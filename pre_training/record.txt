ssh://jeffzhu@172.16.46.217:22/home/jeffzhu/anaconda3/bin/python3.6 -u /home/jeffzhu/AL/pre_training/non_ptrain_cls.py
Namespace(al_method='msg_mask', bald_ft_epochs=5, batch_data_num=100, batchsize=48, data_mix=False, data_mixing_rate=0.5, dataset='qm9', device=1, epochs=800, ft_epochs=5, ft_method='by_valid', init_data_num=5000, k_center_ft_epochs=10, lr=0.0005, mc_sampling_num=80, model_num=4, multi_gpu=False, prop_name='homo', qbc_ft_epochs=5, re_init=False, save_model=False, shuffle=True, test_freq=5, test_use_all=False, use_default=False, use_tb=True, workers=0)
1119_22_10  model SchNetModel(
  (activation): ShiftSoftplus(
    beta=1, threshold=20
    (softplus): Softplus(beta=1, threshold=20)
  )
  (embedding_layer): AtomEmbedding(
    (embedding): Embedding(100, 48, padding_idx=0)
  )
  (rbf_layer): RBFLayer()
  (conv_layers): ModuleList(
    (0): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (1): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (2): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (3): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
  )
  (atom_dense_layer1): Linear(in_features=48, out_features=64, bias=True)
  (atom_dense_layer2): Linear(in_features=64, out_features=1, bias=True)
)  optimizer Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
inference 23.293590545654297
Iteration 1 Mean MSE 0.006674845702946186
Iteration 2 Mean MSE 0.005174547899514437
Iteration 3 Mean MSE 0.004640858620405197
Iteration 4 Mean MSE 0.004152031149715185
Iteration 5 Mean MSE 0.004137295298278332
Iteration 6 Mean MSE 0.00413301307708025
Iteration 7 Mean MSE 0.004131719004362822
Iteration 8 Mean MSE 0.004131695721298456
Iteration 9 Mean MSE 0.004131695721298456
Iteration 10 Mean MSE 0.004131695721298456
start
SchNetModel(
  (activation): ShiftSoftplus(
    beta=1, threshold=20
    (softplus): Softplus(beta=1, threshold=20)
  )
  (embedding_layer): AtomEmbedding(
    (embedding): Embedding(100, 48, padding_idx=0)
  )
  (rbf_layer): RBFLayer()
  (conv_layers): ModuleList(
    (0): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (1): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (2): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (3): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
  )
  (atom_dense_layer1): Linear(in_features=48, out_features=64, bias=True)
  (atom_dense_layer2): Linear(in_features=64, out_features=1, bias=True)
)
-0.23918546736240387 0.021602824330329895
training loss 0.005235239863395691 mae 0.06530282646417618
training loss 0.0006593643104447525 mae 0.01883125266827204
training loss 0.0005493723775264759 mae 0.017069554251461925
training loss 0.0005120919610293493 mae 0.016541310573699855
training loss 0.00048042771340445103 mae 0.01600205717117187
Epoch  0, training: loss: 0.0004762, mae: 0.0159318 test: loss0.0004226, mae:0.0148380
training loss 0.0005376142216846347 mae 0.016777634620666504
training loss 0.00039037551351950745 mae 0.014333507831336235
training loss 0.0003990222655852223 mae 0.014546288020613758
training loss 0.0003963457970742742 mae 0.014507076659838091
training loss 0.0003956224397519856 mae 0.014506957628092355
Epoch  1, training: loss: 0.0003953, mae: 0.0145371 test: loss0.0004289, mae:0.0151779
training loss 0.0005598671268671751 mae 0.015775401145219803
training loss 0.00041211663069678285 mae 0.014625358833547902
training loss 0.00040961945032364335 mae 0.014704749283224045
training loss 0.00039992742643256787 mae 0.014545085772082508
training loss 0.00039527898407321126 mae 0.014491563097606252
Epoch  2, training: loss: 0.0003940, mae: 0.0144950 test: loss0.0004177, mae:0.0150700
training loss 0.0004806162032764405 mae 0.01692270301282406
training loss 0.0003828463908147943 mae 0.014388864293840587
training loss 0.0003923389570508613 mae 0.014435666299766245
training loss 0.00038963458425633676 mae 0.014354559117989826
training loss 0.0003852037284627501 mae 0.01428008129106677
Epoch  3, training: loss: 0.0003845, mae: 0.0142640 test: loss0.0004169, mae:0.0149010
training loss 0.0004190797044429928 mae 0.015397600829601288
training loss 0.00038131370007142643 mae 0.014074004602198507
training loss 0.00038186433704311747 mae 0.014200238271219894
training loss 0.0003891511310843306 mae 0.014336071963983262
training loss 0.0003855091830094655 mae 0.014283715399788384
Epoch  4, training: loss: 0.0003852, mae: 0.0142720 test: loss0.0004084, mae:0.0148120
training loss 0.0003069799568038434 mae 0.013034519739449024
training loss 0.0003972985050138816 mae 0.014461894063096423
training loss 0.000401488386583284 mae 0.01460005235996577
training loss 0.0003855149310802276 mae 0.014333732987427163
training loss 0.0003810676659048486 mae 0.014194452634720668
Epoch  5, training: loss: 0.0003804, mae: 0.0141845 test: loss0.0004145, mae:0.0149586
training loss 0.00036696449387818575 mae 0.01406705379486084
training loss 0.0003771764929885702 mae 0.014351029113373335
training loss 0.00039401407368564123 mae 0.014516410454738848
training loss 0.0003862407877432207 mae 0.014360531831517915
training loss 0.0003785495296617581 mae 0.014240848099748002
Epoch  6, training: loss: 0.0003785, mae: 0.0142488 test: loss0.0004139, mae:0.0152404
training loss 0.0002471277548465878 mae 0.01211531087756157
training loss 0.00038884962194453133 mae 0.014338989139479747
training loss 0.00036657408328201284 mae 0.014035514260798989
training loss 0.0003600829084895687 mae 0.013999015721955051
training loss 0.000351561893956893 mae 0.013848159485380745
Epoch  7, training: loss: 0.0003523, mae: 0.0138768 test: loss0.0003375, mae:0.0136989
training loss 0.000428200961323455 mae 0.014423935674130917
training loss 0.0003176402504893713 mae 0.0132999152821653
training loss 0.00031282872279718654 mae 0.013194548786793017
training loss 0.0003167413128270082 mae 0.01325416724068045
training loss 0.0003102785354278591 mae 0.013120075085418145
Epoch  8, training: loss: 0.0003088, mae: 0.0130880 test: loss0.0003086, mae:0.0128137
training loss 0.000255231891060248 mae 0.010852519422769547
training loss 0.0002819911240686791 mae 0.01255637295909372
training loss 0.000282512111520155 mae 0.012501390604763338
training loss 0.00029239634808233113 mae 0.012691783348702827
training loss 0.0002843540599346695 mae 0.012540500527664792
Epoch  9, training: loss: 0.0002869, mae: 0.0125847 test: loss0.0003034, mae:0.0131242
training loss 0.0002732957655098289 mae 0.012271714396774769
training loss 0.00029060863444636424 mae 0.012793571925630756
training loss 0.0002863227776171049 mae 0.012561435903431757
training loss 0.0002844416726143901 mae 0.012599745894850092
training loss 0.00028399143721161184 mae 0.012570370387156214
Epoch 10, training: loss: 0.0002832, mae: 0.0125308 test: loss0.0003251, mae:0.0130007
training loss 0.00017886538989841938 mae 0.010141500271856785
training loss 0.00027258814838669246 mae 0.01222695346337323
training loss 0.0002761811315079709 mae 0.012431143764590865
training loss 0.00027314977876885196 mae 0.012283386027329408
training loss 0.00026992742616256964 mae 0.012246611509564801
Epoch 11, training: loss: 0.0002679, mae: 0.0122088 test: loss0.0002952, mae:0.0127621
training loss 0.00029234096291475 mae 0.01400058064609766
training loss 0.00025949081749536613 mae 0.012049488084135099
training loss 0.0002689393932987603 mae 0.012191420158205346
training loss 0.0002567815204653642 mae 0.011909756565848917
training loss 0.00025695526583782816 mae 0.011906863164749749
Epoch 12, training: loss: 0.0002569, mae: 0.0118933 test: loss0.0002677, mae:0.0122356
training loss 0.0002531246282160282 mae 0.011714518070220947
training loss 0.00023934389658582703 mae 0.011546762952325394
training loss 0.00024300618895380254 mae 0.011711105326245919
training loss 0.00024288971470869363 mae 0.01169229549855389
training loss 0.00025071857552986994 mae 0.011859956781254778
Epoch 13, training: loss: 0.0002522, mae: 0.0118682 test: loss0.0002901, mae:0.0130987
training loss 0.00023223600874189287 mae 0.011455672793090343
training loss 0.0002295596404663087 mae 0.011339643102723592
training loss 0.00023262617094695832 mae 0.01140098576664482
training loss 0.0002397917791590441 mae 0.011571588719399358
training loss 0.00023575600496416842 mae 0.01146595368847547
Epoch 14, training: loss: 0.0002359, mae: 0.0114646 test: loss0.0002485, mae:0.0117630
training loss 0.00034136627800762653 mae 0.013728756457567215
training loss 0.00023924018622732118 mae 0.011529597125070937
training loss 0.00023288229776610914 mae 0.011343604543864136
training loss 0.00022958617214890607 mae 0.01125572423222444
training loss 0.00022365929480973375 mae 0.01111741174957645
Epoch 15, training: loss: 0.0002212, mae: 0.0110586 test: loss0.0002259, mae:0.0110598
training loss 0.00017329439288005233 mae 0.010580778121948242
training loss 0.0002042133564827964 mae 0.010764925968924576
training loss 0.00021645639532321767 mae 0.011023724207304197
training loss 0.000217169615369142 mae 0.010981636143456035
training loss 0.00021672476103601145 mae 0.010985110527302945
Epoch 16, training: loss: 0.0002152, mae: 0.0109478 test: loss0.0002170, mae:0.0109350
training loss 0.00017281697364524007 mae 0.010073167271912098
training loss 0.00019925961618462364 mae 0.010480328536062847
training loss 0.00020306926977931356 mae 0.010526077766524684
training loss 0.0002062578325349505 mae 0.010652889682609119
training loss 0.0002079538199039923 mae 0.010697696844477259
Epoch 17, training: loss: 0.0002072, mae: 0.0106982 test: loss0.0002320, mae:0.0112271
training loss 0.0002585334295872599 mae 0.011790256015956402
training loss 0.00019916579813249045 mae 0.010663189140020637
training loss 0.0002039322177308962 mae 0.010819811051213504
training loss 0.00020851440781099865 mae 0.010848828683921832
training loss 0.0002089793348539532 mae 0.010780865131919061
Epoch 18, training: loss: 0.0002074, mae: 0.0107456 test: loss0.0002084, mae:0.0107547
training loss 0.00010772237874334678 mae 0.008492784574627876
training loss 0.0001906195372130776 mae 0.010316188081952869
training loss 0.0001953115200077524 mae 0.010464313753539387
training loss 0.00019856321863253408 mae 0.010485874889850226
training loss 0.00019992179961571586 mae 0.01053623648925652
Epoch 19, training: loss: 0.0002002, mae: 0.0105574 test: loss0.0002104, mae:0.0106675
training loss 0.00026306146173737943 mae 0.011003215797245502
training loss 0.0002042631254595357 mae 0.010530435324956976
training loss 0.00020293219059378377 mae 0.010620857330758383
training loss 0.000200676718225243 mae 0.010552368435194555
training loss 0.00020123512064372024 mae 0.010558720094276898
Epoch 20, training: loss: 0.0002017, mae: 0.0105462 test: loss0.0002148, mae:0.0109586
training loss 0.00021807546727359295 mae 0.011041893623769283
training loss 0.00018154343906287833 mae 0.010067287540318917
training loss 0.00019916941708306512 mae 0.010477921630412631
training loss 0.00019562824858415767 mae 0.010394480964477287
training loss 0.0001999608424766713 mae 0.01056358097491795
Epoch 21, training: loss: 0.0002002, mae: 0.0105624 test: loss0.0002152, mae:0.0109652
training loss 0.0001374327257508412 mae 0.008026549592614174
training loss 0.00019358027644806962 mae 0.010390162285344269
training loss 0.00019256111898915935 mae 0.010404760728009265
training loss 0.00019301537684333016 mae 0.010405828405344326
training loss 0.0001936023284936561 mae 0.010387105283452498
Epoch 22, training: loss: 0.0001939, mae: 0.0103984 test: loss0.0001946, mae:0.0102609
training loss 0.0002545482129789889 mae 0.012017502449452877
training loss 0.0002012301911771133 mae 0.010458388219715332
training loss 0.0001882328806729701 mae 0.010279911404272708
training loss 0.0001884169029673278 mae 0.010332278373793066
training loss 0.00019152719001586904 mae 0.010360416891377667
Epoch 23, training: loss: 0.0001905, mae: 0.0103449 test: loss0.0001981, mae:0.0104122
training loss 0.000154260647832416 mae 0.009206590242683887
training loss 0.00018283904914735067 mae 0.010062403337774325
training loss 0.00018170597057646642 mae 0.010114435277104677
training loss 0.00018529360309213903 mae 0.01019702121825112
training loss 0.00018527114395931393 mae 0.010190234985202556
Epoch 24, training: loss: 0.0001872, mae: 0.0102119 test: loss0.0001874, mae:0.0100729
training loss 9.248842252418399e-05 mae 0.007128726691007614
training loss 0.00018626664991732507 mae 0.010364314084689989
training loss 0.00019189606073178762 mae 0.010382146408578548
training loss 0.00018656093228048654 mae 0.010232008176405497
training loss 0.00018538792235504569 mae 0.010177398288149881
Epoch 25, training: loss: 0.0001873, mae: 0.0101945 test: loss0.0001968, mae:0.0104474
training loss 0.00013726696488447487 mae 0.00951537024229765
training loss 0.00017437722400406042 mae 0.00992861065063991
training loss 0.00017897930567666864 mae 0.010022976449552443
training loss 0.0001774526671282885 mae 0.009989368371566795
training loss 0.0001788399545985519 mae 0.010003676100527464
Epoch 26, training: loss: 0.0001790, mae: 0.0100121 test: loss0.0001875, mae:0.0103561
training loss 0.0002257670130347833 mae 0.012147574685513973
training loss 0.00017517193919047716 mae 0.009947382589327355
training loss 0.000173166071498158 mae 0.009865116337222044
training loss 0.00017215007575327492 mae 0.009862769722001053
training loss 0.0001731442551598282 mae 0.009870728253113545
Epoch 27, training: loss: 0.0001739, mae: 0.0098995 test: loss0.0001844, mae:0.0099459
training loss 0.0001210053960676305 mae 0.008795193396508694
training loss 0.0001737384400079392 mae 0.010039996100114842
training loss 0.0001683576707670575 mae 0.009857950003651703
training loss 0.00017378071769424378 mae 0.009976790931125159
training loss 0.00017244366435038706 mae 0.009876406362363655
Epoch 28, training: loss: 0.0001725, mae: 0.0098722 test: loss0.0001798, mae:0.0100350
training loss 0.00010455065785208717 mae 0.0081778010353446
training loss 0.0001637515361996039 mae 0.00972972218604649
training loss 0.0001747957494174416 mae 0.009896072024239765
training loss 0.00017224616147130603 mae 0.009890090822186694
training loss 0.00016992805557949827 mae 0.009786820115604954
Epoch 29, training: loss: 0.0001724, mae: 0.0098402 test: loss0.0001787, mae:0.0100125
training loss 0.0002811483573168516 mae 0.009848085232079029
training loss 0.00016060087121635053 mae 0.00961188067152512
training loss 0.00016418241723993845 mae 0.0096673819024374
training loss 0.00016421179142772887 mae 0.009656396931937785
training loss 0.0001673320554407077 mae 0.00968870153854513
Epoch 30, training: loss: 0.0001686, mae: 0.0097142 test: loss0.0001834, mae:0.0102074
training loss 0.0001304121979046613 mae 0.008631253615021706
training loss 0.00016133188953116427 mae 0.009688545986279554
training loss 0.00016351671113418227 mae 0.009674649100086776
training loss 0.00016364591943627434 mae 0.009612013468679214
training loss 0.00016205610302844506 mae 0.009556023772361122
Epoch 31, training: loss: 0.0001616, mae: 0.0095550 test: loss0.0001612, mae:0.0094859
training loss 0.00010340422159060836 mae 0.00830641109496355
training loss 0.00015434000855676984 mae 0.009349774980150605
training loss 0.0001567901955862151 mae 0.009370911091861162
training loss 0.00016026769539985854 mae 0.009511247298846778
training loss 0.00016290707749271754 mae 0.009590415293304481
Epoch 32, training: loss: 0.0001627, mae: 0.0095872 test: loss0.0001871, mae:0.0103911
training loss 0.00012608067481778562 mae 0.008536536246538162
training loss 0.00014198691068860865 mae 0.009313461772513151
training loss 0.0001527953531021381 mae 0.009401667438963849
training loss 0.00015709778366823198 mae 0.009467632790398322
training loss 0.0001585976855521814 mae 0.009478406278666727
Epoch 33, training: loss: 0.0001582, mae: 0.0094778 test: loss0.0001661, mae:0.0096505
training loss 0.0001255162787856534 mae 0.007924279198050499
training loss 0.00016150900003541368 mae 0.009409458586471338
training loss 0.00015532402615616155 mae 0.009272015750371293
training loss 0.0001551049330891634 mae 0.009353071685559705
training loss 0.000156454487115293 mae 0.009414306380762833
Epoch 34, training: loss: 0.0001571, mae: 0.0094348 test: loss0.0001613, mae:0.0096148
training loss 8.459113450953737e-05 mae 0.0070726629346609116
training loss 0.00016368955987797795 mae 0.009540873963166684
training loss 0.00015525936162315944 mae 0.009369801785243619
training loss 0.00015524837526875178 mae 0.009323548513236428
training loss 0.000153319548506496 mae 0.009307251624596207
Epoch 35, training: loss: 0.0001530, mae: 0.0093070 test: loss0.0001799, mae:0.0101404
training loss 0.00012417930702213198 mae 0.008417751640081406
training loss 0.00015271378737559326 mae 0.009189491689789529
training loss 0.00015728048663839967 mae 0.009354397820511664
training loss 0.0001541157527741921 mae 0.009358093154844855
training loss 0.00015221242315804274 mae 0.009325788194193172
Epoch 36, training: loss: 0.0001511, mae: 0.0093007 test: loss0.0001543, mae:0.0092323
training loss 9.47255757637322e-05 mae 0.007664302363991737
training loss 0.0001448054627319067 mae 0.009081152249492854
training loss 0.00014597914134968293 mae 0.009101812677033763
training loss 0.0001463472760002597 mae 0.009132762075654718
training loss 0.0001464110478557257 mae 0.00913845129375953
Epoch 37, training: loss: 0.0001464, mae: 0.0091423 test: loss0.0001519, mae:0.0093058
training loss 8.271296974271536e-05 mae 0.0070875599049031734
training loss 0.00015176328837268927 mae 0.009296094122178416
training loss 0.00014496965820516108 mae 0.00913053454075119
training loss 0.00014225815028485683 mae 0.009081900110529941
training loss 0.0001497697894547171 mae 0.009244003798812626
Epoch 38, training: loss: 0.0001497, mae: 0.0092279 test: loss0.0001547, mae:0.0093623
training loss 0.00011719559552147985 mae 0.008496268652379513
training loss 0.00014218389882372835 mae 0.009175762899366077
training loss 0.00014873119170540061 mae 0.009188364061646822
training loss 0.00014390726412345016 mae 0.009090456137100586
training loss 0.00014566534113174233 mae 0.009141962025407234
Epoch 39, training: loss: 0.0001453, mae: 0.0091278 test: loss0.0002021, mae:0.0106771
training loss 0.00032891728915274143 mae 0.014384840615093708
training loss 0.00015122655981138132 mae 0.009262130595743656
training loss 0.00015165001644676845 mae 0.009203016260030249
training loss 0.00014918833911852197 mae 0.009141726891158634
training loss 0.0001462138269793946 mae 0.009091236063311648
Epoch 40, training: loss: 0.0001451, mae: 0.0090644 test: loss0.0001452, mae:0.0090387
training loss 0.0001585009740665555 mae 0.008888544514775276
training loss 0.00014761186078734084 mae 0.009061090538606922
training loss 0.0001453604934723255 mae 0.009086178371332361
training loss 0.00013982418993282108 mae 0.008915110799601141
training loss 0.0001375739421515635 mae 0.00888784580970581
Epoch 41, training: loss: 0.0001382, mae: 0.0088805 test: loss0.0001722, mae:0.0098805
training loss 0.00016950699500739574 mae 0.009989731945097446
training loss 0.00014524911924073577 mae 0.009102080035589492
training loss 0.00014327127212142144 mae 0.009095477779386657
training loss 0.00014390079842322188 mae 0.009057381067005611
training loss 0.0001445232339383482 mae 0.009061674978954729
Epoch 42, training: loss: 0.0001439, mae: 0.0090567 test: loss0.0001578, mae:0.0094457
training loss 0.00011740760965039954 mae 0.007600584998726845
training loss 0.00015427689233511757 mae 0.00928989442649717
training loss 0.0001452871803470915 mae 0.009128069090149776
training loss 0.0001425755578644469 mae 0.0090594969301281
training loss 0.0001394747648657756 mae 0.00894576019436047
Epoch 43, training: loss: 0.0001397, mae: 0.0089500 test: loss0.0001567, mae:0.0095494
training loss 0.00015393485955428332 mae 0.010228109546005726
training loss 0.00014303074182380984 mae 0.009013281459463576
training loss 0.00014370233061121744 mae 0.009039410297749654
training loss 0.00014000568698493407 mae 0.008917441372667121
training loss 0.0001391229546682178 mae 0.008902779307716817
Epoch 44, training: loss: 0.0001387, mae: 0.0088962 test: loss0.0001513, mae:0.0093823
training loss 0.00010756853589555249 mae 0.008597544394433498
training loss 0.0001306553499935213 mae 0.008725003878056413
training loss 0.00013239384162882896 mae 0.008696661920904519
training loss 0.00013669428931852186 mae 0.008782715607582528
training loss 0.0001344879146293791 mae 0.008744302933192364
Epoch 45, training: loss: 0.0001346, mae: 0.0087519 test: loss0.0001446, mae:0.0090744
training loss 0.00012129115202696994 mae 0.008693481795489788
training loss 0.00012353119025504508 mae 0.008523728862843092
training loss 0.0001282474927078024 mae 0.008599303095684485
training loss 0.00013527611290495566 mae 0.008799824277740835
training loss 0.00013397708626053379 mae 0.00879542713762442
Epoch 46, training: loss: 0.0001333, mae: 0.0087872 test: loss0.0001406, mae:0.0088704
training loss 0.00013119028881192207 mae 0.008954556658864021
training loss 0.00013704907392179046 mae 0.008823141884789165
training loss 0.00013408364460395537 mae 0.008765807449079979
training loss 0.00013445633825335637 mae 0.008751499654712858
training loss 0.00013408619921772609 mae 0.008775961844815836
Epoch 47, training: loss: 0.0001346, mae: 0.0087832 test: loss0.0001399, mae:0.0090027
training loss 6.963039049878716e-05 mae 0.006502579897642136
training loss 0.00012328967183266822 mae 0.008613030389681751
training loss 0.00013192467858881164 mae 0.008755592846007338
training loss 0.0001298841399804455 mae 0.008670353318336394
training loss 0.00013075746442811034 mae 0.008657228720573647
Epoch 48, training: loss: 0.0001331, mae: 0.0086719 test: loss0.0001594, mae:0.0097445
training loss 0.0001286546903429553 mae 0.009100493974983692
training loss 0.00012804203388262905 mae 0.008616036224160706
training loss 0.00012930422213901354 mae 0.008669368632248422
training loss 0.0001319189390812504 mae 0.008681086055465682
training loss 0.00013231303113462302 mae 0.008674970656559236
Epoch 49, training: loss: 0.0001319, mae: 0.0086592 test: loss0.0001327, mae:0.0087786
training loss 8.411556336795911e-05 mae 0.007285148371011019
training loss 0.00012377776026066084 mae 0.00834112918442663
training loss 0.0001321377242107917 mae 0.00862937075279579
training loss 0.00012824915874990736 mae 0.008599132157566133
training loss 0.00012742506384091402 mae 0.00857354817328168
Epoch 50, training: loss: 0.0001277, mae: 0.0085778 test: loss0.0001364, mae:0.0086845
training loss 0.00014686585927847773 mae 0.009251686744391918
training loss 0.0001224627722324073 mae 0.008417846461502362
training loss 0.00012689318309369783 mae 0.008556704627036458
training loss 0.00012329651106134307 mae 0.008480360046867896
training loss 0.00012344491238510627 mae 0.0084765619431525
Epoch 51, training: loss: 0.0001260, mae: 0.0085000 test: loss0.0001644, mae:0.0095982
training loss 9.035315451910719e-05 mae 0.007515305187553167
training loss 0.00013052916405927934 mae 0.00859886106541928
training loss 0.00012565453437106248 mae 0.008511451852166712
training loss 0.00012524349624422118 mae 0.008531775180595795
training loss 0.00012589726327462061 mae 0.008500265763766727
Epoch 52, training: loss: 0.0001256, mae: 0.0084978 test: loss0.0001384, mae:0.0088690
training loss 0.00010925754759227857 mae 0.008155066519975662
training loss 0.00011847036298337012 mae 0.00842733675723567
training loss 0.00012907935874098631 mae 0.008588730532786639
training loss 0.0001290096960239426 mae 0.00857590395488487
training loss 0.00012651619315205313 mae 0.008530879237999526
Epoch 53, training: loss: 0.0001259, mae: 0.0085180 test: loss0.0001432, mae:0.0088913
training loss 0.00013895540905650705 mae 0.00887217279523611
training loss 0.00012065288391051014 mae 0.008355626790766037
training loss 0.0001192487052141569 mae 0.008285941099655806
training loss 0.00012270620011605804 mae 0.008364382677939751
training loss 0.00011986934234427557 mae 0.008303735291687262
Epoch 54, training: loss: 0.0001196, mae: 0.0083003 test: loss0.0001350, mae:0.0088212
training loss 0.00013143832620698959 mae 0.00883866474032402
training loss 0.00011744750073001556 mae 0.008351072513808807
training loss 0.0001185165178192097 mae 0.00837713075193143
training loss 0.00012203239329276446 mae 0.008461301258166893
training loss 0.0001239211353764693 mae 0.0084610151275251
Epoch 55, training: loss: 0.0001250, mae: 0.0084872 test: loss0.0001289, mae:0.0085039
training loss 7.747833296889439e-05 mae 0.007175091654062271
training loss 0.00011892112410814484 mae 0.0083955466272492
training loss 0.0001167954369430836 mae 0.008279849673704341
training loss 0.00011649594830336753 mae 0.008263243356139842
training loss 0.00011843773433579531 mae 0.00826374074416374
Epoch 56, training: loss: 0.0001193, mae: 0.0082788 test: loss0.0001334, mae:0.0086269
training loss 0.00011576968245208263 mae 0.008367198519408703
training loss 0.00011856996099916562 mae 0.008187902954352254
training loss 0.0001150013938806098 mae 0.008091926081382698
training loss 0.00011645298365319286 mae 0.00816564923707421
training loss 0.00011501688098164843 mae 0.008135509522010887
Epoch 57, training: loss: 0.0001157, mae: 0.0081701 test: loss0.0001309, mae:0.0085376
training loss 7.078820635797456e-05 mae 0.006980506237596273
training loss 0.00011968234848966089 mae 0.008230711123449547
training loss 0.0001174052765893768 mae 0.008251747250151221
training loss 0.00011653693237523881 mae 0.008223459402111588
training loss 0.00011663170391954338 mae 0.008191572159380467
Epoch 58, training: loss: 0.0001166, mae: 0.0081914 test: loss0.0001410, mae:0.0090129
training loss 0.0001232317736139521 mae 0.008532454259693623
training loss 0.00011099563619377566 mae 0.007880500307781441
training loss 0.00011243008951412161 mae 0.007983263572770178
training loss 0.00011175054884693023 mae 0.007993599936514101
training loss 0.00011099969432240382 mae 0.00798045778276389
Epoch 59, training: loss: 0.0001109, mae: 0.0079858 test: loss0.0001322, mae:0.0086404
training loss 0.0003026049234904349 mae 0.008682817220687866
training loss 0.00011745926880679441 mae 0.00811906641933556
training loss 0.00011146508343402514 mae 0.007933553815404375
training loss 0.00011203284026990907 mae 0.008037781253432392
training loss 0.00011179648854929611 mae 0.008030510132214914
Epoch 60, training: loss: 0.0001111, mae: 0.0080050 test: loss0.0001307, mae:0.0085492
training loss 0.00012532580876722932 mae 0.009261843748390675
training loss 0.00011063297093713512 mae 0.007912534398629384
training loss 0.00011004909765238108 mae 0.007893551482201212
training loss 0.00011138565974122699 mae 0.007970557208882262
training loss 0.00011040349974918447 mae 0.007970696159948908
Epoch 61, training: loss: 0.0001113, mae: 0.0079867 test: loss0.0001334, mae:0.0086604
training loss 0.00017190235666930676 mae 0.00902289804071188
training loss 0.00010881308473614683 mae 0.00785744368263027
training loss 0.00010768409229418505 mae 0.007850201980275386
training loss 0.00010836918586121092 mae 0.007841293333061284
training loss 0.0001080329873829626 mae 0.007832016494699674
Epoch 62, training: loss: 0.0001078, mae: 0.0078267 test: loss0.0001390, mae:0.0087322
training loss 0.00010294784442521632 mae 0.007269274443387985
training loss 9.505507392425305e-05 mae 0.007466013853748639
training loss 0.00010124779113863434 mae 0.007705380153464207
training loss 0.00010551560404829972 mae 0.007785695586476891
training loss 0.00010796891510111879 mae 0.00785012856885716
Epoch 63, training: loss: 0.0001078, mae: 0.0078607 test: loss0.0001191, mae:0.0082144
training loss 0.00014255748828873038 mae 0.00865000206977129
training loss 0.00010262091795904744 mae 0.007738322713503651
training loss 0.00010448841437597129 mae 0.007781078444222117
training loss 0.00010541461387707769 mae 0.007775127906228927
training loss 0.00010576908080925491 mae 0.007804938696269224
Epoch 64, training: loss: 0.0001057, mae: 0.0078022 test: loss0.0001151, mae:0.0081040
training loss 6.892839883221313e-05 mae 0.006770389620214701
training loss 0.00010528998038852493 mae 0.007709292576228287
training loss 0.00010564219146634975 mae 0.007708832072923973
training loss 0.00010374521095839957 mae 0.007716448422496681
training loss 0.00010202532353308815 mae 0.007672868761701961
Epoch 65, training: loss: 0.0001024, mae: 0.0076822 test: loss0.0001217, mae:0.0081546
training loss 7.619135431014001e-05 mae 0.006089468020945787
training loss 0.00011110497357816818 mae 0.007928369493753307
training loss 0.00011001447129805602 mae 0.007944457454256496
training loss 0.00010728358207440115 mae 0.007840452780515353
training loss 0.00010672436200823077 mae 0.00783070695785741
Epoch 66, training: loss: 0.0001071, mae: 0.0078360 test: loss0.0001166, mae:0.0080810
training loss 8.620374865131453e-05 mae 0.0077599151991307735
training loss 0.00010687359377591161 mae 0.007742006395596497
training loss 0.00010443441679304707 mae 0.007639850512847745
training loss 0.00010152646266321022 mae 0.007612463247203671
training loss 0.00010142673920597927 mae 0.007614745691288911
Epoch 67, training: loss: 0.0001015, mae: 0.0076252 test: loss0.0001141, mae:0.0079727
training loss 8.06774987722747e-05 mae 0.0063099004328250885
training loss 9.788068735574867e-05 mae 0.007362129915432603
training loss 9.885945830391581e-05 mae 0.007480649761671181
training loss 0.0001002578929240896 mae 0.0075747207707620645
training loss 0.00010153195083813758 mae 0.007608033494273231
Epoch 68, training: loss: 0.0001024, mae: 0.0076271 test: loss0.0001205, mae:0.0081904
training loss 8.79459548741579e-05 mae 0.007273854222148657
training loss 9.913011237193703e-05 mae 0.00750223478740629
training loss 9.918719210827929e-05 mae 0.007562922673429003
training loss 9.761117415121814e-05 mae 0.007528428250418004
training loss 9.751013198210648e-05 mae 0.007518775165729709
Epoch 69, training: loss: 0.0000970, mae: 0.0075026 test: loss0.0001195, mae:0.0081549
training loss 6.281724927248433e-05 mae 0.006485383957624435
training loss 9.598284690154663e-05 mae 0.007452402346055298
training loss 9.547502511598186e-05 mae 0.007391633645695921
training loss 9.52158183197102e-05 mae 0.007445468721504242
training loss 9.592568656543744e-05 mae 0.007470620453209425
Epoch 70, training: loss: 0.0000961, mae: 0.0074764 test: loss0.0001107, mae:0.0078042
training loss 8.532554056728259e-05 mae 0.006978532765060663
training loss 0.00010241163880552861 mae 0.007711561579330296
training loss 0.00010137721623287141 mae 0.0076025790940638235
training loss 9.821499599566353e-05 mae 0.007540145938266191
training loss 9.609766296384421e-05 mae 0.007458990979569023
Epoch 71, training: loss: 0.0000959, mae: 0.0074533 test: loss0.0001170, mae:0.0081599
training loss 0.00010628125164657831 mae 0.00842702854424715
training loss 9.268819371309113e-05 mae 0.0073776097018636915
training loss 9.500599095406527e-05 mae 0.007426860769933993
training loss 9.411345103976085e-05 mae 0.007432202867152081
training loss 9.712357266165492e-05 mae 0.007469467501346002
Epoch 72, training: loss: 0.0000969, mae: 0.0074672 test: loss0.0001184, mae:0.0081709
training loss 9.914606926031411e-05 mae 0.007602529134601355
training loss 9.112341578264592e-05 mae 0.007429040311013952
training loss 9.362417836425744e-05 mae 0.007400909461781828
training loss 9.765563601036617e-05 mae 0.007492026767465259
training loss 9.911862080071156e-05 mae 0.007543334512697975
Epoch 73, training: loss: 0.0000988, mae: 0.0075492 test: loss0.0001210, mae:0.0081617
training loss 9.014352690428495e-05 mae 0.0073693678714334965
training loss 9.26999558692378e-05 mae 0.007384397993412087
training loss 8.950393075252525e-05 mae 0.007279935753950388
training loss 8.654950593967475e-05 mae 0.007173689097684146
training loss 9.107067747738336e-05 mae 0.007281267885767404
Epoch 74, training: loss: 0.0000909, mae: 0.0072808 test: loss0.0001065, mae:0.0077704
training loss 6.15333192399703e-05 mae 0.006214057561010122
training loss 9.156354017071789e-05 mae 0.0073940686461533974
training loss 9.560527495082031e-05 mae 0.007369133445554146
training loss 9.298336705935353e-05 mae 0.007314954375515118
training loss 9.282477735701031e-05 mae 0.007318675747391447
Epoch 75, training: loss: 0.0000928, mae: 0.0073066 test: loss0.0001284, mae:0.0084255
training loss 9.216618491336703e-05 mae 0.007989183068275452
training loss 9.17307202768165e-05 mae 0.007298629091796922
training loss 9.246878312972124e-05 mae 0.007302615510318244
training loss 9.16972656353432e-05 mae 0.007286061245997419
training loss 9.263124759931609e-05 mae 0.007365618843992999
Epoch 76, training: loss: 0.0000933, mae: 0.0073842 test: loss0.0001181, mae:0.0080072
training loss 7.481844659196213e-05 mae 0.0067452252842485905
training loss 9.663829519459063e-05 mae 0.007338644927550181
training loss 9.224992675509646e-05 mae 0.0072362571355361165
training loss 9.003146773682974e-05 mae 0.007219908028125567
training loss 8.95758785190992e-05 mae 0.007216891853036869
Epoch 77, training: loss: 0.0000894, mae: 0.0072116 test: loss0.0001039, mae:0.0076159
training loss 9.14349002414383e-05 mae 0.007186837960034609
training loss 8.087775338436565e-05 mae 0.006952889784074883
training loss 8.734219497482356e-05 mae 0.007118197236357643
training loss 8.939863766126481e-05 mae 0.007137590821906429
training loss 9.077421214100817e-05 mae 0.007213916942310421
Epoch 78, training: loss: 0.0000903, mae: 0.0071779 test: loss0.0001142, mae:0.0078730
training loss 8.850525046000257e-05 mae 0.007107398007065058
training loss 8.370327986103446e-05 mae 0.006956625565448227
training loss 8.55762023498341e-05 mae 0.007018361060815579
training loss 8.590353606288037e-05 mae 0.007043200649863838
training loss 8.436814282481574e-05 mae 0.006994636381851203
Epoch 79, training: loss: 0.0000841, mae: 0.0069902 test: loss0.0001025, mae:0.0076113
training loss 6.324375135591254e-05 mae 0.006493972148746252
training loss 8.472336441057041e-05 mae 0.00699621471850311
training loss 9.402203475042146e-05 mae 0.00722458338973546
training loss 9.214461220498503e-05 mae 0.007212826649084789
training loss 8.933275216012911e-05 mae 0.0071512881958329
Epoch 80, training: loss: 0.0000887, mae: 0.0071352 test: loss0.0001149, mae:0.0080298
training loss 7.262730650836602e-05 mae 0.007103756070137024
training loss 8.551226325353661e-05 mae 0.006988413339736415
training loss 8.409399580870898e-05 mae 0.006960162731569886
training loss 8.19181374592987e-05 mae 0.006901145934293797
training loss 8.19303559771257e-05 mae 0.006883011767131029
Epoch 81, training: loss: 0.0000818, mae: 0.0068772 test: loss0.0000984, mae:0.0073635
training loss 9.262850653612986e-05 mae 0.007320588454604149
training loss 8.663362692174155e-05 mae 0.006942918998937982
training loss 8.524115231795245e-05 mae 0.0070151157797046825
training loss 8.57460818826668e-05 mae 0.00705065028063509
training loss 8.465205751439387e-05 mae 0.007013528130540801
Epoch 82, training: loss: 0.0000844, mae: 0.0070075 test: loss0.0001086, mae:0.0077025
training loss 6.0791135183535516e-05 mae 0.0058306134305894375
training loss 7.809642242957089e-05 mae 0.006811452390370414
training loss 7.980618563061118e-05 mae 0.006889606346514553
training loss 8.182769935402332e-05 mae 0.0069947463201223215
training loss 8.269522489475395e-05 mae 0.006940966143055046
Epoch 83, training: loss: 0.0000831, mae: 0.0069547 test: loss0.0000983, mae:0.0074424
training loss 7.176672806963325e-05 mae 0.006817868445068598
training loss 8.514636675838621e-05 mae 0.006952391791285252
training loss 8.105066518681381e-05 mae 0.006916698621641293
training loss 8.250299935078401e-05 mae 0.00694154358980869
training loss 8.103176568244157e-05 mae 0.006896723448692719
Epoch 84, training: loss: 0.0000812, mae: 0.0068883 test: loss0.0001045, mae:0.0076411
training loss 4.635847290046513e-05 mae 0.0049202474765479565
training loss 7.704053089529387e-05 mae 0.006767864339053633
training loss 8.144113995189005e-05 mae 0.00683187312529524
training loss 8.159358248007425e-05 mae 0.006871559512215539
training loss 8.233082642175249e-05 mae 0.006876428769113708
Epoch 85, training: loss: 0.0000819, mae: 0.0068659 test: loss0.0001043, mae:0.0075139
training loss 7.446920790243894e-05 mae 0.007005821913480759
training loss 7.902731672134799e-05 mae 0.006904676885289304
training loss 7.617863388311368e-05 mae 0.006734779708566938
training loss 8.030485670006715e-05 mae 0.0068870284440402975
training loss 8.03969006705102e-05 mae 0.006842448100429124
Epoch 86, training: loss: 0.0000798, mae: 0.0068260 test: loss0.0000998, mae:0.0073866
training loss 0.00015289646398741752 mae 0.00843149796128273
training loss 7.587937209024736e-05 mae 0.006660028123388103
training loss 7.55190085688846e-05 mae 0.006684232535449291
training loss 7.67916306430807e-05 mae 0.006704958343866054
training loss 7.656484011031307e-05 mae 0.006681910238752318
Epoch 87, training: loss: 0.0000764, mae: 0.0066784 test: loss0.0001019, mae:0.0074117
training loss 7.297434785868973e-05 mae 0.006496199872344732
training loss 7.138365096990109e-05 mae 0.006536473374010302
training loss 7.02151493068512e-05 mae 0.006500487893561621
training loss 7.299591820599937e-05 mae 0.006575808999134806
training loss 7.385744243603667e-05 mae 0.006577767211647324
Epoch 88, training: loss: 0.0000736, mae: 0.0065816 test: loss0.0000967, mae:0.0072656
training loss 3.8539099477929994e-05 mae 0.0051514687947928905
training loss 7.761038311097462e-05 mae 0.006792831371593125
training loss 7.594120699542316e-05 mae 0.006710188979707139
training loss 7.508921593374457e-05 mae 0.006685652767018172
training loss 7.439283481787482e-05 mae 0.00665670118076884
Epoch 89, training: loss: 0.0000756, mae: 0.0066800 test: loss0.0001098, mae:0.0077242
training loss 7.51973784645088e-05 mae 0.006850134581327438
training loss 7.845874293421976e-05 mae 0.00680660571464721
training loss 7.856231219409551e-05 mae 0.006744571168455156
training loss 7.746158217254175e-05 mae 0.006686213161080877
training loss 7.696433774854719e-05 mae 0.0066877110532267145
Epoch 90, training: loss: 0.0000765, mae: 0.0066752 test: loss0.0000994, mae:0.0073207
training loss 4.29447136411909e-05 mae 0.005378196481615305
training loss 7.399420649687503e-05 mae 0.00649664302666982
training loss 7.514920222398935e-05 mae 0.006631411040330879
training loss 7.403078136772124e-05 mae 0.006609645625884763
training loss 7.407274207610532e-05 mae 0.006604312409401236
Epoch 91, training: loss: 0.0000739, mae: 0.0065822 test: loss0.0000954, mae:0.0072433
training loss 4.598329178406857e-05 mae 0.005665723234415054
training loss 6.713246686701368e-05 mae 0.006293823029480727
training loss 6.807326021636515e-05 mae 0.00632686459800542
training loss 6.848374718571985e-05 mae 0.006343978595926076
training loss 7.014854888814565e-05 mae 0.00641749213584027
Epoch 92, training: loss: 0.0000696, mae: 0.0064011 test: loss0.0000948, mae:0.0072468
training loss 9.780099935596809e-05 mae 0.007298467215150595
training loss 7.42565233867579e-05 mae 0.006502680078733201
training loss 7.397607623715886e-05 mae 0.006539978589493743
training loss 7.113911314510481e-05 mae 0.006450170530808093
training loss 7.142497190614276e-05 mae 0.006462609189196462
Epoch 93, training: loss: 0.0000716, mae: 0.0064753 test: loss0.0000953, mae:0.0072095
training loss 6.837622640887275e-05 mae 0.006879036780446768
training loss 6.164093079574991e-05 mae 0.006086410288059826
training loss 6.408079495333078e-05 mae 0.006177370219795715
training loss 6.582557565803616e-05 mae 0.006232298728670702
training loss 6.74715098483744e-05 mae 0.006316556949724457
Epoch 94, training: loss: 0.0000672, mae: 0.0063058 test: loss0.0001006, mae:0.0073843
training loss 5.092370338388719e-05 mae 0.00533890537917614
training loss 6.901153263992027e-05 mae 0.006287890002496687
training loss 7.044255521982945e-05 mae 0.006398293590818597
training loss 7.126181980919566e-05 mae 0.006416157409178698
training loss 7.364535382809813e-05 mae 0.006534528851379357
Epoch 95, training: loss: 0.0000736, mae: 0.0065324 test: loss0.0000986, mae:0.0072951
training loss 4.6967004891484976e-05 mae 0.005478431936353445
training loss 7.051440244300894e-05 mae 0.0063840197855789295
training loss 6.87349343610153e-05 mae 0.006361416181420335
training loss 6.896814864147332e-05 mae 0.006339346339297015
training loss 7.106617910599813e-05 mae 0.006416634267279458
Epoch 96, training: loss: 0.0000713, mae: 0.0064186 test: loss0.0001156, mae:0.0080948
training loss 6.9941277615726e-05 mae 0.007001602556556463
training loss 6.595487374237137e-05 mae 0.006292191480158592
training loss 6.823003059253098e-05 mae 0.006311667612799914
training loss 6.762249155289234e-05 mae 0.006315858754027165
training loss 6.613277226242829e-05 mae 0.006263890192468663
Epoch 97, training: loss: 0.0000663, mae: 0.0062705 test: loss0.0000971, mae:0.0072487
training loss 4.0674487536307424e-05 mae 0.005225033033639193
training loss 5.7126187575882854e-05 mae 0.005854973222549055
training loss 5.967514089837158e-05 mae 0.005912228109370365
training loss 5.925068019479527e-05 mae 0.005931993459184834
training loss 6.26156829571639e-05 mae 0.006085236288088173
Epoch 98, training: loss: 0.0000623, mae: 0.0060646 test: loss0.0000900, mae:0.0069997
training loss 4.875978993368335e-05 mae 0.005451150704175234
training loss 6.11741808254509e-05 mae 0.006014509573944059
training loss 6.235236914207151e-05 mae 0.006110102537613697
training loss 6.31392895410204e-05 mae 0.006143651936597973
training loss 6.32306228225167e-05 mae 0.0061241144323675185
Epoch 99, training: loss: 0.0000630, mae: 0.0061153 test: loss0.0001007, mae:0.0072989
current learning rate: 0.00025
training loss 5.276790398056619e-05 mae 0.005478307604789734
training loss 5.8184766827502705e-05 mae 0.005767551752939528
training loss 5.393226888597859e-05 mae 0.0056259440267366345
training loss 5.240342242485959e-05 mae 0.005559485334754975
training loss 5.217607887328567e-05 mae 0.005552956577279229
Epoch 100, training: loss: 0.0000525, mae: 0.0055695 test: loss0.0000940, mae:0.0071343
training loss 4.708632695837878e-05 mae 0.005281729623675346
training loss 4.74503125516611e-05 mae 0.005391431191717002
training loss 4.989447896348632e-05 mae 0.005441340993591908
training loss 5.111085436101049e-05 mae 0.005471674005334446
training loss 5.120676030684032e-05 mae 0.005495535400904606
Epoch 101, training: loss: 0.0000508, mae: 0.0054773 test: loss0.0000906, mae:0.0069734
training loss 3.488756556180306e-05 mae 0.004957541357725859
training loss 4.978271210668884e-05 mae 0.005341356136270015
training loss 4.872654233730658e-05 mae 0.005382048095216845
training loss 4.7858243528965556e-05 mae 0.00535404364985494
training loss 4.8814038152398725e-05 mae 0.005405093915065503
Epoch 102, training: loss: 0.0000487, mae: 0.0054054 test: loss0.0000844, mae:0.0067436
training loss 6.422842125175521e-05 mae 0.005856897681951523
training loss 5.142388787741461e-05 mae 0.00543754528660108
training loss 4.896154211681925e-05 mae 0.00536391739088046
training loss 4.900684259293771e-05 mae 0.005382163904989695
training loss 4.827467240087643e-05 mae 0.005357834981958295
Epoch 103, training: loss: 0.0000481, mae: 0.0053500 test: loss0.0000854, mae:0.0068124
training loss 3.881996963173151e-05 mae 0.005081166047602892
training loss 4.274061864313861e-05 mae 0.005120094746862556
training loss 4.334186213431041e-05 mae 0.005136661093202558
training loss 4.4222796309410926e-05 mae 0.005202969365031612
training loss 4.608964999493058e-05 mae 0.0052651819131286125
Epoch 104, training: loss: 0.0000459, mae: 0.0052492 test: loss0.0000890, mae:0.0068213
training loss 2.822606074914802e-05 mae 0.004150146618485451
training loss 4.6230073220452626e-05 mae 0.005266036025668476
training loss 4.619448718765871e-05 mae 0.005284547026610315
training loss 4.739097017133363e-05 mae 0.0053088270121605585
training loss 4.632741674234679e-05 mae 0.005268632288354991
Epoch 105, training: loss: 0.0000461, mae: 0.0052596 test: loss0.0000861, mae:0.0068185
training loss 6.323618435999379e-05 mae 0.005935856606811285
training loss 4.6735225980370944e-05 mae 0.005163274550189573
training loss 4.5730750913424016e-05 mae 0.005184816867389745
training loss 4.502068309765617e-05 mae 0.005171964723703186
training loss 4.4963927968493215e-05 mae 0.005188353816093643
Epoch 106, training: loss: 0.0000449, mae: 0.0051791 test: loss0.0000830, mae:0.0066475
training loss 4.325576082919724e-05 mae 0.0053489855490624905
training loss 4.7644395867702734e-05 mae 0.005222695989205557
training loss 4.6412959341294056e-05 mae 0.005238189753882663
training loss 4.536321143889939e-05 mae 0.005198548517085068
training loss 4.475234272288603e-05 mae 0.005175293590039461
Epoch 107, training: loss: 0.0000445, mae: 0.0051577 test: loss0.0000863, mae:0.0067804
training loss 4.097959026694298e-05 mae 0.004933792166411877
training loss 4.405256186146289e-05 mae 0.004972008043242727
training loss 4.415604876112861e-05 mae 0.005048585172724164
training loss 4.638227931578838e-05 mae 0.005217189485799298
training loss 4.621456060360188e-05 mae 0.005226732435548188
Epoch 108, training: loss: 0.0000459, mae: 0.0052149 test: loss0.0000846, mae:0.0067098
training loss 4.5761087676510215e-05 mae 0.005511479917913675
training loss 4.1241889894259736e-05 mae 0.005037888619756582
training loss 4.144665446333555e-05 mae 0.0050359734191125875
training loss 4.4357039317502544e-05 mae 0.005090705179208458
training loss 4.331267945105278e-05 mae 0.005044927609741536
Epoch 109, training: loss: 0.0000436, mae: 0.0050640 test: loss0.0000859, mae:0.0067314
training loss 3.398635089979507e-05 mae 0.004659189842641354
training loss 4.340068438728386e-05 mae 0.0050731695841486546
training loss 4.284374562762378e-05 mae 0.005025097473275545
training loss 4.254157934366766e-05 mae 0.005027006498988198
training loss 4.27777014820592e-05 mae 0.005052452173723436
Epoch 110, training: loss: 0.0000429, mae: 0.0050602 test: loss0.0000853, mae:0.0067349
training loss 4.155339047429152e-05 mae 0.0052177198231220245
training loss 3.903119624523884e-05 mae 0.004865103369286541
training loss 3.859606829900085e-05 mae 0.004854101865174305
training loss 3.960643652743668e-05 mae 0.004924778738645033
training loss 4.088850367573361e-05 mae 0.004952582732232202
Epoch 111, training: loss: 0.0000410, mae: 0.0049609 test: loss0.0000830, mae:0.0066376
training loss 3.2475694752065465e-05 mae 0.004643260966986418
training loss 4.5389950744221065e-05 mae 0.005085016153387581
training loss 4.193609918117301e-05 mae 0.004956198471257148
training loss 4.0970221555996865e-05 mae 0.004933783634942
training loss 4.255893751784848e-05 mae 0.005033998431600815
Epoch 112, training: loss: 0.0000426, mae: 0.0050330 test: loss0.0000836, mae:0.0066671
training loss 4.109580549993552e-05 mae 0.005071698222309351
training loss 4.222873412443589e-05 mae 0.0048932406466965575
training loss 4.075235658164462e-05 mae 0.004880827817014684
training loss 4.061107192803729e-05 mae 0.004896851322792521
training loss 4.078314223486597e-05 mae 0.0049239205108464355
Epoch 113, training: loss: 0.0000408, mae: 0.0049408 test: loss0.0000885, mae:0.0068587
training loss 2.9977805752423592e-05 mae 0.004598849453032017
training loss 3.7324309268311194e-05 mae 0.004778922110905541
training loss 3.776214221529295e-05 mae 0.00477241771526192
training loss 3.951950948128387e-05 mae 0.004832924701372124
training loss 3.9828098861967426e-05 mae 0.004867363264737882
Epoch 114, training: loss: 0.0000399, mae: 0.0048763 test: loss0.0000885, mae:0.0069034
training loss 4.357652869657613e-05 mae 0.005337019916623831
training loss 3.782448848320271e-05 mae 0.004725471842011399
training loss 3.7274534784811814e-05 mae 0.004741755676096175
training loss 3.924404108755764e-05 mae 0.004810703981556739
training loss 4.0217940607620734e-05 mae 0.004891311772983171
Epoch 115, training: loss: 0.0000404, mae: 0.0049022 test: loss0.0000829, mae:0.0066436
training loss 3.459339131950401e-05 mae 0.00474752951413393
training loss 3.763231849434841e-05 mae 0.004801931261431937
training loss 3.71571903376054e-05 mae 0.004783540535349361
training loss 3.849852489550761e-05 mae 0.004797067055293659
training loss 3.8581407397715554e-05 mae 0.004822322087529212
Epoch 116, training: loss: 0.0000386, mae: 0.0048205 test: loss0.0000858, mae:0.0067411
training loss 5.519673868548125e-05 mae 0.005695415195077658
training loss 3.57873795306374e-05 mae 0.0046751565144707765
training loss 3.869048647391196e-05 mae 0.004754618582509387
training loss 3.8036603257672955e-05 mae 0.004743343976246107
training loss 3.792331916707525e-05 mae 0.004767053402654493
Epoch 117, training: loss: 0.0000380, mae: 0.0047712 test: loss0.0000836, mae:0.0066771
training loss 2.893713826779276e-05 mae 0.00436850730329752
training loss 3.523238696039233e-05 mae 0.0046125207342864825
training loss 3.834164092022727e-05 mae 0.00473175116233749
training loss 3.788605886407425e-05 mae 0.004733758391072319
training loss 3.834332541310093e-05 mae 0.004768813001130942
Epoch 118, training: loss: 0.0000382, mae: 0.0047597 test: loss0.0000834, mae:0.0066867
training loss 2.662971382960677e-05 mae 0.004224230069667101
training loss 3.8957958667214914e-05 mae 0.004673767204889479
training loss 3.8019359041331554e-05 mae 0.004734696977135568
training loss 3.679379191429243e-05 mae 0.00468235785783028
training loss 3.799145522509151e-05 mae 0.004781161214859776
Epoch 119, training: loss: 0.0000381, mae: 0.0047880 test: loss0.0000844, mae:0.0067321
training loss 4.231089769746177e-05 mae 0.004786597564816475
training loss 3.688742952933763e-05 mae 0.004735133811539296
training loss 3.558017416064177e-05 mae 0.004643358617541519
training loss 3.719466384675659e-05 mae 0.004671591761158102
training loss 3.6789424330750756e-05 mae 0.004671868839213131
Epoch 120, training: loss: 0.0000366, mae: 0.0046615 test: loss0.0000812, mae:0.0065829
training loss 2.7899834094569087e-05 mae 0.003790699178352952
training loss 3.371795808597396e-05 mae 0.004522917550239783
training loss 3.575316781514108e-05 mae 0.0045517461817010775
training loss 3.5290652902897846e-05 mae 0.004572497241334705
training loss 3.562801541545976e-05 mae 0.004603905243039201
Epoch 121, training: loss: 0.0000357, mae: 0.0046128 test: loss0.0000849, mae:0.0067329
training loss 3.540961552062072e-05 mae 0.004619937855750322
training loss 3.8561720394486034e-05 mae 0.004621914793353747
training loss 3.670476374705093e-05 mae 0.004624317091253431
training loss 3.7231300088240854e-05 mae 0.00468793942821243
training loss 3.658122448686485e-05 mae 0.004669176725049814
Epoch 122, training: loss: 0.0000367, mae: 0.0046820 test: loss0.0000854, mae:0.0066922
training loss 2.8866568754892796e-05 mae 0.0042619346641004086
training loss 3.728401847469037e-05 mae 0.004572116509627771
training loss 3.6274457850613364e-05 mae 0.004578432035955168
training loss 3.6049848232558614e-05 mae 0.0045993006356455255
training loss 3.591639007701175e-05 mae 0.004596453067378621
Epoch 123, training: loss: 0.0000358, mae: 0.0045941 test: loss0.0000859, mae:0.0067585
training loss 2.5811785235418938e-05 mae 0.0038476132322102785
training loss 3.305317784229285e-05 mae 0.004485836366702822
training loss 3.3061599145248064e-05 mae 0.004490645761407986
training loss 3.50100178926272e-05 mae 0.004564344042616078
training loss 3.591203899101056e-05 mae 0.004641045849835178
Epoch 124, training: loss: 0.0000359, mae: 0.0046460 test: loss0.0000878, mae:0.0067983
training loss 2.8950598789379e-05 mae 0.004192275460809469
training loss 3.324563384048102e-05 mae 0.004466593484668169
training loss 3.254161811578385e-05 mae 0.004454967775612628
training loss 3.538497847763957e-05 mae 0.004582185597337834
training loss 3.5499808273106736e-05 mae 0.004598014078691455
Epoch 125, training: loss: 0.0000356, mae: 0.0046036 test: loss0.0000854, mae:0.0067954
training loss 4.303050445741974e-05 mae 0.005681209731847048
training loss 3.296352531156027e-05 mae 0.004480666362260486
training loss 3.25203417749363e-05 mae 0.004442275031515866
training loss 3.372953894077059e-05 mae 0.004469892390948159
training loss 3.378300309702185e-05 mae 0.00448540049901612
Epoch 126, training: loss: 0.0000343, mae: 0.0045222 test: loss0.0000971, mae:0.0071929
training loss 4.617459126166068e-05 mae 0.004954807925969362
training loss 3.9781024123144824e-05 mae 0.0047734394514312335
training loss 3.730590586849641e-05 mae 0.004678322011512695
training loss 3.649055684006169e-05 mae 0.004655382165759308
training loss 3.578727243544038e-05 mae 0.004620506729356094
Epoch 127, training: loss: 0.0000359, mae: 0.0046232 test: loss0.0000825, mae:0.0066251
training loss 4.2568051867419854e-05 mae 0.005260927136987448
training loss 3.132654470106241e-05 mae 0.004337814207389659
training loss 3.125421710930175e-05 mae 0.004362599006715682
training loss 3.366984582806023e-05 mae 0.0044349612921032264
training loss 3.3793213609426486e-05 mae 0.004477134733159668
Epoch 128, training: loss: 0.0000337, mae: 0.0044799 test: loss0.0000861, mae:0.0066918
training loss 3.3974738471442834e-05 mae 0.004735174123197794
training loss 3.197476657720365e-05 mae 0.004387234342193194
training loss 3.3343135805582704e-05 mae 0.00438755822810575
training loss 3.275584630868228e-05 mae 0.004381517995309259
training loss 3.2701919903119794e-05 mae 0.004398809913870189
Epoch 129, training: loss: 0.0000327, mae: 0.0044067 test: loss0.0000824, mae:0.0065903
training loss 3.127919262624346e-05 mae 0.004417126998305321
training loss 3.140557975498001e-05 mae 0.004373565311634951
training loss 3.0338810980124018e-05 mae 0.004273728338429833
training loss 3.048438765176045e-05 mae 0.0043070101401219705
training loss 3.1112882931860385e-05 mae 0.004312809972574385
Epoch 130, training: loss: 0.0000312, mae: 0.0043202 test: loss0.0000822, mae:0.0066234
training loss 2.6267975044902414e-05 mae 0.004290740936994553
training loss 2.9558451013358805e-05 mae 0.0042223729257124904
training loss 3.0228530623290765e-05 mae 0.004283528696598097
training loss 3.148172000264642e-05 mae 0.004307276134484059
training loss 3.1535959270011314e-05 mae 0.004332944534172242
Epoch 131, training: loss: 0.0000318, mae: 0.0043570 test: loss0.0000843, mae:0.0066535
training loss 2.4088983991532587e-05 mae 0.003671445185318589
training loss 3.235060215528767e-05 mae 0.004437611748774846
training loss 3.3248776063454986e-05 mae 0.0044193592263699144
training loss 3.254050197155333e-05 mae 0.004389822248690176
training loss 3.211343732008897e-05 mae 0.004376191951668086
Epoch 132, training: loss: 0.0000323, mae: 0.0043953 test: loss0.0000892, mae:0.0068550
training loss 3.224828469683416e-05 mae 0.004578881897032261
training loss 2.9493961597372794e-05 mae 0.004230078510647894
training loss 2.9406672885922697e-05 mae 0.00422765874064131
training loss 3.0932165815788707e-05 mae 0.004266297187199754
training loss 3.1373673560276205e-05 mae 0.0043248803081426465
Epoch 133, training: loss: 0.0000318, mae: 0.0043537 test: loss0.0000950, mae:0.0071436
training loss 2.392530041106511e-05 mae 0.003554743714630604
training loss 3.3637990282823364e-05 mae 0.0044578900697695885
training loss 3.543434744606561e-05 mae 0.004524256646780687
training loss 3.4138383088193584e-05 mae 0.004488280954283498
training loss 3.342947600423433e-05 mae 0.0044682384364362554
Epoch 134, training: loss: 0.0000333, mae: 0.0044611 test: loss0.0000851, mae:0.0067453
training loss 2.4960419978015125e-05 mae 0.004393252078443766
training loss 3.100766697767254e-05 mae 0.004293091901048433
training loss 2.938509099087492e-05 mae 0.004203838538286268
training loss 2.91127851548499e-05 mae 0.0042114209180192045
training loss 3.0434835604697517e-05 mae 0.00424900903269204
Epoch 135, training: loss: 0.0000306, mae: 0.0042565 test: loss0.0000876, mae:0.0068353
training loss 1.783729749149643e-05 mae 0.0033411832991987467
training loss 2.944440654805386e-05 mae 0.004250369052968773
training loss 2.9948363797050443e-05 mae 0.004184528263193545
training loss 3.069908518155469e-05 mae 0.00427011560975568
training loss 3.124571039327053e-05 mae 0.004325199898897639
Epoch 136, training: loss: 0.0000312, mae: 0.0043251 test: loss0.0000899, mae:0.0067392
training loss 2.3376065655611455e-05 mae 0.0037274977657943964
training loss 3.117906762068456e-05 mae 0.004142255408178065
training loss 3.0045174249193722e-05 mae 0.004183324572565679
training loss 2.980235435251903e-05 mae 0.004184959524169265
training loss 2.9852702217754273e-05 mae 0.004212232841762584
Epoch 137, training: loss: 0.0000299, mae: 0.0042163 test: loss0.0000842, mae:0.0066611
training loss 1.910002356453333e-05 mae 0.0036467909812927246
training loss 2.9557602543968186e-05 mae 0.00424536398392828
training loss 2.9445553883477363e-05 mae 0.004142197881742279
training loss 2.9809546696852087e-05 mae 0.004204384203913887
training loss 3.0438125857888465e-05 mae 0.004264716917315304
Epoch 138, training: loss: 0.0000304, mae: 0.0042631 test: loss0.0000839, mae:0.0066394
training loss 3.494785414659418e-05 mae 0.004893384873867035
training loss 3.072726120116581e-05 mae 0.004157299235207486
training loss 2.940277759215419e-05 mae 0.0041723109915446826
training loss 2.9548249483393395e-05 mae 0.004203438519851753
training loss 2.961988577307741e-05 mae 0.004218809695830984
Epoch 139, training: loss: 0.0000296, mae: 0.0042211 test: loss0.0000866, mae:0.0068181
training loss 2.6288698791177012e-05 mae 0.0038794910069555044
training loss 2.7430050406609564e-05 mae 0.0040882201500090885
training loss 2.7338085556489243e-05 mae 0.004086021403090494
training loss 2.7718058725031055e-05 mae 0.004114719483976727
training loss 2.8700387059445655e-05 mae 0.004138058968889177
Epoch 140, training: loss: 0.0000289, mae: 0.0041515 test: loss0.0000843, mae:0.0067150
training loss 2.1330413801479153e-05 mae 0.0035459089558571577
training loss 2.605745348595448e-05 mae 0.004022185133733585
training loss 2.5350663947554694e-05 mae 0.003957384484958383
training loss 2.6529205015984627e-05 mae 0.004030730290067019
training loss 2.8674449745651938e-05 mae 0.0041269915947570145
Epoch 141, training: loss: 0.0000287, mae: 0.0041294 test: loss0.0000840, mae:0.0066824
training loss 3.003798883582931e-05 mae 0.004137552343308926
training loss 2.7144425200508432e-05 mae 0.004083056290469626
training loss 2.9034085351253958e-05 mae 0.004102803563865106
training loss 2.8384354453202766e-05 mae 0.004110823707679743
training loss 2.828244532566936e-05 mae 0.004120603436604142
Epoch 142, training: loss: 0.0000284, mae: 0.0041314 test: loss0.0000845, mae:0.0066719
training loss 2.894022509281058e-05 mae 0.004198867361992598
training loss 2.8013804676804428e-05 mae 0.00414703598282501
training loss 2.8026054552707267e-05 mae 0.004150004234478467
training loss 2.9601774439175203e-05 mae 0.004195269690641505
training loss 2.9454188571340217e-05 mae 0.004203996606013597
Epoch 143, training: loss: 0.0000293, mae: 0.0041899 test: loss0.0000812, mae:0.0065610
training loss 1.4655740415037144e-05 mae 0.0029217125847935677
training loss 2.4061706979267127e-05 mae 0.0038650225637955403
training loss 2.4254359862488223e-05 mae 0.0038571165768558724
training loss 2.698500623632784e-05 mae 0.003978131920259616
training loss 2.8209352326959908e-05 mae 0.004081669368952584
Epoch 144, training: loss: 0.0000282, mae: 0.0040836 test: loss0.0000897, mae:0.0066856
training loss 2.4040069547481835e-05 mae 0.0040415385738015175
training loss 3.0422527868045022e-05 mae 0.004114592142001378
training loss 2.9434094763514225e-05 mae 0.0041580964755820165
training loss 2.8331727650543115e-05 mae 0.00412659949487359
training loss 2.8044818310123233e-05 mae 0.004114065569505765
Epoch 145, training: loss: 0.0000280, mae: 0.0041068 test: loss0.0000881, mae:0.0068296
training loss 1.9259197870269418e-05 mae 0.0035972397308796644
training loss 2.6978780214467047e-05 mae 0.004058114882996853
training loss 2.8294036473207505e-05 mae 0.004073243117937357
training loss 2.7493614760901703e-05 mae 0.004054932375531028
training loss 2.783812689261171e-05 mae 0.004091824221874201
Epoch 146, training: loss: 0.0000278, mae: 0.0040892 test: loss0.0000856, mae:0.0066999
training loss 2.4735016268095933e-05 mae 0.004152413457632065
training loss 2.600372676277082e-05 mae 0.00398972013727853
training loss 2.523935772572232e-05 mae 0.0039293648891778
training loss 2.6420047112535503e-05 mae 0.00394653668627143
training loss 2.679929096289084e-05 mae 0.003993529782384934
Epoch 147, training: loss: 0.0000267, mae: 0.0039871 test: loss0.0000825, mae:0.0066120
training loss 1.5874169548624195e-05 mae 0.0030537748243659735
training loss 2.4919091843270612e-05 mae 0.0038989204502500154
training loss 2.4779468325919374e-05 mae 0.0038884616719724813
training loss 2.5051843764737256e-05 mae 0.00392598707370756
training loss 2.6322229503384914e-05 mae 0.003980061141264378
Epoch 148, training: loss: 0.0000265, mae: 0.0039922 test: loss0.0000852, mae:0.0066661
training loss 2.5493935027043335e-05 mae 0.004102733451873064
training loss 2.3154293326204457e-05 mae 0.0037616505698465246
training loss 2.5271077614132365e-05 mae 0.0038256846151618973
training loss 2.5779630489927253e-05 mae 0.0039025810223368837
training loss 2.6315207919221738e-05 mae 0.0039562660950555745
Epoch 149, training: loss: 0.0000263, mae: 0.0039544 test: loss0.0000869, mae:0.0068008
training loss 2.8495143851614557e-05 mae 0.004107319284230471
training loss 3.0413515132215044e-05 mae 0.004138040584603362
training loss 2.734331859990039e-05 mae 0.004005627851877917
training loss 2.6711889657368095e-05 mae 0.0039914480182311396
training loss 2.6245097990736504e-05 mae 0.00396942386093239
Epoch 150, training: loss: 0.0000262, mae: 0.0039656 test: loss0.0000834, mae:0.0066628
training loss 2.8132839361205697e-05 mae 0.004116150084882975
training loss 2.517747761172073e-05 mae 0.0038959682216027783
training loss 2.813811387217231e-05 mae 0.004036057664376526
training loss 2.725341075335341e-05 mae 0.0040121972838429035
training loss 2.7100836668797444e-05 mae 0.0040207645237279035
Epoch 151, training: loss: 0.0000271, mae: 0.0040274 test: loss0.0000859, mae:0.0066958
training loss 3.567693056538701e-05 mae 0.004896045662462711
training loss 8.717791734299826e-05 mae 0.006317340860179825
training loss 7.796863644783814e-05 mae 0.0062422640990502775
training loss 6.608956856560582e-05 mae 0.005829480199130955
training loss 5.8991483438239396e-05 mae 0.005564536888902977
Epoch 152, training: loss: 0.0000579, mae: 0.0055244 test: loss0.0000847, mae:0.0066986
training loss 2.1817735614604317e-05 mae 0.003591625252738595
training loss 3.29312812277303e-05 mae 0.004311158933548951
training loss 3.0942843349663165e-05 mae 0.004254881251086987
training loss 2.9975787180129725e-05 mae 0.004224438340532661
training loss 2.9890423763901882e-05 mae 0.004215876929767762
Epoch 153, training: loss: 0.0000300, mae: 0.0042239 test: loss0.0000836, mae:0.0066255
training loss 1.9780169168370776e-05 mae 0.003351363120600581
training loss 2.576318383587789e-05 mae 0.003962670238323363
training loss 2.6308654347609825e-05 mae 0.003895725538529972
training loss 2.5548636411210298e-05 mae 0.003882957309398134
training loss 2.5886024147679274e-05 mae 0.0039323273517969815
Epoch 154, training: loss: 0.0000260, mae: 0.0039381 test: loss0.0000823, mae:0.0066318
training loss 2.4235887394752353e-05 mae 0.003896948881447315
training loss 2.443024311337379e-05 mae 0.0038088316773520967
training loss 2.4588437248318162e-05 mae 0.003838916332230413
training loss 2.571929147335375e-05 mae 0.0038848127306297113
training loss 2.525840261851904e-05 mae 0.0038677686165128605
Epoch 155, training: loss: 0.0000252, mae: 0.0038690 test: loss0.0000809, mae:0.0065435
training loss 1.2139477803430054e-05 mae 0.002858067164197564
training loss 2.270175677860571e-05 mae 0.003732048878994058
training loss 2.3742257920698956e-05 mae 0.0037147708094245435
training loss 2.310590728345852e-05 mae 0.0036942365283257526
training loss 2.3161073035614998e-05 mae 0.0037141131186181338
Epoch 156, training: loss: 0.0000233, mae: 0.0037299 test: loss0.0000834, mae:0.0066259
training loss 2.0445751943043433e-05 mae 0.003493758849799633
training loss 2.2216327534788783e-05 mae 0.003670606492817694
training loss 2.3131203864851503e-05 mae 0.00365598271554797
training loss 2.3347509467451196e-05 mae 0.0037101199110316117
training loss 2.321029597483876e-05 mae 0.0037144858874287915
Epoch 157, training: loss: 0.0000232, mae: 0.0037194 test: loss0.0000822, mae:0.0065720
training loss 2.2014088244759478e-05 mae 0.0036471837665885687
training loss 2.164945744523578e-05 mae 0.003649472907258599
training loss 2.1705386211353906e-05 mae 0.0036510725109146374
training loss 2.2814426494660926e-05 mae 0.0036729631640189725
training loss 2.260000338759511e-05 mae 0.0036670203547720893
Epoch 158, training: loss: 0.0000227, mae: 0.0036726 test: loss0.0000825, mae:0.0065872
training loss 2.5305753297288902e-05 mae 0.003913162741810083
training loss 2.2657477832884375e-05 mae 0.0036970287031841058
training loss 2.291737832865242e-05 mae 0.003724685522017651
training loss 2.3414434737474603e-05 mae 0.0037144164166209724
training loss 2.2973442812588065e-05 mae 0.003709528967043136
Epoch 159, training: loss: 0.0000229, mae: 0.0037042 test: loss0.0000813, mae:0.0065258
training loss 1.768533002177719e-05 mae 0.0032616632524877787
training loss 2.2644610034345252e-05 mae 0.0036927327312821267
training loss 2.2481789222516757e-05 mae 0.003703624012391313
training loss 2.343568397894624e-05 mae 0.0037120488234826483
training loss 2.3358358961179055e-05 mae 0.0037281315435483985
Epoch 160, training: loss: 0.0000233, mae: 0.0037228 test: loss0.0000848, mae:0.0066642
training loss 1.7419335563317873e-05 mae 0.003042530035600066
training loss 2.5167251003628796e-05 mae 0.003728124491103432
training loss 2.4281230901461905e-05 mae 0.0037395180373609362
training loss 2.3955361011878676e-05 mae 0.00374957656993574
training loss 2.3714511787794832e-05 mae 0.0037486464426903753
Epoch 161, training: loss: 0.0000238, mae: 0.0037621 test: loss0.0000821, mae:0.0065961
training loss 2.67101131612435e-05 mae 0.004040638916194439
training loss 2.1070336586882487e-05 mae 0.0035909756278509606
training loss 2.4254794925131998e-05 mae 0.0037530101904885323
training loss 2.4387380746690495e-05 mae 0.0038042361585929113
training loss 2.417473997094694e-05 mae 0.0038090729738460547
Epoch 162, training: loss: 0.0000242, mae: 0.0038141 test: loss0.0000848, mae:0.0065983
training loss 2.1708494386984967e-05 mae 0.0035982707049697638
training loss 2.811458243206075e-05 mae 0.003920746382837201
training loss 2.5001180730025292e-05 mae 0.0038011854786499585
training loss 2.4758129003980054e-05 mae 0.0038086871835495643
training loss 2.4106191545318004e-05 mae 0.00378082726099783
Epoch 163, training: loss: 0.0000240, mae: 0.0037794 test: loss0.0000810, mae:0.0064920
training loss 2.196097921114415e-05 mae 0.003539769211784005
training loss 2.129388549827341e-05 mae 0.0035957896159779204
training loss 2.0730572905809855e-05 mae 0.0035693322257645942
training loss 2.2565871789848704e-05 mae 0.003657123203457191
training loss 2.2079641414745156e-05 mae 0.0036211868050735245
Epoch 164, training: loss: 0.0000220, mae: 0.0036132 test: loss0.0000849, mae:0.0066960
training loss 2.3563168724649586e-05 mae 0.0037092084530740976
training loss 2.3538554873369983e-05 mae 0.0038186548843833737
training loss 2.2699010397706968e-05 mae 0.0037374059706557494
training loss 2.2265516133681758e-05 mae 0.003704106570869092
training loss 2.3366314800019185e-05 mae 0.003734216239970567
Epoch 165, training: loss: 0.0000234, mae: 0.0037417 test: loss0.0000831, mae:0.0065706
training loss 2.9628916308865882e-05 mae 0.004064079839736223
training loss 2.1290366716771937e-05 mae 0.0036161708526824634
training loss 2.0970887475198078e-05 mae 0.0035876735818968844
training loss 2.2258168470541577e-05 mae 0.0036308774786279695
training loss 2.226057745812695e-05 mae 0.0036488694268561423
Epoch 166, training: loss: 0.0000223, mae: 0.0036621 test: loss0.0000884, mae:0.0068190
training loss 2.4617291273898445e-05 mae 0.0038646801840513945
training loss 2.634732920662774e-05 mae 0.0038154704551048133
training loss 2.539655334018817e-05 mae 0.003832692663083869
training loss 2.4385662235370495e-05 mae 0.0037963338294960823
training loss 2.4188234881387583e-05 mae 0.003806350300950347
Epoch 167, training: loss: 0.0000241, mae: 0.0038012 test: loss0.0000839, mae:0.0066399
training loss 3.4234511986142024e-05 mae 0.004262460861355066
training loss 2.16043625936008e-05 mae 0.0036163781221736882
training loss 2.120673978442757e-05 mae 0.0036024026651082937
training loss 2.2445464599405388e-05 mae 0.003643937524568462
training loss 2.2499737044643155e-05 mae 0.003671088959067245
Epoch 168, training: loss: 0.0000225, mae: 0.0036752 test: loss0.0000842, mae:0.0066537
training loss 2.0161811335128732e-05 mae 0.0035798249300569296
training loss 2.1041677677667215e-05 mae 0.003567920262724453
training loss 2.3412817571093767e-05 mae 0.0036706359950954667
training loss 2.2944872512951235e-05 mae 0.0036763819736360705
training loss 2.2715111353774307e-05 mae 0.003687130194620706
Epoch 169, training: loss: 0.0000228, mae: 0.0036911 test: loss0.0000829, mae:0.0065643
training loss 2.2692925995215774e-05 mae 0.0035484067630022764
training loss 2.273694750043682e-05 mae 0.003537246103708942
training loss 2.1885824332124382e-05 mae 0.0035479100384186984
training loss 2.1795481062260614e-05 mae 0.003579858369779904
training loss 2.1974093192624767e-05 mae 0.0036159596100115983
Epoch 170, training: loss: 0.0000220, mae: 0.0036173 test: loss0.0000835, mae:0.0066054
training loss 1.6131090887938626e-05 mae 0.0034266344737261534
training loss 1.8155387556808092e-05 mae 0.0033465047850839654
training loss 1.8754757421798922e-05 mae 0.0033967227462937343
training loss 2.0603143855997236e-05 mae 0.0034824848199719633
training loss 2.0807992483757673e-05 mae 0.003522793201738922
Epoch 171, training: loss: 0.0000209, mae: 0.0035293 test: loss0.0000859, mae:0.0067629
training loss 2.015868085436523e-05 mae 0.003688408061861992
training loss 2.153081862286176e-05 mae 0.0036314371927623065
training loss 2.101287605406055e-05 mae 0.003592100978764419
training loss 2.0708029790554395e-05 mae 0.0035674742743718305
training loss 2.1460245835301773e-05 mae 0.0035858649102535408
Epoch 172, training: loss: 0.0000215, mae: 0.0035820 test: loss0.0000893, mae:0.0068400
training loss 1.5033700037747622e-05 mae 0.0030579061713069677
training loss 2.2768828428382344e-05 mae 0.0035757689289383447
training loss 2.114691420447883e-05 mae 0.0035206742888998868
training loss 2.079075516392562e-05 mae 0.003515976675422183
training loss 2.0530690567261565e-05 mae 0.003508367706599889
Epoch 173, training: loss: 0.0000205, mae: 0.0035073 test: loss0.0000852, mae:0.0066894
training loss 1.451969365007244e-05 mae 0.002970104804262519
training loss 2.0683760839364914e-05 mae 0.0033734052022005986
training loss 1.9547926373465677e-05 mae 0.003354077536317675
training loss 2.044961800023378e-05 mae 0.003455508030050539
training loss 2.0649496280981752e-05 mae 0.003493750131971652
Epoch 174, training: loss: 0.0000207, mae: 0.0034969 test: loss0.0000844, mae:0.0066502
training loss 1.4998448023106903e-05 mae 0.002859036670997739
training loss 1.862955331507335e-05 mae 0.003370896835500995
training loss 2.2393902498259943e-05 mae 0.003582135751142656
training loss 2.220853467039003e-05 mae 0.003613096171533629
training loss 2.170944037459604e-05 mae 0.0035890279701489276
Epoch 175, training: loss: 0.0000217, mae: 0.0035894 test: loss0.0000847, mae:0.0066639
training loss 1.2248378880030941e-05 mae 0.0027046280447393656
training loss 2.2931047401381832e-05 mae 0.003571775851442534
training loss 2.157123334753951e-05 mae 0.003552517892866591
training loss 2.0938303266379774e-05 mae 0.0035251863624604545
training loss 2.1013514634971242e-05 mae 0.0035580448616314582
Epoch 176, training: loss: 0.0000211, mae: 0.0035602 test: loss0.0000832, mae:0.0065802
training loss 1.1133067346236203e-05 mae 0.0028001221362501383
training loss 2.3274820384411962e-05 mae 0.003594567990113122
training loss 2.1805474563446454e-05 mae 0.0035518989183635696
training loss 2.13120777799539e-05 mae 0.0035277841264221644
training loss 2.115899949794722e-05 mae 0.0035481691077833438
Epoch 177, training: loss: 0.0000211, mae: 0.0035468 test: loss0.0000836, mae:0.0065967
training loss 2.087810389639344e-05 mae 0.00362859177403152
training loss 1.6605825330057712e-05 mae 0.0031865810856734423
training loss 1.8366581391074037e-05 mae 0.0033443858777864434
training loss 1.846203950295627e-05 mae 0.003363867900920229
training loss 1.9731980197927136e-05 mae 0.0034309537288969125
Epoch 178, training: loss: 0.0000196, mae: 0.0034241 test: loss0.0000849, mae:0.0066610
training loss 1.8562956029199995e-05 mae 0.003726131049916148
training loss 2.070918986539576e-05 mae 0.00354887752373721
training loss 2.1352612414978804e-05 mae 0.0035212423403443082
training loss 2.126014177164754e-05 mae 0.0035403640987043154
training loss 2.1248478021076536e-05 mae 0.0035545458734517373
Epoch 179, training: loss: 0.0000212, mae: 0.0035548 test: loss0.0000840, mae:0.0066547
training loss 2.0717920051538385e-05 mae 0.0035635263193398714
training loss 2.1239266953118347e-05 mae 0.0034610669393384573
training loss 2.0400912593560306e-05 mae 0.0034445704071607336
training loss 2.0432475585099595e-05 mae 0.0034798949182847675
training loss 2.0639205941186464e-05 mae 0.0035221876937951598
Epoch 180, training: loss: 0.0000207, mae: 0.0035227 test: loss0.0000882, mae:0.0067378
training loss 2.3605294700246304e-05 mae 0.003972785081714392
training loss 2.303655221076755e-05 mae 0.0035738293309787324
training loss 2.2329266123573928e-05 mae 0.0036007716749232297
training loss 2.13485802594624e-05 mae 0.003544168893728924
training loss 2.1108536903870377e-05 mae 0.003532205734967223
Epoch 181, training: loss: 0.0000209, mae: 0.0035232 test: loss0.0000850, mae:0.0066445
training loss 1.6835811038617976e-05 mae 0.003274395130574703
training loss 1.867475307959716e-05 mae 0.0033950829358004478
training loss 2.0260406323935658e-05 mae 0.003438098741105141
training loss 1.9559156489837033e-05 mae 0.003411728002914283
training loss 1.939673470203266e-05 mae 0.0034117832778725976
Epoch 182, training: loss: 0.0000195, mae: 0.0034184 test: loss0.0000833, mae:0.0066271
training loss 2.8693937565549277e-05 mae 0.003844191087409854
training loss 1.7851618716068673e-05 mae 0.003294077268162487
training loss 1.8081351263750588e-05 mae 0.0033369731287112335
training loss 1.8100441045454315e-05 mae 0.0033356499432629313
training loss 1.9472636154770862e-05 mae 0.003416484569324488
Epoch 183, training: loss: 0.0000194, mae: 0.0034110 test: loss0.0000843, mae:0.0066591
training loss 1.6733987649786286e-05 mae 0.003111940110102296
training loss 2.0559611121094418e-05 mae 0.0035392096832248515
training loss 1.9960793168246542e-05 mae 0.0034876809349412684
training loss 2.0578374484548536e-05 mae 0.003474694986624139
training loss 2.1209979246095964e-05 mae 0.0035416834607757447
Epoch 184, training: loss: 0.0000213, mae: 0.0035585 test: loss0.0000846, mae:0.0066665
training loss 1.3385997590376064e-05 mae 0.002929587149992585
training loss 1.9721849140278565e-05 mae 0.003274826912720706
training loss 1.8491756038363943e-05 mae 0.0032578676680822187
training loss 1.883946608347063e-05 mae 0.0033252088843369917
training loss 1.8853100938479228e-05 mae 0.00333745042163304
Epoch 185, training: loss: 0.0000189, mae: 0.0033427 test: loss0.0000843, mae:0.0066001
training loss 1.128304847952677e-05 mae 0.0028004776686429977
training loss 1.6690485804871308e-05 mae 0.0032259149148183707
training loss 1.8863990608044312e-05 mae 0.003320030838085135
training loss 1.9321245628623798e-05 mae 0.003373169562230441
training loss 1.95708893560871e-05 mae 0.003415920502918573
Epoch 186, training: loss: 0.0000196, mae: 0.0034192 test: loss0.0000854, mae:0.0066856
training loss 1.9705277736647986e-05 mae 0.003280817298218608
training loss 2.0887015535645692e-05 mae 0.003349328842744523
training loss 1.9646361822494097e-05 mae 0.003356251266259368
training loss 1.9637747773110785e-05 mae 0.003395798505092693
training loss 1.924624992360765e-05 mae 0.0033803953990729444
Epoch 187, training: loss: 0.0000192, mae: 0.0033825 test: loss0.0000848, mae:0.0066286
training loss 1.5658713891752996e-05 mae 0.0030779268126934767
training loss 1.5535401960768665e-05 mae 0.0031276060149584904
training loss 1.7621576478289875e-05 mae 0.003215774743919176
training loss 1.753703166449494e-05 mae 0.003222485126878153
training loss 1.763952223515859e-05 mae 0.00324946531877663
Epoch 188, training: loss: 0.0000178, mae: 0.0032648 test: loss0.0000849, mae:0.0066388
training loss 1.635161788726691e-05 mae 0.003278885968029499
training loss 1.747908701123807e-05 mae 0.003305285160119334
training loss 1.874920691971236e-05 mae 0.003289711707741907
training loss 1.870268209776085e-05 mae 0.003308397366215062
training loss 1.8730960012665793e-05 mae 0.0033391258848451118
Epoch 189, training: loss: 0.0000187, mae: 0.0033350 test: loss0.0000845, mae:0.0066599
training loss 1.5907580745988525e-05 mae 0.0030484942253679037
training loss 1.73123626511512e-05 mae 0.0032470773439854383
training loss 1.7722642636240698e-05 mae 0.0032794443581548355
training loss 1.7640487457784004e-05 mae 0.003279547256399069
training loss 1.766690938757843e-05 mae 0.0032787665293843873
Epoch 190, training: loss: 0.0000184, mae: 0.0032941 test: loss0.0000848, mae:0.0066421
training loss 1.7088668755604886e-05 mae 0.003366240067407489
training loss 1.667404001850895e-05 mae 0.003222498216428885
training loss 1.7493403181943716e-05 mae 0.0032983227462351035
training loss 1.781489109569954e-05 mae 0.003311795160939165
training loss 1.8216291119185842e-05 mae 0.003295623473776736
Epoch 191, training: loss: 0.0000184, mae: 0.0033113 test: loss0.0000849, mae:0.0066882
training loss 1.764446460583713e-05 mae 0.003342980518937111
training loss 1.606545661428861e-05 mae 0.0031294210211319086
training loss 1.7217208483764423e-05 mae 0.003232797494822062
training loss 1.7123980932416663e-05 mae 0.003228212204077188
training loss 1.7420425551994314e-05 mae 0.003263086407209065
Epoch 192, training: loss: 0.0000178, mae: 0.0032586 test: loss0.0000873, mae:0.0067621
training loss 1.6446034351247363e-05 mae 0.0033683229703456163
training loss 1.812816302773689e-05 mae 0.0033586423058866296
training loss 1.8989603802873545e-05 mae 0.003402617868349547
training loss 1.9579370036688076e-05 mae 0.0034552368609888466
training loss 2.027311619265184e-05 mae 0.003471339677474392
Epoch 193, training: loss: 0.0000204, mae: 0.0034811 test: loss0.0000879, mae:0.0068196
training loss 1.722573688311968e-05 mae 0.0031989908311516047
training loss 2.0012490084827376e-05 mae 0.003476704470813274
training loss 2.0635513528129137e-05 mae 0.0034750346002159725
training loss 1.956303490350672e-05 mae 0.003402343115442439
training loss 1.889112279887238e-05 mae 0.0033552589587663387
Epoch 194, training: loss: 0.0000189, mae: 0.0033545 test: loss0.0000850, mae:0.0066971
training loss 1.0525615834922064e-05 mae 0.002668757690116763
training loss 1.7513469638110038e-05 mae 0.003260130847932077
training loss 1.77655743701138e-05 mae 0.0031949294763294494
training loss 1.7399393199478118e-05 mae 0.0031966136225702753
training loss 1.72468412461397e-05 mae 0.0032029098099032754
Epoch 195, training: loss: 0.0000173, mae: 0.0032057 test: loss0.0000837, mae:0.0065940
training loss 1.7166978068416938e-05 mae 0.003340509021654725
training loss 1.7528304691522327e-05 mae 0.0032497323620333967
training loss 1.7252273786346777e-05 mae 0.0032189609275411544
training loss 1.7827619028338206e-05 mae 0.0032084213249039956
training loss 1.7769572764902366e-05 mae 0.003223323625795061
Epoch 196, training: loss: 0.0000178, mae: 0.0032266 test: loss0.0000867, mae:0.0067827
training loss 1.2965028872713447e-05 mae 0.0031378373969346285
training loss 1.842845520906278e-05 mae 0.00322856506168404
training loss 1.772576770506048e-05 mae 0.0032211602734641565
training loss 1.748737694596525e-05 mae 0.003221118491179996
training loss 1.7178310254754956e-05 mae 0.0032069117551326615
Epoch 197, training: loss: 0.0000172, mae: 0.0032080 test: loss0.0000884, mae:0.0067764
training loss 2.067433342745062e-05 mae 0.003404479706659913
training loss 1.5094990172370923e-05 mae 0.0030520376155846836
training loss 1.6765134304397074e-05 mae 0.0032057226474147917
training loss 1.7875362684677884e-05 mae 0.0032520193918083866
training loss 1.770648758571242e-05 mae 0.003252002773157771
Epoch 198, training: loss: 0.0000178, mae: 0.0032594 test: loss0.0000851, mae:0.0066274
training loss 9.158179636870045e-06 mae 0.0024973133113235235
training loss 1.4988026693982632e-05 mae 0.0030540250853507544
training loss 1.5472543360950716e-05 mae 0.0030868573363243344
training loss 1.6697291134702832e-05 mae 0.0031300754477547486
training loss 1.6573322903951613e-05 mae 0.0031381425348142936
Epoch 199, training: loss: 0.0000166, mae: 0.0031450 test: loss0.0000859, mae:0.0066845
current learning rate: 0.000125
training loss 1.6221047189901583e-05 mae 0.0030251266434788704
training loss 1.3489194298374203e-05 mae 0.002861113636754452
training loss 1.276821642995225e-05 mae 0.0027695765988532415
training loss 1.245604173332531e-05 mae 0.0027476489113717785
training loss 1.2879240938813588e-05 mae 0.002738622232659985
Epoch 200, training: loss: 0.0000129, mae: 0.0027452 test: loss0.0000836, mae:0.0065735
training loss 6.840009973529959e-06 mae 0.00196100608445704
training loss 1.0542338669420489e-05 mae 0.0024642069394901107
training loss 1.1494221492437932e-05 mae 0.0026060360665940262
training loss 1.207306800006938e-05 mae 0.0026250752776133405
training loss 1.196011605239094e-05 mae 0.00263725193298482
Epoch 201, training: loss: 0.0000119, mae: 0.0026340 test: loss0.0000842, mae:0.0065904
training loss 8.234068445744924e-06 mae 0.002068636240437627
training loss 1.1140719941656294e-05 mae 0.002569420199256902
training loss 1.2184640766436398e-05 mae 0.002594932925306482
training loss 1.1727662607857847e-05 mae 0.0025724535877353813
training loss 1.1631359298935333e-05 mae 0.0025755895529896836
Epoch 202, training: loss: 0.0000116, mae: 0.0025722 test: loss0.0000837, mae:0.0065827
training loss 1.2731213246297557e-05 mae 0.0027277779299765825
training loss 1.0998127436323557e-05 mae 0.0025145639163241078
training loss 1.0999234205829078e-05 mae 0.0025403476089681724
training loss 1.1735199183889233e-05 mae 0.002563075784636995
training loss 1.1650091649999781e-05 mae 0.0025753432343857572
Epoch 203, training: loss: 0.0000116, mae: 0.0025720 test: loss0.0000845, mae:0.0065933
training loss 7.701083632127848e-06 mae 0.0021706160623580217
training loss 1.0585557554483835e-05 mae 0.0025215078893975887
training loss 1.0893137344526013e-05 mae 0.0025402802932491106
training loss 1.156132931395863e-05 mae 0.002569721485118054
training loss 1.1515094690787637e-05 mae 0.0025801535345966087
Epoch 204, training: loss: 0.0000115, mae: 0.0025737 test: loss0.0000848, mae:0.0066192
training loss 7.081785952323116e-06 mae 0.002174169523641467
training loss 1.0149716931040282e-05 mae 0.0024683384211076536
training loss 1.0758341071741958e-05 mae 0.002529020212581474
training loss 1.0809756227502401e-05 mae 0.002534681526382782
training loss 1.1449876784983233e-05 mae 0.0025662207357418643
Epoch 205, training: loss: 0.0000114, mae: 0.0025679 test: loss0.0000849, mae:0.0066102
training loss 1.1736047781596426e-05 mae 0.0025309910997748375
training loss 1.2140658086804357e-05 mae 0.002522493648689752
training loss 1.156470782259228e-05 mae 0.0025260966527045218
training loss 1.1402508250543874e-05 mae 0.0025445608065975517
training loss 1.1263382002743586e-05 mae 0.002545595444178217
Epoch 206, training: loss: 0.0000112, mae: 0.0025430 test: loss0.0000852, mae:0.0066178
training loss 1.225113828695612e-05 mae 0.0025887715164572
training loss 1.0228793928713548e-05 mae 0.002477789815862244
training loss 1.0707446369568712e-05 mae 0.0025364579840351158
training loss 1.0899607841204352e-05 mae 0.0025564621831753007
training loss 1.0862902574028603e-05 mae 0.002554617598606505
Epoch 207, training: loss: 0.0000113, mae: 0.0025645 test: loss0.0000860, mae:0.0066794
training loss 1.2044863069604617e-05 mae 0.002823127433657646
training loss 1.2323773440656106e-05 mae 0.0025715288391117666
training loss 1.2040121346937742e-05 mae 0.0026066097244159276
training loss 1.148951800650626e-05 mae 0.0025711468496477084
training loss 1.1445882447147604e-05 mae 0.002580010950152961
Epoch 208, training: loss: 0.0000115, mae: 0.0025861 test: loss0.0000860, mae:0.0066715
training loss 9.604854312783573e-06 mae 0.002504623495042324
training loss 1.0280528412669601e-05 mae 0.002500470926272956
training loss 1.0707144507376704e-05 mae 0.002538537559339242
training loss 1.0750884334299452e-05 mae 0.0025423753140062498
training loss 1.1214793617068271e-05 mae 0.0025510695428165855
Epoch 209, training: loss: 0.0000112, mae: 0.0025531 test: loss0.0000865, mae:0.0066825
training loss 9.81932862487156e-06 mae 0.0023026729468256235
training loss 1.0132490067061397e-05 mae 0.0024530834783160816
training loss 1.0571235954089525e-05 mae 0.0025083779767708916
training loss 1.1010602038548927e-05 mae 0.0025055400798997744
training loss 1.1065818242385111e-05 mae 0.0025231445541799968
Epoch 210, training: loss: 0.0000110, mae: 0.0025194 test: loss0.0000857, mae:0.0066559
training loss 1.1764295777538791e-05 mae 0.0026815980672836304
training loss 1.0457468582827976e-05 mae 0.002506190527449636
training loss 1.0351107471026332e-05 mae 0.0024919483135223824
training loss 1.0615026123848914e-05 mae 0.002520028004478293
training loss 1.108352473093688e-05 mae 0.0025333861857467312
Epoch 211, training: loss: 0.0000111, mae: 0.0025387 test: loss0.0000873, mae:0.0067233
training loss 7.3100363806588575e-06 mae 0.002095139352604747
training loss 1.2452553111121661e-05 mae 0.002611510448760408
training loss 1.2539450079490807e-05 mae 0.002683978072496703
training loss 1.1821387099960407e-05 mae 0.0026232876609825835
training loss 1.1642080748268975e-05 mae 0.002612318337978379
Epoch 212, training: loss: 0.0000116, mae: 0.0026113 test: loss0.0000864, mae:0.0066955
training loss 6.674700216535712e-06 mae 0.0021641345228999853
training loss 9.819852707818533e-06 mae 0.0024327972707539514
training loss 1.1243816894276806e-05 mae 0.002518961234894203
training loss 1.0996013523687739e-05 mae 0.002518324088151042
training loss 1.1134523469877988e-05 mae 0.002539809361744246
Epoch 213, training: loss: 0.0000112, mae: 0.0025440 test: loss0.0000888, mae:0.0067698
training loss 1.0638842468324583e-05 mae 0.002661963691934943
training loss 1.0240958919680186e-05 mae 0.0024927631865127703
training loss 1.0256779070446237e-05 mae 0.0024857770842914976
training loss 1.0812137326793648e-05 mae 0.0024898019125870114
training loss 1.0780312733452351e-05 mae 0.0025017274119682707
Epoch 214, training: loss: 0.0000108, mae: 0.0025008 test: loss0.0000861, mae:0.0066751
training loss 1.8891319996328093e-05 mae 0.003156847320497036
training loss 1.0184584051089853e-05 mae 0.0024758243440266914
training loss 1.1699301426638652e-05 mae 0.002568042478616358
training loss 1.1420379619658404e-05 mae 0.002564785315112414
training loss 1.1321514094149516e-05 mae 0.0025683419144161006
Epoch 215, training: loss: 0.0000113, mae: 0.0025701 test: loss0.0000866, mae:0.0066706
training loss 1.1945966434723232e-05 mae 0.002788523444905877
training loss 1.1358139224338467e-05 mae 0.002461230925594767
training loss 1.0894758766466614e-05 mae 0.0024705476895214443
training loss 1.0932736712741508e-05 mae 0.0025079295244252926
training loss 1.0847466306423387e-05 mae 0.002509924319609813
Epoch 216, training: loss: 0.0000108, mae: 0.0025093 test: loss0.0000878, mae:0.0067334
training loss 1.0968296010105405e-05 mae 0.0023868056014180183
training loss 9.955617458092772e-06 mae 0.0024524736583378974
training loss 9.722653876541707e-06 mae 0.0024089811203798443
training loss 1.0460828539993632e-05 mae 0.002448190332834442
training loss 1.029556452110467e-05 mae 0.002443109968542792
Epoch 217, training: loss: 0.0000103, mae: 0.0024441 test: loss0.0000875, mae:0.0067238
training loss 9.121507900999859e-06 mae 0.002226184820756316
training loss 1.1582750015376774e-05 mae 0.0024835106976074627
training loss 1.0575056570238605e-05 mae 0.00243582231416782
training loss 1.04717845149465e-05 mae 0.0024533516230888126
training loss 1.0373066738816187e-05 mae 0.002456137250219264
Epoch 218, training: loss: 0.0000104, mae: 0.0024642 test: loss0.0000873, mae:0.0067162
training loss 1.0450807167217135e-05 mae 0.0026939131785184145
training loss 1.1550773831236168e-05 mae 0.0024786778775902065
training loss 1.0922612136259841e-05 mae 0.002484503158340507
training loss 1.0685549014604915e-05 mae 0.00248256887599135
training loss 1.0628730563697527e-05 mae 0.002488716409903074
Epoch 219, training: loss: 0.0000106, mae: 0.0024846 test: loss0.0000883, mae:0.0067717
training loss 1.1124552656838205e-05 mae 0.002687477506697178
training loss 9.642952154179369e-06 mae 0.002371415541525566
training loss 9.755597472063202e-06 mae 0.0023986910190894315
training loss 1.057230020398721e-05 mae 0.002450679800479322
training loss 1.0622527674085946e-05 mae 0.00247311935440373
Epoch 220, training: loss: 0.0000106, mae: 0.0024754 test: loss0.0000888, mae:0.0067581
training loss 8.166719453583937e-06 mae 0.0023244796320796013
training loss 1.0096745828538107e-05 mae 0.0024693512047330533
training loss 1.0490279196752041e-05 mae 0.0024372894665005028
training loss 1.0420940541747084e-05 mae 0.0024490909040319592
training loss 1.0289166947591164e-05 mae 0.002444439667244605
Epoch 221, training: loss: 0.0000103, mae: 0.0024505 test: loss0.0000895, mae:0.0067728
training loss 9.864332241704687e-06 mae 0.0025494645815342665
training loss 1.0389739412965035e-05 mae 0.0024743226936598324
training loss 9.743427987211031e-06 mae 0.0024121461244816377
training loss 9.95558983182214e-06 mae 0.002435840801212517
training loss 1.0437120068328918e-05 mae 0.0024617087965905283
Epoch 222, training: loss: 0.0000105, mae: 0.0024658 test: loss0.0000877, mae:0.0067415
training loss 8.694455573277082e-06 mae 0.0024174826685339212
training loss 8.831541248689653e-06 mae 0.0023026596389564805
training loss 9.382452705299011e-06 mae 0.002370681212569521
training loss 9.521407532431103e-06 mae 0.0023862037133881883
training loss 1.0026087231869787e-05 mae 0.0024160192989795214
Epoch 223, training: loss: 0.0000100, mae: 0.0024116 test: loss0.0000889, mae:0.0067677
training loss 1.1213011930522043e-05 mae 0.0026909534353762865
training loss 9.803363390543096e-06 mae 0.002418701276730965
training loss 9.489597016341796e-06 mae 0.0023912013079602245
training loss 1.037794517353949e-05 mae 0.0024370865646552386
training loss 1.0369598317423101e-05 mae 0.002443933330675524
Epoch 224, training: loss: 0.0000103, mae: 0.0024434 test: loss0.0000875, mae:0.0067201
training loss 9.908188076224178e-06 mae 0.002464631572365761
training loss 9.512909855402537e-06 mae 0.0023603428602583854
training loss 9.554669786396485e-06 mae 0.002382337229815081
training loss 1.0393725665531346e-05 mae 0.0024247519827651355
training loss 1.0329460994977865e-05 mae 0.002433834130069545
Epoch 225, training: loss: 0.0000103, mae: 0.0024329 test: loss0.0000878, mae:0.0067465
training loss 1.0116465091414284e-05 mae 0.0026824285741895437
training loss 9.053610153618957e-06 mae 0.002338334313119013
training loss 1.0095004942587984e-05 mae 0.0024047426044904057
training loss 9.855324642317187e-06 mae 0.0023866600784109232
training loss 9.774761015726092e-06 mae 0.0023858420636541948
Epoch 226, training: loss: 0.0000098, mae: 0.0023922 test: loss0.0000876, mae:0.0067055
training loss 8.543659532733727e-06 mae 0.0023266917560249567
training loss 1.0123826327531683e-05 mae 0.0023051442112773653
training loss 9.950355510371458e-06 mae 0.0023600288579185124
training loss 9.673846436620903e-06 mae 0.0023391029006487006
training loss 9.63321080329913e-06 mae 0.0023433832655572794
Epoch 227, training: loss: 0.0000096, mae: 0.0023485 test: loss0.0000906, mae:0.0068570
training loss 1.1695171451719943e-05 mae 0.0025477635208517313
training loss 9.344035271508395e-06 mae 0.0023784023942425847
training loss 9.342088997253106e-06 mae 0.0023651358885875
training loss 9.765985827025304e-06 mae 0.002379632044571223
training loss 9.81129423012339e-06 mae 0.0023942950044976373
Epoch 228, training: loss: 0.0000098, mae: 0.0023914 test: loss0.0000873, mae:0.0066968
training loss 7.334911060752347e-06 mae 0.0019436875591054559
training loss 8.91893445191718e-06 mae 0.0022907205126887443
training loss 8.90848106773954e-06 mae 0.002303450280745136
training loss 8.969599915847998e-06 mae 0.002316943310441709
training loss 9.609019248748504e-06 mae 0.00235634165600554
Epoch 229, training: loss: 0.0000096, mae: 0.0023565 test: loss0.0000883, mae:0.0067080
training loss 8.691228686075192e-06 mae 0.0023141857236623764
training loss 9.703940630809964e-06 mae 0.0024094705552081858
training loss 9.742896459597023e-06 mae 0.0023931086404562585
training loss 9.609381247372352e-06 mae 0.002382493274138365
training loss 9.743835732085396e-06 mae 0.002363578766232256
Epoch 230, training: loss: 0.0000098, mae: 0.0023662 test: loss0.0000903, mae:0.0068224
training loss 7.892957910371479e-06 mae 0.0023066475987434387
training loss 8.797601670570051e-06 mae 0.0022983885913466407
training loss 9.921988591243166e-06 mae 0.002359332544452483
training loss 9.725578552477618e-06 mae 0.002363094904256478
training loss 9.512707739435682e-06 mae 0.0023464334089614777
Epoch 231, training: loss: 0.0000095, mae: 0.0023467 test: loss0.0000888, mae:0.0067196
training loss 1.0233731700282078e-05 mae 0.00256993155926466
training loss 9.430688770211704e-06 mae 0.002373945157445383
training loss 9.15611733962398e-06 mae 0.0023380290650034156
training loss 9.846405766848263e-06 mae 0.002394792740233244
training loss 9.7416700351568e-06 mae 0.002391461063459035
Epoch 232, training: loss: 0.0000097, mae: 0.0023917 test: loss0.0000888, mae:0.0067690
training loss 7.71014401834691e-06 mae 0.0022429435048252344
training loss 9.00569829372769e-06 mae 0.0022890514000227635
training loss 9.754166472198944e-06 mae 0.0023499141956707195
training loss 9.507298333324042e-06 mae 0.002344356776548192
training loss 9.477156602847195e-06 mae 0.0023453496923596037
Epoch 233, training: loss: 0.0000095, mae: 0.0023461 test: loss0.0000888, mae:0.0067818
training loss 8.36171329865465e-06 mae 0.002258368069306016
training loss 8.320560321889759e-06 mae 0.0022347763719438927
training loss 9.346965955478707e-06 mae 0.0023083381866575185
training loss 9.474624956007593e-06 mae 0.002343503890916804
training loss 9.392428395431892e-06 mae 0.002342459241238732
Epoch 234, training: loss: 0.0000094, mae: 0.0023439 test: loss0.0000897, mae:0.0068019
training loss 7.580096735182451e-06 mae 0.0021316849160939455
training loss 9.499204434534615e-06 mae 0.0022568563478724923
training loss 9.213743295380357e-06 mae 0.0022794882914438696
training loss 9.428575677756821e-06 mae 0.0023291542268796092
training loss 9.636822806077824e-06 mae 0.0023711451070976275
Epoch 235, training: loss: 0.0000096, mae: 0.0023717 test: loss0.0000870, mae:0.0066919
training loss 5.723870799556607e-06 mae 0.001942667760886252
training loss 8.875327560364145e-06 mae 0.002274400910691303
training loss 9.167210396974562e-06 mae 0.0023252739372783067
training loss 9.073119711062245e-06 mae 0.0023228227120780987
training loss 9.40663623433507e-06 mae 0.002344274650147156
Epoch 236, training: loss: 0.0000095, mae: 0.0023537 test: loss0.0000897, mae:0.0068220
training loss 7.621187251061201e-06 mae 0.002040438586845994
training loss 8.675481748308386e-06 mae 0.002282405237886397
training loss 8.414504905111714e-06 mae 0.002247246528634488
training loss 9.395104539571973e-06 mae 0.002310416450730174
training loss 9.46584919232132e-06 mae 0.0023382570471762857
Epoch 237, training: loss: 0.0000095, mae: 0.0023394 test: loss0.0000884, mae:0.0067300
training loss 7.994128282007296e-06 mae 0.002283398760482669
training loss 8.625670564773218e-06 mae 0.002288424549167793
training loss 8.581909860998231e-06 mae 0.0022625486991174576
training loss 8.653574869092475e-06 mae 0.0022758514025180733
training loss 9.034193073684541e-06 mae 0.002298005122172447
Epoch 238, training: loss: 0.0000090, mae: 0.0022951 test: loss0.0000894, mae:0.0068099
training loss 9.75772400124697e-06 mae 0.0021955783013254404
training loss 8.541040607073514e-06 mae 0.0022423352119421546
training loss 8.548954102110708e-06 mae 0.0022546429127122797
training loss 8.606852350940782e-06 mae 0.002252339575106635
training loss 8.918546323522732e-06 mae 0.0022687025597569214
Epoch 239, training: loss: 0.0000089, mae: 0.0022711 test: loss0.0000911, mae:0.0068749
training loss 7.850848305679392e-06 mae 0.0021884820889681578
training loss 8.553579641111177e-06 mae 0.002246261250191167
training loss 8.927019901573946e-06 mae 0.0022600979617990484
training loss 8.93733717840098e-06 mae 0.0022662590205398034
training loss 8.95601258672463e-06 mae 0.0022782964912238554
Epoch 240, training: loss: 0.0000089, mae: 0.0022758 test: loss0.0000930, mae:0.0068496
training loss 9.470688382862136e-06 mae 0.0025155283510684967
training loss 8.746394042628153e-06 mae 0.0022755864204144945
training loss 9.278536825798137e-06 mae 0.002299451994211734
training loss 9.491270440703851e-06 mae 0.0023457131977655627
training loss 9.24547682519172e-06 mae 0.0023155889499339444
Epoch 241, training: loss: 0.0000092, mae: 0.0023148 test: loss0.0000898, mae:0.0068129
training loss 1.0195765753451269e-05 mae 0.0026930058375000954
training loss 8.64717763119295e-06 mae 0.0022682538119089954
training loss 9.166810593009055e-06 mae 0.002292755961252174
training loss 9.118168971278477e-06 mae 0.0022936934074818684
training loss 8.9122722987559e-06 mae 0.0022787570377663523
Epoch 242, training: loss: 0.0000089, mae: 0.0022782 test: loss0.0000902, mae:0.0068190
training loss 6.096755896578543e-06 mae 0.0020051125902682543
training loss 7.523531380951786e-06 mae 0.002129940800003562
training loss 7.809153543132538e-06 mae 0.0021563958708602617
training loss 8.4016692349508e-06 mae 0.0022031822170914205
training loss 8.69895267842639e-06 mae 0.002252474192075839
Epoch 243, training: loss: 0.0000087, mae: 0.0022586 test: loss0.0000899, mae:0.0068055
training loss 1.0182345249631908e-05 mae 0.0024624920915812254
training loss 8.474024347307538e-06 mae 0.0022346576146197086
training loss 8.818766367190925e-06 mae 0.002252936321725645
training loss 8.949811723406487e-06 mae 0.002275532480045562
training loss 8.885341402724158e-06 mae 0.0022820096812790178
Epoch 244, training: loss: 0.0000089, mae: 0.0022826 test: loss0.0000916, mae:0.0068799
training loss 1.3122757081873715e-05 mae 0.0025156522169709206
training loss 8.10439188867081e-06 mae 0.0021847621492091934
training loss 7.91999201229004e-06 mae 0.0021641970506067025
training loss 8.001990714631316e-06 mae 0.0021799304669056804
training loss 8.626085568110611e-06 mae 0.0022184549557480635
Epoch 245, training: loss: 0.0000086, mae: 0.0022247 test: loss0.0000893, mae:0.0068475
training loss 1.0718561497924384e-05 mae 0.002665841719135642
training loss 1.0435905248111037e-05 mae 0.002421098231684927
training loss 1.0095156278904401e-05 mae 0.002433244642654046
training loss 9.397540494644653e-06 mae 0.0023580186920154175
training loss 9.17278594078655e-06 mae 0.0023317792092621397
Epoch 246, training: loss: 0.0000092, mae: 0.0023300 test: loss0.0000900, mae:0.0068166
training loss 7.737053238088265e-06 mae 0.002334532095119357
training loss 8.455105359519386e-06 mae 0.002245222868890885
training loss 8.481768683922569e-06 mae 0.002261605714263376
training loss 8.824713247863263e-06 mae 0.002279441782106824
training loss 8.862025834104091e-06 mae 0.0022882967191141333
Epoch 247, training: loss: 0.0000088, mae: 0.0022854 test: loss0.0000926, mae:0.0069094
training loss 7.17028387953178e-06 mae 0.0022884204518049955
training loss 7.722103008763598e-06 mae 0.002127314815023804
training loss 7.835431420467598e-06 mae 0.002145223875096675
training loss 7.78865009822959e-06 mae 0.0021385601621478896
training loss 8.338263446083017e-06 mae 0.002196367855522022
Epoch 248, training: loss: 0.0000084, mae: 0.0022129 test: loss0.0000917, mae:0.0068714
training loss 8.184776561392937e-06 mae 0.002218179404735565
training loss 8.079559009667149e-06 mae 0.0021725632935105004
training loss 7.996342050643244e-06 mae 0.00218029757661948
training loss 8.392208363882621e-06 mae 0.00220623433926075
training loss 8.398909964550212e-06 mae 0.002215485205521707
Epoch 249, training: loss: 0.0000084, mae: 0.0022137 test: loss0.0000902, mae:0.0068018
training loss 1.2989473361812998e-05 mae 0.002760442905128002
training loss 8.479966825189859e-06 mae 0.002176604388902586
training loss 8.847852683120977e-06 mae 0.0022518148986936204
training loss 8.544420579529582e-06 mae 0.0022263263817876577
training loss 8.488292448885149e-06 mae 0.00223278999096941
Epoch 250, training: loss: 0.0000085, mae: 0.0022318 test: loss0.0000920, mae:0.0069144
training loss 8.815307410259265e-06 mae 0.002404151251539588
training loss 8.197109710796642e-06 mae 0.002206984681406003
training loss 7.951702614398993e-06 mae 0.0021726662583538498
training loss 8.101964707724174e-06 mae 0.0021971877608818326
training loss 8.4017697303696e-06 mae 0.002216830432058233
Epoch 251, training: loss: 0.0000085, mae: 0.0022234 test: loss0.0000922, mae:0.0068766
training loss 9.97762253973633e-06 mae 0.0024891176726669073
training loss 8.050586164280702e-06 mae 0.002199194542424498
training loss 8.502300261170605e-06 mae 0.0022185076579064288
training loss 8.719854932431624e-06 mae 0.0022564670818839344
training loss 8.709744507265455e-06 mae 0.002262373029170974
Epoch 252, training: loss: 0.0000087, mae: 0.0022630 test: loss0.0000913, mae:0.0068516
training loss 9.921515811583959e-06 mae 0.002467786194756627
training loss 7.764186086526881e-06 mae 0.002158745995485316
training loss 7.5921523711087404e-06 mae 0.0021238607621871597
training loss 7.83728761318994e-06 mae 0.0021602956189570436
training loss 8.132678057838565e-06 mae 0.002183282077752646
Epoch 253, training: loss: 0.0000081, mae: 0.0021810 test: loss0.0000914, mae:0.0068905
training loss 7.005217867117608e-06 mae 0.0022158240899443626
training loss 7.812725337263306e-06 mae 0.002171516247258029
training loss 7.692656591904895e-06 mae 0.0021360912279567074
training loss 7.784887837874971e-06 mae 0.002152585705113135
training loss 8.098080795673408e-06 mae 0.0021817323071789455
Epoch 254, training: loss: 0.0000081, mae: 0.0021809 test: loss0.0000925, mae:0.0068932
training loss 8.286234333354514e-06 mae 0.0023848318960517645
training loss 7.253927869183541e-06 mae 0.002094483448137694
training loss 9.698713598697962e-06 mae 0.0023412119832332476
training loss 9.330159594879803e-06 mae 0.002312279135198487
training loss 9.041897669138148e-06 mae 0.0022854683544273635
Epoch 255, training: loss: 0.0000090, mae: 0.0022768 test: loss0.0000913, mae:0.0068116
training loss 7.38501103114686e-06 mae 0.002025315770879388
training loss 8.455208462335436e-06 mae 0.0021816490117606583
training loss 8.527320163254747e-06 mae 0.002220419271258951
training loss 8.641703510309065e-06 mae 0.002243640798680633
training loss 8.484143579528738e-06 mae 0.002232662087598288
Epoch 256, training: loss: 0.0000085, mae: 0.0022358 test: loss0.0000910, mae:0.0068611
training loss 7.01825410942547e-06 mae 0.002071957103908062
training loss 8.063049300093835e-06 mae 0.0021388384846824354
training loss 7.87456946733919e-06 mae 0.002139410598775243
training loss 7.97428427309292e-06 mae 0.002147579057580045
training loss 7.885737244152862e-06 mae 0.002142855831519214
Epoch 257, training: loss: 0.0000079, mae: 0.0021403 test: loss0.0000914, mae:0.0068744
training loss 7.002201073191827e-06 mae 0.0020931444596499205
training loss 7.626613510450733e-06 mae 0.00212657728972023
training loss 7.5360615230940605e-06 mae 0.0021048663437624685
training loss 7.995335193522843e-06 mae 0.002148049154214019
training loss 7.786455630571366e-06 mae 0.002124883799781256
Epoch 258, training: loss: 0.0000078, mae: 0.0021239 test: loss0.0000925, mae:0.0068947
training loss 8.438767508778255e-06 mae 0.0022141847293823957
training loss 7.90342464735623e-06 mae 0.002129333518335925
training loss 7.613256644113374e-06 mae 0.0021129179167293814
training loss 7.482377302660206e-06 mae 0.0020975006809026385
training loss 7.498631603582237e-06 mae 0.0020984988659620268
Epoch 259, training: loss: 0.0000075, mae: 0.0020994 test: loss0.0000919, mae:0.0068848
training loss 7.118613666534657e-06 mae 0.0020937202498316765
training loss 8.545003043657156e-06 mae 0.002135912794163268
training loss 8.514961656587862e-06 mae 0.0021917378209475977
training loss 8.302219055092077e-06 mae 0.0021834481108181218
training loss 8.131180657390595e-06 mae 0.00216992155171523
Epoch 260, training: loss: 0.0000081, mae: 0.0021725 test: loss0.0000928, mae:0.0069065
training loss 7.373867902060738e-06 mae 0.002113338327035308
training loss 7.4663731877580196e-06 mae 0.002087179550846271
training loss 7.356359417664665e-06 mae 0.002078850089692243
training loss 8.04980123539522e-06 mae 0.0021499444018210612
training loss 8.104517215017853e-06 mae 0.0021693469685460658
Epoch 261, training: loss: 0.0000081, mae: 0.0021686 test: loss0.0000907, mae:0.0068189
training loss 5.052013420936419e-06 mae 0.0018243748927488923
training loss 7.055742691912148e-06 mae 0.002054508419378716
training loss 7.473404454262576e-06 mae 0.00209715504240761
training loss 8.02540538167542e-06 mae 0.0021655632310935507
training loss 8.128727467376837e-06 mae 0.002185184989282074
Epoch 262, training: loss: 0.0000082, mae: 0.0021905 test: loss0.0000918, mae:0.0069347
training loss 5.2842347031401005e-06 mae 0.001908312551677227
training loss 7.750436630957151e-06 mae 0.002170837993331838
training loss 7.6277711980197875e-06 mae 0.002127161681753501
training loss 7.811904126524887e-06 mae 0.00215466914772494
training loss 7.68111715896977e-06 mae 0.002133195595445103
Epoch 263, training: loss: 0.0000077, mae: 0.0021339 test: loss0.0000911, mae:0.0068191
training loss 6.555748313985532e-06 mae 0.002032278338447213
training loss 7.043411169790631e-06 mae 0.0020203443116271025
training loss 7.2078217064952005e-06 mae 0.0020643792341066775
training loss 7.624416532462721e-06 mae 0.0021118727082460624
training loss 7.652594770512057e-06 mae 0.002120474273505718
Epoch 264, training: loss: 0.0000077, mae: 0.0021239 test: loss0.0000917, mae:0.0068677
training loss 6.116379154263996e-06 mae 0.0017702626064419746
training loss 7.257186267701863e-06 mae 0.002087689404302807
training loss 7.138609488369055e-06 mae 0.0020681412759154002
training loss 7.32702684994642e-06 mae 0.0020938498216267944
training loss 7.597380799644042e-06 mae 0.0021243458295203926
Epoch 265, training: loss: 0.0000076, mae: 0.0021249 test: loss0.0000945, mae:0.0069764
training loss 6.7327177930565085e-06 mae 0.00209258496761322
training loss 7.515824206932974e-06 mae 0.002124598007831795
training loss 7.350116736080677e-06 mae 0.0020922362149720735
training loss 7.4527891241127765e-06 mae 0.0021091573802285535
training loss 7.827055202567559e-06 mae 0.0021534096947708995
Epoch 266, training: loss: 0.0000078, mae: 0.0021467 test: loss0.0000909, mae:0.0068612
training loss 4.609815732692368e-06 mae 0.001633048988878727
training loss 7.695860154390355e-06 mae 0.0021627496234049985
training loss 7.673216565884708e-06 mae 0.0021421790992066556
training loss 7.813867157904722e-06 mae 0.0021589894829910023
training loss 7.778067281752056e-06 mae 0.0021533164536274635
Epoch 267, training: loss: 0.0000077, mae: 0.0021513 test: loss0.0000905, mae:0.0068396
training loss 5.906820206291741e-06 mae 0.0018856009701266885
training loss 6.789128649103861e-06 mae 0.0020219185184139543
training loss 6.9702065331404055e-06 mae 0.0020322150707539934
training loss 7.286354365840222e-06 mae 0.002074410892955169
training loss 7.323907932764833e-06 mae 0.0020831917975542703
Epoch 268, training: loss: 0.0000073, mae: 0.0020845 test: loss0.0000926, mae:0.0069319
training loss 6.746524832124123e-06 mae 0.0021666272077709436
training loss 7.688304536071987e-06 mae 0.002100018017432269
training loss 7.883833542100631e-06 mae 0.002143288678996782
training loss 7.631816017225673e-06 mae 0.0021115298412234477
training loss 7.564867293912179e-06 mae 0.0021067601292676743
Epoch 269, training: loss: 0.0000076, mae: 0.0021050 test: loss0.0000918, mae:0.0068827
training loss 8.23351183498744e-06 mae 0.0022199181839823723
training loss 7.317440915857349e-06 mae 0.002073133435082056
training loss 7.0388458664070845e-06 mae 0.0020398143310062958
training loss 7.038311233728316e-06 mae 0.0020438963794056956
training loss 7.24164745031034e-06 mae 0.0020642016350583578
Epoch 270, training: loss: 0.0000073, mae: 0.0020733 test: loss0.0000922, mae:0.0069214
training loss 7.0464302552863955e-06 mae 0.0018931335071101785
training loss 7.5452239976656e-06 mae 0.002109419864912828
training loss 7.182903451700107e-06 mae 0.0020616309761097374
training loss 7.393474936408581e-06 mae 0.002096354400134639
training loss 7.3011642666303385e-06 mae 0.002086346027839799
Epoch 271, training: loss: 0.0000073, mae: 0.0020928 test: loss0.0000924, mae:0.0069131
training loss 7.163267127907602e-06 mae 0.002095302566885948
training loss 6.721507789359963e-06 mae 0.002003423401665892
training loss 6.763347039733425e-06 mae 0.00201958456044408
training loss 7.253417438341366e-06 mae 0.002062116492083235
training loss 7.394486323419053e-06 mae 0.0020803640450615044
Epoch 272, training: loss: 0.0000074, mae: 0.0020819 test: loss0.0000935, mae:0.0069301
training loss 5.013260761188576e-06 mae 0.0018758507212623954
training loss 6.699918605376939e-06 mae 0.0020241897790601445
training loss 6.823325522850835e-06 mae 0.0020179973550989204
training loss 6.886243940942415e-06 mae 0.0020342835854270215
training loss 7.132433232647453e-06 mae 0.0020610459255453084
Epoch 273, training: loss: 0.0000071, mae: 0.0020614 test: loss0.0000922, mae:0.0068776
training loss 8.137892109516542e-06 mae 0.0022124277893453836
training loss 6.7576758836224265e-06 mae 0.002022590810049545
training loss 6.8625346521363146e-06 mae 0.0020296295844295102
training loss 6.708098927923109e-06 mae 0.0020071218377719366
training loss 7.001201102997836e-06 mae 0.002037710765612994
Epoch 274, training: loss: 0.0000070, mae: 0.0020370 test: loss0.0000921, mae:0.0068681
training loss 7.341751825151732e-06 mae 0.002082810504361987
training loss 6.505384505419665e-06 mae 0.0019824189686344245
training loss 6.645294958373298e-06 mae 0.0019795836576509598
training loss 6.707246796136342e-06 mae 0.0019922686928870855
training loss 6.94397739091833e-06 mae 0.0020265531626562404
Epoch 275, training: loss: 0.0000070, mae: 0.0020287 test: loss0.0000912, mae:0.0068439
training loss 8.540145245206077e-06 mae 0.0023581876885145903
training loss 7.608911604905849e-06 mae 0.002129937802879687
training loss 7.156605727569776e-06 mae 0.0020662990829787477
training loss 7.13413321335377e-06 mae 0.0020669566186774057
training loss 7.103969495285887e-06 mae 0.002059430818754569
Epoch 276, training: loss: 0.0000072, mae: 0.0020651 test: loss0.0000937, mae:0.0069435
training loss 5.624051027552923e-06 mae 0.0018644764786586165
training loss 7.0838663055374315e-06 mae 0.0020714303793604766
training loss 6.776614640384112e-06 mae 0.00203120263551574
training loss 7.319948827892746e-06 mae 0.002051274140291409
training loss 9.079382599605674e-06 mae 0.002264706478732179
Epoch 277, training: loss: 0.0000092, mae: 0.0022762 test: loss0.0000945, mae:0.0069635
training loss 1.0590782039798796e-05 mae 0.002640858991071582
training loss 8.619752959787673e-06 mae 0.00227053935739997
training loss 8.191997980842161e-06 mae 0.0022165986160646287
training loss 8.423434211115045e-06 mae 0.0022198464844795257
training loss 8.141272773720626e-06 mae 0.0021921645691247646
Epoch 278, training: loss: 0.0000081, mae: 0.0021876 test: loss0.0000917, mae:0.0068665
training loss 5.436690571514191e-06 mae 0.0018143797060474753
training loss 7.349197430277642e-06 mae 0.002071161364095614
training loss 6.768510160111306e-06 mae 0.001992431447361734
training loss 6.833353708563229e-06 mae 0.0020031144376844163
training loss 6.788203423430919e-06 mae 0.0019984272788209252
Epoch 279, training: loss: 0.0000069, mae: 0.0020070 test: loss0.0000912, mae:0.0068690
training loss 9.335125469078775e-06 mae 0.0020921023096889257
training loss 5.9709622265098224e-06 mae 0.0018955494560228261
training loss 6.342405050859407e-06 mae 0.0019384882306809179
training loss 6.585995761752021e-06 mae 0.0019698855397754056
training loss 6.6912969100146145e-06 mae 0.001989175882931816
Epoch 280, training: loss: 0.0000067, mae: 0.0019901 test: loss0.0000931, mae:0.0069293
training loss 7.724335773673374e-06 mae 0.0021669568959623575
training loss 6.750033613642184e-06 mae 0.0019930426382879715
training loss 7.0718899678468006e-06 mae 0.0020419141383712543
training loss 7.027967222223719e-06 mae 0.0020312941401876166
training loss 6.992535983854892e-06 mae 0.0020238218472260435
Epoch 281, training: loss: 0.0000070, mae: 0.0020206 test: loss0.0000925, mae:0.0068741
training loss 6.481377113232156e-06 mae 0.0019704762380570173
training loss 6.2862181324567695e-06 mae 0.0019277349141809874
training loss 6.675275573239685e-06 mae 0.0019872789847489337
training loss 6.555075801379224e-06 mae 0.0019758385405659867
training loss 6.662647731886932e-06 mae 0.0019852737009997904
Epoch 282, training: loss: 0.0000067, mae: 0.0019893 test: loss0.0000926, mae:0.0068996
training loss 5.273827355267713e-06 mae 0.0017903918633237481
training loss 6.5562040198725e-06 mae 0.0019727436022139062
training loss 6.72222531963175e-06 mae 0.0020052143879751173
training loss 6.8213385457177035e-06 mae 0.002014867274479213
training loss 6.880820050563542e-06 mae 0.002025649423562732
Epoch 283, training: loss: 0.0000069, mae: 0.0020249 test: loss0.0000930, mae:0.0069280
training loss 6.242713425308466e-06 mae 0.001980078173801303
training loss 6.853530780348342e-06 mae 0.0020029802039703897
training loss 6.530325992013004e-06 mae 0.00195789972947228
training loss 6.502740436057416e-06 mae 0.0019530534762765787
training loss 6.593015893318735e-06 mae 0.001972753793784227
Epoch 284, training: loss: 0.0000066, mae: 0.0019737 test: loss0.0000946, mae:0.0069396
training loss 7.358418315561721e-06 mae 0.002053931588307023
training loss 6.080174011467357e-06 mae 0.001896036848170208
training loss 6.328627876536133e-06 mae 0.0019332383480384064
training loss 6.277387650312592e-06 mae 0.0019202121226610436
training loss 6.380472038033451e-06 mae 0.0019420821142072484
Epoch 285, training: loss: 0.0000064, mae: 0.0019451 test: loss0.0000929, mae:0.0069194
training loss 5.892356057302095e-06 mae 0.0018714290345087647
training loss 6.395393582209303e-06 mae 0.0019402922149382384
training loss 6.58086302638988e-06 mae 0.0019923900484596145
training loss 6.547968735726529e-06 mae 0.001983391054727088
training loss 6.675664673450269e-06 mae 0.0019986472232730603
Epoch 286, training: loss: 0.0000067, mae: 0.0019985 test: loss0.0000929, mae:0.0069176
training loss 4.441555120138219e-06 mae 0.0018107270589098334
training loss 6.312562783362e-06 mae 0.001953353023375658
training loss 6.37437063340996e-06 mae 0.0019545813038078425
training loss 6.583766876337301e-06 mae 0.0019866670697730104
training loss 6.816042107165055e-06 mae 0.00200672455847875
Epoch 287, training: loss: 0.0000068, mae: 0.0020058 test: loss0.0000949, mae:0.0069828
training loss 6.671451501460979e-06 mae 0.0020249627996236086
training loss 6.698793034350577e-06 mae 0.001999171804545411
training loss 6.388495988869523e-06 mae 0.0019346212398832535
training loss 6.4068722739774855e-06 mae 0.0019449323858288638
training loss 6.587552980587377e-06 mae 0.0019796784151697635
Epoch 288, training: loss: 0.0000066, mae: 0.0019869 test: loss0.0000935, mae:0.0069768
training loss 7.989591722434852e-06 mae 0.002167058177292347
training loss 6.583504081587021e-06 mae 0.0019773372754856356
training loss 6.857648364836343e-06 mae 0.0020124445390417287
training loss 6.796982681154853e-06 mae 0.0020052458688505745
training loss 6.801121922373697e-06 mae 0.0020064307105685806
Epoch 289, training: loss: 0.0000068, mae: 0.0020108 test: loss0.0000934, mae:0.0069371
training loss 6.089141152187949e-06 mae 0.00190820824354887
training loss 6.500841138011883e-06 mae 0.001959180867975103
training loss 6.328550488442396e-06 mae 0.0019346523308425697
training loss 6.4618282534179375e-06 mae 0.0019452605729564925
training loss 6.594030703887094e-06 mae 0.0019744450141865734
Epoch 290, training: loss: 0.0000066, mae: 0.0019795 test: loss0.0000946, mae:0.0069817
training loss 6.256500000745291e-06 mae 0.0018472628435119987
training loss 6.812649287751936e-06 mae 0.002011152000312566
training loss 6.443635523435946e-06 mae 0.0019648835895177304
training loss 6.402814081514026e-06 mae 0.0019514540083928418
training loss 6.364257293868076e-06 mae 0.001952207074454286
Epoch 291, training: loss: 0.0000064, mae: 0.0019502 test: loss0.0000941, mae:0.0069575
training loss 6.167867013573414e-06 mae 0.0016730846837162971
training loss 6.813150802747046e-06 mae 0.0020008797542758134
training loss 6.354127030549942e-06 mae 0.0019453143076233494
training loss 6.615363679956748e-06 mae 0.001988065901631799
training loss 6.642608288747224e-06 mae 0.0019936383654368096
Epoch 292, training: loss: 0.0000067, mae: 0.0019988 test: loss0.0000943, mae:0.0069924
training loss 1.7096828742069192e-05 mae 0.0022087146062403917
training loss 7.0605428310741175e-06 mae 0.002026755842106308
training loss 6.736283579596036e-06 mae 0.001997182454618783
training loss 6.591864030872105e-06 mae 0.0019782591481625265
training loss 6.586569380166808e-06 mae 0.001981868365760987
Epoch 293, training: loss: 0.0000066, mae: 0.0019771 test: loss0.0000942, mae:0.0069548
training loss 6.4888740780588705e-06 mae 0.001852632500231266
training loss 5.987587526618097e-06 mae 0.0018565913242306199
training loss 6.2884236812540544e-06 mae 0.0019365665733371638
training loss 6.269023348272791e-06 mae 0.0019335874505896152
training loss 6.4010656700050794e-06 mae 0.001953202749217921
Epoch 294, training: loss: 0.0000064, mae: 0.0019514 test: loss0.0000941, mae:0.0069987
training loss 3.949190158891724e-06 mae 0.001543495454825461
training loss 6.3569613811363095e-06 mae 0.0019483027030147758
training loss 6.387544837509429e-06 mae 0.0019469971406633156
training loss 6.356394610309697e-06 mae 0.0019435054863130798
training loss 6.389231004812527e-06 mae 0.0019497049408292954
Epoch 295, training: loss: 0.0000064, mae: 0.0019510 test: loss0.0000952, mae:0.0070000
training loss 4.7943840399966575e-06 mae 0.0017312495037913322
training loss 6.2171455363782385e-06 mae 0.0019243387916289712
training loss 6.2116564392211055e-06 mae 0.001913144809913148
training loss 6.192917939886878e-06 mae 0.001912429462347352
training loss 6.115741731961107e-06 mae 0.0019050024569136513
Epoch 296, training: loss: 0.0000061, mae: 0.0019055 test: loss0.0000935, mae:0.0069308
training loss 7.915634341770783e-06 mae 0.002130127279087901
training loss 6.641850752524024e-06 mae 0.0019409763033264409
training loss 6.708866330522104e-06 mae 0.001975459855805973
training loss 6.810923483703993e-06 mae 0.0019960793102197115
training loss 6.8925059149568784e-06 mae 0.0020072762524264306
Epoch 297, training: loss: 0.0000069, mae: 0.0020072 test: loss0.0000937, mae:0.0069603
training loss 6.643501819780795e-06 mae 0.0020826098043471575
training loss 5.75552233867176e-06 mae 0.0018538891534120138
training loss 5.827297904448137e-06 mae 0.001854431924944983
training loss 5.926034430781358e-06 mae 0.00187300941259261
training loss 6.083325249178346e-06 mae 0.0018983886227363826
Epoch 298, training: loss: 0.0000061, mae: 0.0019037 test: loss0.0000942, mae:0.0069628
training loss 4.502108822634909e-06 mae 0.0015487956115975976
training loss 6.464372174877117e-06 mae 0.0019670172284046807
training loss 6.169129495775803e-06 mae 0.0019154102728231857
training loss 6.189782264161918e-06 mae 0.0019233019225336366
training loss 6.323143045154947e-06 mae 0.001942492440569942
Epoch 299, training: loss: 0.0000063, mae: 0.0019483 test: loss0.0000953, mae:0.0070112
current learning rate: 6.25e-05
training loss 7.45447505323682e-06 mae 0.002207930199801922
training loss 5.455867748798268e-06 mae 0.0017845260604814278
training loss 5.1046730831249885e-06 mae 0.0017179747822113553
training loss 4.940677678215607e-06 mae 0.0016810095179764343
training loss 4.864246937730021e-06 mae 0.001664203384877252
Epoch 300, training: loss: 0.0000049, mae: 0.0016639 test: loss0.0000940, mae:0.0069550
training loss 5.60030321139493e-06 mae 0.001769620575942099
training loss 4.563090790136759e-06 mae 0.0015870242279169026
training loss 4.486903649329646e-06 mae 0.0015646713131135055
training loss 4.449962684913577e-06 mae 0.001562781031366857
training loss 4.478199646958578e-06 mae 0.0015709079855320911
Epoch 301, training: loss: 0.0000045, mae: 0.0015669 test: loss0.0000938, mae:0.0069393
training loss 3.7416841678350465e-06 mae 0.001276995986700058
training loss 4.167029008421392e-06 mae 0.0015157947371549465
training loss 4.131430101227638e-06 mae 0.00150724862457834
training loss 4.255978622899157e-06 mae 0.0015320460672878866
training loss 4.296553104958285e-06 mae 0.001542219008314447
Epoch 302, training: loss: 0.0000043, mae: 0.0015437 test: loss0.0000933, mae:0.0069272
training loss 3.862045559799299e-06 mae 0.0013271821662783623
training loss 4.0746541683392036e-06 mae 0.0014996702723460748
training loss 4.347948223506019e-06 mae 0.0015431598951523697
training loss 4.389791860111117e-06 mae 0.0015542021935608307
training loss 4.379453817384626e-06 mae 0.0015587080078922322
Epoch 303, training: loss: 0.0000044, mae: 0.0015577 test: loss0.0000944, mae:0.0069768
training loss 3.664411678983015e-06 mae 0.0014907745644450188
training loss 4.223578134413257e-06 mae 0.0015229219068572221
training loss 4.306127079466438e-06 mae 0.0015342663466414014
training loss 4.361078452841909e-06 mae 0.0015517583260288004
training loss 4.308222528311829e-06 mae 0.0015457324036142201
Epoch 304, training: loss: 0.0000043, mae: 0.0015444 test: loss0.0000949, mae:0.0070110
training loss 2.9042614642094122e-06 mae 0.0012900764122605324
training loss 4.306139357247635e-06 mae 0.001542667312748438
training loss 4.183591466127188e-06 mae 0.0015259313957286206
training loss 4.188971164068116e-06 mae 0.001523955725133418
training loss 4.235849204049565e-06 mae 0.0015339093706779065
Epoch 305, training: loss: 0.0000043, mae: 0.0015393 test: loss0.0000945, mae:0.0069730
training loss 5.275287549011409e-06 mae 0.0017080666730180383
training loss 4.153498223911133e-06 mae 0.0014880111431885585
training loss 4.056916249382002e-06 mae 0.0014890896451502736
training loss 4.2765203934383135e-06 mae 0.0015402685762968972
training loss 4.256001430753958e-06 mae 0.001540474397876295
Epoch 306, training: loss: 0.0000043, mae: 0.0015439 test: loss0.0000943, mae:0.0069672
training loss 4.322846962168114e-06 mae 0.0015053398674353957
training loss 4.0062003630309315e-06 mae 0.0015010947210933358
training loss 4.199672942080058e-06 mae 0.0015321010539289747
training loss 4.257392849515762e-06 mae 0.001535408949750839
training loss 4.216443299195252e-06 mae 0.0015298201765086663
Epoch 307, training: loss: 0.0000042, mae: 0.0015297 test: loss0.0000947, mae:0.0069608
training loss 2.9396026093309047e-06 mae 0.0012759059900417924
training loss 3.927613340024169e-06 mae 0.0014646606629385671
training loss 4.01292412235388e-06 mae 0.0014909446253302963
training loss 4.135323302274243e-06 mae 0.001515501956702117
training loss 4.216413076030214e-06 mae 0.001531811228337053
Epoch 308, training: loss: 0.0000043, mae: 0.0015366 test: loss0.0000958, mae:0.0070147
training loss 5.359974238672294e-06 mae 0.0018132654950022697
training loss 4.196390007421667e-06 mae 0.0015220432405305259
training loss 4.246963889686817e-06 mae 0.0015325545296441805
training loss 4.26059074055089e-06 mae 0.0015357911088719769
training loss 4.29232833464912e-06 mae 0.001545814791490412
Epoch 309, training: loss: 0.0000043, mae: 0.0015416 test: loss0.0000956, mae:0.0070007
training loss 3.6437097605812596e-06 mae 0.0015236452454701066
training loss 4.235137856655865e-06 mae 0.001554375270144174
training loss 4.141414892008396e-06 mae 0.0015360664802206924
training loss 4.119860299975479e-06 mae 0.0015279201803025826
training loss 4.2414952538331505e-06 mae 0.0015464160523020597
Epoch 310, training: loss: 0.0000043, mae: 0.0015506 test: loss0.0000960, mae:0.0070211
training loss 4.869998065260006e-06 mae 0.0016296800458803773
training loss 3.9047230882037995e-06 mae 0.0014843973217020725
training loss 4.08760701414547e-06 mae 0.0015119807183797847
training loss 4.291797919049349e-06 mae 0.0015458643030905666
training loss 4.333052371108366e-06 mae 0.0015548028524464638
Epoch 311, training: loss: 0.0000043, mae: 0.0015522 test: loss0.0000954, mae:0.0070097
training loss 3.364758640600485e-06 mae 0.0013376217102631927
training loss 3.980419679090332e-06 mae 0.0014878808881393546
training loss 4.227853408040816e-06 mae 0.0015394396925492602
training loss 4.248061585959952e-06 mae 0.0015440735717717295
training loss 4.287835621997245e-06 mae 0.0015496152102141018
Epoch 312, training: loss: 0.0000043, mae: 0.0015526 test: loss0.0000956, mae:0.0070164
training loss 4.170869488007156e-06 mae 0.001562014571391046
training loss 4.026231658381042e-06 mae 0.001498404248873247
training loss 4.114556252779317e-06 mae 0.0015235008570448601
training loss 4.150376333272535e-06 mae 0.0015348171297189413
training loss 4.230931048667445e-06 mae 0.0015461727599173191
Epoch 313, training: loss: 0.0000042, mae: 0.0015449 test: loss0.0000949, mae:0.0069797
training loss 3.0173214327078313e-06 mae 0.0011831661686301231
training loss 3.954625709393798e-06 mae 0.0014969070565284178
training loss 4.00137386269208e-06 mae 0.0015013609069980465
training loss 4.1564414266831124e-06 mae 0.0015222218407268653
training loss 4.234057789643027e-06 mae 0.0015410892091997296
Epoch 314, training: loss: 0.0000042, mae: 0.0015423 test: loss0.0000960, mae:0.0070293
training loss 3.1159197533270344e-06 mae 0.0013512898003682494
training loss 4.011869827545346e-06 mae 0.0014955150785253327
training loss 4.079127190587062e-06 mae 0.0015081786965527157
training loss 4.1193731902733835e-06 mae 0.0015164058484346758
training loss 4.110407228754821e-06 mae 0.001518975239622393
Epoch 315, training: loss: 0.0000041, mae: 0.0015195 test: loss0.0000955, mae:0.0069994
training loss 4.211821305943886e-06 mae 0.0015593146672472358
training loss 4.032385384241078e-06 mae 0.0015187862479840131
training loss 4.109341284449759e-06 mae 0.0015329489908596076
training loss 4.0811729009898465e-06 mae 0.001527381340035589
training loss 4.13734719201888e-06 mae 0.0015380372477472944
Epoch 316, training: loss: 0.0000042, mae: 0.0015437 test: loss0.0000967, mae:0.0070663
training loss 6.213700089574559e-06 mae 0.0017729249084368348
training loss 4.0971905620751265e-06 mae 0.001519209894753408
training loss 4.165824079656419e-06 mae 0.001523941920120454
training loss 4.121361767311522e-06 mae 0.0015207219827266425
training loss 4.162186953285007e-06 mae 0.0015307298226661943
Epoch 317, training: loss: 0.0000042, mae: 0.0015377 test: loss0.0000963, mae:0.0070475
training loss 3.6112439829594223e-06 mae 0.0015349624445661902
training loss 4.068247244058332e-06 mae 0.0015224794396107978
training loss 4.12309541636457e-06 mae 0.0015254835021857281
training loss 4.163942971357731e-06 mae 0.001532112617532889
training loss 4.09755487123053e-06 mae 0.00152070470216837
Epoch 318, training: loss: 0.0000041, mae: 0.0015252 test: loss0.0000964, mae:0.0070661
training loss 3.4655738545552595e-06 mae 0.00120713806245476
training loss 3.90498295625245e-06 mae 0.001448885964540144
training loss 4.060387932485816e-06 mae 0.0015055832907207092
training loss 4.1971897496976775e-06 mae 0.001531960679347695
training loss 4.1776227384281656e-06 mae 0.0015316833458164363
Epoch 319, training: loss: 0.0000042, mae: 0.0015302 test: loss0.0000969, mae:0.0070451
training loss 2.42363739744178e-06 mae 0.0012012518709525466
training loss 3.6835695495670616e-06 mae 0.001446479429289991
training loss 3.899936222305679e-06 mae 0.0014776026675625159
training loss 4.008249402732986e-06 mae 0.0015005279966130462
training loss 4.071466604506182e-06 mae 0.0015107946735644594
Epoch 320, training: loss: 0.0000041, mae: 0.0015118 test: loss0.0000959, mae:0.0070252
training loss 3.670943669931148e-06 mae 0.0013922437792643905
training loss 4.157170781350123e-06 mae 0.001526407089413089
training loss 4.064912414238766e-06 mae 0.001516923823596744
training loss 4.1220875245083205e-06 mae 0.0015300142436923576
training loss 4.150851189488052e-06 mae 0.0015340274384022879
Epoch 321, training: loss: 0.0000041, mae: 0.0015331 test: loss0.0000963, mae:0.0070269
training loss 3.1903018680168316e-06 mae 0.0013544019311666489
training loss 4.024812641564138e-06 mae 0.00149706482439868
training loss 3.889487287562932e-06 mae 0.0014758787629664829
training loss 3.912024271511542e-06 mae 0.001484975574186553
training loss 4.07607717364438e-06 mae 0.0015174987597335412
Epoch 322, training: loss: 0.0000041, mae: 0.0015176 test: loss0.0000961, mae:0.0070583
training loss 2.5922972781700082e-06 mae 0.0010986359557136893
training loss 3.758718990224331e-06 mae 0.001456323602492464
training loss 3.834979402694948e-06 mae 0.0014769065742482345
training loss 3.878464967301806e-06 mae 0.0014779679168829853
training loss 3.960606446340555e-06 mae 0.0014886963934132558
Epoch 323, training: loss: 0.0000040, mae: 0.0014882 test: loss0.0000965, mae:0.0070440
training loss 3.973136699642055e-06 mae 0.001491684466600418
training loss 3.6891411207975853e-06 mae 0.00143657705299191
training loss 3.817865314823115e-06 mae 0.0014647834192002464
training loss 3.945503898564569e-06 mae 0.0014883290686404074
training loss 4.0302750379968826e-06 mae 0.0015059272018469756
Epoch 324, training: loss: 0.0000040, mae: 0.0015035 test: loss0.0000968, mae:0.0070671
training loss 3.342188165333937e-06 mae 0.0014386080438271165
training loss 4.09552821307158e-06 mae 0.0015380642166836005
training loss 3.980955988040345e-06 mae 0.001501903998305891
training loss 3.988364365669376e-06 mae 0.0015010459299217781
training loss 3.975500363816342e-06 mae 0.0014998110252159148
Epoch 325, training: loss: 0.0000040, mae: 0.0014995 test: loss0.0000964, mae:0.0070506
training loss 2.529982111809659e-06 mae 0.0012097671860828996
training loss 3.8568339991173796e-06 mae 0.001461045366858004
training loss 3.9971741763552125e-06 mae 0.0014933368276417397
training loss 4.002144858483933e-06 mae 0.0014986685324546616
training loss 3.977046675056803e-06 mae 0.0014998507680167535
Epoch 326, training: loss: 0.0000040, mae: 0.0015061 test: loss0.0000967, mae:0.0070535
training loss 3.1019787911645835e-06 mae 0.0013892017304897308
training loss 3.7096459441183435e-06 mae 0.0014584465819757943
training loss 3.7729228581318445e-06 mae 0.0014703285080463725
training loss 3.897863990122588e-06 mae 0.0014871391105397752
training loss 3.939294923843569e-06 mae 0.00149171148132607
Epoch 327, training: loss: 0.0000040, mae: 0.0014935 test: loss0.0000966, mae:0.0070550
training loss 3.1778217817191035e-06 mae 0.0012146156514063478
training loss 3.5769331170027437e-06 mae 0.0014219851388285556
training loss 3.7238972643733927e-06 mae 0.0014307541521450522
training loss 3.908733572113503e-06 mae 0.0014752383784865902
training loss 3.915063903557675e-06 mae 0.0014806215967566
Epoch 328, training: loss: 0.0000039, mae: 0.0014824 test: loss0.0000970, mae:0.0070807
training loss 3.985247076343512e-06 mae 0.001547025516629219
training loss 3.7426180048950144e-06 mae 0.0014595845189201186
training loss 3.725965841410228e-06 mae 0.0014463426927934478
training loss 3.833726734698354e-06 mae 0.001467303903341688
training loss 3.868562397027251e-06 mae 0.0014800982345914962
Epoch 329, training: loss: 0.0000039, mae: 0.0014842 test: loss0.0000977, mae:0.0070910
training loss 2.7595312985795317e-06 mae 0.0012607472017407417
training loss 3.4297459113112572e-06 mae 0.0014063195626744453
training loss 3.779964776194812e-06 mae 0.0014618455702070105
training loss 3.8134624496374982e-06 mae 0.0014676861488106135
training loss 3.893592138614513e-06 mae 0.0014836679262558543
Epoch 330, training: loss: 0.0000039, mae: 0.0014833 test: loss0.0000970, mae:0.0071009
training loss 4.204135166219203e-06 mae 0.0016549589345231652
training loss 4.100514272567141e-06 mae 0.001515318879255039
training loss 4.009622892965017e-06 mae 0.0014981455763221005
training loss 3.937471751819945e-06 mae 0.0014865645303834992
training loss 3.87108864525721e-06 mae 0.0014751974214679223
Epoch 331, training: loss: 0.0000039, mae: 0.0014742 test: loss0.0000970, mae:0.0070674
training loss 4.550743597064866e-06 mae 0.0014396201586350799
training loss 3.527248371064357e-06 mae 0.001405292417735373
training loss 3.7921731903468136e-06 mae 0.0014703990609182208
training loss 3.864732433016225e-06 mae 0.001483469975214846
training loss 3.84887113425102e-06 mae 0.0014792377779617053
Epoch 332, training: loss: 0.0000039, mae: 0.0014800 test: loss0.0000969, mae:0.0070753
training loss 4.494410404731752e-06 mae 0.00146347691770643
training loss 3.644804896972417e-06 mae 0.0014298274204609734
training loss 3.7401276239335746e-06 mae 0.0014475769412906677
training loss 3.84467394070686e-06 mae 0.0014664675660326534
training loss 3.8511442652412555e-06 mae 0.0014751527370973053
Epoch 333, training: loss: 0.0000038, mae: 0.0014757 test: loss0.0000958, mae:0.0070199
training loss 2.581864464445971e-06 mae 0.0013159066438674927
training loss 3.853015405362455e-06 mae 0.0014875433293591238
training loss 3.762792156485734e-06 mae 0.0014605787651853102
training loss 3.854698951462167e-06 mae 0.0014736810521783906
training loss 3.839873439252883e-06 mae 0.0014709917468190262
Epoch 334, training: loss: 0.0000038, mae: 0.0014668 test: loss0.0000963, mae:0.0070535
training loss 3.2468403787788702e-06 mae 0.0013779721921309829
training loss 3.7372658381155266e-06 mae 0.001457059107647807
training loss 3.6834756672148375e-06 mae 0.0014465693845459734
training loss 3.7178241661079056e-06 mae 0.001453811357519435
training loss 3.786464086301547e-06 mae 0.001463454270003298
Epoch 335, training: loss: 0.0000038, mae: 0.0014571 test: loss0.0000966, mae:0.0070577
training loss 4.737292783829616e-06 mae 0.0015783678973093629
training loss 3.5779192767579083e-06 mae 0.0014127807819521894
training loss 3.683128701739784e-06 mae 0.0014332595504912564
training loss 3.7197123621856903e-06 mae 0.0014386575982060959
training loss 3.7774615146471173e-06 mae 0.0014506922287644071
Epoch 336, training: loss: 0.0000038, mae: 0.0014506 test: loss0.0000975, mae:0.0071076
training loss 3.033281927855569e-06 mae 0.0013903466751798987
training loss 3.4247511491997223e-06 mae 0.0013775588218670556
training loss 3.508833393731614e-06 mae 0.0013972185609751556
training loss 3.6453405335195815e-06 mae 0.001426431884686925
training loss 3.691503750711101e-06 mae 0.00143633924936644
Epoch 337, training: loss: 0.0000037, mae: 0.0014425 test: loss0.0000987, mae:0.0071462
training loss 3.1339607176050777e-06 mae 0.0013543739914894104
training loss 3.72953002393022e-06 mae 0.0014654898550361393
training loss 3.6830710478801e-06 mae 0.001448562185482887
training loss 3.7450986488596797e-06 mae 0.0014530772139743858
training loss 3.758568559148548e-06 mae 0.0014577944095788605
Epoch 338, training: loss: 0.0000038, mae: 0.0014585 test: loss0.0000973, mae:0.0070878
training loss 3.600342779463972e-06 mae 0.001360794180072844
training loss 3.450812574984037e-06 mae 0.0013873023391865634
training loss 3.5603518314235437e-06 mae 0.0014092714000161332
training loss 3.6803252628758864e-06 mae 0.0014335046351165665
training loss 3.6957751399766052e-06 mae 0.0014397977829322362
Epoch 339, training: loss: 0.0000037, mae: 0.0014426 test: loss0.0000976, mae:0.0070924
training loss 3.6372177874000045e-06 mae 0.0014620976289734244
training loss 3.7354041425857625e-06 mae 0.001457869271546894
training loss 3.742698248799461e-06 mae 0.0014563837160107374
training loss 3.7965199275855183e-06 mae 0.0014674107621991319
training loss 3.7762605732731293e-06 mae 0.0014642492803717167
Epoch 340, training: loss: 0.0000038, mae: 0.0014676 test: loss0.0000970, mae:0.0070776
training loss 2.1591138192889048e-06 mae 0.0011338364565744996
training loss 3.5267813299309207e-06 mae 0.0014184077564791283
training loss 3.4803695807748237e-06 mae 0.001408215292319102
training loss 3.685269382232632e-06 mae 0.001444335189256324
training loss 3.792948666444711e-06 mae 0.0014624359889479533
Epoch 341, training: loss: 0.0000038, mae: 0.0014622 test: loss0.0000974, mae:0.0070842
training loss 2.6280565634806408e-06 mae 0.0012207971885800362
training loss 3.635803431743446e-06 mae 0.0014173138074065538
training loss 3.7251106946449467e-06 mae 0.0014488700014253213
training loss 3.634414002704786e-06 mae 0.0014334473780522876
training loss 3.635995412946272e-06 mae 0.0014346574487590891
Epoch 342, training: loss: 0.0000037, mae: 0.0014369 test: loss0.0000977, mae:0.0070814
training loss 2.767868636510684e-06 mae 0.0012278417125344276
training loss 3.554792890688131e-06 mae 0.0013982578084858902
training loss 3.6013504241775158e-06 mae 0.001407216380276524
training loss 3.717710050121193e-06 mae 0.001438245515721475
training loss 3.7839457995438694e-06 mae 0.0014555122794824384
Epoch 343, training: loss: 0.0000038, mae: 0.0014544 test: loss0.0000986, mae:0.0071204
training loss 4.1338112168887164e-06 mae 0.001514628529548645
training loss 3.874348047194121e-06 mae 0.0014878762657662816
training loss 3.7050954638784376e-06 mae 0.00144657830472593
training loss 3.651059128271106e-06 mae 0.001439713839046805
training loss 3.7232801330458124e-06 mae 0.0014546663113475878
Epoch 344, training: loss: 0.0000037, mae: 0.0014576 test: loss0.0000990, mae:0.0071242
training loss 2.668132538019563e-06 mae 0.001279480173252523
training loss 3.5957609524677326e-06 mae 0.001409034680702961
training loss 3.6095701176182587e-06 mae 0.0014191371651532333
training loss 3.6396683335383465e-06 mae 0.0014287170172292763
training loss 3.652128543536712e-06 mae 0.0014327702834155056
Epoch 345, training: loss: 0.0000037, mae: 0.0014315 test: loss0.0000989, mae:0.0071316
training loss 2.2893818822922185e-06 mae 0.0011395517503842711
training loss 3.5129822288795637e-06 mae 0.0013836901132747823
training loss 3.494989683342355e-06 mae 0.0013841220781686577
training loss 3.514579605700348e-06 mae 0.0013980118346567088
training loss 3.518629134658168e-06 mae 0.0013994351652015316
Epoch 346, training: loss: 0.0000036, mae: 0.0014067 test: loss0.0000979, mae:0.0071111
training loss 1.5882093293839716e-06 mae 0.0009656312759034336
training loss 3.3598571819454148e-06 mae 0.001352659255430541
training loss 3.393856793469006e-06 mae 0.0013643792446582843
training loss 3.434736554301095e-06 mae 0.0013775164751021422
training loss 3.473909841818908e-06 mae 0.0013880025608747717
Epoch 347, training: loss: 0.0000035, mae: 0.0013949 test: loss0.0000988, mae:0.0071495
training loss 3.699987018990214e-06 mae 0.0013858660822734237
training loss 3.476402525848244e-06 mae 0.0014194951132487726
training loss 3.5908724306288004e-06 mae 0.001425072528873709
training loss 3.5719367008444154e-06 mae 0.0014206239925111518
training loss 3.549126232378761e-06 mae 0.0014165146406670448
Epoch 348, training: loss: 0.0000036, mae: 0.0014151 test: loss0.0000986, mae:0.0071306
training loss 3.2059535897133173e-06 mae 0.0012870269129052758
training loss 3.7111149942896144e-06 mae 0.0014401084524305425
training loss 3.685896702714528e-06 mae 0.0014332394048774454
training loss 3.668468361896078e-06 mae 0.0014335017632816415
training loss 3.622213093695787e-06 mae 0.0014316061574187302
Epoch 349, training: loss: 0.0000036, mae: 0.0014331 test: loss0.0000985, mae:0.0071394
training loss 2.2375718344846973e-06 mae 0.0011539831757545471
training loss 3.3069751608826324e-06 mae 0.0013711488116349953
training loss 3.324604911018234e-06 mae 0.0013705603167469991
training loss 3.401050948532006e-06 mae 0.0013878923140786938
training loss 3.529043350686128e-06 mae 0.0014084789876487867
Epoch 350, training: loss: 0.0000035, mae: 0.0014101 test: loss0.0000996, mae:0.0071809
training loss 2.3939558104757452e-06 mae 0.0010442578932270408
training loss 3.354931559264443e-06 mae 0.0013605178100988267
training loss 3.2096168295106284e-06 mae 0.0013334163548747576
training loss 3.3970892377962354e-06 mae 0.0013793298418769326
training loss 3.4791971587249324e-06 mae 0.0013954375568772687
Epoch 351, training: loss: 0.0000035, mae: 0.0013948 test: loss0.0000994, mae:0.0071597
training loss 2.4181065327866236e-06 mae 0.0012239674106240273
training loss 3.4011208317322454e-06 mae 0.0014022369572308426
training loss 3.495574289202226e-06 mae 0.0014097333089264092
training loss 3.4907436513745075e-06 mae 0.0014055078422854238
training loss 3.5338298329547096e-06 mae 0.001412398325202904
Epoch 352, training: loss: 0.0000035, mae: 0.0014094 test: loss0.0000992, mae:0.0071678
training loss 3.940921942557907e-06 mae 0.0013814102858304977
training loss 3.430770653258329e-06 mae 0.0013657461374760695
training loss 3.3729724223256683e-06 mae 0.00136403574741971
training loss 3.4198598440591806e-06 mae 0.001384292594280513
training loss 3.4485610519789327e-06 mae 0.0013925915486086614
Epoch 353, training: loss: 0.0000035, mae: 0.0013947 test: loss0.0000993, mae:0.0071652
training loss 3.958738034270937e-06 mae 0.0015067191561684012
training loss 3.422448917173789e-06 mae 0.001392405466450488
training loss 3.43799173255194e-06 mae 0.0013889739575731282
training loss 3.5177970147327e-06 mae 0.001405877079691318
training loss 3.4828520429344743e-06 mae 0.0013979595868074473
Epoch 354, training: loss: 0.0000035, mae: 0.0013979 test: loss0.0000989, mae:0.0071354
training loss 2.4602015855634818e-06 mae 0.001145102665759623
training loss 3.2674261136907816e-06 mae 0.0013703439457743776
training loss 3.3459143763832223e-06 mae 0.0013721428285952259
training loss 3.3925380999573085e-06 mae 0.0013841575671554806
training loss 3.4166939780904485e-06 mae 0.001389147842937701
Epoch 355, training: loss: 0.0000034, mae: 0.0013924 test: loss0.0000994, mae:0.0071535
training loss 2.4248861336673144e-06 mae 0.0012560108443722129
training loss 3.205897310270337e-06 mae 0.0013427458082636197
training loss 3.420650700988544e-06 mae 0.0013837548166439676
training loss 3.468737405389359e-06 mae 0.0013904482810850563
training loss 3.3908801841975206e-06 mae 0.001375892077658362
Epoch 356, training: loss: 0.0000034, mae: 0.0013743 test: loss0.0000986, mae:0.0071390
training loss 3.4636630061868345e-06 mae 0.0013640612596645951
training loss 3.1621351514759752e-06 mae 0.0013283046142782506
training loss 3.3135998405263723e-06 mae 0.001370858922623799
training loss 3.32271578739868e-06 mae 0.0013744372281628287
training loss 3.436131775779178e-06 mae 0.0013910069327861705
Epoch 357, training: loss: 0.0000035, mae: 0.0013964 test: loss0.0000994, mae:0.0071503
training loss 2.1177004327910254e-06 mae 0.0010903911897912621
training loss 3.5969079724108105e-06 mae 0.0014287536466202025
training loss 3.4260735755296002e-06 mae 0.00138678069809673
training loss 3.5263368319334078e-06 mae 0.001401411007476094
training loss 3.4823488910086017e-06 mae 0.0013961401024939891
Epoch 358, training: loss: 0.0000035, mae: 0.0013940 test: loss0.0000999, mae:0.0071692
training loss 2.039226501437952e-06 mae 0.0009846193715929985
training loss 3.1876383793453842e-06 mae 0.001328677682014292
training loss 3.175358617929557e-06 mae 0.0013318979187112415
training loss 3.2602171673740026e-06 mae 0.0013519084417964306
training loss 3.371320364898729e-06 mae 0.0013760597041381794
Epoch 359, training: loss: 0.0000034, mae: 0.0013754 test: loss0.0000997, mae:0.0071702
training loss 2.14651481655892e-06 mae 0.0011650113156065345
training loss 3.3922013525465016e-06 mae 0.0013755362722840084
training loss 3.349163189386223e-06 mae 0.0013626446079527306
training loss 3.4232458461545168e-06 mae 0.0013818544561555299
training loss 3.4630281743601487e-06 mae 0.0013919936684747938
Epoch 360, training: loss: 0.0000034, mae: 0.0013903 test: loss0.0001008, mae:0.0072024
training loss 3.13040186483704e-06 mae 0.0013074427843093872
training loss 3.106026267731996e-06 mae 0.0013186819851398468
training loss 3.2042468992675834e-06 mae 0.0013370886733202207
training loss 3.2266586725242054e-06 mae 0.0013460539434018803
training loss 3.249459482571753e-06 mae 0.0013464869652175114
Epoch 361, training: loss: 0.0000032, mae: 0.0013457 test: loss0.0000987, mae:0.0071320
training loss 2.3834998046368128e-06 mae 0.001232030801475048
training loss 3.0398507778598146e-06 mae 0.0013016941580537924
training loss 3.2262863354358484e-06 mae 0.0013331033428900386
training loss 3.236770281028352e-06 mae 0.0013373688153231781
training loss 3.3004523867197394e-06 mae 0.0013553533261173886
Epoch 362, training: loss: 0.0000033, mae: 0.0013601 test: loss0.0000996, mae:0.0071758
training loss 2.544887138355989e-06 mae 0.0013237505918368697
training loss 3.3276887667371355e-06 mae 0.0013645903421931112
training loss 3.2362553710218783e-06 mae 0.0013552638315964525
training loss 3.317353237989001e-06 mae 0.0013681911051563222
training loss 3.4256881635023736e-06 mae 0.0013944968098742116
Epoch 363, training: loss: 0.0000034, mae: 0.0013937 test: loss0.0000995, mae:0.0071762
training loss 2.6541317765804706e-06 mae 0.0012244997778907418
training loss 3.0783428874379826e-06 mae 0.0013118978811190556
training loss 3.1709820393756455e-06 mae 0.001320393826342234
training loss 3.183806871217428e-06 mae 0.001331248015338024
training loss 3.2635061372944895e-06 mae 0.001346692763760093
Epoch 364, training: loss: 0.0000033, mae: 0.0013478 test: loss0.0000990, mae:0.0071499
training loss 2.314974608452758e-06 mae 0.0010751750087365508
training loss 3.211436690292555e-06 mae 0.0013496435815742347
training loss 3.2161479616166114e-06 mae 0.0013541523476666068
training loss 3.307205207534778e-06 mae 0.0013659697597297456
training loss 3.338172285822197e-06 mae 0.0013777161070924087
Epoch 365, training: loss: 0.0000033, mae: 0.0013771 test: loss0.0000995, mae:0.0071637
training loss 3.727893272298388e-06 mae 0.0014423327520489693
training loss 3.3223293730414158e-06 mae 0.0013770130839582317
training loss 3.3553289758946464e-06 mae 0.0013726388425842893
training loss 3.3325344731071673e-06 mae 0.0013651199972670256
training loss 3.321459823509305e-06 mae 0.0013635956924941523
Epoch 366, training: loss: 0.0000033, mae: 0.0013625 test: loss0.0000989, mae:0.0071286
training loss 3.188327127645607e-06 mae 0.0012937579303979874
training loss 3.1108666052222915e-06 mae 0.0013157379455115722
training loss 3.1989690418557456e-06 mae 0.001329202069509037
training loss 3.3841817627721835e-06 mae 0.0013794256416642856
training loss 3.475930061450069e-06 mae 0.001403391936652603
Epoch 367, training: loss: 0.0000035, mae: 0.0014043 test: loss0.0001008, mae:0.0072021
training loss 3.0463045277429046e-06 mae 0.0012912327656522393
training loss 3.2069625046692546e-06 mae 0.0013501320465687007
training loss 3.212581904930143e-06 mae 0.0013579593891840393
training loss 3.2450170850862422e-06 mae 0.0013611382652160447
training loss 3.2384824194128086e-06 mae 0.001350818138479242
Epoch 368, training: loss: 0.0000033, mae: 0.0013549 test: loss0.0001004, mae:0.0072155
training loss 6.461670636781491e-06 mae 0.0014932568883523345
training loss 3.250965097750475e-06 mae 0.0013364811642460673
training loss 3.254725510991607e-06 mae 0.0013438359682107037
training loss 3.2909711374347966e-06 mae 0.0013605119114655341
training loss 3.3411221459152617e-06 mae 0.0013700732277863814
Epoch 369, training: loss: 0.0000033, mae: 0.0013700 test: loss0.0000992, mae:0.0071728
training loss 2.2888420971867163e-06 mae 0.0011758161708712578
training loss 3.5305148747854637e-06 mae 0.001403936029722293
training loss 3.29739556140213e-06 mae 0.0013622676511064614
training loss 3.2860884496266406e-06 mae 0.0013689998381567613
training loss 3.3260094729050603e-06 mae 0.0013762208099120214
Epoch 370, training: loss: 0.0000033, mae: 0.0013787 test: loss0.0001000, mae:0.0071827
training loss 4.257796263118507e-06 mae 0.0014908077428117394
training loss 3.1785489829203205e-06 mae 0.0013377773963535824
training loss 3.306217224397385e-06 mae 0.0013721650812110989
training loss 3.427392574313303e-06 mae 0.0013908167668026164
training loss 3.406429234236207e-06 mae 0.0013831548959790587
Epoch 371, training: loss: 0.0000034, mae: 0.0013829 test: loss0.0000996, mae:0.0071715
training loss 2.9213570087449625e-06 mae 0.0013422755291685462
training loss 3.0365097022606243e-06 mae 0.0013079683241598748
training loss 3.0665907654182978e-06 mae 0.001315778129996898
training loss 3.099836767011242e-06 mae 0.0013191119635890894
training loss 3.112455490836013e-06 mae 0.0013251946894428806
Epoch 372, training: loss: 0.0000031, mae: 0.0013279 test: loss0.0000995, mae:0.0071696
training loss 4.234207608533325e-06 mae 0.0014901179820299149
training loss 2.7938674431389505e-06 mae 0.0012507259197460086
training loss 3.0173692430883477e-06 mae 0.0012927391799166796
training loss 3.1497376182975038e-06 mae 0.0013285479870692688
training loss 3.1934603004710464e-06 mae 0.001339566773985193
Epoch 373, training: loss: 0.0000032, mae: 0.0013456 test: loss0.0001001, mae:0.0071828
training loss 2.485914365024655e-06 mae 0.0012048346688970923
training loss 3.0376690650276337e-06 mae 0.0012908725538199732
training loss 3.029700602238866e-06 mae 0.0012976143585222933
training loss 3.0126841322922e-06 mae 0.0012909547716459353
training loss 3.0373515495676063e-06 mae 0.001299327610359431
Epoch 374, training: loss: 0.0000030, mae: 0.0013026 test: loss0.0000992, mae:0.0071628
training loss 3.2666487186361337e-06 mae 0.0013419674942269921
training loss 2.8727336364360836e-06 mae 0.0012786823151377485
training loss 3.0301596032784637e-06 mae 0.0013081285846650156
training loss 3.07814733537421e-06 mae 0.0013120969547735927
training loss 3.1259157714740277e-06 mae 0.00132680749022564
Epoch 375, training: loss: 0.0000031, mae: 0.0013258 test: loss0.0001016, mae:0.0072359
training loss 3.03726324091258e-06 mae 0.0012954818084836006
training loss 3.021984282752908e-06 mae 0.001316422302111545
training loss 3.0744560947346377e-06 mae 0.0013119980737815915
training loss 3.122089214266212e-06 mae 0.0013232631921083569
training loss 3.171611736624029e-06 mae 0.0013332233468269865
Epoch 376, training: loss: 0.0000032, mae: 0.0013356 test: loss0.0001010, mae:0.0072245
training loss 3.5869800285581732e-06 mae 0.0014338502660393715
training loss 3.1896124130748912e-06 mae 0.001335411151076722
training loss 3.0419008894703906e-06 mae 0.0013114226397615625
training loss 3.0932941828590904e-06 mae 0.0013196434057009287
training loss 3.1257712002776633e-06 mae 0.0013267557757596173
Epoch 377, training: loss: 0.0000031, mae: 0.0013241 test: loss0.0000996, mae:0.0071808
training loss 3.7206734759820392e-06 mae 0.0014770282432436943
training loss 2.9752080585168706e-06 mae 0.001294706539506568
training loss 2.960213359893707e-06 mae 0.001291290233326652
training loss 3.0524314969089215e-06 mae 0.0013055617439973832
training loss 3.0348731018452627e-06 mae 0.001304808966132039
Epoch 378, training: loss: 0.0000031, mae: 0.0013078 test: loss0.0001006, mae:0.0072269
training loss 2.7543371743377065e-06 mae 0.0013383111217990518
training loss 2.925773314392941e-06 mae 0.0012849755174316028
training loss 3.079948639046195e-06 mae 0.0013133319403681129
training loss 3.0983665236522493e-06 mae 0.0013223796533975772
training loss 3.1025263708885466e-06 mae 0.0013223199720554684
Epoch 379, training: loss: 0.0000031, mae: 0.0013261 test: loss0.0001006, mae:0.0072227
training loss 3.413670356167131e-06 mae 0.001377517357468605
training loss 2.9626070027363457e-06 mae 0.0012895743768917875
training loss 3.034598535123223e-06 mae 0.0013118682943266068
training loss 3.095130859977063e-06 mae 0.0013259730790587548
training loss 3.1392485243260334e-06 mae 0.0013321478253425065
Epoch 380, training: loss: 0.0000032, mae: 0.0013355 test: loss0.0001008, mae:0.0072191
training loss 2.1774767446913756e-06 mae 0.0011939700925722718
training loss 2.9916637935921073e-06 mae 0.0013085334874488707
training loss 3.0118166135946376e-06 mae 0.0013059867046874862
training loss 3.0347566923368194e-06 mae 0.0013065952657327193
training loss 3.0673074905877673e-06 mae 0.0013105585514938457
Epoch 381, training: loss: 0.0000031, mae: 0.0013097 test: loss0.0001002, mae:0.0071736
training loss 2.6937705115415156e-06 mae 0.0012670423602685332
training loss 2.776859103095394e-06 mae 0.0012461291283697763
training loss 2.7975906059419678e-06 mae 0.001248872404950348
training loss 2.936997236024551e-06 mae 0.001278002256456614
training loss 3.0248935960044403e-06 mae 0.0013025595609038433
Epoch 382, training: loss: 0.0000030, mae: 0.0012995 test: loss0.0000998, mae:0.0071619
training loss 3.5446416859485907e-06 mae 0.0012795092770829797
training loss 3.114731440541172e-06 mae 0.0013122618901452014
training loss 2.9997230546658736e-06 mae 0.001294171855448386
training loss 3.0116348253668175e-06 mae 0.0013040963260438428
training loss 3.065116189799622e-06 mae 0.001315170674601024
Epoch 383, training: loss: 0.0000031, mae: 0.0013185 test: loss0.0001009, mae:0.0072105
training loss 5.054871053289389e-06 mae 0.001732610515318811
training loss 3.1334861530779683e-06 mae 0.0013217858721356038
training loss 3.063989351680407e-06 mae 0.001316754678195103
training loss 3.0729798492383836e-06 mae 0.0013204362819322834
training loss 3.0453772707453526e-06 mae 0.0013093136506391444
Epoch 384, training: loss: 0.0000031, mae: 0.0013103 test: loss0.0001001, mae:0.0071874
training loss 2.575686494310503e-06 mae 0.0012277677888050675
training loss 2.872210052819534e-06 mae 0.0012683225019524493
training loss 2.9319205792165643e-06 mae 0.0012841935530028279
training loss 3.007760806333363e-06 mae 0.0013006193893314003
training loss 3.012615978611814e-06 mae 0.0013017085799599877
Epoch 385, training: loss: 0.0000030, mae: 0.0013000 test: loss0.0001014, mae:0.0072112
training loss 2.2168799205246614e-06 mae 0.0009973958367481828
training loss 2.949570136265453e-06 mae 0.0012744116761228617
training loss 2.939751263313318e-06 mae 0.0012789712216022722
training loss 3.016323351114006e-06 mae 0.0013001267744277576
training loss 2.9735380087854835e-06 mae 0.0012945444847281035
Epoch 386, training: loss: 0.0000030, mae: 0.0012960 test: loss0.0001012, mae:0.0072386
training loss 2.6143836748815374e-06 mae 0.0012333514168858528
training loss 2.92586904985607e-06 mae 0.001279278354737542
training loss 2.9418516827952238e-06 mae 0.0012700696208394395
training loss 2.94699216268276e-06 mae 0.0012812474098778171
training loss 2.9791825071113793e-06 mae 0.001289149699964333
Epoch 387, training: loss: 0.0000030, mae: 0.0012882 test: loss0.0001009, mae:0.0072249
training loss 2.981308398375404e-06 mae 0.0012114228447899222
training loss 2.780115111960971e-06 mae 0.0012513097711181378
training loss 2.915570828215752e-06 mae 0.0012818150237285636
training loss 2.8752803711167324e-06 mae 0.0012806828449085502
training loss 2.9603368679615525e-06 mae 0.0012927476455114636
Epoch 388, training: loss: 0.0000029, mae: 0.0012899 test: loss0.0001004, mae:0.0071940
training loss 3.1288636819226667e-06 mae 0.001256264396943152
training loss 2.805136015619307e-06 mae 0.0012492003642917413
training loss 2.9308050310610627e-06 mae 0.0012735366794778642
training loss 2.9229739916957355e-06 mae 0.0012734937365937809
training loss 2.9738298010430575e-06 mae 0.0012888187422088135
Epoch 389, training: loss: 0.0000030, mae: 0.0012893 test: loss0.0001012, mae:0.0072446
training loss 2.1267292140692007e-06 mae 0.0011473093181848526
training loss 2.8675459968023156e-06 mae 0.0012864492418171434
training loss 2.8112165441283163e-06 mae 0.0012601681173511664
training loss 2.888821226382342e-06 mae 0.0012720686399735604
training loss 2.943751833887987e-06 mae 0.0012818885412859491
Epoch 390, training: loss: 0.0000029, mae: 0.0012829 test: loss0.0001012, mae:0.0072111
training loss 3.574191168809193e-06 mae 0.0013882449129596353
training loss 2.9495891063632774e-06 mae 0.0012775229806007416
training loss 2.9969129422854605e-06 mae 0.0012875792165840101
training loss 3.0089226978611646e-06 mae 0.0012946785793837086
training loss 2.954625951070145e-06 mae 0.0012886226029524496
Epoch 391, training: loss: 0.0000030, mae: 0.0012942 test: loss0.0001019, mae:0.0072440
training loss 1.8266233610120253e-06 mae 0.0010489305714145303
training loss 2.89371248482876e-06 mae 0.001284464972331097
training loss 2.999345414005658e-06 mae 0.0013054743176326153
training loss 2.9973242989164396e-06 mae 0.00130599264066284
training loss 3.0273411774675397e-06 mae 0.0013127271459549114
Epoch 392, training: loss: 0.0000030, mae: 0.0013119 test: loss0.0001012, mae:0.0072369
training loss 3.5151708743796917e-06 mae 0.0014636156847700477
training loss 2.7930728033514277e-06 mae 0.0012454287954788727
training loss 2.889322471187686e-06 mae 0.0012687423461264387
training loss 2.963360436706654e-06 mae 0.0012875762903008189
training loss 2.9873485421679424e-06 mae 0.0012939090047620087
Epoch 393, training: loss: 0.0000030, mae: 0.0012952 test: loss0.0001015, mae:0.0072357
training loss 3.9213168747664895e-06 mae 0.0013599625090137124
training loss 2.782422553519056e-06 mae 0.0012560093617888493
training loss 2.8746416542207496e-06 mae 0.0012741731243475457
training loss 2.9140244049698747e-06 mae 0.0012815349954272933
training loss 2.9519816133241383e-06 mae 0.0012885004195699755
Epoch 394, training: loss: 0.0000029, mae: 0.0012862 test: loss0.0001010, mae:0.0072221
training loss 3.2663053843862144e-06 mae 0.0013737492263317108
training loss 2.74632305031535e-06 mae 0.0012315771252592552
training loss 2.818134830569292e-06 mae 0.0012453463323884601
training loss 2.882795604891244e-06 mae 0.0012598720896489527
training loss 2.8694519108226202e-06 mae 0.001261782753095958
Epoch 395, training: loss: 0.0000029, mae: 0.0012616 test: loss0.0001007, mae:0.0072334
training loss 1.8497730707167648e-06 mae 0.0010758911957964301
training loss 2.9147464379004274e-06 mae 0.0012722406513514177
training loss 2.8980818855740335e-06 mae 0.0012687166339007787
training loss 2.86403961590562e-06 mae 0.001268008667574764
training loss 2.8784613048185225e-06 mae 0.0012712991044657369
Epoch 396, training: loss: 0.0000029, mae: 0.0012755 test: loss0.0001006, mae:0.0072254
training loss 2.109806928274338e-06 mae 0.0011344676604494452
training loss 2.7345315540479337e-06 mae 0.0012488931408334597
training loss 2.773534502597216e-06 mae 0.0012537490577658288
training loss 2.8581243948744497e-06 mae 0.0012704478044815312
training loss 2.881941433127879e-06 mae 0.0012754604726594259
Epoch 397, training: loss: 0.0000029, mae: 0.0012767 test: loss0.0001009, mae:0.0072274
training loss 3.6536355310090585e-06 mae 0.0013228598982095718
training loss 2.783316468665754e-06 mae 0.0012326253470782117
training loss 2.9813421554869214e-06 mae 0.0012723191567502992
training loss 2.944655569153356e-06 mae 0.001270109729761171
training loss 2.927822824903843e-06 mae 0.0012774440872283714
Epoch 398, training: loss: 0.0000029, mae: 0.0012770 test: loss0.0001014, mae:0.0072216
training loss 2.0971203866793076e-06 mae 0.0010651942575350404
training loss 2.705700841729327e-06 mae 0.0012352623812416014
training loss 2.7276041669435947e-06 mae 0.0012380792021677621
training loss 2.85791282366945e-06 mae 0.001267731202448662
training loss 2.990178861399551e-06 mae 0.0012971831028655504
Epoch 399, training: loss: 0.0000030, mae: 0.0012954 test: loss0.0001018, mae:0.0072761
current learning rate: 3.125e-05
training loss 4.022338544018567e-06 mae 0.0015698755159974098
training loss 2.446878798924873e-06 mae 0.0011438830303685632
training loss 2.312509372973857e-06 mae 0.0011120756182100367
training loss 2.3522034071926546e-06 mae 0.001110657718455086
training loss 2.345759240264969e-06 mae 0.001112478864564565
Epoch 400, training: loss: 0.0000023, mae: 0.0011094 test: loss0.0001006, mae:0.0072176
training loss 1.8230479099656804e-06 mae 0.0009627643157728016
training loss 2.077861497537907e-06 mae 0.0010261705702207253
training loss 2.1849427568695044e-06 mae 0.0010493798792306887
training loss 2.2205801609448356e-06 mae 0.0010603597925032265
training loss 2.2035551927756243e-06 mae 0.0010578590416261441
Epoch 401, training: loss: 0.0000022, mae: 0.0010577 test: loss0.0001014, mae:0.0072362
training loss 2.0804445739486255e-06 mae 0.0009112966363318264
training loss 1.991720889200937e-06 mae 0.0010115959985163429
training loss 2.074884904875872e-06 mae 0.001029581691963727
training loss 2.1305263868235e-06 mae 0.0010418795536438759
training loss 2.1372903519527174e-06 mae 0.0010437466620018161
Epoch 402, training: loss: 0.0000022, mae: 0.0010497 test: loss0.0001020, mae:0.0072646
training loss 2.672893060662318e-06 mae 0.00111455621663481
training loss 2.0536676745406092e-06 mae 0.0010170385768801412
training loss 2.0481835592660652e-06 mae 0.0010118224366457377
training loss 2.121704711342149e-06 mae 0.0010332074236056904
training loss 2.1557653680523326e-06 mae 0.001043630388868852
Epoch 403, training: loss: 0.0000022, mae: 0.0010450 test: loss0.0001020, mae:0.0072632
training loss 1.9071894712396897e-06 mae 0.000895336561370641
training loss 1.913731507868885e-06 mae 0.0009820704798068048
training loss 2.1841644016234625e-06 mae 0.0010399594758913877
training loss 2.204341486904375e-06 mae 0.0010498823345151558
training loss 2.1641829117646953e-06 mae 0.0010477184831739314
Epoch 404, training: loss: 0.0000022, mae: 0.0010512 test: loss0.0001033, mae:0.0072996
training loss 2.255903382319957e-06 mae 0.001206384040415287
training loss 2.1766653037725604e-06 mae 0.0010466528953710459
training loss 2.2020157672105215e-06 mae 0.0010575898992137448
training loss 2.200084166674396e-06 mae 0.0010643463365160029
training loss 2.1900475819315193e-06 mae 0.0010602085176966533
Epoch 405, training: loss: 0.0000022, mae: 0.0010619 test: loss0.0001031, mae:0.0072954
training loss 1.5041669030324556e-06 mae 0.0008142360602505505
training loss 2.0533587305591007e-06 mae 0.0010175638622147778
training loss 2.1780210632002335e-06 mae 0.0010505788792431866
training loss 2.158697591824342e-06 mae 0.0010449288425263983
training loss 2.1856958961050186e-06 mae 0.0010560252185359568
Epoch 406, training: loss: 0.0000022, mae: 0.0010556 test: loss0.0001021, mae:0.0072614
training loss 1.7837310224422254e-06 mae 0.001033803797326982
training loss 2.0434469714767635e-06 mae 0.0010088706724619603
training loss 2.118371718110987e-06 mae 0.0010342475684995919
training loss 2.144228406733656e-06 mae 0.0010410974311815043
training loss 2.1935857813079033e-06 mae 0.0010537779690183487
Epoch 407, training: loss: 0.0000022, mae: 0.0010535 test: loss0.0001022, mae:0.0072694
training loss 1.4737712490386912e-06 mae 0.0009685459663160145
training loss 2.0516743073612276e-06 mae 0.0010267245074204521
training loss 2.111966812726048e-06 mae 0.0010409707533158732
training loss 2.2108891353910556e-06 mae 0.0010640019032832802
training loss 2.2006255386797446e-06 mae 0.0010642344983461418
Epoch 408, training: loss: 0.0000022, mae: 0.0010685 test: loss0.0001039, mae:0.0073304
training loss 1.3583561440100311e-06 mae 0.0008792442386038601
training loss 2.140430218983682e-06 mae 0.0010351037969538832
training loss 2.136759877488982e-06 mae 0.001032941016290189
training loss 2.124814953213412e-06 mae 0.0010362181362282754
training loss 2.148492597911652e-06 mae 0.001047183597050329
Epoch 409, training: loss: 0.0000022, mae: 0.0010485 test: loss0.0001020, mae:0.0072567
training loss 2.1587729861494154e-06 mae 0.0010563958203420043
training loss 2.0454386266661174e-06 mae 0.0010209527544622476
training loss 2.1284706600445217e-06 mae 0.0010383538244808518
training loss 2.145741733263232e-06 mae 0.0010500170706018858
training loss 2.1756599017631978e-06 mae 0.0010569193160082607
Epoch 410, training: loss: 0.0000022, mae: 0.0010556 test: loss0.0001023, mae:0.0072779
training loss 1.9678207081597066e-06 mae 0.0009878972778096795
training loss 2.0844468655709273e-06 mae 0.0010332251492632077
training loss 2.098630419156804e-06 mae 0.0010401875917145076
training loss 2.1119726489808605e-06 mae 0.0010473069003228076
training loss 2.145611974516773e-06 mae 0.001053265557133255
Epoch 411, training: loss: 0.0000022, mae: 0.0010563 test: loss0.0001021, mae:0.0072591
training loss 2.565083605077234e-06 mae 0.0011960901319980621
training loss 2.2769709566020148e-06 mae 0.0010777991938441262
training loss 2.249708719917558e-06 mae 0.0010664170022389975
training loss 2.193287865121956e-06 mae 0.0010568397936861452
training loss 2.158335779093892e-06 mae 0.0010545579293761314
Epoch 412, training: loss: 0.0000022, mae: 0.0010552 test: loss0.0001026, mae:0.0072758
training loss 1.3653731230078847e-06 mae 0.0008664242923259735
training loss 2.11350807906759e-06 mae 0.001049428949972578
training loss 2.1596948457018067e-06 mae 0.0010468153476641318
training loss 2.1595356840395434e-06 mae 0.0010499701656351817
training loss 2.160474550058535e-06 mae 0.0010560241325721336
Epoch 413, training: loss: 0.0000021, mae: 0.0010530 test: loss0.0001035, mae:0.0073034
training loss 1.8534078662924003e-06 mae 0.0010699763661250472
training loss 2.0715683492609036e-06 mae 0.0010411887139757622
training loss 2.141519678051475e-06 mae 0.001056945009622723
training loss 2.1028774550142398e-06 mae 0.001043901891703564
training loss 2.1205592885501155e-06 mae 0.0010491827886843527
Epoch 414, training: loss: 0.0000021, mae: 0.0010535 test: loss0.0001022, mae:0.0072623
training loss 3.0982689622760518e-06 mae 0.0011935107177123427
training loss 1.8959864068985514e-06 mae 0.0010054525063263582
training loss 2.0740215253825595e-06 mae 0.0010377471842388116
training loss 2.0557916532077386e-06 mae 0.0010342709878182032
training loss 2.1330547457607885e-06 mae 0.0010539588402025406
Epoch 415, training: loss: 0.0000021, mae: 0.0010555 test: loss0.0001047, mae:0.0073279
training loss 2.2978863398748217e-06 mae 0.0011712427949532866
training loss 2.0895858717588403e-06 mae 0.0010600756710011732
training loss 2.0851862549678506e-06 mae 0.0010449062839224194
training loss 2.127849292933297e-06 mae 0.0010538676372784746
training loss 2.154543822211624e-06 mae 0.0010604570273411885
Epoch 416, training: loss: 0.0000022, mae: 0.0010599 test: loss0.0001026, mae:0.0072842
training loss 1.9585729660320794e-06 mae 0.0009993513813242316
training loss 2.1095140865844438e-06 mae 0.0010211091390445683
training loss 2.156923860728975e-06 mae 0.001040094713611289
training loss 2.172626423751787e-06 mae 0.0010507716184377134
training loss 2.145287099955029e-06 mae 0.0010543539535152308
Epoch 417, training: loss: 0.0000022, mae: 0.0010591 test: loss0.0001030, mae:0.0072897
training loss 1.947972805282916e-06 mae 0.0010883841896429658
training loss 2.175752941264578e-06 mae 0.0010540804428541483
training loss 2.1787013809957325e-06 mae 0.0010688387896026494
training loss 2.1416211564651384e-06 mae 0.0010640655523967862
training loss 2.1692744200609293e-06 mae 0.0010663490300530116
Epoch 418, training: loss: 0.0000022, mae: 0.0010653 test: loss0.0001025, mae:0.0072783
training loss 1.4298378800958744e-06 mae 0.000864407338667661
training loss 2.1011236680458422e-06 mae 0.0010351873740243416
training loss 2.072004177773465e-06 mae 0.0010304299637324884
training loss 2.130545926278174e-06 mae 0.0010430889712208697
training loss 2.1321809478776495e-06 mae 0.0010470396483465286
Epoch 419, training: loss: 0.0000021, mae: 0.0010477 test: loss0.0001037, mae:0.0073160
training loss 1.738979904075677e-06 mae 0.0010096170008182526
training loss 2.075535915127166e-06 mae 0.0010269252108592614
training loss 2.0318834004899716e-06 mae 0.0010162346015481444
training loss 2.042985059817861e-06 mae 0.0010180963813913185
training loss 2.0760991739861587e-06 mae 0.0010294514011466571
Epoch 420, training: loss: 0.0000021, mae: 0.0010301 test: loss0.0001024, mae:0.0072801
training loss 2.192930196542875e-06 mae 0.001089390367269516
training loss 1.9927411906738762e-06 mae 0.0010100530720699363
training loss 2.0911855994434e-06 mae 0.0010312399705918046
training loss 2.075553169145721e-06 mae 0.0010278821130289349
training loss 2.087160224851886e-06 mae 0.0010342763016343267
Epoch 421, training: loss: 0.0000021, mae: 0.0010309 test: loss0.0001030, mae:0.0072892
training loss 2.2857241219753632e-06 mae 0.0010338962310925126
training loss 2.0560107134538243e-06 mae 0.0010149470968719794
training loss 2.0392175730815485e-06 mae 0.0010170091714682334
training loss 2.072193836024992e-06 mae 0.001032367244205555
training loss 2.064431537382463e-06 mae 0.0010307922392309792
Epoch 422, training: loss: 0.0000021, mae: 0.0010308 test: loss0.0001034, mae:0.0073104
training loss 2.0748857423313893e-06 mae 0.0010036081075668335
training loss 1.997257213053458e-06 mae 0.0010292299397691503
training loss 2.1255644490806707e-06 mae 0.001061770433933465
training loss 2.098829961912708e-06 mae 0.001052702447501264
training loss 2.1140443268721814e-06 mae 0.0010529646727566921
Epoch 423, training: loss: 0.0000021, mae: 0.0010521 test: loss0.0001029, mae:0.0072939
training loss 2.629413529575686e-06 mae 0.0011398358037695289
training loss 1.9261536651499094e-06 mae 0.001007502871182035
training loss 2.0049226085719415e-06 mae 0.0010164141064823262
training loss 2.0647055095157104e-06 mae 0.0010352419482679753
training loss 2.0994656474432278e-06 mae 0.0010441406528622988
Epoch 424, training: loss: 0.0000021, mae: 0.0010426 test: loss0.0001038, mae:0.0073306
training loss 1.1553544254638837e-06 mae 0.0008488427847623825
training loss 2.0240903439776497e-06 mae 0.0010006835207105706
training loss 2.035367943150986e-06 mae 0.001013125973583042
training loss 2.000403793281534e-06 mae 0.0010092914476836506
training loss 2.0473634193723075e-06 mae 0.0010214071582993537
Epoch 425, training: loss: 0.0000020, mae: 0.0010217 test: loss0.0001037, mae:0.0073217
training loss 1.707388378235919e-06 mae 0.000988790183328092
training loss 2.051887845794352e-06 mae 0.0010361286638048942
training loss 2.133783903891617e-06 mae 0.0010511936109264082
training loss 2.1211837082809997e-06 mae 0.0010473931606069515
training loss 2.105395642547365e-06 mae 0.001048440096577844
Epoch 426, training: loss: 0.0000021, mae: 0.0010430 test: loss0.0001032, mae:0.0073040
training loss 3.005302460223902e-06 mae 0.001251228735782206
training loss 2.0799176120496727e-06 mae 0.0010459443255254597
training loss 1.973925458343621e-06 mae 0.0010161716485220823
training loss 1.9933963824239265e-06 mae 0.0010265713682496881
training loss 2.051340217760968e-06 mae 0.001035968796234225
Epoch 427, training: loss: 0.0000021, mae: 0.0010385 test: loss0.0001032, mae:0.0073096
training loss 1.0933541716440232e-06 mae 0.000749990635085851
training loss 1.8754754333427875e-06 mae 0.0009958098195127997
training loss 1.9472278818464493e-06 mae 0.001017514442780636
training loss 2.0049148275237833e-06 mae 0.0010303472244297931
training loss 2.0796701544814325e-06 mae 0.0010452131246707751
Epoch 428, training: loss: 0.0000021, mae: 0.0010451 test: loss0.0001042, mae:0.0073610
training loss 1.3107320455674198e-06 mae 0.0008506014128215611
training loss 2.0438336315576216e-06 mae 0.0010269481549496015
training loss 2.0421868841799754e-06 mae 0.001027645022230278
training loss 2.013737766735976e-06 mae 0.0010196041980363574
training loss 2.0574435420237586e-06 mae 0.0010305669781895566
Epoch 429, training: loss: 0.0000021, mae: 0.0010317 test: loss0.0001034, mae:0.0073136
training loss 1.6243824347839109e-06 mae 0.0009580810437910259
training loss 1.998137464602669e-06 mae 0.00101912221939796
training loss 1.9933978091337328e-06 mae 0.0010143470890488068
training loss 2.029332598334238e-06 mae 0.0010228307758895865
training loss 2.034220318789868e-06 mae 0.0010251298993339403
Epoch 430, training: loss: 0.0000020, mae: 0.0010253 test: loss0.0001040, mae:0.0073334
training loss 1.0894657407334307e-06 mae 0.0007877119933255017
training loss 1.8630158173633103e-06 mae 0.0009778742940968596
training loss 1.909638347070081e-06 mae 0.0009861523273981885
training loss 2.0136555041448298e-06 mae 0.00101167654205385
training loss 2.0186007110798123e-06 mae 0.0010181659176964218
Epoch 431, training: loss: 0.0000020, mae: 0.0010178 test: loss0.0001042, mae:0.0073316
training loss 2.4920379928516923e-06 mae 0.0011773323640227318
training loss 1.9372304847160663e-06 mae 0.0009975159979041886
training loss 1.9826958558915432e-06 mae 0.001003892636987021
training loss 1.9785267862676393e-06 mae 0.0010037833611968136
training loss 1.9974218260548656e-06 mae 0.0010126285030233175
Epoch 432, training: loss: 0.0000020, mae: 0.0010169 test: loss0.0001041, mae:0.0073394
training loss 2.26362794819579e-06 mae 0.0010246442398056388
training loss 2.1396486710157255e-06 mae 0.0010405363270775505
training loss 2.0561307329750562e-06 mae 0.001025463920662936
training loss 2.0255175933100608e-06 mae 0.001020219024357121
training loss 2.0404942756839544e-06 mae 0.00102880120970105
Epoch 433, training: loss: 0.0000020, mae: 0.0010280 test: loss0.0001035, mae:0.0073236
training loss 1.5616948303431855e-06 mae 0.0009303747792728245
training loss 1.9654802205128796e-06 mae 0.0010054628695726538
training loss 1.974075519904158e-06 mae 0.0010069630530635176
training loss 1.977242272944853e-06 mae 0.001008974883585108
training loss 2.0025746901758876e-06 mae 0.0010163785932604137
Epoch 434, training: loss: 0.0000020, mae: 0.0010159 test: loss0.0001038, mae:0.0073202
training loss 2.965507746921503e-06 mae 0.0010208521271124482
training loss 1.9669764262342883e-06 mae 0.001000730399562813
training loss 1.963431997194107e-06 mae 0.0009979899565799918
training loss 1.9979489010685506e-06 mae 0.0010078775438954989
training loss 2.001508848074742e-06 mae 0.0010168517584129425
Epoch 435, training: loss: 0.0000020, mae: 0.0010183 test: loss0.0001034, mae:0.0073114
training loss 2.046229383267928e-06 mae 0.001054033637046814
training loss 1.92165345638654e-06 mae 0.0010062625640820638
training loss 1.8755564587090611e-06 mae 0.0009915090942902874
training loss 1.8865663137239006e-06 mae 0.0009891215067307484
training loss 1.977592925565356e-06 mae 0.001008747215324837
Epoch 436, training: loss: 0.0000020, mae: 0.0010115 test: loss0.0001034, mae:0.0072978
training loss 2.28560452342208e-06 mae 0.001042557298205793
training loss 1.9035601582675328e-06 mae 0.0009738105810338667
training loss 1.933468241185267e-06 mae 0.0009892540269227019
training loss 1.9661095270056125e-06 mae 0.0009987138247191418
training loss 1.9768270659817558e-06 mae 0.0010039974649816714
Epoch 437, training: loss: 0.0000020, mae: 0.0010043 test: loss0.0001036, mae:0.0073195
training loss 1.6195341459024348e-06 mae 0.0009451955556869507
training loss 1.8728724974019258e-06 mae 0.000977150937222748
training loss 1.8940968734658787e-06 mae 0.0009872567768457652
training loss 1.978943352910482e-06 mae 0.0010030870877101481
training loss 2.0069619729915377e-06 mae 0.0010191813581599974
Epoch 438, training: loss: 0.0000020, mae: 0.0010160 test: loss0.0001049, mae:0.0073670
training loss 1.5993545048331725e-06 mae 0.0009742838446982205
training loss 1.951375913770736e-06 mae 0.0010190691716749876
training loss 1.9751873417202896e-06 mae 0.0010170620067349388
training loss 1.9630589726063855e-06 mae 0.0010153567433875321
training loss 1.989539754418248e-06 mae 0.0010165179444277725
Epoch 439, training: loss: 0.0000020, mae: 0.0010164 test: loss0.0001043, mae:0.0073398
training loss 1.425909431418404e-06 mae 0.0009302630205638707
training loss 1.780164292959657e-06 mae 0.0009604137203674398
training loss 1.8099949708884389e-06 mae 0.0009657698885150401
training loss 1.8523579642051302e-06 mae 0.0009719671327651583
training loss 1.9369080016203307e-06 mae 0.0009946557562867766
Epoch 440, training: loss: 0.0000019, mae: 0.0009934 test: loss0.0001047, mae:0.0073517
training loss 2.608273689475027e-06 mae 0.0011700053000822663
training loss 1.8176595551053386e-06 mae 0.0009635342958419786
training loss 1.9094564565718185e-06 mae 0.0009917916492605124
training loss 1.9189786005592514e-06 mae 0.0009932328579910347
training loss 1.9472185731479065e-06 mae 0.0010043599301675769
Epoch 441, training: loss: 0.0000020, mae: 0.0010083 test: loss0.0001041, mae:0.0073443
training loss 2.042930873358273e-06 mae 0.0010138187790289521
training loss 1.8491160276693445e-06 mae 0.0009747538247657024
training loss 1.864982055345122e-06 mae 0.000977901192635556
training loss 1.8648979141518486e-06 mae 0.0009821315380029185
training loss 1.9309127853389264e-06 mae 0.0009981973423386253
Epoch 442, training: loss: 0.0000020, mae: 0.0010021 test: loss0.0001041, mae:0.0073269
training loss 1.4954639482311904e-06 mae 0.0009290541638620198
training loss 1.9630317348285225e-06 mae 0.0009988005987514614
training loss 1.994529321847841e-06 mae 0.0010153392276791213
training loss 1.9515300359430603e-06 mae 0.0010107548915879762
training loss 1.9646903187415975e-06 mae 0.0010133476140769886
Epoch 443, training: loss: 0.0000020, mae: 0.0010123 test: loss0.0001048, mae:0.0073659
training loss 1.3139466545908363e-06 mae 0.000805743329692632
training loss 1.9205051870028235e-06 mae 0.0009784213719232116
training loss 1.8849189547548276e-06 mae 0.0009849768184992194
training loss 1.906900819605822e-06 mae 0.0009935345147606845
training loss 1.9189487759161864e-06 mae 0.000998171492634962
Epoch 444, training: loss: 0.0000019, mae: 0.0009963 test: loss0.0001040, mae:0.0073334
training loss 2.3970544589246856e-06 mae 0.0011496819788590074
training loss 1.9306740488686686e-06 mae 0.0010106624183975448
training loss 1.89921598296741e-06 mae 0.000996416146488915
training loss 1.904408825203967e-06 mae 0.000993694916185583
training loss 1.943040072250488e-06 mae 0.0010019498500759838
Epoch 445, training: loss: 0.0000019, mae: 0.0010035 test: loss0.0001038, mae:0.0073331
training loss 2.4900805328798015e-06 mae 0.0010628175223246217
training loss 1.7859979870277752e-06 mae 0.0009541633965757986
training loss 1.8512542469985007e-06 mae 0.0009762432569139958
training loss 1.9564510456266905e-06 mae 0.0009969653082172697
training loss 1.9221246095266504e-06 mae 0.0009956747537886545
Epoch 446, training: loss: 0.0000019, mae: 0.0009965 test: loss0.0001061, mae:0.0073638
training loss 1.2370102240311098e-06 mae 0.0008593161473982036
training loss 1.784241899882795e-06 mae 0.0009450702391583107
training loss 1.863290938870543e-06 mae 0.0009708352989775337
training loss 1.9015462979173837e-06 mae 0.0009913620637692756
training loss 1.906179604634756e-06 mae 0.0009956116201958402
Epoch 447, training: loss: 0.0000019, mae: 0.0009975 test: loss0.0001047, mae:0.0073651
training loss 1.761594830895774e-06 mae 0.0009246570989489555
training loss 1.9525062741337804e-06 mae 0.0009815619925183114
training loss 1.8575436758222315e-06 mae 0.0009752252821506243
training loss 1.8918650791709184e-06 mae 0.0009862527028435455
training loss 1.91533014083969e-06 mae 0.0009921044062021465
Epoch 448, training: loss: 0.0000019, mae: 0.0009914 test: loss0.0001049, mae:0.0073721
training loss 1.5263170780599467e-06 mae 0.0008773729205131531
training loss 1.725461923824161e-06 mae 0.0009470532942727645
training loss 1.8554539209536318e-06 mae 0.0009748274644855227
training loss 1.8588278550357203e-06 mae 0.0009751166313399396
training loss 1.926736215986538e-06 mae 0.000994851169250535
Epoch 449, training: loss: 0.0000019, mae: 0.0009930 test: loss0.0001050, mae:0.0073854
training loss 3.312812395961373e-06 mae 0.001228680252097547
training loss 1.8818052578853002e-06 mae 0.0009979515407216172
training loss 1.9673598509855028e-06 mae 0.0010115784952546114
training loss 1.9781594900697678e-06 mae 0.0010159142879732203
training loss 1.938376425782033e-06 mae 0.0010092852490059846
Epoch 450, training: loss: 0.0000019, mae: 0.0010112 test: loss0.0001052, mae:0.0073917
training loss 2.2299946067505516e-06 mae 0.0012237693881615996
training loss 1.8833739467677957e-06 mae 0.0009783137528061431
training loss 1.8601102552376766e-06 mae 0.0009693332422192736
training loss 1.9139147151305376e-06 mae 0.0009858379890642665
training loss 1.9233627813603685e-06 mae 0.0009917830121680279
Epoch 451, training: loss: 0.0000019, mae: 0.0009936 test: loss0.0001046, mae:0.0073430
training loss 2.1983523765811697e-06 mae 0.001062437891960144
training loss 1.9510025071230857e-06 mae 0.0010124580277239574
training loss 1.9461585766563995e-06 mae 0.001002227011514772
training loss 1.9347825033814205e-06 mae 0.0010033811632210743
training loss 1.8999896484563944e-06 mae 0.0009945610364950005
Epoch 452, training: loss: 0.0000019, mae: 0.0009981 test: loss0.0001055, mae:0.0073996
training loss 3.4674376365728676e-06 mae 0.001162653206847608
training loss 1.7821568476125212e-06 mae 0.0009612764640455592
training loss 1.855701822419763e-06 mae 0.000972954177707987
training loss 1.8440916797946605e-06 mae 0.0009703341611685687
training loss 1.8742533923804287e-06 mae 0.0009826175840710527
Epoch 453, training: loss: 0.0000019, mae: 0.0009826 test: loss0.0001040, mae:0.0073293
training loss 1.1656755987132783e-06 mae 0.0008327982504852116
training loss 1.937321293749561e-06 mae 0.0009755287787365707
training loss 1.996494018943723e-06 mae 0.0010026385223880262
training loss 1.9354890900410946e-06 mae 0.0009944180873938102
training loss 1.87379518210584e-06 mae 0.0009783748566261057
Epoch 454, training: loss: 0.0000019, mae: 0.0009790 test: loss0.0001048, mae:0.0073736
training loss 1.5479232615689398e-06 mae 0.0009500899468548596
training loss 1.8260712910405882e-06 mae 0.000973657257489714
training loss 1.7898151300230016e-06 mae 0.0009656831003163049
training loss 1.7921698842252493e-06 mae 0.0009635002136427837
training loss 1.8402644676355154e-06 mae 0.0009738694284509741
Epoch 455, training: loss: 0.0000018, mae: 0.0009757 test: loss0.0001053, mae:0.0073671
training loss 1.1386997584850178e-06 mae 0.0007923608645796776
training loss 1.803754980532996e-06 mae 0.0009704309027167218
training loss 1.8718705974822684e-06 mae 0.0009792108811428862
training loss 1.8640477575797756e-06 mae 0.0009809233928993594
training loss 1.8654681051188008e-06 mae 0.0009837410524510665
Epoch 456, training: loss: 0.0000019, mae: 0.0009832 test: loss0.0001053, mae:0.0073894
training loss 1.7190624248542008e-06 mae 0.000933133065700531
training loss 1.839824394620319e-06 mae 0.0009712541542089011
training loss 1.7948321206039514e-06 mae 0.0009566391737541495
training loss 1.8429229271532672e-06 mae 0.0009685961324536541
training loss 1.837603938282027e-06 mae 0.0009680297749872268
Epoch 457, training: loss: 0.0000019, mae: 0.0009714 test: loss0.0001053, mae:0.0073827
training loss 2.0980294266337296e-06 mae 0.0010674201184883714
training loss 1.8260050361348864e-06 mae 0.0009677548557245998
training loss 1.7847991018405276e-06 mae 0.0009641072731280686
training loss 1.7882548193495713e-06 mae 0.0009615236387544912
training loss 1.8315412050279561e-06 mae 0.0009749591135100197
Epoch 458, training: loss: 0.0000018, mae: 0.0009760 test: loss0.0001055, mae:0.0073894
training loss 1.4966077515055076e-06 mae 0.0010131619637832046
training loss 1.8985388112691995e-06 mae 0.000989905709181638
training loss 1.949965544756899e-06 mae 0.0010082451736157997
training loss 1.8553292167647945e-06 mae 0.0009850605128067758
training loss 1.8827897795618536e-06 mae 0.0009877957404818174
Epoch 459, training: loss: 0.0000019, mae: 0.0009875 test: loss0.0001053, mae:0.0073688
training loss 1.2789399761459208e-06 mae 0.0008878819644451141
training loss 1.7773072669364873e-06 mae 0.000955710298263048
training loss 1.7447535616057476e-06 mae 0.0009456590632908047
training loss 1.7596084568224538e-06 mae 0.0009513399748760738
training loss 1.8283963830375185e-06 mae 0.0009692213796333998
Epoch 460, training: loss: 0.0000018, mae: 0.0009749 test: loss0.0001051, mae:0.0073669
training loss 1.7784583405955345e-06 mae 0.0009737368673086166
training loss 1.8109432254081472e-06 mae 0.0009618505011495275
training loss 1.8960725621654282e-06 mae 0.000985432672660134
training loss 1.8539402418017871e-06 mae 0.000977585878954054
training loss 1.8303205486029296e-06 mae 0.0009750213705128368
Epoch 461, training: loss: 0.0000018, mae: 0.0009785 test: loss0.0001048, mae:0.0073683
training loss 1.2910658142573084e-06 mae 0.0008528620819561183
training loss 1.8417969180982073e-06 mae 0.0009851513074382264
training loss 1.8417054880136687e-06 mae 0.0009755999619823579
training loss 1.8307541846833743e-06 mae 0.0009740809617033665
training loss 1.8524066229189494e-06 mae 0.0009811265525565634
Epoch 462, training: loss: 0.0000019, mae: 0.0009816 test: loss0.0001058, mae:0.0073988
training loss 1.2173344430266297e-06 mae 0.0008756760507822037
training loss 1.569013604426693e-06 mae 0.0008835864114100296
training loss 1.6387570627053305e-06 mae 0.0009059790201429833
training loss 1.712782045841181e-06 mae 0.0009364469323017397
training loss 1.7856406946326456e-06 mae 0.0009579854262691566
Epoch 463, training: loss: 0.0000018, mae: 0.0009624 test: loss0.0001061, mae:0.0074050
training loss 2.554015736677684e-06 mae 0.0011168814962729812
training loss 1.6559592181158006e-06 mae 0.0009307481711456444
training loss 1.7385467425909983e-06 mae 0.0009442965158879168
training loss 1.800156889766372e-06 mae 0.0009590672558044014
training loss 1.824360229373052e-06 mae 0.0009665358310625585
Epoch 464, training: loss: 0.0000018, mae: 0.0009688 test: loss0.0001065, mae:0.0074115
training loss 2.3246566343004815e-06 mae 0.0010771149536594748
training loss 1.6632228574267437e-06 mae 0.0009338559340868217
training loss 1.735840502835056e-06 mae 0.0009490143430493035
training loss 1.782103817463665e-06 mae 0.0009602197096683086
training loss 1.8206464237076244e-06 mae 0.0009726277171444169
Epoch 465, training: loss: 0.0000018, mae: 0.0009711 test: loss0.0001052, mae:0.0073689
training loss 1.0887234793699463e-06 mae 0.0007340808515436947
training loss 1.7802858843763306e-06 mae 0.0009559178690645185
training loss 1.8411949896506142e-06 mae 0.0009662837428640167
training loss 1.821418547948775e-06 mae 0.0009638278472194117
training loss 1.7925918537591103e-06 mae 0.0009588761740498506
Epoch 466, training: loss: 0.0000018, mae: 0.0009618 test: loss0.0001056, mae:0.0073898
training loss 1.5262986607922358e-06 mae 0.0009634022717364132
training loss 1.795379987737867e-06 mae 0.0009743760501984137
training loss 1.688371547529506e-06 mae 0.0009354898898567231
training loss 1.7601832657557968e-06 mae 0.0009536830957405369
training loss 1.7843803549967832e-06 mae 0.000961572610349186
Epoch 467, training: loss: 0.0000018, mae: 0.0009639 test: loss0.0001062, mae:0.0074143
training loss 2.1396785996330436e-06 mae 0.0010566472774371505
training loss 1.72115288282215e-06 mae 0.0009539758012739613
training loss 1.756694461623693e-06 mae 0.0009567625595874498
training loss 1.7507639668020299e-06 mae 0.0009577681907199771
training loss 1.7858619450558242e-06 mae 0.0009681561522171909
Epoch 468, training: loss: 0.0000018, mae: 0.0009703 test: loss0.0001059, mae:0.0074016
training loss 1.2069822332705371e-06 mae 0.0008789387647993863
training loss 1.687032009862841e-06 mae 0.0009305201432046788
training loss 1.7127638009039607e-06 mae 0.0009372903646135902
training loss 1.7635884146721275e-06 mae 0.0009500023763201241
training loss 1.7687040167062936e-06 mae 0.0009548840743132782
Epoch 469, training: loss: 0.0000018, mae: 0.0009592 test: loss0.0001054, mae:0.0073971
training loss 1.310691573053191e-06 mae 0.000902002677321434
training loss 1.6342020172469704e-06 mae 0.0009340539793757831
training loss 1.779558977202691e-06 mae 0.0009624581815130862
training loss 1.7670848017930873e-06 mae 0.0009575254717126293
training loss 1.7830881182844542e-06 mae 0.0009615669836086311
Epoch 470, training: loss: 0.0000018, mae: 0.0009666 test: loss0.0001061, mae:0.0074174
training loss 2.9866714612580836e-06 mae 0.001202157698571682
training loss 1.8754909961792102e-06 mae 0.0009745003315437511
training loss 1.8982081772913143e-06 mae 0.0009844905655314715
training loss 1.8139413224698186e-06 mae 0.0009716160927701809
training loss 1.824387631860186e-06 mae 0.0009721498177687987
Epoch 471, training: loss: 0.0000018, mae: 0.0009711 test: loss0.0001065, mae:0.0074248
training loss 1.8557466319180094e-06 mae 0.0008686268702149391
training loss 1.83204421470388e-06 mae 0.0009587374002234463
training loss 1.727395975662922e-06 mae 0.0009329547208853733
training loss 1.731760484958703e-06 mae 0.0009363933720021503
training loss 1.7785426357092645e-06 mae 0.0009552353760806742
Epoch 472, training: loss: 0.0000018, mae: 0.0009595 test: loss0.0001050, mae:0.0073723
training loss 1.314237920269079e-06 mae 0.0007674666121602058
training loss 1.8038222285266624e-06 mae 0.0009572051383335799
training loss 1.765605416062274e-06 mae 0.0009560302876185001
training loss 1.7552769020990154e-06 mae 0.00095509645946953
training loss 1.7548492291668364e-06 mae 0.0009551932435802451
Epoch 473, training: loss: 0.0000018, mae: 0.0009588 test: loss0.0001087, mae:0.0074403
training loss 9.812524695007596e-07 mae 0.0007062402437441051
training loss 1.5423385706544289e-06 mae 0.0008843467461273949
training loss 1.6304470591421698e-06 mae 0.0009137885743005221
training loss 1.700284441606388e-06 mae 0.0009352940305086804
training loss 1.7422295108285176e-06 mae 0.0009485551544733864
Epoch 474, training: loss: 0.0000018, mae: 0.0009525 test: loss0.0001061, mae:0.0074114
training loss 2.742131300692563e-06 mae 0.0010419556638225913
training loss 1.7673919980516787e-06 mae 0.0009544255102381987
training loss 1.743067660655551e-06 mae 0.0009346687457395143
training loss 1.726725270823438e-06 mae 0.0009363847283572451
training loss 1.7295301173889673e-06 mae 0.0009413540692865588
Epoch 475, training: loss: 0.0000017, mae: 0.0009398 test: loss0.0001054, mae:0.0073899
training loss 1.2225931413922808e-06 mae 0.0008248997037298977
training loss 1.7313065975887344e-06 mae 0.0009288949910186084
training loss 1.7184134683045364e-06 mae 0.0009356697209843313
training loss 1.706463313707629e-06 mae 0.0009350402277604433
training loss 1.7183025632411005e-06 mae 0.000942161378846851
Epoch 476, training: loss: 0.0000017, mae: 0.0009426 test: loss0.0001054, mae:0.0073826
training loss 1.3432535297397408e-06 mae 0.0009070113301277161
training loss 1.7606079488948174e-06 mae 0.000938089660314076
training loss 1.7780201723875467e-06 mae 0.0009464319036941569
training loss 1.7316211331291426e-06 mae 0.0009385411479565874
training loss 1.7274567770172733e-06 mae 0.0009419376210689508
Epoch 477, training: loss: 0.0000017, mae: 0.0009425 test: loss0.0001061, mae:0.0074101
training loss 1.409294441145903e-06 mae 0.0008879431406967342
training loss 1.8387647541577975e-06 mae 0.0009722926918709394
training loss 1.7941038984393135e-06 mae 0.0009695588176525979
training loss 1.761599368581675e-06 mae 0.0009634551042475003
training loss 1.7842331175329923e-06 mae 0.0009666412161774378
Epoch 478, training: loss: 0.0000018, mae: 0.0009650 test: loss0.0001062, mae:0.0074192
training loss 1.7230839830517652e-06 mae 0.0008942869608290493
training loss 1.5939292184692232e-06 mae 0.0008993805739937315
training loss 1.7230973907821366e-06 mae 0.000931711661131444
training loss 1.7312650245442867e-06 mae 0.0009421559150133356
training loss 1.7328059729337827e-06 mae 0.000945493570512707
Epoch 479, training: loss: 0.0000017, mae: 0.0009465 test: loss0.0001058, mae:0.0073932
training loss 4.076924142282223e-06 mae 0.0012000709539279342
training loss 1.7310285117816297e-06 mae 0.0009373732746633536
training loss 1.6442955747701456e-06 mae 0.0009164103804764774
training loss 1.6362308559302545e-06 mae 0.0009174318644927434
training loss 1.6656210147598519e-06 mae 0.0009226875622358304
Epoch 480, training: loss: 0.0000017, mae: 0.0009294 test: loss0.0001058, mae:0.0073986
training loss 1.8925442191175534e-06 mae 0.0010284557938575745
training loss 1.6401790559279563e-06 mae 0.0009299572465904786
training loss 1.6999123818770903e-06 mae 0.000937221741815568
training loss 1.6983699902686798e-06 mae 0.0009352238077843035
training loss 1.7111212977130403e-06 mae 0.000939407579042481
Epoch 481, training: loss: 0.0000017, mae: 0.0009394 test: loss0.0001062, mae:0.0074110
training loss 1.3951022310720873e-06 mae 0.0009175551240332425
training loss 1.6358698120729607e-06 mae 0.000921871490301747
training loss 1.566024155694646e-06 mae 0.0008967147579896125
training loss 1.667202634161372e-06 mae 0.0009251741899812706
training loss 1.6913734285069276e-06 mae 0.000930889525835342
Epoch 482, training: loss: 0.0000017, mae: 0.0009330 test: loss0.0001068, mae:0.0074211
training loss 1.0296929531250498e-06 mae 0.000808586657512933
training loss 1.6196275340667388e-06 mae 0.0009192485903280185
training loss 1.658424984685049e-06 mae 0.0009375162501035648
training loss 1.7325575158973773e-06 mae 0.0009525178099785076
training loss 1.7418546429040885e-06 mae 0.0009540304314548994
Epoch 483, training: loss: 0.0000018, mae: 0.0009566 test: loss0.0001053, mae:0.0073699
training loss 1.1453886372692068e-06 mae 0.0008387221023440361
training loss 1.675898223159796e-06 mae 0.0009325898695262332
training loss 1.7157855863572846e-06 mae 0.0009400047434466236
training loss 1.680667752912056e-06 mae 0.0009312662750864956
training loss 1.7112876099617012e-06 mae 0.0009383800015576293
Epoch 484, training: loss: 0.0000017, mae: 0.0009431 test: loss0.0001062, mae:0.0073994
training loss 1.6509540046172333e-06 mae 0.0009375943918712437
training loss 1.6017697460341092e-06 mae 0.0009236709379097992
training loss 1.6950023841177832e-06 mae 0.0009330215219565033
training loss 1.7164382981134277e-06 mae 0.0009433263365413674
training loss 1.6982038660690837e-06 mae 0.0009415612254287138
Epoch 485, training: loss: 0.0000017, mae: 0.0009458 test: loss0.0001061, mae:0.0074087
training loss 1.2923243275508867e-06 mae 0.0008170781657099724
training loss 1.7704412417675914e-06 mae 0.0009471561723187858
training loss 1.7147092787334006e-06 mae 0.0009406082687143345
training loss 1.652519971164249e-06 mae 0.0009244147415229716
training loss 1.692076856482829e-06 mae 0.0009364791180294434
Epoch 486, training: loss: 0.0000017, mae: 0.0009364 test: loss0.0001060, mae:0.0074001
training loss 1.0722021670517279e-06 mae 0.0007451968267560005
training loss 1.5734989748285868e-06 mae 0.0008996486271201981
training loss 1.602470352637365e-06 mae 0.0009085372889110139
training loss 1.6348953849293616e-06 mae 0.0009200866457494285
training loss 1.7009894961925576e-06 mae 0.0009370223492782437
Epoch 487, training: loss: 0.0000017, mae: 0.0009346 test: loss0.0001062, mae:0.0074122
training loss 1.1415970675443532e-06 mae 0.000710787542629987
training loss 1.5985919091298116e-06 mae 0.0009045833950022273
training loss 1.619606413023215e-06 mae 0.0009099990383048755
training loss 1.6348629792868065e-06 mae 0.0009176105253910339
training loss 1.6407411520171378e-06 mae 0.0009150673506146675
Epoch 488, training: loss: 0.0000017, mae: 0.0009204 test: loss0.0001069, mae:0.0074437
training loss 1.8615361341289827e-06 mae 0.0009569280664436519
training loss 1.7454189504610885e-06 mae 0.0009460301565316815
training loss 1.6849914380478673e-06 mae 0.0009305269888482323
training loss 1.6853265009895824e-06 mae 0.0009338100335178338
training loss 1.6984608254714308e-06 mae 0.0009381209525617937
Epoch 489, training: loss: 0.0000017, mae: 0.0009387 test: loss0.0001069, mae:0.0074444
training loss 1.407043441759015e-06 mae 0.0008553136140108109
training loss 1.7219844686027785e-06 mae 0.0009372828439261547
training loss 1.696280424532755e-06 mae 0.0009272834487803431
training loss 1.664856401719782e-06 mae 0.0009273010292953955
training loss 1.6514918284833837e-06 mae 0.0009240461380188519
Epoch 490, training: loss: 0.0000016, mae: 0.0009238 test: loss0.0001066, mae:0.0074235
training loss 1.7482352632214315e-06 mae 0.0009511944954283535
training loss 1.5819266458689235e-06 mae 0.000896705257450687
training loss 1.6142797158584534e-06 mae 0.0009029399849114813
training loss 1.6145362379900337e-06 mae 0.0009081489321720227
training loss 1.630840755812791e-06 mae 0.000914884904638137
Epoch 491, training: loss: 0.0000016, mae: 0.0009184 test: loss0.0001066, mae:0.0074314
training loss 1.8424480003886856e-06 mae 0.00102894043084234
training loss 1.5421479969625262e-06 mae 0.000891651091960204
training loss 1.610603243926283e-06 mae 0.0009066889484385303
training loss 1.614264168070828e-06 mae 0.0009094425118589981
training loss 1.6179834848678107e-06 mae 0.000912149808895473
Epoch 492, training: loss: 0.0000016, mae: 0.0009146 test: loss0.0001068, mae:0.0074435
training loss 1.5308550018744427e-06 mae 0.0009198331390507519
training loss 1.6568522416010265e-06 mae 0.0008919985003897227
training loss 1.5784091249428238e-06 mae 0.0008860088015749754
training loss 1.595495332472541e-06 mae 0.0009001042043932079
training loss 1.6068017749372548e-06 mae 0.0009072286230905806
Epoch 493, training: loss: 0.0000016, mae: 0.0009052 test: loss0.0001068, mae:0.0074385
training loss 1.2628511285583954e-06 mae 0.0008086422458291054
training loss 1.5341596285817979e-06 mae 0.000879575161710747
training loss 1.5386793926730814e-06 mae 0.0008843573908745728
training loss 1.5978639092715484e-06 mae 0.000900707310929105
training loss 1.6311293000770062e-06 mae 0.0009139760296597187
Epoch 494, training: loss: 0.0000016, mae: 0.0009169 test: loss0.0001079, mae:0.0074668
training loss 1.395509912072157e-06 mae 0.0008487629820592701
training loss 1.5833536898682331e-06 mae 0.0009120067150112899
training loss 1.6382041000571632e-06 mae 0.00091861866732672
training loss 1.6481950193121147e-06 mae 0.0009240676014412762
training loss 1.6473910913625897e-06 mae 0.0009221280574571543
Epoch 495, training: loss: 0.0000016, mae: 0.0009237 test: loss0.0001067, mae:0.0074145
training loss 1.8982567553393892e-06 mae 0.0009895615512505174
training loss 1.4748664456169985e-06 mae 0.0008765950584418926
training loss 1.5505146973169981e-06 mae 0.000890827087667024
training loss 1.5924181026991173e-06 mae 0.0009113240463578989
training loss 1.6516899716325908e-06 mae 0.0009273546426418002
Epoch 496, training: loss: 0.0000017, mae: 0.0009321 test: loss0.0001064, mae:0.0074194
training loss 1.387184624945803e-06 mae 0.00089296541409567
training loss 1.4619433618374352e-06 mae 0.0008842135053675841
training loss 1.518091465024356e-06 mae 0.0008907714376853097
training loss 1.6171233777345602e-06 mae 0.0009144142725726973
training loss 1.6407454438366634e-06 mae 0.0009215583431705908
Epoch 497, training: loss: 0.0000016, mae: 0.0009221 test: loss0.0001065, mae:0.0074244
training loss 1.5187162034635548e-06 mae 0.0009050006046891212
training loss 1.4380154980512194e-06 mae 0.0008665873468214392
training loss 1.5839432336124989e-06 mae 0.0009034498413738862
training loss 1.6212180072059538e-06 mae 0.0009145281194333015
training loss 1.6197408362090956e-06 mae 0.0009183422812904741
Epoch 498, training: loss: 0.0000016, mae: 0.0009181 test: loss0.0001070, mae:0.0074429
training loss 1.995729689951986e-06 mae 0.0010600151726976037
training loss 1.6572695636907658e-06 mae 0.0009218385778641438
training loss 1.6211713866348624e-06 mae 0.0009179317904175212
training loss 1.631615303580812e-06 mae 0.0009179150073523846
training loss 1.598493187615513e-06 mae 0.0009128956778081531
Epoch 499, training: loss: 0.0000016, mae: 0.0009201 test: loss0.0001072, mae:0.0074524
current learning rate: 1.5625e-05
training loss 1.3525332178687677e-06 mae 0.0008036801591515541
training loss 1.521596565382032e-06 mae 0.0008679787781290419
training loss 1.4640180704263193e-06 mae 0.0008449903631693511
training loss 1.4471024931463155e-06 mae 0.0008378638762561298
training loss 1.4310316063149087e-06 mae 0.0008332342545348987
Epoch 500, training: loss: 0.0000014, mae: 0.0008312 test: loss0.0001074, mae:0.0074365
training loss 2.0361808310553897e-06 mae 0.0010698838159441948
training loss 1.3125254509741712e-06 mae 0.0007857230843464826
training loss 1.3475295576323463e-06 mae 0.0007915112076414806
training loss 1.3343202580162083e-06 mae 0.0007940637881001026
training loss 1.3729363145182156e-06 mae 0.0008012748064776759
Epoch 501, training: loss: 0.0000014, mae: 0.0008029 test: loss0.0001069, mae:0.0074462
training loss 1.0751119816632126e-06 mae 0.0007126191630959511
training loss 1.2252431799077065e-06 mae 0.0007668135963909914
training loss 1.355948096105938e-06 mae 0.0007936231328617082
training loss 1.3675835759088107e-06 mae 0.0008010701133733901
training loss 1.3803246090096246e-06 mae 0.00080656048561451
Epoch 502, training: loss: 0.0000014, mae: 0.0008077 test: loss0.0001069, mae:0.0074265
training loss 1.084769678527664e-06 mae 0.0007815987919457257
training loss 1.3434275029241836e-06 mae 0.0008038506140092424
training loss 1.3816389106509767e-06 mae 0.0008034590755054486
training loss 1.3743191199400887e-06 mae 0.0008001015166943238
training loss 1.3756540898607483e-06 mae 0.0008055662961597947
Epoch 503, training: loss: 0.0000014, mae: 0.0008077 test: loss0.0001074, mae:0.0074564
training loss 2.0525010313576786e-06 mae 0.0009531133691780269
training loss 1.4076009631257783e-06 mae 0.0008011211951573688
training loss 1.3627298252379995e-06 mae 0.0007940204381841313
training loss 1.3647857595843498e-06 mae 0.0007997449515973344
training loss 1.3848108706404684e-06 mae 0.0008052049519781447
Epoch 504, training: loss: 0.0000014, mae: 0.0008030 test: loss0.0001072, mae:0.0074541
training loss 1.7557462115291855e-06 mae 0.0008968950132839382
training loss 1.3272617584512845e-06 mae 0.0007771518459205754
training loss 1.344424178866332e-06 mae 0.0007893407015973645
training loss 1.357691700910521e-06 mae 0.0007969548936165397
training loss 1.3542711542130799e-06 mae 0.0007985206632191472
Epoch 505, training: loss: 0.0000014, mae: 0.0008040 test: loss0.0001069, mae:0.0074292
training loss 1.4123476148597547e-06 mae 0.0008201412856578827
training loss 1.3212581393418073e-06 mae 0.000788996371614071
training loss 1.323178987649503e-06 mae 0.0007946701019559757
training loss 1.3500469940150433e-06 mae 0.0007967695999360141
training loss 1.368235746269591e-06 mae 0.0008035013411629628
Epoch 506, training: loss: 0.0000014, mae: 0.0008070 test: loss0.0001070, mae:0.0074472
training loss 1.4971087693993468e-06 mae 0.0007850537076592445
training loss 1.2983686220275773e-06 mae 0.0007773581401481494
training loss 1.3373505494921191e-06 mae 0.0007881053560874473
training loss 1.3583912529147768e-06 mae 0.0008003438089739851
training loss 1.3776170022759998e-06 mae 0.0008049111744384304
Epoch 507, training: loss: 0.0000014, mae: 0.0008051 test: loss0.0001078, mae:0.0074699
training loss 1.4469218285739771e-06 mae 0.0007711012731306255
training loss 1.2920065137668821e-06 mae 0.0007894725764772912
training loss 1.2972652909033203e-06 mae 0.0007924996322196089
training loss 1.3547059692780221e-06 mae 0.0008049308027862416
training loss 1.3787534887276588e-06 mae 0.0008120785321952282
Epoch 508, training: loss: 0.0000014, mae: 0.0008128 test: loss0.0001075, mae:0.0074679
training loss 1.4766607137062238e-06 mae 0.0009874152019619942
training loss 1.3228862084200523e-06 mae 0.0007991116555571995
training loss 1.3398852374983567e-06 mae 0.000803414539152775
training loss 1.330597311285323e-06 mae 0.0008010460658998085
training loss 1.3610519507851879e-06 mae 0.0008061425126185502
Epoch 509, training: loss: 0.0000014, mae: 0.0008077 test: loss0.0001074, mae:0.0074562
training loss 7.548700864390412e-07 mae 0.0006549786776304245
training loss 1.2654387368385475e-06 mae 0.0007744251805193282
training loss 1.2923659369334924e-06 mae 0.0007894779987734659
training loss 1.3214353650755283e-06 mae 0.000794545376892294
training loss 1.3860847684215388e-06 mae 0.0008077885618488961
Epoch 510, training: loss: 0.0000014, mae: 0.0008065 test: loss0.0001071, mae:0.0074471
training loss 5.314935265232634e-07 mae 0.0005803592503070831
training loss 1.2671990103341407e-06 mae 0.0007687516421463121
training loss 1.3132929575251614e-06 mae 0.000790574756741413
training loss 1.3393343024953063e-06 mae 0.0007946295929696437
training loss 1.3537332941481343e-06 mae 0.000800899193455368
Epoch 511, training: loss: 0.0000014, mae: 0.0008057 test: loss0.0001073, mae:0.0074545
training loss 1.695651462796377e-06 mae 0.0008378864149563015
training loss 1.245667622733177e-06 mae 0.0007705462516705487
training loss 1.304567698154264e-06 mae 0.0007919298725820487
training loss 1.3433985433243986e-06 mae 0.0007990501934372577
training loss 1.3692415990504264e-06 mae 0.0008071817924495586
Epoch 512, training: loss: 0.0000014, mae: 0.0008068 test: loss0.0001073, mae:0.0074550
training loss 1.2077007340849377e-06 mae 0.0007939739152789116
training loss 1.3169159458089673e-06 mae 0.0007962417005853472
training loss 1.3882079066647883e-06 mae 0.0008118482932476051
training loss 1.3909420991598423e-06 mae 0.0008089900450836496
training loss 1.3798824401740817e-06 mae 0.0008097040931467169
Epoch 513, training: loss: 0.0000014, mae: 0.0008086 test: loss0.0001069, mae:0.0074281
training loss 2.4831069822539575e-06 mae 0.0010042475769296288
training loss 1.2273079385201581e-06 mae 0.0007625432027613414
training loss 1.2624984278075018e-06 mae 0.0007755923010570243
training loss 1.3332534982790045e-06 mae 0.0007928796816040571
training loss 1.3590114698688388e-06 mae 0.000797011418788189
Epoch 514, training: loss: 0.0000014, mae: 0.0007990 test: loss0.0001082, mae:0.0074862
training loss 1.0494228490642854e-06 mae 0.0007496497710235417
training loss 1.2753028321568826e-06 mae 0.000797388994125832
training loss 1.3395217851806103e-06 mae 0.0008032113229533159
training loss 1.3197396470253724e-06 mae 0.0007981256734493453
training loss 1.3641955184025683e-06 mae 0.0008091455897714816
Epoch 515, training: loss: 0.0000014, mae: 0.0008072 test: loss0.0001074, mae:0.0074562
training loss 1.7659530158198322e-06 mae 0.0009042620658874512
training loss 1.308452704720634e-06 mae 0.0007886489312274052
training loss 1.3165363636496955e-06 mae 0.0007922379271182608
training loss 1.347045474405291e-06 mae 0.0008006419648033942
training loss 1.3760839081202284e-06 mae 0.0008069345186318534
Epoch 516, training: loss: 0.0000014, mae: 0.0008042 test: loss0.0001075, mae:0.0074660
training loss 9.143739134742646e-07 mae 0.000665475323330611
training loss 1.2870705344379105e-06 mae 0.0007819204474799336
training loss 1.2692407132349359e-06 mae 0.0007722620629834585
training loss 1.3159739901823673e-06 mae 0.0007855703287899792
training loss 1.349728008722535e-06 mae 0.0007966743893365362
Epoch 517, training: loss: 0.0000013, mae: 0.0007933 test: loss0.0001076, mae:0.0074740
training loss 7.220619409054052e-07 mae 0.0006323866546154022
training loss 1.4024680391836828e-06 mae 0.0007998331863086158
training loss 1.3354494551166867e-06 mae 0.0007934438959132916
training loss 1.346430758568559e-06 mae 0.0007963547855461424
training loss 1.3501899085874913e-06 mae 0.0007972848006241505
Epoch 518, training: loss: 0.0000013, mae: 0.0007972 test: loss0.0001082, mae:0.0074845
training loss 1.0281806908096769e-06 mae 0.0007794781704433262
training loss 1.3193395803308005e-06 mae 0.0007760925743473217
training loss 1.3479958382877968e-06 mae 0.000795053098567457
training loss 1.350211363348493e-06 mae 0.0007942519690348019
training loss 1.3510683196958533e-06 mae 0.0007968686497306904
Epoch 519, training: loss: 0.0000013, mae: 0.0007959 test: loss0.0001087, mae:0.0075041
training loss 1.0409353308205027e-06 mae 0.0007031308487057686
training loss 1.203663931819927e-06 mae 0.0007617412723929565
training loss 1.2956292253100869e-06 mae 0.0007776807103822429
training loss 1.351851846347385e-06 mae 0.0007941237610721625
training loss 1.3389485352671739e-06 mae 0.0007928398464440334
Epoch 520, training: loss: 0.0000013, mae: 0.0007934 test: loss0.0001077, mae:0.0074741
training loss 1.0645946986187482e-06 mae 0.0006872735102660954
training loss 1.1545828574169574e-06 mae 0.0007539002895903062
training loss 1.3217035629649852e-06 mae 0.0007903150935187051
training loss 1.3108845461079445e-06 mae 0.0007909271412757252
training loss 1.3503807514580188e-06 mae 0.0007990742752685284
Epoch 521, training: loss: 0.0000013, mae: 0.0007993 test: loss0.0001075, mae:0.0074677
training loss 1.5141469020818477e-06 mae 0.0008588529308326542
training loss 1.3458258382236049e-06 mae 0.0007946823641438696
training loss 1.3107091314787517e-06 mae 0.0007898675521033459
training loss 1.3198784836879219e-06 mae 0.0007911019643232079
training loss 1.3330544558712558e-06 mae 0.0007965037621214838
Epoch 522, training: loss: 0.0000013, mae: 0.0007965 test: loss0.0001082, mae:0.0074853
training loss 1.8474424905434716e-06 mae 0.0010131197050213814
training loss 1.4027015363442882e-06 mae 0.0008054943024819973
training loss 1.3617609995142827e-06 mae 0.0007917805664739252
training loss 1.3462567490053618e-06 mae 0.0007917910959997172
training loss 1.3402624754189904e-06 mae 0.000797789459885909
Epoch 523, training: loss: 0.0000013, mae: 0.0007999 test: loss0.0001076, mae:0.0074654
training loss 2.2866220206196886e-06 mae 0.0010142145911231637
training loss 1.2244664017713403e-06 mae 0.0007640973256225242
training loss 1.293976420174208e-06 mae 0.0007833000543379767
training loss 1.2805445815363494e-06 mae 0.000782087495757131
training loss 1.3363137506091704e-06 mae 0.0007949212443810973
Epoch 524, training: loss: 0.0000013, mae: 0.0007938 test: loss0.0001080, mae:0.0074874
training loss 8.617155344836647e-07 mae 0.0006696935743093491
training loss 1.3478511603210008e-06 mae 0.0007954808819454677
training loss 1.3351810449952758e-06 mae 0.000792305596693797
training loss 1.3589946604873977e-06 mae 0.0008001151018227075
training loss 1.3420432954565813e-06 mae 0.0007946941050566475
Epoch 525, training: loss: 0.0000013, mae: 0.0007921 test: loss0.0001130, mae:0.0075259
training loss 6.990543965912366e-07 mae 0.0006304001435637474
training loss 1.2122321298228796e-06 mae 0.0007617308920267605
training loss 1.2841287638600467e-06 mae 0.0007807601353909709
training loss 1.29199874576419e-06 mae 0.0007836734980116567
training loss 1.3205900394093275e-06 mae 0.0007909837043357649
Epoch 526, training: loss: 0.0000013, mae: 0.0007917 test: loss0.0001078, mae:0.0074601
training loss 5.580185415965389e-07 mae 0.0005395908956415951
training loss 1.3591225805451836e-06 mae 0.0008040576540938048
training loss 1.318851848237041e-06 mae 0.0007907241360413479
training loss 1.343911573625495e-06 mae 0.0007944965560840359
training loss 1.330701847034934e-06 mae 0.0007918957566995343
Epoch 527, training: loss: 0.0000013, mae: 0.0007911 test: loss0.0001080, mae:0.0074751
training loss 1.3361196806727094e-06 mae 0.0008499420364387333
training loss 1.2011395401090096e-06 mae 0.0007674969737326689
training loss 1.2924332356017816e-06 mae 0.0007854613451296372
training loss 1.3023798545638535e-06 mae 0.0007870127597287152
training loss 1.3096112027922766e-06 mae 0.0007902022725693994
Epoch 528, training: loss: 0.0000013, mae: 0.0007904 test: loss0.0001081, mae:0.0074808
training loss 1.3299313650350086e-06 mae 0.0007951008155941963
training loss 1.2592193690900735e-06 mae 0.000768013870316174
training loss 1.294430830967031e-06 mae 0.0007738882599693563
training loss 1.3179541615469946e-06 mae 0.0007841451731911358
training loss 1.3031721697817869e-06 mae 0.000784914313632513
Epoch 529, training: loss: 0.0000013, mae: 0.0007874 test: loss0.0001080, mae:0.0074645
training loss 5.461743057821877e-07 mae 0.0005345151876099408
training loss 1.1538603953964664e-06 mae 0.0007511373385604398
training loss 1.2135807839225141e-06 mae 0.0007669654862706247
training loss 1.2735255147663048e-06 mae 0.0007790897472527202
training loss 1.3129019370312776e-06 mae 0.000791203061788255
Epoch 530, training: loss: 0.0000013, mae: 0.0007947 test: loss0.0001084, mae:0.0075003
training loss 1.2596073020176846e-06 mae 0.0008061357657425106
training loss 1.2983633445062383e-06 mae 0.0007709707635655707
training loss 1.3468044553834755e-06 mae 0.0007873435993448342
training loss 1.315222610532183e-06 mae 0.0007822078283388932
training loss 1.3217970536966308e-06 mae 0.0007896923161209075
Epoch 531, training: loss: 0.0000013, mae: 0.0007891 test: loss0.0001081, mae:0.0074871
training loss 1.2358965477687889e-06 mae 0.0007868490065447986
training loss 1.3653340448292835e-06 mae 0.0008020930131440799
training loss 1.3506448305300788e-06 mae 0.000795791330770359
training loss 1.319292020347926e-06 mae 0.0007876281226393057
training loss 1.3172761920988927e-06 mae 0.0007902521217839596
Epoch 532, training: loss: 0.0000013, mae: 0.0007885 test: loss0.0001077, mae:0.0074654
training loss 1.8709947653405834e-06 mae 0.0008608025382272899
training loss 1.2568643618507455e-06 mae 0.0007515880701533868
training loss 1.257888491260595e-06 mae 0.000758843365042369
training loss 1.2872411833083504e-06 mae 0.0007770212412663793
training loss 1.3045292222731733e-06 mae 0.0007828430629414113
Epoch 533, training: loss: 0.0000013, mae: 0.0007832 test: loss0.0001081, mae:0.0074769
training loss 7.592253155053186e-07 mae 0.0006623932276852429
training loss 1.2091557065934248e-06 mae 0.0007599074868302719
training loss 1.3044674581619392e-06 mae 0.000785374783422097
training loss 1.2957457030046227e-06 mae 0.0007835793800787718
training loss 1.3120954334133791e-06 mae 0.0007875646477268632
Epoch 534, training: loss: 0.0000013, mae: 0.0007872 test: loss0.0001081, mae:0.0074902
training loss 9.392995252710534e-07 mae 0.0007586733554489911
training loss 1.2882661621915756e-06 mae 0.0007597614164171995
training loss 1.3063898437745718e-06 mae 0.0007683235513954654
training loss 1.2856233302015326e-06 mae 0.0007696051271215994
training loss 1.2991324527985257e-06 mae 0.0007787795463143686
Epoch 535, training: loss: 0.0000013, mae: 0.0007778 test: loss0.0001085, mae:0.0074794
training loss 1.6143989114425494e-06 mae 0.0009284618427045643
training loss 1.3162643602870458e-06 mae 0.0007876513843132438
training loss 1.3169729551563175e-06 mae 0.0007901657009635601
training loss 1.3142873851486595e-06 mae 0.0007877681945567791
training loss 1.2966258756437336e-06 mae 0.0007821112788347208
Epoch 536, training: loss: 0.0000013, mae: 0.0007827 test: loss0.0001084, mae:0.0074945
training loss 1.388919031342084e-06 mae 0.0008000393281690776
training loss 1.2630687140200262e-06 mae 0.0007587783654932589
training loss 1.242035566092184e-06 mae 0.0007588845806768034
training loss 1.2653048345885136e-06 mae 0.0007724210215942156
training loss 1.2813406994012705e-06 mae 0.0007778335022693732
Epoch 537, training: loss: 0.0000013, mae: 0.0007775 test: loss0.0001086, mae:0.0074926
training loss 1.774127667886205e-06 mae 0.0008672606199979782
training loss 1.155199351044347e-06 mae 0.0007445202300361557
training loss 1.2430421271225293e-06 mae 0.0007629312127067863
training loss 1.2868412070384388e-06 mae 0.0007774260179185932
training loss 1.2854413105332925e-06 mae 0.0007788164679259895
Epoch 538, training: loss: 0.0000013, mae: 0.0007808 test: loss0.0001084, mae:0.0074826
training loss 1.1509104069773457e-06 mae 0.0007191824843175709
training loss 1.3876292814752652e-06 mae 0.0007936551261866721
training loss 1.3207330921589587e-06 mae 0.0007845802214757381
training loss 1.3026567443155982e-06 mae 0.0007804865632375812
training loss 1.2886379055661884e-06 mae 0.0007759986973230946
Epoch 539, training: loss: 0.0000013, mae: 0.0007762 test: loss0.0001083, mae:0.0074985
training loss 8.19026070075779e-07 mae 0.000735732726752758
training loss 1.179427304613651e-06 mae 0.00073830169219268
training loss 1.2431596300671976e-06 mae 0.0007654333102175652
training loss 1.2630890143136406e-06 mae 0.0007716147028625209
training loss 1.2799834124661324e-06 mae 0.0007812992981820372
Epoch 540, training: loss: 0.0000013, mae: 0.0007815 test: loss0.0001082, mae:0.0074962
training loss 8.182125270650431e-07 mae 0.0006766365841031075
training loss 1.215498954542115e-06 mae 0.0007485435538304349
training loss 1.2316415446246825e-06 mae 0.0007522162692598295
training loss 1.2433388120982444e-06 mae 0.000762267674848326
training loss 1.2710974283606063e-06 mae 0.0007730838907847818
Epoch 541, training: loss: 0.0000013, mae: 0.0007722 test: loss0.0001085, mae:0.0075016
training loss 7.729117328381108e-07 mae 0.0007086924160830677
training loss 1.2893857234196143e-06 mae 0.0007831789872736906
training loss 1.2668362222655668e-06 mae 0.0007734058435204077
training loss 1.2834155692694036e-06 mae 0.000774483175621374
training loss 1.2762299315083553e-06 mae 0.0007752795910476633
Epoch 542, training: loss: 0.0000013, mae: 0.0007790 test: loss0.0001084, mae:0.0074912
training loss 8.809342944005039e-07 mae 0.0005977072869427502
training loss 1.2866584525311864e-06 mae 0.000778767404114535
training loss 1.2442575756168564e-06 mae 0.0007649791518590493
training loss 1.237089008821428e-06 mae 0.0007642663331834954
training loss 1.2639710215393985e-06 mae 0.0007712366752991388
Epoch 543, training: loss: 0.0000013, mae: 0.0007743 test: loss0.0001082, mae:0.0074866
training loss 7.782119268995302e-07 mae 0.0006382015417329967
training loss 1.2798969128109053e-06 mae 0.0007782785413677202
training loss 1.2665168010851915e-06 mae 0.0007787590142164269
training loss 1.271963328306641e-06 mae 0.0007784319906618866
training loss 1.272751951653989e-06 mae 0.0007785642768640941
Epoch 544, training: loss: 0.0000013, mae: 0.0007788 test: loss0.0001086, mae:0.0075044
training loss 9.844397936831228e-07 mae 0.0007687133620493114
training loss 1.2283356357677218e-06 mae 0.0007394175924013792
training loss 1.2757202154766183e-06 mae 0.0007655729965011746
training loss 1.2309526490096818e-06 mae 0.0007596646812559408
training loss 1.2768661073566743e-06 mae 0.0007758940537043137
Epoch 545, training: loss: 0.0000013, mae: 0.0007756 test: loss0.0001084, mae:0.0074920
training loss 1.3888211469748057e-06 mae 0.0007506844704039395
training loss 1.2155980175744024e-06 mae 0.0007459467223288454
training loss 1.24559393737958e-06 mae 0.0007642738853631968
training loss 1.2578682086744728e-06 mae 0.0007676241666137163
training loss 1.2536611476870212e-06 mae 0.0007678709699209339
Epoch 546, training: loss: 0.0000013, mae: 0.0007712 test: loss0.0001087, mae:0.0075014
training loss 1.0273637371938094e-06 mae 0.0007056233589537442
training loss 1.262630745508895e-06 mae 0.0007640719915941067
training loss 1.3132875725961348e-06 mae 0.0007789050647297343
training loss 1.2564634329350309e-06 mae 0.0007651057356462772
training loss 1.2513007429596245e-06 mae 0.0007637453254483713
Epoch 547, training: loss: 0.0000013, mae: 0.0007670 test: loss0.0001086, mae:0.0075084
training loss 1.7215376146850758e-06 mae 0.0007759944419376552
training loss 1.1789278844657127e-06 mae 0.0007511605200467306
training loss 1.2410146079182955e-06 mae 0.0007657703296926871
training loss 1.2689027871586805e-06 mae 0.0007745780602763623
training loss 1.2622278923649521e-06 mae 0.0007738577203700013
Epoch 548, training: loss: 0.0000013, mae: 0.0007723 test: loss0.0001096, mae:0.0075299
training loss 2.5617118808440864e-06 mae 0.0009797312086448073
training loss 1.3014753249660546e-06 mae 0.0007712578505971561
training loss 1.2442383442500817e-06 mae 0.0007620238765871321
training loss 1.2311334024233907e-06 mae 0.0007622916284572357
training loss 1.246590636470157e-06 mae 0.000766534651907633
Epoch 549, training: loss: 0.0000012, mae: 0.0007669 test: loss0.0001085, mae:0.0074983
training loss 1.2793551604772802e-06 mae 0.0007552330498583615
training loss 1.1961229072134231e-06 mae 0.0007534607795213222
training loss 1.1675976649334335e-06 mae 0.000743471755125182
training loss 1.21355266542768e-06 mae 0.0007590881127876458
training loss 1.2462366012345344e-06 mae 0.0007699112956926446
Epoch 550, training: loss: 0.0000013, mae: 0.0007702 test: loss0.0001083, mae:0.0074865
training loss 1.4366178220370784e-06 mae 0.0008150488138198853
training loss 1.1771195353699736e-06 mae 0.0007399457179274702
training loss 1.2019946729475965e-06 mae 0.0007489405259476184
training loss 1.2242464288418285e-06 mae 0.0007554550985929045
training loss 1.2446040769098551e-06 mae 0.0007644301240995008
Epoch 551, training: loss: 0.0000013, mae: 0.0007661 test: loss0.0001081, mae:0.0074870
training loss 9.697084806248313e-07 mae 0.000657531141769141
training loss 1.3009575666853612e-06 mae 0.0007837243508273626
training loss 1.2786961257582943e-06 mae 0.0007769568589294677
training loss 1.2646688896170693e-06 mae 0.0007758437546389486
training loss 1.255398551088932e-06 mae 0.0007730643855713644
Epoch 552, training: loss: 0.0000013, mae: 0.0007748 test: loss0.0001085, mae:0.0074921
training loss 7.238849661916902e-07 mae 0.0006142982165329158
training loss 1.248962324133768e-06 mae 0.0007788844271928219
training loss 1.2414953155460152e-06 mae 0.0007693489678065746
training loss 1.231936200145729e-06 mae 0.00076594167012746
training loss 1.2415954229050952e-06 mae 0.0007665777058845654
Epoch 553, training: loss: 0.0000012, mae: 0.0007679 test: loss0.0001093, mae:0.0075179
training loss 1.1340412129356991e-06 mae 0.0007983765681274235
training loss 1.2116269357754984e-06 mae 0.0007447531103275207
training loss 1.1943298489421806e-06 mae 0.0007505926893134186
training loss 1.241952981028035e-06 mae 0.0007627823011138609
training loss 1.2507771817011538e-06 mae 0.0007666590209732953
Epoch 554, training: loss: 0.0000012, mae: 0.0007644 test: loss0.0001089, mae:0.0075059
training loss 1.176283035420056e-06 mae 0.0007624185527674854
training loss 1.214352637897023e-06 mae 0.0007548104314243092
training loss 1.2451867904825243e-06 mae 0.0007648269258587079
training loss 1.2293452878427782e-06 mae 0.0007607182206444148
training loss 1.2333263936538263e-06 mae 0.0007620223393010446
Epoch 555, training: loss: 0.0000012, mae: 0.0007638 test: loss0.0001088, mae:0.0074975
training loss 1.1341522849761532e-06 mae 0.0007727897609584033
training loss 1.2710611266428197e-06 mae 0.0007623273036990531
training loss 1.254865290912667e-06 mae 0.0007578869915618055
training loss 1.2311228862026786e-06 mae 0.0007591192883868017
training loss 1.2361193851414458e-06 mae 0.0007649351544712959
Epoch 556, training: loss: 0.0000013, mae: 0.0007688 test: loss0.0001091, mae:0.0075259
training loss 1.5111887705643312e-06 mae 0.0008564588497392833
training loss 1.1908434281198707e-06 mae 0.0007437455568316519
training loss 1.210741647630111e-06 mae 0.0007559757542538244
training loss 1.2454303225921057e-06 mae 0.0007645367758058701
training loss 1.2418719585021697e-06 mae 0.0007656078284087394
Epoch 557, training: loss: 0.0000012, mae: 0.0007671 test: loss0.0001097, mae:0.0075365
training loss 1.1086951872130157e-06 mae 0.0007476189057342708
training loss 1.2229345713876683e-06 mae 0.0007405427533347961
training loss 1.1958212069495658e-06 mae 0.0007416717575558712
training loss 1.202181903482711e-06 mae 0.0007496232901998975
training loss 1.2224377143755297e-06 mae 0.0007593378612009428
Epoch 558, training: loss: 0.0000012, mae: 0.0007592 test: loss0.0001095, mae:0.0075274
training loss 9.324794518761337e-07 mae 0.0007088857819326222
training loss 1.1419053260611052e-06 mae 0.000736349394691049
training loss 1.216340495888614e-06 mae 0.0007550025371475016
training loss 1.1744645222213102e-06 mae 0.0007424562778601436
training loss 1.2175030799165983e-06 mae 0.0007549363370308312
Epoch 559, training: loss: 0.0000012, mae: 0.0007579 test: loss0.0001090, mae:0.0075212
training loss 1.4800313010709942e-06 mae 0.0008851864258758724
training loss 1.1695772780414836e-06 mae 0.0007413613664753295
training loss 1.144677951299287e-06 mae 0.0007353755639690793
training loss 1.1969401644075441e-06 mae 0.0007498544484333731
training loss 1.2275129515966178e-06 mae 0.0007628724578448305
Epoch 560, training: loss: 0.0000012, mae: 0.0007613 test: loss0.0001090, mae:0.0075188
training loss 1.1082863693445688e-06 mae 0.000757769972551614
training loss 1.2239367626588157e-06 mae 0.00073712367984448
training loss 1.2138363620642215e-06 mae 0.0007486766740785782
training loss 1.2284782177184684e-06 mae 0.0007562118802922744
training loss 1.2181097043985877e-06 mae 0.0007578166165801494
Epoch 561, training: loss: 0.0000012, mae: 0.0007587 test: loss0.0001088, mae:0.0075149
training loss 1.030281396197097e-06 mae 0.0007253953372128308
training loss 1.249206346243477e-06 mae 0.000757665867579407
training loss 1.2271488138608087e-06 mae 0.0007591737304021153
training loss 1.224423330455128e-06 mae 0.000759815442181556
training loss 1.2409034665532871e-06 mae 0.0007661186353840968
Epoch 562, training: loss: 0.0000012, mae: 0.0007617 test: loss0.0001088, mae:0.0075026
training loss 1.1760181450881646e-06 mae 0.000700428441632539
training loss 1.1365314094291536e-06 mae 0.0007444235745032192
training loss 1.1466565595556225e-06 mae 0.0007445472984178234
training loss 1.1900229085298127e-06 mae 0.0007540282048593394
training loss 1.2145234803931576e-06 mae 0.0007556968182383281
Epoch 563, training: loss: 0.0000012, mae: 0.0007561 test: loss0.0001092, mae:0.0075209
training loss 1.3100710702929064e-06 mae 0.0007422743365168571
training loss 1.1352608748670624e-06 mae 0.000720207525791564
training loss 1.2006353292780658e-06 mae 0.0007471047493970336
training loss 1.233468150423914e-06 mae 0.0007589222633569742
training loss 1.2226229518062293e-06 mae 0.000758939519679795
Epoch 564, training: loss: 0.0000012, mae: 0.0007567 test: loss0.0001092, mae:0.0075280
training loss 1.0267070820191293e-06 mae 0.0007202339475043118
training loss 1.1614234663328732e-06 mae 0.0007405973292941994
training loss 1.1559570709485536e-06 mae 0.0007311821264551382
training loss 1.1993041154321735e-06 mae 0.0007462060658742258
training loss 1.2165783366142123e-06 mae 0.0007534869813327837
Epoch 565, training: loss: 0.0000012, mae: 0.0007535 test: loss0.0001093, mae:0.0075277
training loss 7.611142223140632e-07 mae 0.0006484827026724815
training loss 1.12768206220465e-06 mae 0.0007300195615172532
training loss 1.2087349367754206e-06 mae 0.0007478675556064832
training loss 1.1901067907819509e-06 mae 0.0007485171187505594
training loss 1.2004085499370941e-06 mae 0.0007525245971521316
Epoch 566, training: loss: 0.0000012, mae: 0.0007546 test: loss0.0001099, mae:0.0075401
training loss 1.684568019300059e-06 mae 0.0008558705449104309
training loss 1.1848306561053805e-06 mae 0.000752355163807378
training loss 1.1637775910480514e-06 mae 0.0007422208742210918
training loss 1.1911767119167141e-06 mae 0.0007504572539084545
training loss 1.207217991633065e-06 mae 0.0007535592057459538
Epoch 567, training: loss: 0.0000012, mae: 0.0007546 test: loss0.0001090, mae:0.0075095
training loss 9.348382263851818e-07 mae 0.000642529979813844
training loss 1.080364782245513e-06 mae 0.0007188078740785154
training loss 1.1450732339617753e-06 mae 0.000733514712019224
training loss 1.1858315345710906e-06 mae 0.0007462744342412997
training loss 1.2072607733758361e-06 mae 0.0007534523948329725
Epoch 568, training: loss: 0.0000012, mae: 0.0007525 test: loss0.0001093, mae:0.0075192
training loss 1.9593865090428153e-06 mae 0.0009970543906092644
training loss 1.1715021572604648e-06 mae 0.0007314015220960274
training loss 1.1465566060316167e-06 mae 0.000732795560690989
training loss 1.1802738799137812e-06 mae 0.0007444314650161229
training loss 1.204881633368666e-06 mae 0.0007543872402742302
Epoch 569, training: loss: 0.0000012, mae: 0.0007533 test: loss0.0001100, mae:0.0075513
training loss 8.761016943026334e-07 mae 0.0006376327946782112
training loss 1.0900058919774248e-06 mae 0.000720752071584666
training loss 1.0902525887239342e-06 mae 0.0007168476488256811
training loss 1.1557031496745397e-06 mae 0.0007376634868151749
training loss 1.1966788766151144e-06 mae 0.000747972439374515
Epoch 570, training: loss: 0.0000012, mae: 0.0007504 test: loss0.0001095, mae:0.0075307
training loss 1.0951661124636303e-06 mae 0.0006326719303615391
training loss 1.08969196583744e-06 mae 0.0007173351780283175
training loss 1.120408160879901e-06 mae 0.000730005879874922
training loss 1.139387972297667e-06 mae 0.0007357151594108573
training loss 1.179195637338687e-06 mae 0.0007430740441855932
Epoch 571, training: loss: 0.0000012, mae: 0.0007472 test: loss0.0001126, mae:0.0075757
training loss 5.1614756557683e-07 mae 0.0005315157468430698
training loss 1.1849966572789723e-06 mae 0.000735828765969285
training loss 1.1886769682000214e-06 mae 0.000741244946712499
training loss 1.197160075784365e-06 mae 0.0007446582243309068
training loss 1.1947338939150647e-06 mae 0.0007487491829626597
Epoch 572, training: loss: 0.0000012, mae: 0.0007520 test: loss0.0001091, mae:0.0075132
training loss 8.933362209972984e-07 mae 0.0005958008696325123
training loss 1.055177267182765e-06 mae 0.0006952513946338975
training loss 1.14066884237326e-06 mae 0.0007291549616746862
training loss 1.1538386221046727e-06 mae 0.0007385907238673118
training loss 1.1814316372744731e-06 mae 0.0007469478969413562
Epoch 573, training: loss: 0.0000012, mae: 0.0007463 test: loss0.0001095, mae:0.0075391
training loss 1.1344291124260053e-06 mae 0.0007863957434892654
training loss 1.0696439802564322e-06 mae 0.0007146970424846764
training loss 1.1338708957012358e-06 mae 0.0007321881882426801
training loss 1.170279343254127e-06 mae 0.0007429965384326111
training loss 1.182557776102876e-06 mae 0.0007489053423884811
Epoch 574, training: loss: 0.0000012, mae: 0.0007490 test: loss0.0001093, mae:0.0075262
training loss 1.4882307368679903e-06 mae 0.0008752156863920391
training loss 1.149417341018895e-06 mae 0.0007266925617038984
training loss 1.1286928663076373e-06 mae 0.0007263951508978405
training loss 1.1319733924072064e-06 mae 0.0007306140497101734
training loss 1.1741649421681316e-06 mae 0.0007412353933346342
Epoch 575, training: loss: 0.0000012, mae: 0.0007427 test: loss0.0001100, mae:0.0075395
training loss 7.414541300931887e-07 mae 0.0006256137858144939
training loss 1.1592576746735695e-06 mae 0.0007573319987083476
training loss 1.1737802517675642e-06 mae 0.0007490868911280825
training loss 1.1978600759481584e-06 mae 0.0007542273195827422
training loss 1.1805607314770228e-06 mae 0.0007483383094189947
Epoch 576, training: loss: 0.0000012, mae: 0.0007505 test: loss0.0001100, mae:0.0075503
training loss 9.411048154106538e-07 mae 0.0006737724761478603
training loss 1.200669611106979e-06 mae 0.0007599201099034035
training loss 1.2467296432807395e-06 mae 0.0007672238873668236
training loss 1.2021820568846527e-06 mae 0.0007533216918356075
training loss 1.1735625356940641e-06 mae 0.0007452405113159256
Epoch 577, training: loss: 0.0000012, mae: 0.0007453 test: loss0.0001100, mae:0.0075503
training loss 8.532560968887992e-07 mae 0.0007042273064143956
training loss 1.1229147517889372e-06 mae 0.0007152401490648294
training loss 1.1996166507503972e-06 mae 0.0007467872594566184
training loss 1.1802350225431859e-06 mae 0.0007423458473266575
training loss 1.1738037200578655e-06 mae 0.0007434913702367046
Epoch 578, training: loss: 0.0000012, mae: 0.0007440 test: loss0.0001095, mae:0.0075303
training loss 1.1081255024691927e-06 mae 0.0007764774491079152
training loss 1.0617972305663345e-06 mae 0.0006929124538859756
training loss 1.0808899832123773e-06 mae 0.0007077651461448041
training loss 1.1459626114324752e-06 mae 0.000727025486316088
training loss 1.152518639790161e-06 mae 0.0007355068352731383
Epoch 579, training: loss: 0.0000012, mae: 0.0007401 test: loss0.0001105, mae:0.0075642
training loss 8.696239888195123e-07 mae 0.0007176107610575855
training loss 1.2106158139574168e-06 mae 0.0007693040967626754
training loss 1.1950501633993266e-06 mae 0.0007582762040022653
training loss 1.18964791494249e-06 mae 0.0007572775908640974
training loss 1.1928561244571732e-06 mae 0.0007559668092608489
Epoch 580, training: loss: 0.0000012, mae: 0.0007544 test: loss0.0001099, mae:0.0075406
training loss 1.8741501435215469e-06 mae 0.0009398323600180447
training loss 1.0583248455549988e-06 mae 0.000711988715349934
training loss 1.1688119717755307e-06 mae 0.0007374428926118751
training loss 1.1538108678348558e-06 mae 0.0007344485146443774
training loss 1.1679224809115418e-06 mae 0.0007421632725351598
Epoch 581, training: loss: 0.0000012, mae: 0.0007439 test: loss0.0001113, mae:0.0075724
training loss 5.41796737252298e-07 mae 0.0005811508744955063
training loss 1.2289396277439809e-06 mae 0.0007538903377922799
training loss 1.1835269153362055e-06 mae 0.0007472405372081453
training loss 1.1636815846585955e-06 mae 0.0007452428942648014
training loss 1.1736981300136953e-06 mae 0.0007463445093781589
Epoch 582, training: loss: 0.0000012, mae: 0.0007452 test: loss0.0001102, mae:0.0075627
training loss 6.988307745814382e-07 mae 0.0005882168188691139
training loss 1.1360306601483325e-06 mae 0.000736609409453676
training loss 1.1961592603895923e-06 mae 0.000748689054732792
training loss 1.18210706492596e-06 mae 0.0007437890841138385
training loss 1.1775274272246914e-06 mae 0.0007395874485906347
Epoch 583, training: loss: 0.0000012, mae: 0.0007393 test: loss0.0001099, mae:0.0075461
training loss 1.0494289881535224e-06 mae 0.0006642704829573631
training loss 1.1212604455718692e-06 mae 0.0007164182629911048
training loss 1.1179282446031682e-06 mae 0.0007158714654339055
training loss 1.1492434151818239e-06 mae 0.0007271920552474159
training loss 1.1491884111113793e-06 mae 0.0007336328212410419
Epoch 584, training: loss: 0.0000012, mae: 0.0007353 test: loss0.0001094, mae:0.0075211
training loss 1.349995386590308e-06 mae 0.000767274759709835
training loss 1.0961620537451456e-06 mae 0.0007262388358403948
training loss 1.1166632128890233e-06 mae 0.0007299659935075164
training loss 1.1612062340850728e-06 mae 0.0007426630738746401
training loss 1.1638967571660278e-06 mae 0.0007435702660876274
Epoch 585, training: loss: 0.0000012, mae: 0.0007454 test: loss0.0001101, mae:0.0075480
training loss 7.294979127436818e-07 mae 0.0005989282508380711
training loss 1.1521194202415762e-06 mae 0.0007275919140060889
training loss 1.1016989293297556e-06 mae 0.0007200761546039642
training loss 1.1215490063787964e-06 mae 0.0007222063470530072
training loss 1.1501732582693232e-06 mae 0.0007321147978785382
Epoch 586, training: loss: 0.0000012, mae: 0.0007332 test: loss0.0001101, mae:0.0075507
training loss 1.1249059070905787e-06 mae 0.0007142458925954998
training loss 1.139372460222469e-06 mae 0.0007347779681760015
training loss 1.1136339846060686e-06 mae 0.0007215960834193792
training loss 1.1371078877144234e-06 mae 0.0007287593240794166
training loss 1.1617462507220175e-06 mae 0.0007392307862162183
Epoch 587, training: loss: 0.0000012, mae: 0.0007401 test: loss0.0001100, mae:0.0075564
training loss 6.584775178453128e-07 mae 0.0005878228694200516
training loss 1.096222584741483e-06 mae 0.00071776694819039
training loss 1.1040961192966374e-06 mae 0.0007256031817858024
training loss 1.1211840328557233e-06 mae 0.000727103875845128
training loss 1.1494463047536986e-06 mae 0.000736641183852297
Epoch 588, training: loss: 0.0000012, mae: 0.0007391 test: loss0.0001099, mae:0.0075511
training loss 1.3267126632854342e-06 mae 0.0007875906303524971
training loss 1.2022979031005922e-06 mae 0.000746182808606867
training loss 1.163584113778643e-06 mae 0.0007407387051763622
training loss 1.145380191127702e-06 mae 0.0007364036450208788
training loss 1.1540514006625036e-06 mae 0.0007391620848144852
Epoch 589, training: loss: 0.0000012, mae: 0.0007401 test: loss0.0001100, mae:0.0075562
training loss 9.023080451697751e-07 mae 0.0006314165075309575
training loss 1.1401469746532727e-06 mae 0.0007248153275463221
training loss 1.087456558034229e-06 mae 0.0007071254932261672
training loss 1.111848298809708e-06 mae 0.0007171483757822399
training loss 1.1331434506024573e-06 mae 0.0007244338565540791
Epoch 590, training: loss: 0.0000011, mae: 0.0007246 test: loss0.0001101, mae:0.0075497
training loss 1.4337309721668134e-06 mae 0.0007914354209788144
training loss 1.062067051786168e-06 mae 0.000702874475916592
training loss 1.054395515567275e-06 mae 0.0007020837274647439
training loss 1.1235594935433312e-06 mae 0.0007196383220020263
training loss 1.1254386843357857e-06 mae 0.000721266721398454
Epoch 591, training: loss: 0.0000011, mae: 0.0007221 test: loss0.0001103, mae:0.0075518
training loss 1.141308871410729e-06 mae 0.0007006789674051106
training loss 1.1030970010021217e-06 mae 0.000721878558739691
training loss 1.1432135891320088e-06 mae 0.0007409197869042101
training loss 1.1522007219050976e-06 mae 0.0007387826634364337
training loss 1.137832341204946e-06 mae 0.0007320166487578046
Epoch 592, training: loss: 0.0000011, mae: 0.0007323 test: loss0.0001102, mae:0.0075482
training loss 9.235668585461099e-07 mae 0.0007459794287569821
training loss 1.0343898791278681e-06 mae 0.0006845140802290509
training loss 1.0508311841629912e-06 mae 0.0006983477661459897
training loss 1.114705026750005e-06 mae 0.0007188625981420624
training loss 1.1309603718896304e-06 mae 0.0007285497505194621
Epoch 593, training: loss: 0.0000011, mae: 0.0007303 test: loss0.0001101, mae:0.0075584
training loss 1.3020886626691208e-06 mae 0.0007642582058906555
training loss 1.145210695076636e-06 mae 0.0007272753350929743
training loss 1.118623517845888e-06 mae 0.000726998645909094
training loss 1.1677667803811371e-06 mae 0.0007412298462995088
training loss 1.1336151821164535e-06 mae 0.0007296901057817193
Epoch 594, training: loss: 0.0000011, mae: 0.0007333 test: loss0.0001102, mae:0.0075617
training loss 1.4013694453751668e-06 mae 0.0008683474734425545
training loss 1.079007236013222e-06 mae 0.0007126767361792279
training loss 1.0957253222598114e-06 mae 0.0007146865522516611
training loss 1.1081031599940088e-06 mae 0.0007234953967576829
training loss 1.1278087911884105e-06 mae 0.0007297822615405103
Epoch 595, training: loss: 0.0000011, mae: 0.0007297 test: loss0.0001100, mae:0.0075427
training loss 1.2883762110504904e-06 mae 0.0006554564461112022
training loss 1.2039943481242662e-06 mae 0.0007401773085196813
training loss 1.1817811869690612e-06 mae 0.0007371500968393677
training loss 1.1520583643385977e-06 mae 0.000734024086738906
training loss 1.1336143216880853e-06 mae 0.0007311402659617661
Epoch 596, training: loss: 0.0000011, mae: 0.0007315 test: loss0.0001120, mae:0.0076035
training loss 1.6855365174706094e-06 mae 0.000715884380042553
training loss 1.086535572834263e-06 mae 0.000723114200666839
training loss 1.1524981317452872e-06 mae 0.0007380633194248494
training loss 1.1489497660326773e-06 mae 0.0007402411618097284
training loss 1.1424006608357654e-06 mae 0.0007381213455928941
Epoch 597, training: loss: 0.0000011, mae: 0.0007407 test: loss0.0001103, mae:0.0075633
training loss 6.645905727964418e-07 mae 0.000598008104134351
training loss 1.1142384944467302e-06 mae 0.0007266626218516453
training loss 1.1236445907410834e-06 mae 0.0007277533151485987
training loss 1.1338191224984151e-06 mae 0.0007345940133661574
training loss 1.1297495120460604e-06 mae 0.0007329733046964829
Epoch 598, training: loss: 0.0000011, mae: 0.0007302 test: loss0.0001104, mae:0.0075607
training loss 1.662596673668304e-06 mae 0.0008806933765299618
training loss 1.1376569091225974e-06 mae 0.0007227025401102851
training loss 1.0752854618681118e-06 mae 0.0007113349539836373
training loss 1.1025283747630668e-06 mae 0.0007201024525434966
training loss 1.111251676848811e-06 mae 0.0007247090381940836
Epoch 599, training: loss: 0.0000011, mae: 0.0007283 test: loss0.0001101, mae:0.0075546
current learning rate: 7.8125e-06
training loss 8.638926374260336e-07 mae 0.00070139643503353
training loss 1.1752811918968832e-06 mae 0.0007292078796536754
training loss 1.1066460289573954e-06 mae 0.0007057861337959214
training loss 1.0948291241602898e-06 mae 0.000700792897144172
training loss 1.0609596644987153e-06 mae 0.0006868164625542413
Epoch 600, training: loss: 0.0000011, mae: 0.0006839 test: loss0.0001100, mae:0.0075548
training loss 1.6146010466400185e-06 mae 0.0007919147610664368
training loss 9.915933880361708e-07 mae 0.0006619267497786923
training loss 9.916449277742427e-07 mae 0.0006656660009659643
training loss 1.0405356124164317e-06 mae 0.0006747796178628866
training loss 1.0513182910279796e-06 mae 0.0006766429146930149
Epoch 601, training: loss: 0.0000010, mae: 0.0006732 test: loss0.0001101, mae:0.0075518
training loss 4.705142657712713e-07 mae 0.0005188097129575908
training loss 1.0032282511483397e-06 mae 0.000657398494499206
training loss 1.0490597513143986e-06 mae 0.0006656761028767001
training loss 1.0418016235828251e-06 mae 0.0006720891894251477
training loss 1.0392248075684398e-06 mae 0.0006746897233281729
Epoch 602, training: loss: 0.0000010, mae: 0.0006714 test: loss0.0001099, mae:0.0075477
training loss 9.21293747069285e-07 mae 0.0006213595042936504
training loss 9.77081184623339e-07 mae 0.0006557700769774909
training loss 1.055309445462734e-06 mae 0.000674589338255954
training loss 1.0462320800201612e-06 mae 0.0006724389723659377
training loss 1.0329025925913626e-06 mae 0.0006710367166641078
Epoch 603, training: loss: 0.0000010, mae: 0.0006715 test: loss0.0001101, mae:0.0075462
training loss 9.479479672336311e-07 mae 0.0006209106068126857
training loss 1.0191359064372271e-06 mae 0.000668230439723451
training loss 1.032421599318533e-06 mae 0.0006669622896087014
training loss 1.0450908755441352e-06 mae 0.0006767926340235013
training loss 1.0396511176557267e-06 mae 0.0006770890738076038
Epoch 604, training: loss: 0.0000010, mae: 0.0006752 test: loss0.0001104, mae:0.0075672
training loss 1.0496024742678856e-06 mae 0.0006722463294863701
training loss 8.988046136205502e-07 mae 0.0006301796443842571
training loss 9.657724802147439e-07 mae 0.0006561460527291725
training loss 1.0240325362951663e-06 mae 0.000669811351150768
training loss 1.0308827480984848e-06 mae 0.000672903204532526
Epoch 605, training: loss: 0.0000010, mae: 0.0006733 test: loss0.0001105, mae:0.0075526
training loss 9.935089337886893e-07 mae 0.0006799893453717232
training loss 9.529365584904893e-07 mae 0.0006636986874190031
training loss 9.610235541922431e-07 mae 0.0006620180427147499
training loss 1.0057272553896708e-06 mae 0.0006692126263796494
training loss 1.0187945881241026e-06 mae 0.0006726778794916355
Epoch 606, training: loss: 0.0000010, mae: 0.0006736 test: loss0.0001105, mae:0.0075597
training loss 9.851625009105192e-07 mae 0.0006630488787777722
training loss 9.622022781852638e-07 mae 0.0006548573157987466
training loss 9.869388118812201e-07 mae 0.0006626594202514592
training loss 1.0051473066442785e-06 mae 0.0006672554176812683
training loss 1.0217815648486605e-06 mae 0.0006710648421135578
Epoch 607, training: loss: 0.0000010, mae: 0.0006723 test: loss0.0001103, mae:0.0075641
training loss 1.2473244623834034e-06 mae 0.0007353005930781364
training loss 9.887630890083456e-07 mae 0.0006479574253271317
training loss 9.923400977420374e-07 mae 0.0006569619247307173
training loss 1.023576919729351e-06 mae 0.0006684844710922719
training loss 1.0194355975012442e-06 mae 0.0006697516435460739
Epoch 608, training: loss: 0.0000010, mae: 0.0006704 test: loss0.0001110, mae:0.0075805
training loss 1.0441459608045989e-06 mae 0.000693698413670063
training loss 9.834557003534211e-07 mae 0.0006531631241908626
training loss 1.059984959616792e-06 mae 0.0006748204042277493
training loss 1.016258022747139e-06 mae 0.0006681222245806445
training loss 1.014915229112078e-06 mae 0.0006689173726895499
Epoch 609, training: loss: 0.0000010, mae: 0.0006704 test: loss0.0001105, mae:0.0075655
training loss 1.2276583447601297e-06 mae 0.0007477563922293484
training loss 1.020755634301043e-06 mae 0.0006696128528382555
training loss 1.0218525877779953e-06 mae 0.0006714844252075075
training loss 1.003817555277129e-06 mae 0.000669023275702254
training loss 1.0293953562635786e-06 mae 0.0006738269284010784
Epoch 610, training: loss: 0.0000010, mae: 0.0006728 test: loss0.0001107, mae:0.0075753
training loss 7.136805493246356e-07 mae 0.0005859984084963799
training loss 9.679427245257604e-07 mae 0.0006451845681523063
training loss 9.909833923669315e-07 mae 0.000657457767266522
training loss 9.934030869195248e-07 mae 0.0006629450440203265
training loss 1.0164218369919892e-06 mae 0.0006685665588818992
Epoch 611, training: loss: 0.0000010, mae: 0.0006714 test: loss0.0001117, mae:0.0075952
training loss 3.8523421608260833e-07 mae 0.0004910699208267033
training loss 9.001346214455565e-07 mae 0.0006390490766623803
training loss 9.922571845805525e-07 mae 0.0006634344126093247
training loss 1.0093174228583568e-06 mae 0.0006636415998719708
training loss 1.0187812890265141e-06 mae 0.0006687947012025361
Epoch 612, training: loss: 0.0000010, mae: 0.0006683 test: loss0.0001103, mae:0.0075610
training loss 8.987288424577855e-07 mae 0.0006892755627632141
training loss 9.99252166571287e-07 mae 0.0006647370909751558
training loss 1.0056356169534774e-06 mae 0.0006667466509723023
training loss 1.0208878267957406e-06 mae 0.0006684244428036598
training loss 1.0203585317123355e-06 mae 0.0006733088383731428
Epoch 613, training: loss: 0.0000010, mae: 0.0006730 test: loss0.0001104, mae:0.0075677
training loss 1.5428940969286487e-06 mae 0.000849427655339241
training loss 1.0433900609216068e-06 mae 0.000674383152121057
training loss 1.0107094638978298e-06 mae 0.0006656958015594114
training loss 1.0196666737450958e-06 mae 0.0006698888683358561
training loss 1.0183491589932635e-06 mae 0.0006705090640731316
Epoch 614, training: loss: 0.0000010, mae: 0.0006712 test: loss0.0001105, mae:0.0075704
training loss 9.387576369590533e-07 mae 0.0006365592707879841
training loss 1.0148892867538747e-06 mae 0.0006738196785662177
training loss 9.824896611656072e-07 mae 0.0006649590671952299
training loss 1.0275392030402245e-06 mae 0.0006728991434372812
training loss 1.025160873860385e-06 mae 0.0006716270551412127
Epoch 615, training: loss: 0.0000010, mae: 0.0006712 test: loss0.0001104, mae:0.0075618
training loss 7.888913273745857e-07 mae 0.0006209404091350734
training loss 9.537743402037732e-07 mae 0.0006468695737923695
training loss 9.744492279486897e-07 mae 0.0006564148559954274
training loss 9.69354726336276e-07 mae 0.0006567950102975528
training loss 1.0130748350504963e-06 mae 0.0006686383768550086
Epoch 616, training: loss: 0.0000010, mae: 0.0006702 test: loss0.0001106, mae:0.0075624
training loss 1.437096216250211e-06 mae 0.0007087141275405884
training loss 1.0413834503402078e-06 mae 0.0006685285867812734
training loss 1.010478195322582e-06 mae 0.0006640378488366039
training loss 1.0129392144667e-06 mae 0.0006663884579385969
training loss 1.0141848443466949e-06 mae 0.0006692285396955648
Epoch 617, training: loss: 0.0000010, mae: 0.0006724 test: loss0.0001103, mae:0.0075639
training loss 7.725022328486375e-07 mae 0.0004900188068859279
training loss 1.0046209672955408e-06 mae 0.000661012525989802
training loss 1.0012116050991538e-06 mae 0.0006586437924456145
training loss 9.896692924126878e-07 mae 0.0006589800580435536
training loss 1.0056496043559606e-06 mae 0.0006659114144021989
Epoch 618, training: loss: 0.0000010, mae: 0.0006681 test: loss0.0001109, mae:0.0075779
training loss 1.6828163325044443e-06 mae 0.0008396847988478839
training loss 1.0158681348849649e-06 mae 0.000665205815583285
training loss 1.0637170877282977e-06 mae 0.0006778771060606518
training loss 1.0233960528170284e-06 mae 0.000670109954331107
training loss 1.012932145201615e-06 mae 0.0006687303758006245
Epoch 619, training: loss: 0.0000010, mae: 0.0006708 test: loss0.0001107, mae:0.0075826
training loss 7.94771892742574e-07 mae 0.0006265330011956394
training loss 1.0070180046702321e-06 mae 0.0006767844155832539
training loss 1.03097784908146e-06 mae 0.0006763641754197561
training loss 1.007718045933124e-06 mae 0.0006688222187318349
training loss 1.0032997515770436e-06 mae 0.0006652227184441009
Epoch 620, training: loss: 0.0000010, mae: 0.0006646 test: loss0.0001104, mae:0.0075608
training loss 1.2419008044162183e-06 mae 0.0007401636685244739
training loss 1.0115367750390136e-06 mae 0.0006552314255660509
training loss 1.0201458772119413e-06 mae 0.0006708109628588957
training loss 1.0123291931058272e-06 mae 0.0006648175025571584
training loss 1.0043444298269745e-06 mae 0.0006616898309624759
Epoch 621, training: loss: 0.0000010, mae: 0.0006635 test: loss0.0001108, mae:0.0075715
training loss 8.327695013576886e-07 mae 0.000653938390314579
training loss 9.970833014663457e-07 mae 0.000659594752862319
training loss 1.0051224607384868e-06 mae 0.0006707935628024766
training loss 1.0264446607997167e-06 mae 0.0006762023707574218
training loss 1.0123990457023003e-06 mae 0.0006697949976834066
Epoch 622, training: loss: 0.0000010, mae: 0.0006693 test: loss0.0001117, mae:0.0075883
training loss 6.022265210958722e-07 mae 0.000564704358112067
training loss 9.296541043559223e-07 mae 0.0006401931328297246
training loss 9.217082124158168e-07 mae 0.000641855826465376
training loss 9.536092502668716e-07 mae 0.0006519614611690656
training loss 9.914802357212214e-07 mae 0.0006614833634661798
Epoch 623, training: loss: 0.0000010, mae: 0.0006654 test: loss0.0001107, mae:0.0075737
training loss 1.0501884162295028e-06 mae 0.0007197791710495949
training loss 9.067899531387142e-07 mae 0.0006475729180737305
training loss 9.60189640413728e-07 mae 0.000653728957421672
training loss 9.817090845908694e-07 mae 0.0006603436206856013
training loss 1.0055682755637243e-06 mae 0.0006659393005052219
Epoch 624, training: loss: 0.0000010, mae: 0.0006670 test: loss0.0001111, mae:0.0075848
training loss 8.654763519189146e-07 mae 0.000471312552690506
training loss 9.773998332253968e-07 mae 0.0006492667969581031
training loss 9.875535185826026e-07 mae 0.0006557668410564336
training loss 1.0071151827021173e-06 mae 0.0006632169113812094
training loss 9.989257859583792e-07 mae 0.0006611739328439207
Epoch 625, training: loss: 0.0000010, mae: 0.0006615 test: loss0.0001112, mae:0.0075947
training loss 7.100240395629953e-07 mae 0.0005374497850425541
training loss 9.367730548743282e-07 mae 0.000641123639926424
training loss 9.828720158899464e-07 mae 0.0006508746148100914
training loss 9.969793828058872e-07 mae 0.0006624141336798
training loss 9.96624971798952e-07 mae 0.000663161088958086
Epoch 626, training: loss: 0.0000010, mae: 0.0006631 test: loss0.0001110, mae:0.0075861
training loss 1.054198605743295e-06 mae 0.0007078098133206367
training loss 9.700592229097203e-07 mae 0.0006386512284413637
training loss 9.606224768778685e-07 mae 0.000642253614162178
training loss 9.744394908044728e-07 mae 0.0006519538155349824
training loss 9.957165214889497e-07 mae 0.0006605868264847433
Epoch 627, training: loss: 0.0000010, mae: 0.0006629 test: loss0.0001111, mae:0.0075936
training loss 1.1206352610315662e-06 mae 0.0006069264491088688
training loss 9.503957150681182e-07 mae 0.0006414678526412258
training loss 9.611581971161158e-07 mae 0.0006514818985061259
training loss 9.86383154604904e-07 mae 0.0006592237037477016
training loss 9.994838207707685e-07 mae 0.0006637596305163194
Epoch 628, training: loss: 0.0000010, mae: 0.0006661 test: loss0.0001113, mae:0.0075996
training loss 4.6712759171896323e-07 mae 0.00048141894512809813
training loss 1.0172135210839272e-06 mae 0.0006638875455988172
training loss 9.860020827095427e-07 mae 0.0006614689349823882
training loss 1.002190501159497e-06 mae 0.0006616874449556407
training loss 1.0074429056246357e-06 mae 0.0006652164380981201
Epoch 629, training: loss: 0.0000010, mae: 0.0006630 test: loss0.0001112, mae:0.0076031
training loss 8.989402431325288e-07 mae 0.000554736063349992
training loss 9.465319001443346e-07 mae 0.0006396193445349734
training loss 9.809679783198715e-07 mae 0.0006525559739283478
training loss 9.907596577912488e-07 mae 0.0006609975261265251
training loss 1.0041178496969725e-06 mae 0.0006652531580100604
Epoch 630, training: loss: 0.0000010, mae: 0.0006644 test: loss0.0001106, mae:0.0075674
training loss 1.1187227073605754e-06 mae 0.0007189859752543271
training loss 9.327242452730156e-07 mae 0.0006532264193154723
training loss 9.56709590634028e-07 mae 0.0006590383412606902
training loss 9.596103118247809e-07 mae 0.0006579653853751003
training loss 9.82935247366955e-07 mae 0.0006620117448112785
Epoch 631, training: loss: 0.0000010, mae: 0.0006650 test: loss0.0001109, mae:0.0075926
training loss 8.417710546382295e-07 mae 0.0006448269705288112
training loss 9.5572485580466e-07 mae 0.0006489693084457777
training loss 9.879655807469706e-07 mae 0.0006651891352802439
training loss 1.0050622758539227e-06 mae 0.000668611895564376
training loss 1.0048369766373889e-06 mae 0.0006666020789955145
Epoch 632, training: loss: 0.0000010, mae: 0.0006641 test: loss0.0001109, mae:0.0075740
training loss 4.699051316947589e-07 mae 0.00048778890050016344
training loss 1.0455930671870356e-06 mae 0.000665898172883317
training loss 9.533875700612092e-07 mae 0.000646431873884458
training loss 9.940413567303324e-07 mae 0.0006601286528551866
training loss 9.931129071035017e-07 mae 0.0006612656296020031
Epoch 633, training: loss: 0.0000010, mae: 0.0006613 test: loss0.0001115, mae:0.0076089
training loss 5.445783131108328e-07 mae 0.0005055942456237972
training loss 9.565870991358609e-07 mae 0.0006598142179998331
training loss 9.88862092074192e-07 mae 0.0006592436491899046
training loss 9.949972448848354e-07 mae 0.0006599017462865852
training loss 9.994847541623301e-07 mae 0.0006622674839341763
Epoch 634, training: loss: 0.0000010, mae: 0.0006604 test: loss0.0001111, mae:0.0075856
training loss 5.207517119742988e-07 mae 0.0005362543161027133
training loss 1.0017744439299387e-06 mae 0.0006395317648318322
training loss 9.794321753719098e-07 mae 0.0006482935312323937
training loss 1.0114193804008899e-06 mae 0.0006588021010339311
training loss 9.967721046963049e-07 mae 0.0006605045799286433
Epoch 635, training: loss: 0.0000010, mae: 0.0006616 test: loss0.0001114, mae:0.0076005
training loss 1.0106956551680923e-06 mae 0.0006760889664292336
training loss 8.930428560135769e-07 mae 0.0006391244065062162
training loss 9.724663038135684e-07 mae 0.0006543154010461587
training loss 9.711801977718047e-07 mae 0.000652943140116292
training loss 9.888167421672625e-07 mae 0.0006570916141288363
Epoch 636, training: loss: 0.0000010, mae: 0.0006589 test: loss0.0001109, mae:0.0075779
training loss 6.06466812769213e-07 mae 0.0005647090147249401
training loss 9.387141818371996e-07 mae 0.0006499601249564805
training loss 9.438550519727941e-07 mae 0.0006517647813987172
training loss 9.747584620027131e-07 mae 0.0006545851113496734
training loss 9.849366276468112e-07 mae 0.0006565866664350869
Epoch 637, training: loss: 0.0000010, mae: 0.0006573 test: loss0.0001111, mae:0.0075860
training loss 1.4377834531842382e-06 mae 0.000789376616012305
training loss 9.605303839456538e-07 mae 0.0006452326436855777
training loss 9.793257015982169e-07 mae 0.0006554291867720202
training loss 9.728176897151695e-07 mae 0.0006552136264932619
training loss 9.828594422844572e-07 mae 0.0006567209700425967
Epoch 638, training: loss: 0.0000010, mae: 0.0006589 test: loss0.0001107, mae:0.0075640
training loss 9.61012915468018e-07 mae 0.0006995850126259029
training loss 9.639125835292364e-07 mae 0.000642438932760235
training loss 1.0108130770686785e-06 mae 0.0006603106610260275
training loss 1.0265764332557987e-06 mae 0.0006630171758403374
training loss 1.002275527201196e-06 mae 0.0006615722917404901
Epoch 639, training: loss: 0.0000010, mae: 0.0006578 test: loss0.0001108, mae:0.0075751
training loss 1.0033764965555747e-06 mae 0.0006087885121814907
training loss 9.752382396197625e-07 mae 0.0006546445257103473
training loss 9.677730415682852e-07 mae 0.0006475093371654391
training loss 9.547415233618757e-07 mae 0.0006515559598121992
training loss 9.790198524554025e-07 mae 0.0006559494557322482
Epoch 640, training: loss: 0.0000010, mae: 0.0006553 test: loss0.0001113, mae:0.0075978
training loss 4.7634830480092205e-07 mae 0.0005417028442025185
training loss 8.84838801410154e-07 mae 0.0006151305529855044
training loss 9.609702218394076e-07 mae 0.0006398952377056425
training loss 9.646543972978523e-07 mae 0.0006463707597010354
training loss 9.856244221271077e-07 mae 0.000656379999700628
Epoch 641, training: loss: 0.0000010, mae: 0.0006556 test: loss0.0001112, mae:0.0075970
training loss 1.0705349495765404e-06 mae 0.0006470925291068852
training loss 9.599398805783174e-07 mae 0.0006464196371334588
training loss 9.340892732568435e-07 mae 0.0006431984786626577
training loss 9.330874434375715e-07 mae 0.0006409383011559357
training loss 9.806509967010562e-07 mae 0.000656045255261654
Epoch 642, training: loss: 0.0000010, mae: 0.0006553 test: loss0.0001110, mae:0.0075844
training loss 7.672524588997476e-07 mae 0.0006095503340475261
training loss 1.024943789692164e-06 mae 0.0006646668491656799
training loss 9.960825426526637e-07 mae 0.0006639609893664052
training loss 9.548428443090924e-07 mae 0.0006533420503228928
training loss 9.834858489832169e-07 mae 0.000659407886203879
Epoch 643, training: loss: 0.0000010, mae: 0.0006594 test: loss0.0001107, mae:0.0075680
training loss 9.946508043867652e-07 mae 0.0006475106929428875
training loss 1.0058592248269103e-06 mae 0.0006471445821636521
training loss 9.682671875993095e-07 mae 0.0006464286041699609
training loss 9.685052533910158e-07 mae 0.0006518766464394158
training loss 9.856010623099873e-07 mae 0.0006579834430951126
Epoch 644, training: loss: 0.0000010, mae: 0.0006582 test: loss0.0001110, mae:0.0075838
training loss 1.1770886203521513e-06 mae 0.0007338924333453178
training loss 9.067942264263204e-07 mae 0.0006379790789009456
training loss 9.497834502335446e-07 mae 0.0006469615321896862
training loss 9.599840874195088e-07 mae 0.0006503171595091014
training loss 9.823292666861744e-07 mae 0.0006583408604315787
Epoch 645, training: loss: 0.0000010, mae: 0.0006574 test: loss0.0001120, mae:0.0076232
training loss 1.296173991249816e-06 mae 0.0007047491963021457
training loss 9.602902650840406e-07 mae 0.000645906802531624
training loss 9.942539968010754e-07 mae 0.000658300365229128
training loss 9.815710779928142e-07 mae 0.0006549882186755589
training loss 9.760166960010942e-07 mae 0.000654594930159901
Epoch 646, training: loss: 0.0000010, mae: 0.0006548 test: loss0.0001120, mae:0.0076149
training loss 1.4353578308146098e-06 mae 0.0008405586704611778
training loss 9.87836798748165e-07 mae 0.000653935804638975
training loss 9.913730189685408e-07 mae 0.0006632525462306812
training loss 9.912320725284529e-07 mae 0.0006574022638401788
training loss 9.790327132082187e-07 mae 0.0006532949065055174
Epoch 647, training: loss: 0.0000010, mae: 0.0006528 test: loss0.0001109, mae:0.0075774
training loss 6.730963946210977e-07 mae 0.0005590046639554203
training loss 9.584636001470836e-07 mae 0.0006455128462346017
training loss 9.717206304379366e-07 mae 0.000649136344159926
training loss 9.790972358403457e-07 mae 0.0006583716667531715
training loss 9.710741673770187e-07 mae 0.0006556159145411902
Epoch 648, training: loss: 0.0000010, mae: 0.0006543 test: loss0.0001112, mae:0.0075815
training loss 1.789861016732175e-06 mae 0.0007515931501984596
training loss 9.703092570469058e-07 mae 0.0006547274288273984
training loss 9.720668032005591e-07 mae 0.0006514407041952897
training loss 9.533725171907567e-07 mae 0.0006493682275751344
training loss 9.704680199838233e-07 mae 0.000653243961046677
Epoch 649, training: loss: 0.0000010, mae: 0.0006532 test: loss0.0001114, mae:0.0075991
training loss 3.8087304687906e-07 mae 0.0004765127378050238
training loss 8.898584453350742e-07 mae 0.0006124528185418789
training loss 8.872144287624456e-07 mae 0.0006206684591972482
training loss 9.302146169516738e-07 mae 0.000638558260251503
training loss 9.568182843313893e-07 mae 0.0006472537236401712
Epoch 650, training: loss: 0.0000010, mae: 0.0006508 test: loss0.0001121, mae:0.0076242
training loss 8.462077403237345e-07 mae 0.0006542137707583606
training loss 9.967039786448258e-07 mae 0.0006549634511911255
training loss 1.0200079700125305e-06 mae 0.0006598692746385748
training loss 9.892268640847181e-07 mae 0.0006563038829658547
training loss 9.774853777964015e-07 mae 0.0006553544256018374
Epoch 651, training: loss: 0.0000010, mae: 0.0006533 test: loss0.0001116, mae:0.0076045
training loss 5.760226144957414e-07 mae 0.0004740810545627028
training loss 1.0032722585430177e-06 mae 0.0006460184003135153
training loss 9.269642751603735e-07 mae 0.0006356693079261719
training loss 9.338574061801421e-07 mae 0.0006398081072892947
training loss 9.608785039722647e-07 mae 0.0006489136535153073
Epoch 652, training: loss: 0.0000010, mae: 0.0006512 test: loss0.0001122, mae:0.0076083
training loss 6.714458891110553e-07 mae 0.0005486871232278645
training loss 8.807115339586029e-07 mae 0.0006340217232183717
training loss 9.163107193054912e-07 mae 0.0006392063667518224
training loss 9.354834229053803e-07 mae 0.0006478862237148232
training loss 9.777100586209269e-07 mae 0.0006554778912903932
Epoch 653, training: loss: 0.0000010, mae: 0.0006533 test: loss0.0001108, mae:0.0075726
training loss 1.5937871467031073e-06 mae 0.0008328625117428601
training loss 1.0768093155619168e-06 mae 0.0006814542684165358
training loss 9.887384489146741e-07 mae 0.0006565746809770859
training loss 9.58960721233014e-07 mae 0.0006489636878011166
training loss 9.565402770270908e-07 mae 0.0006473385055598091
Epoch 654, training: loss: 0.0000010, mae: 0.0006486 test: loss0.0001113, mae:0.0076007
training loss 1.3385050579017843e-06 mae 0.0007605999708175659
training loss 1.0030964000518497e-06 mae 0.0006508768579520868
training loss 9.68905967488579e-07 mae 0.0006465017974219919
training loss 9.496291561435714e-07 mae 0.0006446458275170858
training loss 9.590579263466278e-07 mae 0.0006493880566665948
Epoch 655, training: loss: 0.0000010, mae: 0.0006509 test: loss0.0001110, mae:0.0075705
training loss 1.6975901644400437e-06 mae 0.0008810320869088173
training loss 9.604957663035676e-07 mae 0.0006452538803511975
training loss 9.23018594203493e-07 mae 0.000639335393829566
training loss 9.534965132036249e-07 mae 0.0006450112151222558
training loss 9.58238093371212e-07 mae 0.0006488916033818685
Epoch 656, training: loss: 0.0000010, mae: 0.0006502 test: loss0.0001119, mae:0.0076195
training loss 7.295534487639088e-07 mae 0.0006260884110815823
training loss 9.356226326854441e-07 mae 0.0006462274628289626
training loss 9.100826031440776e-07 mae 0.0006375206236169663
training loss 9.57727303932417e-07 mae 0.0006530798330983232
training loss 9.636359161459269e-07 mae 0.0006502560917311229
Epoch 657, training: loss: 0.0000010, mae: 0.0006513 test: loss0.0001110, mae:0.0075821
training loss 5.70379768305429e-07 mae 0.0005500456318259239
training loss 9.02512490327746e-07 mae 0.000622921559945954
training loss 9.748604853002953e-07 mae 0.0006530458274145268
training loss 9.366797811202291e-07 mae 0.0006425114165812679
training loss 9.6232598995961e-07 mae 0.0006532891307707265
Epoch 658, training: loss: 0.0000010, mae: 0.0006528 test: loss0.0001116, mae:0.0075988
training loss 8.183774866665772e-07 mae 0.0005449528689496219
training loss 8.832351883769413e-07 mae 0.0006222781627073738
training loss 9.141478834984502e-07 mae 0.0006297457574792943
training loss 9.249876456837923e-07 mae 0.0006329601020351769
training loss 9.616000087209689e-07 mae 0.0006467887973331787
Epoch 659, training: loss: 0.0000010, mae: 0.0006474 test: loss0.0001116, mae:0.0076163
training loss 8.329708975907124e-07 mae 0.0005478135426528752
training loss 8.767482881921751e-07 mae 0.0006187445199911428
training loss 9.147329607084528e-07 mae 0.0006274336414171917
training loss 9.378088431267377e-07 mae 0.0006362381323768035
training loss 9.542141480328587e-07 mae 0.0006458996350596663
Epoch 660, training: loss: 0.0000010, mae: 0.0006463 test: loss0.0001119, mae:0.0076205
training loss 5.124199446981947e-07 mae 0.0005009112064726651
training loss 9.371985000000802e-07 mae 0.000634822270298815
training loss 9.410941817516839e-07 mae 0.0006438278426568367
training loss 9.500899522365385e-07 mae 0.0006454179203369044
training loss 9.605814736360618e-07 mae 0.0006484429620435712
Epoch 661, training: loss: 0.0000010, mae: 0.0006475 test: loss0.0001189, mae:0.0076686
training loss 1.5861875226619304e-06 mae 0.0007906866376288235
training loss 1.0174473336697772e-06 mae 0.0006710863289852428
training loss 9.567807234190585e-07 mae 0.0006557611312306863
training loss 9.708094952979353e-07 mae 0.0006518922938922086
training loss 9.513700403129319e-07 mae 0.000645282391032473
Epoch 662, training: loss: 0.0000010, mae: 0.0006471 test: loss0.0001113, mae:0.0076003
training loss 1.4820925571257249e-06 mae 0.0007902790675871074
training loss 9.429621055754552e-07 mae 0.0006481951870727264
training loss 9.066670242132791e-07 mae 0.0006311831677143492
training loss 9.355984414225342e-07 mae 0.0006445548952008224
training loss 9.607973615472579e-07 mae 0.0006502915053009818
Epoch 663, training: loss: 0.0000010, mae: 0.0006502 test: loss0.0001116, mae:0.0076072
training loss 7.35142066332628e-07 mae 0.0005487113376148045
training loss 8.890669146596479e-07 mae 0.0006338857312905877
training loss 9.13165153649358e-07 mae 0.0006353129681181348
training loss 9.171074671843635e-07 mae 0.0006366700494109918
training loss 9.489048755684018e-07 mae 0.0006497514889394835
Epoch 664, training: loss: 0.0000010, mae: 0.0006500 test: loss0.0001119, mae:0.0076174
training loss 1.650580884415831e-06 mae 0.0008993701194413006
training loss 9.34539743264609e-07 mae 0.0006373490699950387
training loss 9.824324983542688e-07 mae 0.000650336397097142
training loss 9.664146190763422e-07 mae 0.0006460215565239678
training loss 9.631768550582758e-07 mae 0.0006489621029948975
Epoch 665, training: loss: 0.0000010, mae: 0.0006477 test: loss0.0001117, mae:0.0076069
training loss 9.9025055533275e-07 mae 0.000657619268167764
training loss 1.0176177190313705e-06 mae 0.0006758762419671185
training loss 9.646318228859775e-07 mae 0.000650165019335576
training loss 9.676546013212606e-07 mae 0.0006496945960024709
training loss 9.610741332142026e-07 mae 0.0006502460050095448
Epoch 666, training: loss: 0.0000010, mae: 0.0006489 test: loss0.0001115, mae:0.0076057
training loss 1.3803145293422858e-06 mae 0.0007678139954805374
training loss 8.225971999077135e-07 mae 0.0006120838824084357
training loss 8.843859855050602e-07 mae 0.0006241749298970886
training loss 9.173660540140169e-07 mae 0.0006284739431656819
training loss 9.426319502692463e-07 mae 0.0006405643393941201
Epoch 667, training: loss: 0.0000009, mae: 0.0006431 test: loss0.0001115, mae:0.0076018
training loss 5.343209750208189e-07 mae 0.0005317274481058121
training loss 9.116247065990404e-07 mae 0.000624164912676183
training loss 9.45079087763012e-07 mae 0.0006315523938916734
training loss 9.719129491560237e-07 mae 0.0006437831437246047
training loss 9.521293481976045e-07 mae 0.0006450672769474225
Epoch 668, training: loss: 0.0000009, mae: 0.0006449 test: loss0.0001116, mae:0.0076063
training loss 5.563426270782657e-07 mae 0.0005358690395951271
training loss 9.139157499453621e-07 mae 0.000629276271188157
training loss 9.033286521526403e-07 mae 0.0006345000324337421
training loss 9.365270542412564e-07 mae 0.0006392093748085523
training loss 9.505867503736648e-07 mae 0.0006420710270318541
Epoch 669, training: loss: 0.0000009, mae: 0.0006415 test: loss0.0001115, mae:0.0075995
training loss 2.069596575893229e-06 mae 0.0008639516308903694
training loss 1.0441495335806609e-06 mae 0.0006558345995989501
training loss 9.775785338544012e-07 mae 0.0006446641899234994
training loss 9.409083965530216e-07 mae 0.0006365422354101369
training loss 9.449389281924784e-07 mae 0.0006411818077090422
Epoch 670, training: loss: 0.0000009, mae: 0.0006405 test: loss0.0001117, mae:0.0076107
training loss 1.3801912928101956e-06 mae 0.0008923308923840523
training loss 9.170329115156218e-07 mae 0.0006382751282003217
training loss 9.051274365348114e-07 mae 0.0006261867169051037
training loss 9.265275190310958e-07 mae 0.0006347008111936022
training loss 9.308003276843161e-07 mae 0.0006378063486381641
Epoch 671, training: loss: 0.0000009, mae: 0.0006406 test: loss0.0001116, mae:0.0076083
training loss 6.716809934914636e-07 mae 0.0005683724884875119
training loss 8.737881175563291e-07 mae 0.0006228389073501102
training loss 9.088075678324226e-07 mae 0.0006313046574763177
training loss 9.187081966818081e-07 mae 0.0006356797329865591
training loss 9.483390960522953e-07 mae 0.0006430968772550459
Epoch 672, training: loss: 0.0000009, mae: 0.0006421 test: loss0.0001113, mae:0.0075872
training loss 8.742443355913565e-07 mae 0.0006852351943962276
training loss 8.776271571088157e-07 mae 0.0006172292090851963
training loss 9.162090261477462e-07 mae 0.0006325902191963562
training loss 9.385279253627305e-07 mae 0.0006397776560093097
training loss 9.372983462055882e-07 mae 0.0006400888446319978
Epoch 673, training: loss: 0.0000009, mae: 0.0006420 test: loss0.0001131, mae:0.0076494
training loss 5.541239715967095e-07 mae 0.0005217923899181187
training loss 9.500211225107097e-07 mae 0.0006380781694543641
training loss 9.10206693734466e-07 mae 0.00063287518171612
training loss 9.168578564387513e-07 mae 0.0006365813126028921
training loss 9.31350804160217e-07 mae 0.0006391124167671864
Epoch 674, training: loss: 0.0000009, mae: 0.0006421 test: loss0.0001119, mae:0.0076124
training loss 1.051833237397659e-06 mae 0.0006368476897478104
training loss 8.939268542128371e-07 mae 0.0006246817230895235
training loss 9.033375205701919e-07 mae 0.0006290170096672407
training loss 9.3466502429969e-07 mae 0.000640329186702826
training loss 9.366285275373956e-07 mae 0.0006420211688399931
Epoch 675, training: loss: 0.0000009, mae: 0.0006423 test: loss0.0001124, mae:0.0076326
training loss 8.452084330201615e-07 mae 0.0005562647129409015
training loss 9.789504899264801e-07 mae 0.0006519263688995337
training loss 9.516698072491293e-07 mae 0.0006407779044388694
training loss 9.354380063312734e-07 mae 0.0006374905575128483
training loss 9.457409746924509e-07 mae 0.0006403780929785943
Epoch 676, training: loss: 0.0000009, mae: 0.0006400 test: loss0.0001114, mae:0.0075861
training loss 8.764995413912402e-07 mae 0.0005933760548941791
training loss 8.792937130826484e-07 mae 0.0006139978581546423
training loss 8.947840161220098e-07 mae 0.0006228750191241652
training loss 9.181729831423898e-07 mae 0.0006336180239974443
training loss 9.325241602148786e-07 mae 0.000638146844631135
Epoch 677, training: loss: 0.0000009, mae: 0.0006408 test: loss0.0001113, mae:0.0075917
training loss 8.369743795810791e-07 mae 0.0005590400542132556
training loss 9.481279814245482e-07 mae 0.0006304323632160528
training loss 9.188037281411636e-07 mae 0.0006323710944397501
training loss 9.309673194347968e-07 mae 0.0006363221439563775
training loss 9.341330065689724e-07 mae 0.0006385743031591012
Epoch 678, training: loss: 0.0000009, mae: 0.0006407 test: loss0.0001114, mae:0.0076000
training loss 6.896444233461807e-07 mae 0.00045429449528455734
training loss 8.037198872622028e-07 mae 0.0005966377645438794
training loss 8.653107414320943e-07 mae 0.0006189259877981673
training loss 9.037896722704004e-07 mae 0.0006318250620916535
training loss 9.228836228514445e-07 mae 0.0006376389523826784
Epoch 679, training: loss: 0.0000009, mae: 0.0006394 test: loss0.0001112, mae:0.0075892
training loss 1.004230284706864e-06 mae 0.000708412379026413
training loss 9.382621869338206e-07 mae 0.0006387856242237793
training loss 9.590418702004225e-07 mae 0.0006446017380020035
training loss 9.241107881391616e-07 mae 0.0006335371937941982
training loss 9.257712468951384e-07 mae 0.0006381477892748661
Epoch 680, training: loss: 0.0000009, mae: 0.0006396 test: loss0.0001116, mae:0.0076050
training loss 8.271442197838041e-07 mae 0.0005906385486014187
training loss 9.205277318441777e-07 mae 0.0006316590520060236
training loss 9.178601147249364e-07 mae 0.0006302732750681221
training loss 9.178345645306388e-07 mae 0.0006305450289849414
training loss 9.306620146372862e-07 mae 0.000636717557254947
Epoch 681, training: loss: 0.0000009, mae: 0.0006389 test: loss0.0001117, mae:0.0076060
training loss 9.305226171818504e-07 mae 0.0006931250100024045
training loss 8.636539023115677e-07 mae 0.00061605018495089
training loss 9.110629312180219e-07 mae 0.0006293062781131285
training loss 9.313980632403957e-07 mae 0.0006375742917387002
training loss 9.249995772636826e-07 mae 0.0006386655122470298
Epoch 682, training: loss: 0.0000009, mae: 0.0006399 test: loss0.0001138, mae:0.0076789
training loss 7.724070201220457e-07 mae 0.0006372962961904705
training loss 8.04249159121658e-07 mae 0.0006046107212337207
training loss 8.504258838875462e-07 mae 0.0006162378940463878
training loss 8.981810921211598e-07 mae 0.0006277214615090768
training loss 9.25535939037915e-07 mae 0.0006373831119031564
Epoch 683, training: loss: 0.0000009, mae: 0.0006383 test: loss0.0001126, mae:0.0076164
training loss 7.201358585007256e-07 mae 0.000601831532549113
training loss 8.994410548296525e-07 mae 0.0006229951013308748
training loss 9.067731120426694e-07 mae 0.0006303010173579955
training loss 8.96849356222266e-07 mae 0.0006318158232637783
training loss 9.248970259385944e-07 mae 0.0006376685065261331
Epoch 684, training: loss: 0.0000009, mae: 0.0006389 test: loss0.0001119, mae:0.0076028
training loss 8.314591468661092e-07 mae 0.0006778929382562637
training loss 9.343070859376539e-07 mae 0.0006478207143635362
training loss 9.042980029744735e-07 mae 0.0006345621708407074
training loss 9.102004344637578e-07 mae 0.0006356425608418856
training loss 9.210932303538365e-07 mae 0.0006380238883320094
Epoch 685, training: loss: 0.0000009, mae: 0.0006392 test: loss0.0001134, mae:0.0076520
training loss 5.240090104052797e-07 mae 0.0005072482745163143
training loss 8.69716314234799e-07 mae 0.0006202808375853825
training loss 8.726119318961906e-07 mae 0.0006176222134197109
training loss 9.11313264474386e-07 mae 0.0006299586309388349
training loss 9.191087090967263e-07 mae 0.0006347009723899483
Epoch 686, training: loss: 0.0000009, mae: 0.0006347 test: loss0.0001118, mae:0.0076092
training loss 1.2322209386184113e-06 mae 0.0006511748651973903
training loss 9.39729405855342e-07 mae 0.0006462093739423388
training loss 9.377301156215111e-07 mae 0.0006408560404434136
training loss 9.352071313270964e-07 mae 0.0006400638308903412
training loss 9.269887586913288e-07 mae 0.000640822034120671
Epoch 687, training: loss: 0.0000009, mae: 0.0006412 test: loss0.0001121, mae:0.0076204
training loss 4.608807557815453e-07 mae 0.0005265318905003369
training loss 8.416791070644914e-07 mae 0.0006104316356975365
training loss 8.677635661916779e-07 mae 0.0006179747916297011
training loss 8.852116252079298e-07 mae 0.0006270937082690762
training loss 9.21703184276475e-07 mae 0.0006368215988153843
Epoch 688, training: loss: 0.0000009, mae: 0.0006368 test: loss0.0001130, mae:0.0076521
training loss 1.1013361245204578e-06 mae 0.0007291557267308235
training loss 8.920649547155123e-07 mae 0.0006229811617374128
training loss 8.610778095744806e-07 mae 0.0006134558747416491
training loss 8.916997277664517e-07 mae 0.0006234666771280104
training loss 9.101445340405268e-07 mae 0.0006298981754651377
Epoch 689, training: loss: 0.0000009, mae: 0.0006342 test: loss0.0001119, mae:0.0076188
training loss 5.96462712110224e-07 mae 0.000538090243935585
training loss 9.48231485209612e-07 mae 0.0006330820227352282
training loss 9.141458431855373e-07 mae 0.0006275631306983425
training loss 9.168605144332529e-07 mae 0.0006288945255107218
training loss 9.166059195942706e-07 mae 0.0006309210116319376
Epoch 690, training: loss: 0.0000009, mae: 0.0006319 test: loss0.0001120, mae:0.0076189
training loss 1.0217307817583787e-06 mae 0.0006561397458426654
training loss 9.208938982006728e-07 mae 0.0006266288948930141
training loss 9.398204763509077e-07 mae 0.000633546401629003
training loss 9.190140253804505e-07 mae 0.0006302094028411518
training loss 9.230433558278763e-07 mae 0.0006312816664437529
Epoch 691, training: loss: 0.0000009, mae: 0.0006293 test: loss0.0001122, mae:0.0076369
training loss 5.046098863203952e-07 mae 0.0005335059831850231
training loss 8.278123094022898e-07 mae 0.0006003677297164412
training loss 8.883864190560382e-07 mae 0.0006263823640915466
training loss 9.076080922288433e-07 mae 0.0006290749893045757
training loss 9.148786861445823e-07 mae 0.0006311896595344022
Epoch 692, training: loss: 0.0000009, mae: 0.0006327 test: loss0.0001117, mae:0.0076020
training loss 3.350412498548394e-07 mae 0.00042900009430013597
training loss 8.77710605476857e-07 mae 0.0006143665226523344
training loss 8.650004636000637e-07 mae 0.000614521504289904
training loss 8.745654854736144e-07 mae 0.0006208469409307476
training loss 9.172937790485754e-07 mae 0.0006331410235841179
Epoch 693, training: loss: 0.0000009, mae: 0.0006332 test: loss0.0001123, mae:0.0076311
training loss 7.394621093226306e-07 mae 0.0006010252982378006
training loss 9.465224067360965e-07 mae 0.0006413274500564689
training loss 9.29118300237491e-07 mae 0.0006403747800792944
training loss 9.136026935886654e-07 mae 0.0006339300361412596
training loss 9.143253250772767e-07 mae 0.000633249335009512
Epoch 694, training: loss: 0.0000009, mae: 0.0006337 test: loss0.0001120, mae:0.0076152
training loss 1.1412735148041975e-06 mae 0.0007991160382516682
training loss 9.381139883388123e-07 mae 0.0006250879406591184
training loss 9.14384519274331e-07 mae 0.0006272774912984027
training loss 8.871430789392451e-07 mae 0.0006248977893750665
training loss 9.15887446141068e-07 mae 0.000633498740791394
Epoch 695, training: loss: 0.0000009, mae: 0.0006343 test: loss0.0001119, mae:0.0076049
training loss 7.185660138020467e-07 mae 0.0005404899711720645
training loss 9.032411889629477e-07 mae 0.0006170067548130948
training loss 9.326161349722179e-07 mae 0.000630800592388017
training loss 9.068452418778901e-07 mae 0.0006259543453466564
training loss 9.133838485995152e-07 mae 0.0006314246744132915
Epoch 696, training: loss: 0.0000009, mae: 0.0006307 test: loss0.0001122, mae:0.0076263
training loss 5.064726451564638e-07 mae 0.0005112141370773315
training loss 9.200910088726662e-07 mae 0.0006312297499629064
training loss 9.041392266424575e-07 mae 0.0006266876635611941
training loss 8.96927278761916e-07 mae 0.000626211945855657
training loss 9.152770892240426e-07 mae 0.0006323764158588886
Epoch 697, training: loss: 0.0000009, mae: 0.0006318 test: loss0.0001121, mae:0.0076197
training loss 8.935967343859375e-07 mae 0.0006523163174279034
training loss 7.902037193126294e-07 mae 0.000604200162057418
training loss 8.389802318750225e-07 mae 0.0006108935611174983
training loss 8.788328784703481e-07 mae 0.0006226973643985928
training loss 9.101751443629848e-07 mae 0.0006303698773176144
Epoch 698, training: loss: 0.0000009, mae: 0.0006324 test: loss0.0001127, mae:0.0076361
training loss 6.719528755638748e-07 mae 0.0005739293992519379
training loss 9.749131610239123e-07 mae 0.0006481595000699527
training loss 9.58853604515753e-07 mae 0.0006464919052893607
training loss 9.314544193492847e-07 mae 0.0006378764677515245
training loss 9.162137588271716e-07 mae 0.0006340394695451605
Epoch 699, training: loss: 0.0000009, mae: 0.0006332 test: loss0.0001127, mae:0.0076372
current learning rate: 3.90625e-06
training loss 1.0789684665724053e-06 mae 0.0006139834295026958
training loss 8.725676085465665e-07 mae 0.0006110815282262789
training loss 8.65426972857019e-07 mae 0.0006080296479562058
training loss 8.558798753224647e-07 mae 0.0006052740303713164
training loss 8.615877656787308e-07 mae 0.0006037738121494611
Epoch 700, training: loss: 0.0000009, mae: 0.0006087 test: loss0.0001121, mae:0.0076245
training loss 7.27193764760159e-07 mae 0.0005562503938563168
training loss 8.922717226516182e-07 mae 0.0006062444961428936
training loss 9.030386755818321e-07 mae 0.0006096743408585945
training loss 8.691052604362333e-07 mae 0.0006016944112974706
training loss 8.705478176781429e-07 mae 0.000604018985278747
Epoch 701, training: loss: 0.0000009, mae: 0.0006050 test: loss0.0001119, mae:0.0076122
training loss 6.558232712450263e-07 mae 0.0005546768079511821
training loss 8.314769271531558e-07 mae 0.0005930310883331022
training loss 8.535250813538638e-07 mae 0.000601352559861331
training loss 8.48335750597983e-07 mae 0.0005986291550127605
training loss 8.657748751018708e-07 mae 0.0006021879951061858
Epoch 702, training: loss: 0.0000009, mae: 0.0006016 test: loss0.0001123, mae:0.0076146
training loss 7.647834081581095e-07 mae 0.0006320082466118038
training loss 7.758392642057621e-07 mae 0.0005778949667800071
training loss 8.277802044964425e-07 mae 0.0005871710228609232
training loss 8.296837862923252e-07 mae 0.0005891462132667766
training loss 8.652967701255871e-07 mae 0.0006007818732206797
Epoch 703, training: loss: 0.0000009, mae: 0.0006003 test: loss0.0001123, mae:0.0076253
training loss 8.340588806277083e-07 mae 0.0006424912135116756
training loss 8.965072840608484e-07 mae 0.0006022627993861178
training loss 8.764423023196718e-07 mae 0.0005994990962502169
training loss 8.524109760413237e-07 mae 0.0005961278286170435
training loss 8.636921420622181e-07 mae 0.0006017743520651806
Epoch 704, training: loss: 0.0000009, mae: 0.0006018 test: loss0.0001120, mae:0.0076060
training loss 5.736374077969231e-07 mae 0.0004812596889678389
training loss 7.936895932115314e-07 mae 0.0005813048508785226
training loss 7.988902106800725e-07 mae 0.0005834136889569718
training loss 8.219546056531741e-07 mae 0.0005888548523144462
training loss 8.544035127603464e-07 mae 0.0005984205708593428
Epoch 705, training: loss: 0.0000009, mae: 0.0006023 test: loss0.0001131, mae:0.0076661
training loss 1.1702665005941526e-06 mae 0.0006047201459296048
training loss 7.778614142174701e-07 mae 0.0005757939184595849
training loss 8.7204953211488e-07 mae 0.0006007432907653768
training loss 8.631443589419724e-07 mae 0.0006001983872932437
training loss 8.672791835099486e-07 mae 0.0006036166036825868
Epoch 706, training: loss: 0.0000009, mae: 0.0006036 test: loss0.0001124, mae:0.0076236
training loss 6.24166489160416e-07 mae 0.000472482293844223
training loss 8.121836761650534e-07 mae 0.0005826803121724913
training loss 8.534204368407871e-07 mae 0.0005925215580754239
training loss 8.520269070822004e-07 mae 0.0005955273842143004
training loss 8.660635364465597e-07 mae 0.000600652951939467
Epoch 707, training: loss: 0.0000009, mae: 0.0006009 test: loss0.0001123, mae:0.0076212
training loss 7.308759109037055e-07 mae 0.0005796995828859508
training loss 8.748129592580274e-07 mae 0.0006079499868849544
training loss 8.296771009009711e-07 mae 0.0005923225797970991
training loss 8.558948098014306e-07 mae 0.000599963836439743
training loss 8.66788168785215e-07 mae 0.0006023098356136819
Epoch 708, training: loss: 0.0000009, mae: 0.0006027 test: loss0.0001123, mae:0.0076274
training loss 1.1374303312550182e-06 mae 0.0006520096212625504
training loss 8.123416702186232e-07 mae 0.0005878713324337321
training loss 8.341448495450617e-07 mae 0.0005911944458828227
training loss 8.623389531984029e-07 mae 0.0005978981897601896
training loss 8.614422287805541e-07 mae 0.0005991889105354485
Epoch 709, training: loss: 0.0000009, mae: 0.0006008 test: loss0.0001126, mae:0.0076368
training loss 8.408137546211947e-07 mae 0.0006176971946842968
training loss 7.772102692320449e-07 mae 0.0005722792485874951
training loss 8.551912631250755e-07 mae 0.0005984914706360625
training loss 8.641657462434197e-07 mae 0.0006031306834031045
training loss 8.681911472947026e-07 mae 0.0006029076106262519
Epoch 710, training: loss: 0.0000009, mae: 0.0006004 test: loss0.0001122, mae:0.0076250
training loss 5.759160899287963e-07 mae 0.0005479284445755184
training loss 7.84600832931199e-07 mae 0.0005762913017807639
training loss 8.413586553529913e-07 mae 0.0005907451827309583
training loss 8.451726111257777e-07 mae 0.0005977439554562387
training loss 8.67310921368282e-07 mae 0.0006025660508231205
Epoch 711, training: loss: 0.0000009, mae: 0.0006022 test: loss0.0001123, mae:0.0076240
training loss 5.029176577409089e-07 mae 0.0004230473714414984
training loss 7.822813154903356e-07 mae 0.0005694541146558725
training loss 8.282375767751069e-07 mae 0.0005887157798371284
training loss 8.575378324965173e-07 mae 0.0005970070131249502
training loss 8.631299766280584e-07 mae 0.0006009291830549099
Epoch 712, training: loss: 0.0000009, mae: 0.0006029 test: loss0.0001128, mae:0.0076390
training loss 3.9820534425416554e-07 mae 0.00044714813702739775
training loss 7.830850552404715e-07 mae 0.00056898983380319
training loss 8.236475157265867e-07 mae 0.0005863290507747508
training loss 8.560830953445846e-07 mae 0.000597282443103066
training loss 8.632929937966447e-07 mae 0.0006000801688639342
Epoch 713, training: loss: 0.0000009, mae: 0.0006019 test: loss0.0001127, mae:0.0076481
training loss 3.4404081361572025e-07 mae 0.0004447801038622856
training loss 7.436246695711888e-07 mae 0.000563627173416499
training loss 8.26655548871488e-07 mae 0.0005892635781852766
training loss 8.584457784033493e-07 mae 0.0006011492086633026
training loss 8.552010382288981e-07 mae 0.0005999012570480689
Epoch 714, training: loss: 0.0000009, mae: 0.0006018 test: loss0.0001124, mae:0.0076319
training loss 6.610087552871846e-07 mae 0.0005842270329594612
training loss 8.529998188921537e-07 mae 0.0006004740636996633
training loss 8.550492501142637e-07 mae 0.000598933347466282
training loss 8.754167740202598e-07 mae 0.0006041762755184581
training loss 8.638973475113132e-07 mae 0.0006011206548387622
Epoch 715, training: loss: 0.0000009, mae: 0.0006007 test: loss0.0001125, mae:0.0076367
training loss 9.07401044969447e-07 mae 0.000623698637355119
training loss 7.978198820022526e-07 mae 0.0005785260051625837
training loss 8.213135042650368e-07 mae 0.000584897496749066
training loss 8.489761886464191e-07 mae 0.0005952204187903305
training loss 8.594702844126698e-07 mae 0.0005990341001662857
Epoch 716, training: loss: 0.0000009, mae: 0.0005995 test: loss0.0001118, mae:0.0076018
training loss 1.1740235095203388e-06 mae 0.0007421700283885002
training loss 8.346647169499941e-07 mae 0.000594011293929618
training loss 8.449169285714193e-07 mae 0.000596829664681365
training loss 8.472520910488382e-07 mae 0.0005955124738015577
training loss 8.581439781306213e-07 mae 0.000598188815899054
Epoch 717, training: loss: 0.0000009, mae: 0.0006000 test: loss0.0001127, mae:0.0076467
training loss 1.1216520761081483e-06 mae 0.0007156000356189907
training loss 8.739196487692655e-07 mae 0.0006024249339722752
training loss 8.385712491043805e-07 mae 0.0005885884113895641
training loss 8.401154862994437e-07 mae 0.0005887265533782442
training loss 8.604102343901135e-07 mae 0.0005971273880530684
Epoch 718, training: loss: 0.0000009, mae: 0.0005987 test: loss0.0001182, mae:0.0076855
training loss 8.09548339475441e-07 mae 0.0005554007366299629
training loss 8.487753519807929e-07 mae 0.0006023445120994368
training loss 8.829172478015444e-07 mae 0.0006032937744028385
training loss 8.551977350032526e-07 mae 0.0005967389764834739
training loss 8.562076441850324e-07 mae 0.0005998517178399812
Epoch 719, training: loss: 0.0000009, mae: 0.0005996 test: loss0.0001122, mae:0.0076223
training loss 2.9538475132540043e-07 mae 0.00037901135510765016
training loss 9.246177194868592e-07 mae 0.0006095888578177737
training loss 8.764883522426227e-07 mae 0.000604170653219842
training loss 8.657464315655674e-07 mae 0.0006027111354968603
training loss 8.524537985526852e-07 mae 0.0005986307695825396
Epoch 720, training: loss: 0.0000009, mae: 0.0005986 test: loss0.0001120, mae:0.0076150
training loss 7.864289273129543e-07 mae 0.0005768304690718651
training loss 8.574228103293088e-07 mae 0.0005881027004672397
training loss 8.615561332961455e-07 mae 0.0005966381413059881
training loss 8.663273066516416e-07 mae 0.0005956863930883711
training loss 8.537164791617338e-07 mae 0.0005956156463397023
Epoch 721, training: loss: 0.0000009, mae: 0.0005965 test: loss0.0001129, mae:0.0076468
training loss 3.4811401405931974e-07 mae 0.00044482448720373213
training loss 8.401642026447186e-07 mae 0.0006012229024039984
training loss 8.633803389275813e-07 mae 0.0006003103851188566
training loss 8.767875129745375e-07 mae 0.0006018076413881576
training loss 8.505320327128006e-07 mae 0.0005987189137909688
Epoch 722, training: loss: 0.0000009, mae: 0.0005992 test: loss0.0001130, mae:0.0076472
training loss 7.990820449776947e-07 mae 0.0005864659324288368
training loss 7.344645813886658e-07 mae 0.0005675289459188707
training loss 8.297748150193956e-07 mae 0.0005899292856597776
training loss 8.195159329192248e-07 mae 0.0005891232462969935
training loss 8.490198351593329e-07 mae 0.0005983897425183349
Epoch 723, training: loss: 0.0000009, mae: 0.0005986 test: loss0.0001126, mae:0.0076415
training loss 8.469814929412678e-07 mae 0.0005717476014979184
training loss 8.584682672442476e-07 mae 0.0005996889051269081
training loss 7.818260663646804e-07 mae 0.0005784407871807604
training loss 8.134555714409336e-07 mae 0.0005865012585040529
training loss 8.526498711591504e-07 mae 0.0005978166960603767
Epoch 724, training: loss: 0.0000009, mae: 0.0005988 test: loss0.0001122, mae:0.0076235
training loss 4.602528349550994e-07 mae 0.0005243020132184029
training loss 8.659208329025241e-07 mae 0.0005934708072648694
training loss 8.303121023913047e-07 mae 0.0005856975996613244
training loss 8.531261837018947e-07 mae 0.0005958437580778725
training loss 8.632683486395301e-07 mae 0.0005996255928057414
Epoch 725, training: loss: 0.0000009, mae: 0.0005980 test: loss0.0001123, mae:0.0076253
training loss 7.571159699182317e-07 mae 0.0006334672798402607
training loss 8.252191566596451e-07 mae 0.0005950501979575219
training loss 8.50776436688855e-07 mae 0.0005998964283712169
training loss 8.561204990670886e-07 mae 0.0005995011651373488
training loss 8.570919508891812e-07 mae 0.000598858469264322
Epoch 726, training: loss: 0.0000009, mae: 0.0005989 test: loss0.0001129, mae:0.0076440
training loss 8.121987775666639e-07 mae 0.0005918601527810097
training loss 9.187414168179969e-07 mae 0.0006210554074998216
training loss 8.431470925445116e-07 mae 0.0005952208034929332
training loss 8.495013415594806e-07 mae 0.0005957304996543176
training loss 8.520272050967138e-07 mae 0.0005985081072176221
Epoch 727, training: loss: 0.0000009, mae: 0.0005975 test: loss0.0001128, mae:0.0076500
training loss 4.306622543026606e-07 mae 0.00046965977526269853
training loss 8.406259127571271e-07 mae 0.0005951939628906915
training loss 8.462039721960108e-07 mae 0.0005949261269214021
training loss 8.679684791122664e-07 mae 0.0005987044367923814
training loss 8.574532700716044e-07 mae 0.0005988104989119712
Epoch 728, training: loss: 0.0000009, mae: 0.0005992 test: loss0.0001122, mae:0.0076211
training loss 1.3675330592377577e-06 mae 0.000697597221005708
training loss 8.211238742196836e-07 mae 0.0005843029574344992
training loss 8.199124199383718e-07 mae 0.000584906807957715
training loss 8.314749630768494e-07 mae 0.0005908988258918588
training loss 8.578851964159357e-07 mae 0.0005989672602513407
Epoch 729, training: loss: 0.0000009, mae: 0.0005981 test: loss0.0001125, mae:0.0076459
training loss 7.424968657687714e-07 mae 0.0005368292331695557
training loss 7.85178616747633e-07 mae 0.0005802917533882838
training loss 7.605337451999425e-07 mae 0.0005720961527175057
training loss 8.288048688325414e-07 mae 0.0005910588794199764
training loss 8.531666896852128e-07 mae 0.0005966234950473841
Epoch 730, training: loss: 0.0000009, mae: 0.0005952 test: loss0.0001123, mae:0.0076271
training loss 8.756223905947991e-07 mae 0.0005630602245219052
training loss 8.858305195600206e-07 mae 0.0005977991180877914
training loss 8.327761793337924e-07 mae 0.0005863053706304016
training loss 8.3513060004732e-07 mae 0.0005907976490272741
training loss 8.381699454341417e-07 mae 0.0005939255017655163
Epoch 731, training: loss: 0.0000009, mae: 0.0005965 test: loss0.0001123, mae:0.0076224
training loss 8.779088602750562e-07 mae 0.000607202120590955
training loss 8.130393574251061e-07 mae 0.0005862969709971154
training loss 8.377663507957604e-07 mae 0.0005889553423984772
training loss 8.511065436175838e-07 mae 0.0005949724716942341
training loss 8.535319083685902e-07 mae 0.0005980747194719779
Epoch 732, training: loss: 0.0000008, mae: 0.0005968 test: loss0.0001125, mae:0.0076341
training loss 1.5549852605545311e-06 mae 0.0006824368610978127
training loss 8.413277879860876e-07 mae 0.000576838996494189
training loss 8.29241481641934e-07 mae 0.0005847064041680115
training loss 8.450437890690033e-07 mae 0.0005936223185938839
training loss 8.499983499321354e-07 mae 0.0005956526546836348
Epoch 733, training: loss: 0.0000009, mae: 0.0005971 test: loss0.0001128, mae:0.0076428
training loss 7.069208436405461e-07 mae 0.0005442897672764957
training loss 8.511846457997094e-07 mae 0.0005969505080286705
training loss 8.671840444409584e-07 mae 0.000600712060914513
training loss 8.548506920025876e-07 mae 0.0005997717212839356
training loss 8.552439654031513e-07 mae 0.0005982602618933554
Epoch 734, training: loss: 0.0000008, mae: 0.0005962 test: loss0.0001124, mae:0.0076275
training loss 1.0469901781107183e-06 mae 0.0006199625204317272
training loss 8.40694425795507e-07 mae 0.0005892580260942672
training loss 8.51919458788267e-07 mae 0.0005900210085426374
training loss 8.36148329135961e-07 mae 0.0005890526134206214
training loss 8.43065641726947e-07 mae 0.0005942229087411223
Epoch 735, training: loss: 0.0000008, mae: 0.0005948 test: loss0.0001129, mae:0.0076378
training loss 1.0121947298102896e-06 mae 0.0006317893858067691
training loss 8.027586243672118e-07 mae 0.0005837978239344689
training loss 8.305621754448787e-07 mae 0.0005931213306779468
training loss 8.425333062510521e-07 mae 0.000595528840176786
training loss 8.504115575406739e-07 mae 0.0005972489055225266
Epoch 736, training: loss: 0.0000008, mae: 0.0005972 test: loss0.0001129, mae:0.0076445
training loss 9.186397278426739e-07 mae 0.0006144391372799873
training loss 8.525807402293136e-07 mae 0.0005928307775106719
training loss 8.188299701298161e-07 mae 0.0005837420553734322
training loss 8.384858871212544e-07 mae 0.0005898973019870726
training loss 8.516832455691866e-07 mae 0.0005952413781749929
Epoch 737, training: loss: 0.0000008, mae: 0.0005957 test: loss0.0001125, mae:0.0076319
training loss 1.3536199503505486e-06 mae 0.0007280874997377396
training loss 8.401605802699873e-07 mae 0.0005968555233979998
training loss 8.667616308648937e-07 mae 0.0005968609792503095
training loss 8.725942107628195e-07 mae 0.0005987480152490865
training loss 8.534342730801238e-07 mae 0.0005977901215120614
Epoch 738, training: loss: 0.0000008, mae: 0.0005971 test: loss0.0001122, mae:0.0076206
training loss 4.400995692321885e-07 mae 0.0004300624132156372
training loss 8.138810348630916e-07 mae 0.0005763546465530847
training loss 8.269973010733836e-07 mae 0.0005846515546857132
training loss 8.361789875962796e-07 mae 0.0005884604156171498
training loss 8.333266373511921e-07 mae 0.0005896822133840559
Epoch 739, training: loss: 0.0000008, mae: 0.0005922 test: loss0.0001126, mae:0.0076340
training loss 3.6486940757640696e-07 mae 0.00042342767119407654
training loss 8.333865728931697e-07 mae 0.0005873888308711935
training loss 8.308625647732515e-07 mae 0.000592623799543662
training loss 8.381223683756122e-07 mae 0.000591805354508553
training loss 8.446911409403436e-07 mae 0.0005949786868564952
Epoch 740, training: loss: 0.0000008, mae: 0.0005964 test: loss0.0001125, mae:0.0076358
training loss 1.671239260758739e-06 mae 0.000853665464092046
training loss 8.934211762732282e-07 mae 0.0006068149506223993
training loss 8.465308015934394e-07 mae 0.0005965034790350917
training loss 8.301848128444332e-07 mae 0.0005912035439604439
training loss 8.435468728791662e-07 mae 0.0005946169205969062
Epoch 741, training: loss: 0.0000008, mae: 0.0005943 test: loss0.0001129, mae:0.0076498
training loss 3.516068716180598e-07 mae 0.0004255309177096933
training loss 7.899265257242849e-07 mae 0.0005735326701017351
training loss 7.818901908064105e-07 mae 0.0005727237684647597
training loss 8.227096102006505e-07 mae 0.0005889450659010164
training loss 8.350954808904941e-07 mae 0.0005930546591225175
Epoch 742, training: loss: 0.0000008, mae: 0.0005948 test: loss0.0001125, mae:0.0076300
training loss 6.943539574422175e-07 mae 0.0005558772827498615
training loss 7.898214512218093e-07 mae 0.0005756321251301057
training loss 8.305987788076834e-07 mae 0.0005852582787813638
training loss 8.388513649885032e-07 mae 0.0005905343985203527
training loss 8.441556877417026e-07 mae 0.0005949144245852801
Epoch 743, training: loss: 0.0000009, mae: 0.0005949 test: loss0.0001129, mae:0.0076492
training loss 9.203574791172286e-07 mae 0.0006259142537601292
training loss 8.40249469444875e-07 mae 0.0005861629170420416
training loss 8.238738456780669e-07 mae 0.0005866277877628657
training loss 8.47381211910009e-07 mae 0.0005915864189201661
training loss 8.467245332561178e-07 mae 0.000592243101416545
Epoch 744, training: loss: 0.0000008, mae: 0.0005909 test: loss0.0001134, mae:0.0076609
training loss 9.153615678769711e-07 mae 0.0005650066887028515
training loss 8.61424052577515e-07 mae 0.0005811867058761052
training loss 8.311438707528623e-07 mae 0.000583501263247287
training loss 8.224978561367718e-07 mae 0.0005858087933788833
training loss 8.387560820349779e-07 mae 0.0005923075210801169
Epoch 745, training: loss: 0.0000008, mae: 0.0005936 test: loss0.0001130, mae:0.0076554
training loss 5.230967303759826e-07 mae 0.0005134561215527356
training loss 8.253850770264156e-07 mae 0.0005931793729199425
training loss 8.588792509148208e-07 mae 0.0005947733845441871
training loss 8.387809834247916e-07 mae 0.0005902338349753804
training loss 8.40058838739039e-07 mae 0.0005911856140264898
Epoch 746, training: loss: 0.0000008, mae: 0.0005925 test: loss0.0001126, mae:0.0076432
training loss 8.353615612577414e-07 mae 0.0005573375965468585
training loss 7.39884395737617e-07 mae 0.0005700518559737534
training loss 8.060691934216797e-07 mae 0.0005805779076734612
training loss 8.201704153389192e-07 mae 0.0005831193209345152
training loss 8.372572465027834e-07 mae 0.0005900360403793279
Epoch 747, training: loss: 0.0000008, mae: 0.0005916 test: loss0.0001123, mae:0.0076275
training loss 8.718555477571499e-07 mae 0.0006015387480147183
training loss 8.185707152805147e-07 mae 0.000588114417897647
training loss 8.266112601096114e-07 mae 0.0005877928619300666
training loss 8.167497412990912e-07 mae 0.0005868325096289839
training loss 8.463060651568081e-07 mae 0.0005948126290876191
Epoch 748, training: loss: 0.0000008, mae: 0.0005926 test: loss0.0001126, mae:0.0076370
training loss 1.5014335303931148e-06 mae 0.00075547891901806
training loss 8.842529268804354e-07 mae 0.0005955511131141262
training loss 8.839610826374937e-07 mae 0.0006014304967495697
training loss 8.552621930874503e-07 mae 0.000596975214780919
training loss 8.50026124234617e-07 mae 0.0005945883090700491
Epoch 749, training: loss: 0.0000008, mae: 0.0005925 test: loss0.0001136, mae:0.0076684
training loss 5.76246577566053e-07 mae 0.0005040916730649769
training loss 8.286458211716985e-07 mae 0.0005842697396920081
training loss 8.349073297186705e-07 mae 0.0005895736011201607
training loss 8.164745545912333e-07 mae 0.0005862702906982889
training loss 8.399889033051441e-07 mae 0.0005938126429028126
Epoch 750, training: loss: 0.0000008, mae: 0.0005933 test: loss0.0001127, mae:0.0076431
training loss 6.690455052194011e-07 mae 0.0004967895220033824
training loss 7.611480887313593e-07 mae 0.0005688452499681244
training loss 7.953045015071465e-07 mae 0.0005742953395779881
training loss 8.238607321592577e-07 mae 0.000586066869378941
training loss 8.420699671703247e-07 mae 0.0005921549060954179
Epoch 751, training: loss: 0.0000008, mae: 0.0005907 test: loss0.0001128, mae:0.0076390
training loss 1.2378135352264508e-06 mae 0.0008092733914963901
training loss 8.312711110656713e-07 mae 0.0005955362844271769
training loss 8.20150398569672e-07 mae 0.000587102337532889
training loss 8.168309652743926e-07 mae 0.0005869638118492008
training loss 8.355946536356446e-07 mae 0.0005914403765059233
Epoch 752, training: loss: 0.0000008, mae: 0.0005906 test: loss0.0001140, mae:0.0076808
training loss 1.4180944845065824e-06 mae 0.0007196366786956787
training loss 8.801092525791496e-07 mae 0.0006011944422817916
training loss 8.501843977077588e-07 mae 0.0005974241170460489
training loss 8.42565170093007e-07 mae 0.0005944210838215019
training loss 8.337458912188568e-07 mae 0.0005923314649896668
Epoch 753, training: loss: 0.0000008, mae: 0.0005907 test: loss0.0001129, mae:0.0076514
training loss 5.798355573460867e-07 mae 0.000589866831433028
training loss 8.168420344654624e-07 mae 0.0005856023104998339
training loss 8.43254086210364e-07 mae 0.0005961398010275852
training loss 8.397139195431431e-07 mae 0.0005909484504284149
training loss 8.442172427305596e-07 mae 0.0005926112864463039
Epoch 754, training: loss: 0.0000008, mae: 0.0005914 test: loss0.0001134, mae:0.0076650
training loss 9.027093597069324e-07 mae 0.0006140184705145657
training loss 7.715567796688394e-07 mae 0.0005620902697510068
training loss 7.797696566335124e-07 mae 0.0005728517521269585
training loss 8.095145040891459e-07 mae 0.0005834901557744956
training loss 8.290172507347069e-07 mae 0.0005880222940789678
Epoch 755, training: loss: 0.0000008, mae: 0.0005908 test: loss0.0001126, mae:0.0076299
training loss 9.011275210468739e-07 mae 0.0006213219021447003
training loss 8.301346238737346e-07 mae 0.0005851943769455685
training loss 8.536952767408006e-07 mae 0.000592484056493175
training loss 8.491092160809614e-07 mae 0.0005933610920559931
training loss 8.405480471298068e-07 mae 0.0005924856417351374
Epoch 756, training: loss: 0.0000008, mae: 0.0005914 test: loss0.0001140, mae:0.0076511
training loss 7.938977546473325e-07 mae 0.0005747455288656056
training loss 7.773378278144781e-07 mae 0.0005709299601970568
training loss 7.923729614535346e-07 mae 0.000580273251945964
training loss 8.34118358324942e-07 mae 0.0005931009674161079
training loss 8.342330403790051e-07 mae 0.000589814092541132
Epoch 757, training: loss: 0.0000008, mae: 0.0005911 test: loss0.0001144, mae:0.0076728
training loss 4.7655385060352273e-07 mae 0.0005216064746491611
training loss 9.122628926781865e-07 mae 0.0006072558251474349
training loss 8.685057020493965e-07 mae 0.0006001729138960859
training loss 8.694648630091179e-07 mae 0.0005988480350918552
training loss 8.439308906851115e-07 mae 0.0005929353639694275
Epoch 758, training: loss: 0.0000008, mae: 0.0005892 test: loss0.0001128, mae:0.0076509
training loss 6.826064122833486e-07 mae 0.0005962311406619847
training loss 8.205682532064324e-07 mae 0.000588642462960207
training loss 8.412850588841378e-07 mae 0.000596213346983463
training loss 8.292325903375546e-07 mae 0.0005894447723095189
training loss 8.338451205268773e-07 mae 0.000590940348662677
Epoch 759, training: loss: 0.0000008, mae: 0.0005899 test: loss0.0001133, mae:0.0076713
training loss 7.630313234585628e-07 mae 0.000620183243881911
training loss 7.430875967884278e-07 mae 0.0005705594416141657
training loss 7.8607000994628e-07 mae 0.0005821098906615071
training loss 8.100129710875077e-07 mae 0.0005840976061052739
training loss 8.323863104679457e-07 mae 0.0005904643711534365
Epoch 760, training: loss: 0.0000008, mae: 0.0005926 test: loss0.0001137, mae:0.0076774
training loss 3.1299597935685597e-07 mae 0.00039949206984601915
training loss 8.165301519021144e-07 mae 0.0005809724617375097
training loss 8.333583161631826e-07 mae 0.0005894357071714568
training loss 8.525640049244871e-07 mae 0.0005972393840822756
training loss 8.250845676467356e-07 mae 0.0005870202752594739
Epoch 761, training: loss: 0.0000008, mae: 0.0005885 test: loss0.0001128, mae:0.0076425
training loss 5.727712846237409e-07 mae 0.000555477396119386
training loss 8.021660562462787e-07 mae 0.000579011902742672
training loss 8.132170027539983e-07 mae 0.0005855635935233187
training loss 8.120581572065691e-07 mae 0.000584842397026496
training loss 8.345709727366137e-07 mae 0.0005912174544857807
Epoch 762, training: loss: 0.0000008, mae: 0.0005884 test: loss0.0001126, mae:0.0076369
training loss 6.276026738305518e-07 mae 0.0005409745499491692
training loss 8.892671873992167e-07 mae 0.0006095200050028736
training loss 8.639349830997554e-07 mae 0.000594876470311665
training loss 8.362277818364507e-07 mae 0.0005857069130111117
training loss 8.283133409330595e-07 mae 0.0005872590537993841
Epoch 763, training: loss: 0.0000008, mae: 0.0005868 test: loss0.0001126, mae:0.0076400
training loss 1.627334199838515e-06 mae 0.0007536119665019214
training loss 7.939610542525413e-07 mae 0.000580613355850801
training loss 8.179739949139163e-07 mae 0.0005893813792528529
training loss 8.311466153496222e-07 mae 0.0005873307211414003
training loss 8.329996855301297e-07 mae 0.0005893547352014192
Epoch 764, training: loss: 0.0000008, mae: 0.0005890 test: loss0.0001131, mae:0.0076562
training loss 8.732421861168405e-07 mae 0.0005741004715673625
training loss 8.091107460194168e-07 mae 0.0005778440242341042
training loss 8.239966927483171e-07 mae 0.0005767397262100681
training loss 8.240587974055808e-07 mae 0.000582948865967229
training loss 8.33292960382847e-07 mae 0.0005873515910934658
Epoch 765, training: loss: 0.0000008, mae: 0.0005868 test: loss0.0001134, mae:0.0076590
training loss 8.836869369588385e-07 mae 0.0005581015720963478
training loss 7.86633754213493e-07 mae 0.0005776079373854194
training loss 8.047837848895898e-07 mae 0.0005751061748879235
training loss 8.224361455472267e-07 mae 0.0005807950132871852
training loss 8.254459628324861e-07 mae 0.0005830733478532651
Epoch 766, training: loss: 0.0000008, mae: 0.0005864 test: loss0.0001133, mae:0.0076600
training loss 4.6455531332867395e-07 mae 0.0004422705096658319
training loss 7.51084899338464e-07 mae 0.0005756574369026528
training loss 7.980470310021788e-07 mae 0.0005774747218364978
training loss 8.225864672155109e-07 mae 0.0005855100951774281
training loss 8.338311822943302e-07 mae 0.0005897479457606964
Epoch 767, training: loss: 0.0000008, mae: 0.0005885 test: loss0.0001131, mae:0.0076571
training loss 1.2302991763135651e-06 mae 0.0006197585607878864
training loss 7.81054211250764e-07 mae 0.0005726158834637743
training loss 8.342641415097068e-07 mae 0.000588915552563578
training loss 8.231307774900677e-07 mae 0.0005835260558312153
training loss 8.266818144790444e-07 mae 0.0005874733426967238
Epoch 768, training: loss: 0.0000008, mae: 0.0005863 test: loss0.0001126, mae:0.0076298
training loss 6.991128884692444e-07 mae 0.0005751270800828934
training loss 8.151950872419832e-07 mae 0.0005877781870510137
training loss 8.372947882048047e-07 mae 0.0005864624509191382
training loss 8.326426467984547e-07 mae 0.0005889454201582376
training loss 8.279421802034907e-07 mae 0.0005877754930175138
Epoch 769, training: loss: 0.0000008, mae: 0.0005904 test: loss0.0001131, mae:0.0076701
training loss 1.1854881449835375e-06 mae 0.0008468339219689369
training loss 8.411549477690036e-07 mae 0.0005874109640489241
training loss 7.983964586133374e-07 mae 0.0005778917595559714
training loss 8.269204179266985e-07 mae 0.0005843109822838218
training loss 8.234993269363381e-07 mae 0.0005854982888529921
Epoch 770, training: loss: 0.0000008, mae: 0.0005863 test: loss0.0001134, mae:0.0076670
training loss 6.051483865121554e-07 mae 0.0005344466189853847
training loss 8.175785217636254e-07 mae 0.0005885675947830153
training loss 8.379287302126038e-07 mae 0.000591175029305557
training loss 8.216737166281109e-07 mae 0.0005858662090686748
training loss 8.314137193591349e-07 mae 0.0005882549703709635
Epoch 771, training: loss: 0.0000008, mae: 0.0005886 test: loss0.0001133, mae:0.0076615
training loss 1.0424014362797607e-06 mae 0.0006779643590562046
training loss 8.116937728314666e-07 mae 0.0005792372023421977
training loss 8.261578418253943e-07 mae 0.000582622094160536
training loss 8.419487620352246e-07 mae 0.0005891439086674336
training loss 8.199443279965582e-07 mae 0.0005839507627493203
Epoch 772, training: loss: 0.0000008, mae: 0.0005855 test: loss0.0001127, mae:0.0076349
training loss 9.985097904063878e-07 mae 0.0006115188007242978
training loss 7.786954105491603e-07 mae 0.0005683912541808598
training loss 8.112398373918726e-07 mae 0.0005766607268879394
training loss 7.940587760539897e-07 mae 0.0005785676776755305
training loss 8.162784011751932e-07 mae 0.0005858839872937227
Epoch 773, training: loss: 0.0000008, mae: 0.0005879 test: loss0.0001130, mae:0.0076538
training loss 1.136065179707657e-06 mae 0.0006238752976059914
training loss 8.246352699777214e-07 mae 0.0005808605968851741
training loss 8.614517072925479e-07 mae 0.0005860816961670719
training loss 8.117264309662731e-07 mae 0.000577950774960294
training loss 8.208573680591299e-07 mae 0.0005844117558401413
Epoch 774, training: loss: 0.0000008, mae: 0.0005839 test: loss0.0001136, mae:0.0076602
training loss 9.773365263754386e-07 mae 0.0005838082288391888
training loss 7.795989743089212e-07 mae 0.0005708132709568256
training loss 7.990949872673989e-07 mae 0.0005767541547942133
training loss 8.367667806393628e-07 mae 0.0005865267251266647
training loss 8.140124728965727e-07 mae 0.000583079949065357
Epoch 775, training: loss: 0.0000008, mae: 0.0005860 test: loss0.0001128, mae:0.0076422
training loss 5.854023470419634e-07 mae 0.0005605301703326404
training loss 7.424064284466523e-07 mae 0.0005724471035029957
training loss 7.963777525309534e-07 mae 0.0005776063679708262
training loss 7.982968359643589e-07 mae 0.0005814516294318935
training loss 8.194240160040617e-07 mae 0.0005850693483463954
Epoch 776, training: loss: 0.0000008, mae: 0.0005872 test: loss0.0001129, mae:0.0076435
training loss 7.386067295556131e-07 mae 0.0005499062244780362
training loss 8.730861537185098e-07 mae 0.0005930084934887275
training loss 8.176575551990009e-07 mae 0.0005853035657823383
training loss 8.330204917796595e-07 mae 0.000587559834782602
training loss 8.211537537457149e-07 mae 0.0005864670292522985
Epoch 777, training: loss: 0.0000008, mae: 0.0005863 test: loss0.0001128, mae:0.0076426
training loss 4.5483253074962704e-07 mae 0.0004520422371570021
training loss 7.997844223235221e-07 mae 0.0005711705518621659
training loss 8.180151453281256e-07 mae 0.0005836777387896903
training loss 8.115567452673739e-07 mae 0.0005835430322304392
training loss 8.209780433851359e-07 mae 0.0005866640070175277
Epoch 778, training: loss: 0.0000008, mae: 0.0005848 test: loss0.0001130, mae:0.0076502
training loss 5.731264423047833e-07 mae 0.0005470383912324905
training loss 8.016564371629619e-07 mae 0.0005783965045303181
training loss 8.499229440107823e-07 mae 0.0005862857676157269
training loss 8.520125957162743e-07 mae 0.0005885969613453635
training loss 8.28517796883295e-07 mae 0.0005871122915805113
Epoch 779, training: loss: 0.0000008, mae: 0.0005851 test: loss0.0001128, mae:0.0076449
training loss 4.918786089547211e-07 mae 0.0004769874212797731
training loss 8.247570759504372e-07 mae 0.0005863315400704013
training loss 8.401332243663838e-07 mae 0.0005896452602257233
training loss 8.33665292089935e-07 mae 0.0005901310057456404
training loss 8.255972431076922e-07 mae 0.0005883059074316843
Epoch 780, training: loss: 0.0000008, mae: 0.0005850 test: loss0.0001129, mae:0.0076545
training loss 7.674466360185761e-07 mae 0.0005847833235748112
training loss 8.291524336839882e-07 mae 0.0005943863093396469
training loss 8.061508212746713e-07 mae 0.0005826956749833788
training loss 7.860801996201684e-07 mae 0.0005748252808779408
training loss 8.164943704629757e-07 mae 0.0005836259034770399
Epoch 781, training: loss: 0.0000008, mae: 0.0005845 test: loss0.0001131, mae:0.0076497
training loss 3.312454737169901e-07 mae 0.0003745493886526674
training loss 8.455818970685191e-07 mae 0.0005730003456347712
training loss 8.181920838359753e-07 mae 0.0005781939665275155
training loss 8.130307987353295e-07 mae 0.0005802045561240626
training loss 8.131659778831848e-07 mae 0.0005819965472481954
Epoch 782, training: loss: 0.0000008, mae: 0.0005836 test: loss0.0001134, mae:0.0076690
training loss 4.0211355667452153e-07 mae 0.00045353639870882034
training loss 7.794358576601926e-07 mae 0.000564664453907194
training loss 7.906382556443937e-07 mae 0.000568037021024577
training loss 8.048203538550634e-07 mae 0.0005796204021567519
training loss 8.126494003760459e-07 mae 0.0005810914743015205
Epoch 783, training: loss: 0.0000008, mae: 0.0005830 test: loss0.0001190, mae:0.0077115
training loss 4.932754222863878e-07 mae 0.0004948101122863591
training loss 8.350819980790344e-07 mae 0.0005841351476014026
training loss 8.126140121284703e-07 mae 0.0005799790087621657
training loss 8.047555885905365e-07 mae 0.0005776574519086653
training loss 8.185646520365688e-07 mae 0.0005848879056556419
Epoch 784, training: loss: 0.0000008, mae: 0.0005850 test: loss0.0001128, mae:0.0076434
training loss 1.0269615131619503e-06 mae 0.000714199326466769
training loss 8.229877105729127e-07 mae 0.0005762501826336352
training loss 8.104118629064196e-07 mae 0.0005805051089080032
training loss 8.293506436721385e-07 mae 0.0005878518996235994
training loss 8.188577885985792e-07 mae 0.0005845455124114748
Epoch 785, training: loss: 0.0000008, mae: 0.0005826 test: loss0.0001128, mae:0.0076332
training loss 5.26849589732592e-07 mae 0.0005057441885583103
training loss 7.389007415511908e-07 mae 0.0005621117740577342
training loss 7.564139446904027e-07 mae 0.0005640113924510114
training loss 7.789404790232375e-07 mae 0.0005732183934448425
training loss 8.051659382651373e-07 mae 0.0005802800790557352
Epoch 786, training: loss: 0.0000008, mae: 0.0005823 test: loss0.0001132, mae:0.0076681
training loss 8.186986519831407e-07 mae 0.0005546060274355114
training loss 7.749825341289026e-07 mae 0.0005678877225328311
training loss 8.015112532665637e-07 mae 0.0005809640143606076
training loss 8.16196866396499e-07 mae 0.000586045079861478
training loss 8.142769132887654e-07 mae 0.0005838645302716049
Epoch 787, training: loss: 0.0000008, mae: 0.0005849 test: loss0.0001131, mae:0.0076540
training loss 2.592514931620826e-07 mae 0.0003871054796036333
training loss 8.36348548586209e-07 mae 0.0005803637768087141
training loss 7.896787784118555e-07 mae 0.0005704816139768808
training loss 7.990079879787514e-07 mae 0.0005762745957399124
training loss 8.12440319040054e-07 mae 0.0005810858297511584
Epoch 788, training: loss: 0.0000008, mae: 0.0005817 test: loss0.0001131, mae:0.0076610
training loss 4.4705507207254414e-07 mae 0.00042182797915302217
training loss 7.768367129524538e-07 mae 0.000577073377630144
training loss 8.062256004806009e-07 mae 0.000581503319288927
training loss 7.950607935840133e-07 mae 0.0005791156268632351
training loss 8.087556237802541e-07 mae 0.0005829087261629491
Epoch 789, training: loss: 0.0000008, mae: 0.0005825 test: loss0.0001138, mae:0.0076740
training loss 4.5141771920498286e-07 mae 0.00044340858585201204
training loss 8.227722977001882e-07 mae 0.000570642970748903
training loss 8.250215528570864e-07 mae 0.0005797197460773223
training loss 8.04176386056184e-07 mae 0.0005774190493270333
training loss 8.083948114516494e-07 mae 0.0005809971467598773
Epoch 790, training: loss: 0.0000008, mae: 0.0005826 test: loss0.0001138, mae:0.0076788
training loss 1.2754861700159381e-06 mae 0.0006973833660595119
training loss 8.321044801692443e-07 mae 0.000585546584653796
training loss 8.122089325590226e-07 mae 0.0005799463098882558
training loss 8.21109255267456e-07 mae 0.0005847757756602114
training loss 8.112854220372577e-07 mae 0.0005824283752601999
Epoch 791, training: loss: 0.0000008, mae: 0.0005825 test: loss0.0001131, mae:0.0076585
training loss 4.1556188534741523e-07 mae 0.00047890408313833177
training loss 7.950793381755206e-07 mae 0.0005794720479981134
training loss 8.423771799531853e-07 mae 0.0005940087433526348
training loss 8.362353816321516e-07 mae 0.0005902096176973315
training loss 8.184453483054946e-07 mae 0.0005835887546817867
Epoch 792, training: loss: 0.0000008, mae: 0.0005829 test: loss0.0001136, mae:0.0076710
training loss 5.543650445360981e-07 mae 0.0005181769956834614
training loss 7.448593434593717e-07 mae 0.000562905443503576
training loss 7.59122299471771e-07 mae 0.000570650002688612
training loss 7.929405592824195e-07 mae 0.0005748814106024552
training loss 8.103462231140913e-07 mae 0.0005804984088968576
Epoch 793, training: loss: 0.0000008, mae: 0.0005803 test: loss0.0001132, mae:0.0076645
training loss 7.893189035712567e-07 mae 0.0006438717246055603
training loss 8.002791698867012e-07 mae 0.000578703074008409
training loss 8.169726399809505e-07 mae 0.0005854237395324883
training loss 8.125449252834821e-07 mae 0.0005832367352308669
training loss 8.16394458539023e-07 mae 0.0005832664685645859
Epoch 794, training: loss: 0.0000008, mae: 0.0005811 test: loss0.0001135, mae:0.0076725
training loss 1.181236257252749e-06 mae 0.0006460149888880551
training loss 8.029469025881125e-07 mae 0.0005739385116041875
training loss 7.97069009026342e-07 mae 0.0005772710209574068
training loss 7.900663529319403e-07 mae 0.0005741824882817358
training loss 8.048840705574116e-07 mae 0.0005783519943057099
Epoch 795, training: loss: 0.0000008, mae: 0.0005799 test: loss0.0001151, mae:0.0077132
training loss 6.244642349884089e-07 mae 0.0005269587854854763
training loss 7.775300655696042e-07 mae 0.0005688368840435264
training loss 8.130405776660379e-07 mae 0.0005760399816617702
training loss 8.079131154339103e-07 mae 0.0005778253677112998
training loss 8.148045644453637e-07 mae 0.0005818307799384446
Epoch 796, training: loss: 0.0000008, mae: 0.0005816 test: loss0.0001131, mae:0.0076399
training loss 5.946379815213731e-07 mae 0.0005422181566245854
training loss 7.740500724567565e-07 mae 0.0005656506263596172
training loss 7.856688981600301e-07 mae 0.0005685083437273005
training loss 8.06415967538922e-07 mae 0.0005775922662671043
training loss 8.066138105614887e-07 mae 0.0005780950904504135
Epoch 797, training: loss: 0.0000008, mae: 0.0005814 test: loss0.0001132, mae:0.0076561
training loss 3.9234123505593743e-07 mae 0.00049533013952896
training loss 8.250525979189893e-07 mae 0.0005814607675169027
training loss 8.097047867750006e-07 mae 0.0005766804278759043
training loss 8.144669063037344e-07 mae 0.0005817787674724409
training loss 8.058299825893638e-07 mae 0.0005780578762077405
Epoch 798, training: loss: 0.0000008, mae: 0.0005800 test: loss0.0001141, mae:0.0076787
training loss 5.587169766840816e-07 mae 0.000463937409222126
training loss 7.337228171264523e-07 mae 0.0005594115794462827
training loss 7.658949965503994e-07 mae 0.0005666384159928499
training loss 7.992072075041011e-07 mae 0.0005771870177859033
training loss 8.047343989368194e-07 mae 0.0005794530554873804
Epoch 799, training: loss: 0.0000008, mae: 0.0005809 test: loss0.0001131, mae:0.0076573
current learning rate: 1.953125e-06

Process finished with exit code 0






















ssh://jeffzhu@172.16.46.217:22/home/jeffzhu/anaconda3/bin/python3.6 -u /home/jeffzhu/AL/pre_training/non_ptrain_cls.py
Namespace(al_method='msg_mask', bald_ft_epochs=5, batch_data_num=100, batchsize=48, data_mix=False, data_mixing_rate=0.5, dataset='qm9', device=1, epochs=800, ft_epochs=5, ft_method='by_valid', init_data_num=5000, k_center_ft_epochs=10, lr=0.0005, mc_sampling_num=80, model_num=4, multi_gpu=False, prop_name='homo', qbc_ft_epochs=5, re_init=False, save_model=False, shuffle=True, test_freq=5, test_use_all=False, use_default=False, use_tb=True, workers=0)
1119_22_37  model SchNetModel(
  (activation): ShiftSoftplus(
    beta=1, threshold=20
    (softplus): Softplus(beta=1, threshold=20)
  )
  (embedding_layer): AtomEmbedding(
    (embedding): Embedding(100, 48, padding_idx=0)
  )
  (rbf_layer): RBFLayer()
  (conv_layers): ModuleList(
    (0): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (1): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (2): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (3): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
  )
  (atom_dense_layer1): Linear(in_features=48, out_features=64, bias=True)
  (atom_dense_layer2): Linear(in_features=64, out_features=1, bias=True)
)  optimizer Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
inference 26.37046980857849
Iteration 1 Mean MSE 0.0025128056295216084
Iteration 2 Mean MSE 0.0018228215631097555
Iteration 3 Mean MSE 0.0016259904950857162
Iteration 4 Mean MSE 0.0015678324270993471
Iteration 5 Mean MSE 0.0015634200535714626
Iteration 6 Mean MSE 0.0015632244758307934
Iteration 7 Mean MSE 0.0015631687128916383
Iteration 8 Mean MSE 0.0015631746500730515
Iteration 9 Mean MSE 0.0015631223795935512
Iteration 10 Mean MSE 0.0015630939742550254
start
SchNetModel(
  (activation): ShiftSoftplus(
    beta=1, threshold=20
    (softplus): Softplus(beta=1, threshold=20)
  )
  (embedding_layer): AtomEmbedding(
    (embedding): Embedding(100, 48, padding_idx=0)
  )
  (rbf_layer): RBFLayer()
  (conv_layers): ModuleList(
    (0): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (1): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (2): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (3): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
  )
  (atom_dense_layer1): Linear(in_features=48, out_features=64, bias=True)
  (atom_dense_layer2): Linear(in_features=64, out_features=1, bias=True)
)
-0.23928554356098175 0.022318677976727486
training loss 0.03308054804801941 mae 0.16586504876613617
training loss 0.0012730425891384265 mae 0.02246757998478179
training loss 0.0008704774243074774 mae 0.019231017503776763
training loss 0.0007250819845581487 mae 0.01787646238796955
training loss 0.0006600356783424805 mae 0.017425963576345585
Epoch  0, training: loss: 0.0006503, mae: 0.0173285 test: loss0.0004333, mae:0.0153480
training loss 0.0007502177613787353 mae 0.019736958667635918
training loss 0.00043154562716189694 mae 0.015280409423890064
training loss 0.000441429852871086 mae 0.01536888204109255
training loss 0.0004426106328332784 mae 0.015421415420113414
training loss 0.0004356678210248689 mae 0.015316051013990119
Epoch  1, training: loss: 0.0004328, mae: 0.0152706 test: loss0.0004209, mae:0.0151257
training loss 0.0003565526276361197 mae 0.014530867338180542
training loss 0.00042011266444152333 mae 0.015227165438380893
training loss 0.00043249681564900877 mae 0.015307807907609657
training loss 0.0004303491067616209 mae 0.015212109330918225
training loss 0.0004272058820299834 mae 0.015148864887580651
Epoch  2, training: loss: 0.0004279, mae: 0.0151749 test: loss0.0004182, mae:0.0150101
training loss 0.00023769175459165126 mae 0.01199692115187645
training loss 0.00040719503458068863 mae 0.014775096858833348
training loss 0.00040815662048609557 mae 0.014782256944713612
training loss 0.00041738440284955734 mae 0.014997559747603159
training loss 0.00042021894210368053 mae 0.0150154205595157
Epoch  3, training: loss: 0.0004205, mae: 0.0150114 test: loss0.0004110, mae:0.0147889
training loss 0.0003827039327006787 mae 0.014819562435150146
training loss 0.00039573000543111694 mae 0.01473679232831095
training loss 0.0004043188178047335 mae 0.014719689089694234
training loss 0.00040910266947297283 mae 0.014778062621045187
training loss 0.0004194738712296151 mae 0.014924975695894722
Epoch  4, training: loss: 0.0004175, mae: 0.0149087 test: loss0.0004176, mae:0.0150933
training loss 0.0002974658564198762 mae 0.013363131321966648
training loss 0.00043565103117668743 mae 0.015177561098015777
training loss 0.00041085476917906267 mae 0.014878553338348866
training loss 0.0004106818314600082 mae 0.014832398274432347
training loss 0.00041831201656237344 mae 0.014960798110571962
Epoch  5, training: loss: 0.0004180, mae: 0.0149437 test: loss0.0004079, mae:0.0148855
training loss 0.000582330278120935 mae 0.015471021644771099
training loss 0.00041425040313590553 mae 0.015030690414064074
training loss 0.00041626888459276326 mae 0.015017170274611748
training loss 0.0004161451259953284 mae 0.014929234229077566
training loss 0.0004103285285403295 mae 0.014838330424506574
Epoch  6, training: loss: 0.0004106, mae: 0.0148479 test: loss0.0004465, mae:0.0156718
training loss 0.0003036320849787444 mae 0.01305413618683815
training loss 0.0004147207549905551 mae 0.014786980954893664
training loss 0.0004013444465299202 mae 0.014590321200908996
training loss 0.0004074573110163037 mae 0.014805448186022556
training loss 0.000396012394229733 mae 0.014658629231673862
Epoch  7, training: loss: 0.0003954, mae: 0.0146595 test: loss0.0003922, mae:0.0152051
training loss 0.0004627110902220011 mae 0.017401933670043945
training loss 0.00033871500664984106 mae 0.013874486636589559
training loss 0.00035527598033290313 mae 0.014062869546953407
training loss 0.0003353845015274581 mae 0.013675116625497278
training loss 0.00033023212137180303 mae 0.013549038709433223
Epoch  8, training: loss: 0.0003267, mae: 0.0134831 test: loss0.0003085, mae:0.0132086
training loss 0.00025762899895198643 mae 0.012616218067705631
training loss 0.0002923435582916307 mae 0.013042176854522792
training loss 0.0002944159922581905 mae 0.012998573122416984
training loss 0.00029542970059390206 mae 0.012943548175031384
training loss 0.000292106283671079 mae 0.012856769741545275
Epoch  9, training: loss: 0.0002915, mae: 0.0128291 test: loss0.0003399, mae:0.0138837
training loss 0.0003309864259790629 mae 0.01515122503042221
training loss 0.00030089997259589535 mae 0.012969516309014722
training loss 0.00029086875688920903 mae 0.012752008670479946
training loss 0.00028364979367108027 mae 0.012668135606788644
training loss 0.0002849069137641101 mae 0.012702203939543731
Epoch 10, training: loss: 0.0002858, mae: 0.0127069 test: loss0.0003051, mae:0.0131703
training loss 0.00019484102085698396 mae 0.010079928673803806
training loss 0.000279896268872179 mae 0.012378985774429405
training loss 0.0002679726338569574 mae 0.012249030412571266
training loss 0.0002709944544179637 mae 0.012356823125225024
training loss 0.0002739792926783156 mae 0.012389633299514131
Epoch 11, training: loss: 0.0002733, mae: 0.0123806 test: loss0.0002811, mae:0.0125507
training loss 0.00021996522264089435 mae 0.01146556343883276
training loss 0.0002626163300638106 mae 0.0122027406834212
training loss 0.0002575932198986733 mae 0.01207059989729435
training loss 0.0002611903354717536 mae 0.012185850501405877
training loss 0.00026282020448504346 mae 0.012179570175262526
Epoch 12, training: loss: 0.0002642, mae: 0.0122011 test: loss0.0002686, mae:0.0120821
training loss 0.0002419683150947094 mae 0.01175099890679121
training loss 0.00027528673800749373 mae 0.012412311856214909
training loss 0.0002813956684849022 mae 0.012496436913820479
training loss 0.00027696958014283596 mae 0.012431041026080876
training loss 0.00027275823278379263 mae 0.0124262718330902
Epoch 13, training: loss: 0.0002713, mae: 0.0123942 test: loss0.0002670, mae:0.0120573
training loss 0.00020791545102838427 mae 0.011140736751258373
training loss 0.0002536774340756785 mae 0.011992587634891855
training loss 0.0002723093018382973 mae 0.012378894262901037
training loss 0.0002673512606284836 mae 0.012302083166821902
training loss 0.00026599785752608385 mae 0.012228954027392968
Epoch 14, training: loss: 0.0002635, mae: 0.0121805 test: loss0.0002871, mae:0.0125485
training loss 0.00020234425028320402 mae 0.011119349859654903
training loss 0.0002554656461785164 mae 0.01188306702191339
training loss 0.0002512686884740718 mae 0.011835036651775389
training loss 0.0002556447544334063 mae 0.011978007624826289
training loss 0.0002580770739239517 mae 0.012034962171531135
Epoch 15, training: loss: 0.0002584, mae: 0.0120594 test: loss0.0003132, mae:0.0136922
training loss 0.00024980262969620526 mae 0.012612446211278439
training loss 0.0002518858506364802 mae 0.011947914492338894
training loss 0.00026102463268423633 mae 0.012114651183053703
training loss 0.0002602586580616953 mae 0.01209784678757092
training loss 0.00026228668168042356 mae 0.012144963908013872
Epoch 16, training: loss: 0.0002619, mae: 0.0121389 test: loss0.0002642, mae:0.0120118
training loss 0.0002857334620784968 mae 0.01254794280976057
training loss 0.0002466734323545596 mae 0.011695179850885683
training loss 0.00025358242304052804 mae 0.01187287971819982
training loss 0.0002539502957427954 mae 0.011909743037000793
training loss 0.0002540527285549868 mae 0.011941255176838356
Epoch 17, training: loss: 0.0002543, mae: 0.0119633 test: loss0.0002554, mae:0.0118660
training loss 0.00022401784372050315 mae 0.010535708628594875
training loss 0.0002578705450515355 mae 0.011905868099454571
training loss 0.0002552455689960924 mae 0.011927333411456333
training loss 0.00025990987472183723 mae 0.01201833453374785
training loss 0.0002578159021584327 mae 0.012008397191281047
Epoch 18, training: loss: 0.0002555, mae: 0.0119711 test: loss0.0002641, mae:0.0121110
training loss 0.0002484878059476614 mae 0.012176916934549809
training loss 0.0002578590633050886 mae 0.012162656048495394
training loss 0.00024840535040158724 mae 0.011954041446863423
training loss 0.00025093836519236536 mae 0.011921554939537645
training loss 0.00025099042604824495 mae 0.011899301216970035
Epoch 19, training: loss: 0.0002500, mae: 0.0118704 test: loss0.0002599, mae:0.0119973
training loss 0.00015701998199801892 mae 0.009170052595436573
training loss 0.0002553683164068406 mae 0.012039983425946798
training loss 0.0002505694716127986 mae 0.011872777733767385
training loss 0.0002477864775014887 mae 0.011783832640985379
training loss 0.00025207714620886117 mae 0.011890229986823013
Epoch 20, training: loss: 0.0002523, mae: 0.0118964 test: loss0.0002609, mae:0.0119813
training loss 0.00021887799084652215 mae 0.01128754299134016
training loss 0.00025249759674610974 mae 0.011716375419614363
training loss 0.00025891255149917486 mae 0.011970295976515447
training loss 0.00025611273120990075 mae 0.01192624300767648
training loss 0.0002540650786785642 mae 0.011923961526718888
Epoch 21, training: loss: 0.0002531, mae: 0.0119180 test: loss0.0002752, mae:0.0120499
training loss 0.00023765680089127272 mae 0.011156827211380005
training loss 0.00024633756673246544 mae 0.011767004379162602
training loss 0.00024605157436736124 mae 0.011762939370467816
training loss 0.0002494194225962458 mae 0.011825332600031269
training loss 0.000250232612329885 mae 0.011864940494539873
Epoch 22, training: loss: 0.0002499, mae: 0.0118559 test: loss0.0002775, mae:0.0126387
training loss 0.0003198491467628628 mae 0.012885533273220062
training loss 0.00024560040180279195 mae 0.01174978336648029
training loss 0.0002511920578003225 mae 0.011830077248413371
training loss 0.0002462651468975321 mae 0.011728736337595035
training loss 0.00024371881066040783 mae 0.011689525801892309
Epoch 23, training: loss: 0.0002427, mae: 0.0116667 test: loss0.0002553, mae:0.0118914
training loss 0.00017626496264711022 mae 0.010729622095823288
training loss 0.000247598966120231 mae 0.011739565424767196
training loss 0.00024214534116055963 mae 0.011696926714612704
training loss 0.0002412320081328061 mae 0.011687573113336866
training loss 0.0002383250313181435 mae 0.01161597543094882
Epoch 24, training: loss: 0.0002379, mae: 0.0116210 test: loss0.0002396, mae:0.0114807
training loss 0.000198316338355653 mae 0.010071120224893093
training loss 0.0002898460647260186 mae 0.012737603273754025
training loss 0.0002612710193013957 mae 0.012112595048723829
training loss 0.0002541995809405012 mae 0.011981739576239856
training loss 0.0002530238827005891 mae 0.011945208557765583
Epoch 25, training: loss: 0.0002543, mae: 0.0119450 test: loss0.0002665, mae:0.0122620
training loss 0.00020387301628943533 mae 0.010748648084700108
training loss 0.00025479991574470397 mae 0.012066451704823504
training loss 0.00023814282808821684 mae 0.011652767888759036
training loss 0.00023677707622783708 mae 0.011593531527315935
training loss 0.00023650640958574926 mae 0.011576991769211798
Epoch 26, training: loss: 0.0002368, mae: 0.0115910 test: loss0.0002572, mae:0.0118205
training loss 0.00024362692784052342 mae 0.011527677066624165
training loss 0.00023026110098797682 mae 0.01146727743759459
training loss 0.00023198237750416453 mae 0.011476748281776316
training loss 0.00023193493689177552 mae 0.011428702067559129
training loss 0.00023445386871196258 mae 0.011512008884838268
Epoch 27, training: loss: 0.0002332, mae: 0.0115039 test: loss0.0002361, mae:0.0113909
training loss 0.00021322444081306458 mae 0.010784465819597244
training loss 0.00022965705854813658 mae 0.011433209456941661
training loss 0.00022246818483800232 mae 0.01125751673658886
training loss 0.0002249995812126867 mae 0.011301951439718145
training loss 0.00022838285094531333 mae 0.011362859024782093
Epoch 28, training: loss: 0.0002281, mae: 0.0113630 test: loss0.0002346, mae:0.0112477
training loss 0.00017843280511442572 mae 0.010073126293718815
training loss 0.00023236401637618924 mae 0.011372345937963796
training loss 0.0002242785889717593 mae 0.01122176991591212
training loss 0.00022506540236818837 mae 0.011261356023097083
training loss 0.00022880339485202767 mae 0.011312543690223163
Epoch 29, training: loss: 0.0002278, mae: 0.0113053 test: loss0.0002759, mae:0.0123301
training loss 0.0002149368665413931 mae 0.01199378352612257
training loss 0.00021799742449622826 mae 0.010989816567184878
training loss 0.000227184124788878 mae 0.011329624018087833
training loss 0.0002204396881742113 mae 0.011178135878054904
training loss 0.00022169947045025845 mae 0.011193597098506654
Epoch 30, training: loss: 0.0002211, mae: 0.0111804 test: loss0.0002332, mae:0.0113363
training loss 0.00012261107622180134 mae 0.008685366250574589
training loss 0.00022568824993761475 mae 0.011406843020927673
training loss 0.00022173917948586471 mae 0.01120294345137064
training loss 0.00022665283364970052 mae 0.011352091734698476
training loss 0.00022386949769579188 mae 0.011321194532480256
Epoch 31, training: loss: 0.0002228, mae: 0.0113013 test: loss0.0002214, mae:0.0109880
training loss 0.0002461076946929097 mae 0.011685078032314777
training loss 0.000216808855446859 mae 0.011034966811683835
training loss 0.0002202614196714494 mae 0.01114163698978944
training loss 0.00022063249936450698 mae 0.011145563176308845
training loss 0.00021899036203907219 mae 0.011081935664110669
Epoch 32, training: loss: 0.0002188, mae: 0.0110635 test: loss0.0002184, mae:0.0108890
training loss 0.00019452204287517816 mae 0.00990266539156437
training loss 0.00019217777712494315 mae 0.010432121608698481
training loss 0.0002028923373644028 mae 0.010664042385092169
training loss 0.00020495131446067122 mae 0.010676999733666117
training loss 0.00020472154751609986 mae 0.010733576855200586
Epoch 33, training: loss: 0.0002053, mae: 0.0107581 test: loss0.0002324, mae:0.0111903
training loss 0.0001968049182323739 mae 0.010749752633273602
training loss 0.0002187294133667213 mae 0.01132322723666827
training loss 0.00021470439767852276 mae 0.011121063576181337
training loss 0.00021250610676746724 mae 0.011037567621158643
training loss 0.00020883138046399525 mae 0.010872942203220295
Epoch 34, training: loss: 0.0002077, mae: 0.0108362 test: loss0.0002111, mae:0.0108415
training loss 0.00024617675808258355 mae 0.01236786786466837
training loss 0.0001956653386336185 mae 0.010557435708595254
training loss 0.00019717844399637412 mae 0.010559620566223516
training loss 0.00020001077044806118 mae 0.010580265783513614
training loss 0.00020304356258743638 mae 0.010703637556576015
Epoch 35, training: loss: 0.0002046, mae: 0.0107145 test: loss0.0002209, mae:0.0110169
training loss 0.0001506942353444174 mae 0.009102112613618374
training loss 0.00019191576989125645 mae 0.010514132651116916
training loss 0.00019426954178817312 mae 0.010518209958024837
training loss 0.00020037263701978258 mae 0.01065933148049756
training loss 0.00020010188510508238 mae 0.010646661289444019
Epoch 36, training: loss: 0.0002006, mae: 0.0106465 test: loss0.0002054, mae:0.0106130
training loss 0.00030018025427125394 mae 0.01301464531570673
training loss 0.00021520615857797582 mae 0.01083170743111302
training loss 0.0002009568976279864 mae 0.010632475644423822
training loss 0.0002008358973374479 mae 0.010638980188758564
training loss 0.00019345161126693716 mae 0.010437727903493163
Epoch 37, training: loss: 0.0001926, mae: 0.0104191 test: loss0.0002129, mae:0.0107376
training loss 0.00021734235633630306 mae 0.010650395415723324
training loss 0.00017541782218876166 mae 0.01006905534578597
training loss 0.00017832683079177508 mae 0.01002006433295584
training loss 0.00018198649686744237 mae 0.010115805004926901
training loss 0.00018587594048817421 mae 0.010233255726428912
Epoch 38, training: loss: 0.0001869, mae: 0.0102555 test: loss0.0001994, mae:0.0105140
training loss 0.00028946297243237495 mae 0.013155565597116947
training loss 0.00018860340973261895 mae 0.010288354584618528
training loss 0.00018934684066161843 mae 0.01034738172749334
training loss 0.00018511390028635525 mae 0.010257905324464599
training loss 0.00018594220469749778 mae 0.010228706289787054
Epoch 39, training: loss: 0.0001859, mae: 0.0102370 test: loss0.0001994, mae:0.0103934
training loss 0.00024816373479552567 mae 0.011586382985115051
training loss 0.00018742481602113874 mae 0.01039636303104606
training loss 0.00018434281212261568 mae 0.010249676076824416
training loss 0.00018659223764225073 mae 0.010254012124723946
training loss 0.0001852266547725935 mae 0.010185350358152568
Epoch 40, training: loss: 0.0001847, mae: 0.0101691 test: loss0.0001909, mae:0.0101053
training loss 0.00012026665353914723 mae 0.007902637124061584
training loss 0.00016706810043508406 mae 0.00974553577857567
training loss 0.0001786418692387484 mae 0.01003387404402884
training loss 0.00018062468474370035 mae 0.010051240774475978
training loss 0.00017805526544750012 mae 0.009983008421623887
Epoch 41, training: loss: 0.0001772, mae: 0.0099544 test: loss0.0001858, mae:0.0100331
training loss 0.00010320521687390283 mae 0.00827879086136818
training loss 0.00016266228168901486 mae 0.00966805346565796
training loss 0.0001615966868466965 mae 0.009562621635412523
training loss 0.00016423090242908723 mae 0.009658621602525183
training loss 0.0001648459480173332 mae 0.009641941822138586
Epoch 42, training: loss: 0.0001650, mae: 0.0096305 test: loss0.0002098, mae:0.0106319
training loss 0.00011160975554957986 mae 0.00795696396380663
training loss 0.00016372281571224736 mae 0.009468293328787769
training loss 0.0001591260123958737 mae 0.00948797964426403
training loss 0.00016342389561345376 mae 0.009623417445760687
training loss 0.00016499217031309285 mae 0.009654738673875427
Epoch 43, training: loss: 0.0001658, mae: 0.0096498 test: loss0.0001910, mae:0.0101971
training loss 0.0001242861762875691 mae 0.00886946078389883
training loss 0.0001737389566009298 mae 0.009711860505608369
training loss 0.00016288257584074923 mae 0.009513909946942681
training loss 0.0001643781232881199 mae 0.009587869334916604
training loss 0.00016622634597188697 mae 0.009625642532621746
Epoch 44, training: loss: 0.0001662, mae: 0.0096437 test: loss0.0001702, mae:0.0097116
training loss 0.00011404138058423996 mae 0.008528923615813255
training loss 0.00015543816513016674 mae 0.00941713054792261
training loss 0.00016220817878372071 mae 0.009548639837136063
training loss 0.0001653418575555381 mae 0.009615984628137366
training loss 0.0001647375417691979 mae 0.009633364491813366
Epoch 45, training: loss: 0.0001645, mae: 0.0096308 test: loss0.0001630, mae:0.0094286
training loss 0.0001507956039858982 mae 0.00933129247277975
training loss 0.00015490935495073051 mae 0.009385942043188742
training loss 0.00015117162461092897 mae 0.009302613731141725
training loss 0.00015512614067457307 mae 0.009436914117525744
training loss 0.00015722529392021094 mae 0.009461173840997678
Epoch 46, training: loss: 0.0001581, mae: 0.0094667 test: loss0.0001771, mae:0.0098741
training loss 0.00017032022878993303 mae 0.010211572982370853
training loss 0.00014421715747505682 mae 0.009010943593274735
training loss 0.00015197337426690126 mae 0.009292595342059834
training loss 0.00015470822952149603 mae 0.009394437972061484
training loss 0.00015347691146982363 mae 0.009337949019796512
Epoch 47, training: loss: 0.0001550, mae: 0.0093565 test: loss0.0001726, mae:0.0096395
training loss 0.00016248873725999147 mae 0.008552932180464268
training loss 0.00016922375182141825 mae 0.009649401576276504
training loss 0.00016320501580963486 mae 0.009541019221011662
training loss 0.00015948564078217833 mae 0.009483363127816988
training loss 0.00015825992589845405 mae 0.009434177901651429
Epoch 48, training: loss: 0.0001569, mae: 0.0094027 test: loss0.0001563, mae:0.0093296
training loss 0.00011947562597924843 mae 0.00885887909680605
training loss 0.00014971457672871503 mae 0.009093980578815233
training loss 0.00015967695208469237 mae 0.009420203893344003
training loss 0.0001561941622061785 mae 0.00935287776384626
training loss 0.00015499854609642453 mae 0.00934910218680824
Epoch 49, training: loss: 0.0001547, mae: 0.0093368 test: loss0.0001574, mae:0.0093811
training loss 0.00014505871513392776 mae 0.00944873970001936
training loss 0.0001314109307999138 mae 0.008767277075379502
training loss 0.00014352002623433362 mae 0.009044447088485012
training loss 0.00014696410335392218 mae 0.009099988417934309
training loss 0.00014555241568135651 mae 0.009048553804556532
Epoch 50, training: loss: 0.0001451, mae: 0.0090561 test: loss0.0001531, mae:0.0091597
training loss 0.00012974806304555386 mae 0.009194742888212204
training loss 0.0001300048565562796 mae 0.008725752460532912
training loss 0.00013962897143152649 mae 0.008784828440166346
training loss 0.00014049156736339665 mae 0.008835238327475767
training loss 0.00014275836330663013 mae 0.008932830267747981
Epoch 51, training: loss: 0.0001430, mae: 0.0089482 test: loss0.0001491, mae:0.0090669
training loss 7.0041925937403e-05 mae 0.005935713183134794
training loss 0.00013744874726684178 mae 0.008751156055094566
training loss 0.00013607489035895914 mae 0.008834476412117188
training loss 0.00013724904509229645 mae 0.008873381189360529
training loss 0.00013950501326590302 mae 0.008908352710362244
Epoch 52, training: loss: 0.0001384, mae: 0.0088823 test: loss0.0001472, mae:0.0090973
training loss 0.00014490708417724818 mae 0.009126227349042892
training loss 0.00012908293109492158 mae 0.008453513933893514
training loss 0.00013182087851680197 mae 0.008661810645261907
training loss 0.00013180269235926246 mae 0.008641130321763996
training loss 0.00013303820881659545 mae 0.00866930521978297
Epoch 53, training: loss: 0.0001337, mae: 0.0086887 test: loss0.0001435, mae:0.0089133
training loss 7.926118996692821e-05 mae 0.007485011126846075
training loss 0.000138417066801933 mae 0.00882840956396916
training loss 0.0001325219202026527 mae 0.00866273282519956
training loss 0.00013127292978900356 mae 0.008639186340081961
training loss 0.00013186555648345936 mae 0.008679342263873978
Epoch 54, training: loss: 0.0001308, mae: 0.0086447 test: loss0.0001439, mae:0.0088324
training loss 0.00024902314180508256 mae 0.009952765889465809
training loss 0.00013324507345518493 mae 0.008523598066805037
training loss 0.0001277068047352034 mae 0.00849020968901344
training loss 0.0001266359269949731 mae 0.008473004624617607
training loss 0.00012548941334382868 mae 0.008442626934758016
Epoch 55, training: loss: 0.0001256, mae: 0.0084427 test: loss0.0001358, mae:0.0087911
training loss 7.468385592801496e-05 mae 0.0068814740516245365
training loss 0.00012235228938402097 mae 0.008360814622731182
training loss 0.00012752129281363867 mae 0.00844684602821817
training loss 0.00013029331352757537 mae 0.008566906697283315
training loss 0.00012643647169713416 mae 0.008478565718422626
Epoch 56, training: loss: 0.0001269, mae: 0.0084754 test: loss0.0001407, mae:0.0088381
training loss 0.00011306724627502263 mae 0.008539603091776371
training loss 0.00011294252822956289 mae 0.008104945989508254
training loss 0.00011854701052912011 mae 0.008215510871925269
training loss 0.0001221976426978586 mae 0.008325935853336807
training loss 0.00012269282567796102 mae 0.008368091759693554
Epoch 57, training: loss: 0.0001227, mae: 0.0083646 test: loss0.0001549, mae:0.0092956
training loss 0.0001298685383517295 mae 0.008723647333681583
training loss 0.0001252939145356946 mae 0.008602688362931503
training loss 0.00012037275342609603 mae 0.008412768736961162
training loss 0.00012214189703307966 mae 0.00839525684159225
training loss 0.00012725283394235214 mae 0.008515255259285072
Epoch 58, training: loss: 0.0001270, mae: 0.0085043 test: loss0.0001578, mae:0.0093953
training loss 0.00022753863595426083 mae 0.0113374600186944
training loss 0.00011722184047125755 mae 0.008235148546815505
training loss 0.00012280938273473264 mae 0.008330297015496708
training loss 0.00012279328378804013 mae 0.008331985114179308
training loss 0.00012057987361406764 mae 0.008281219509703602
Epoch 59, training: loss: 0.0001203, mae: 0.0082657 test: loss0.0001380, mae:0.0088746
training loss 0.00011020503006875515 mae 0.007517265621572733
training loss 0.00012599941093118533 mae 0.00851529066943947
training loss 0.0001211670800204398 mae 0.008319263397208825
training loss 0.00012282169666754186 mae 0.008348841337326744
training loss 0.00012450336977614735 mae 0.008410637468488805
Epoch 60, training: loss: 0.0001247, mae: 0.0084224 test: loss0.0001256, mae:0.0084684
training loss 0.00010085163376061246 mae 0.007835241965949535
training loss 0.00012338988980285223 mae 0.008327676722889438
training loss 0.00011421116390254 mae 0.008076446734848293
training loss 0.00011594937913835248 mae 0.008144811588333342
training loss 0.0001179925170052329 mae 0.008208871186613592
Epoch 61, training: loss: 0.0001180, mae: 0.0082111 test: loss0.0001285, mae:0.0084110
training loss 7.935948815429583e-05 mae 0.0071289390325546265
training loss 0.00011176898167868069 mae 0.008140718582652364
training loss 0.00011461180233394283 mae 0.008134309230095676
training loss 0.00011343298081327104 mae 0.008121972363935599
training loss 0.00011464180504084813 mae 0.008088088536458975
Epoch 62, training: loss: 0.0001133, mae: 0.0080447 test: loss0.0001412, mae:0.0087629
training loss 0.00016823048645164818 mae 0.009961292147636414
training loss 0.00011005082805971981 mae 0.007921675968842178
training loss 0.00011040298906466135 mae 0.007953920235654503
training loss 0.00011329217677063223 mae 0.008040326974348514
training loss 0.00011366526892354282 mae 0.008053376601165646
Epoch 63, training: loss: 0.0001128, mae: 0.0080221 test: loss0.0001280, mae:0.0085759
training loss 8.788841660134494e-05 mae 0.007634112145751715
training loss 0.00011168572967232877 mae 0.00802748296044621
training loss 0.00011237899647906911 mae 0.008059682513559515
training loss 0.00011273929188166748 mae 0.008030329590404273
training loss 0.00011019989050967057 mae 0.007920036518677554
Epoch 64, training: loss: 0.0001103, mae: 0.0079192 test: loss0.0001172, mae:0.0080588
training loss 9.313649934483692e-05 mae 0.007784314453601837
training loss 0.00010757567981593584 mae 0.007911987383576001
training loss 0.0001058685322799881 mae 0.007868151286487829
training loss 0.00010738810534945176 mae 0.00787472124797422
training loss 0.00010842739451147813 mae 0.007890888499049708
Epoch 65, training: loss: 0.0001090, mae: 0.0079208 test: loss0.0001427, mae:0.0089801
training loss 9.083053009817377e-05 mae 0.00838287454098463
training loss 0.00011131043149861413 mae 0.00791020272299647
training loss 0.00010371587696942809 mae 0.0076849327819182136
training loss 0.00010640865963936122 mae 0.0077893492636202976
training loss 0.00010871630105096148 mae 0.007889551332971053
Epoch 66, training: loss: 0.0001086, mae: 0.0078846 test: loss0.0001284, mae:0.0085205
training loss 0.0002797411580104381 mae 0.009701903909444809
training loss 0.00010987780450366199 mae 0.007875131990979703
training loss 0.00010718841242285829 mae 0.007790349165696912
training loss 0.0001074549872769329 mae 0.007767157388592006
training loss 0.00010749364533260071 mae 0.007802014765028487
Epoch 67, training: loss: 0.0001073, mae: 0.0078022 test: loss0.0001277, mae:0.0083979
training loss 0.00011195764091098681 mae 0.008280218578875065
training loss 0.00010310636778241115 mae 0.0077555231731750215
training loss 0.00010160556056428386 mae 0.007675037853014055
training loss 0.00011054197576287088 mae 0.007873004786546851
training loss 0.00011618978205479833 mae 0.00807443221530585
Epoch 68, training: loss: 0.0001156, mae: 0.0080658 test: loss0.0001322, mae:0.0086036
training loss 0.00010597525397315621 mae 0.008307741023600101
training loss 0.00010600963444220742 mae 0.007877781631096321
training loss 0.00010398684566152411 mae 0.007778219270086525
training loss 0.0001046637742499133 mae 0.007792343320756756
training loss 0.00010537525707860455 mae 0.007810400643696388
Epoch 69, training: loss: 0.0001048, mae: 0.0077910 test: loss0.0001275, mae:0.0083522
training loss 6.353611388476565e-05 mae 0.006258373614400625
training loss 0.00010383624341521486 mae 0.007785414929921721
training loss 0.00010680155717654818 mae 0.007847474878746097
training loss 0.0001031263937267 mae 0.0076979594274319205
training loss 0.0001025052340232086 mae 0.0076707448342361005
Epoch 70, training: loss: 0.0001017, mae: 0.0076453 test: loss0.0001268, mae:0.0085454
training loss 0.00013193910126574337 mae 0.008589910343289375
training loss 0.00010189686373433135 mae 0.007424727392693361
training loss 0.0001043706468426355 mae 0.007654628601146512
training loss 0.00010989491863635718 mae 0.00785722133598679
training loss 0.00010628273688310024 mae 0.007790383736867071
Epoch 71, training: loss: 0.0001051, mae: 0.0077643 test: loss0.0001147, mae:0.0079310
training loss 4.8741985665401444e-05 mae 0.005548002663999796
training loss 9.792316518096236e-05 mae 0.007499013470449284
training loss 9.60174292420489e-05 mae 0.007461383141171518
training loss 9.883451017736925e-05 mae 0.007531916007202191
training loss 9.866386204720497e-05 mae 0.007525055456109604
Epoch 72, training: loss: 0.0000986, mae: 0.0075348 test: loss0.0001258, mae:0.0083141
training loss 6.404494342859834e-05 mae 0.0060440488159656525
training loss 9.236317782779226e-05 mae 0.007323234542912126
training loss 9.218420310200673e-05 mae 0.00731843406113215
training loss 9.545802656447033e-05 mae 0.007465707807376093
training loss 9.924314204610379e-05 mae 0.007553498466865195
Epoch 73, training: loss: 0.0001001, mae: 0.0075870 test: loss0.0001260, mae:0.0085403
training loss 0.00016708315524738282 mae 0.009983677417039871
training loss 0.0001021386644923968 mae 0.007669979377704508
training loss 0.0001044443250624555 mae 0.007713457071545102
training loss 0.00010024219877988605 mae 0.007599552513550449
training loss 0.00010227552185136832 mae 0.007665185023803466
Epoch 74, training: loss: 0.0001022, mae: 0.0076695 test: loss0.0001121, mae:0.0079204
training loss 9.275742922909558e-05 mae 0.007679654750972986
training loss 9.571170841238698e-05 mae 0.007533196642921835
training loss 9.9554278109497e-05 mae 0.007531032620901518
training loss 9.975604798636301e-05 mae 0.007561860276102425
training loss 9.930531195462196e-05 mae 0.007563672009948174
Epoch 75, training: loss: 0.0000990, mae: 0.0075451 test: loss0.0001135, mae:0.0080237
training loss 7.244182779686525e-05 mae 0.005827099084854126
training loss 0.00010069484001267519 mae 0.007562383169344825
training loss 9.615115554384325e-05 mae 0.007468756311342561
training loss 9.320944289188054e-05 mae 0.007363759924885847
training loss 9.287169170004095e-05 mae 0.0073493580977937465
Epoch 76, training: loss: 0.0000934, mae: 0.0073553 test: loss0.0001174, mae:0.0081464
training loss 6.742182449670509e-05 mae 0.006356367841362953
training loss 8.74223426531078e-05 mae 0.0071035387158832125
training loss 8.935325525154703e-05 mae 0.007192310190178677
training loss 9.056060782723155e-05 mae 0.007237975630563812
training loss 9.139579982440036e-05 mae 0.007257785233414139
Epoch 77, training: loss: 0.0000912, mae: 0.0072542 test: loss0.0001052, mae:0.0076689
training loss 9.247328125638887e-05 mae 0.006979051977396011
training loss 8.25936425398515e-05 mae 0.006918998124698801
training loss 8.803980129207302e-05 mae 0.007110325020212349
training loss 9.164660832237928e-05 mae 0.0072525909990812385
training loss 9.411966034824573e-05 mae 0.0073330750809380086
Epoch 78, training: loss: 0.0000938, mae: 0.0073203 test: loss0.0001129, mae:0.0079128
training loss 6.16908582742326e-05 mae 0.006058929022401571
training loss 8.77668916312841e-05 mae 0.007103855940787232
training loss 9.064213153047459e-05 mae 0.007269679338182554
training loss 9.15928647746656e-05 mae 0.007314158572710509
training loss 9.232484808308075e-05 mae 0.007310896303356435
Epoch 79, training: loss: 0.0000922, mae: 0.0073043 test: loss0.0001088, mae:0.0078872
training loss 7.482895307475701e-05 mae 0.006918579339981079
training loss 8.547171726640241e-05 mae 0.007171870209276676
training loss 9.078347453522484e-05 mae 0.007282549450819446
training loss 9.145715416737501e-05 mae 0.007303656310163784
training loss 9.424534956582902e-05 mae 0.0073971131825773286
Epoch 80, training: loss: 0.0000933, mae: 0.0073609 test: loss0.0001104, mae:0.0077513
training loss 5.788827547803521e-05 mae 0.0062090735882520676
training loss 8.354035954278731e-05 mae 0.007035465929291995
training loss 8.918548143027343e-05 mae 0.0071539688062402275
training loss 8.888543167711232e-05 mae 0.007130688284554623
training loss 8.792386387228106e-05 mae 0.007117447234801392
Epoch 81, training: loss: 0.0000877, mae: 0.0071078 test: loss0.0001066, mae:0.0077080
training loss 5.73905163037125e-05 mae 0.005734721198678017
training loss 8.662387364322557e-05 mae 0.007059060164964667
training loss 8.859394704876924e-05 mae 0.0071291271043886035
training loss 8.750259892634164e-05 mae 0.007106439549697945
training loss 8.819321184132393e-05 mae 0.007120321745710881
Epoch 82, training: loss: 0.0000890, mae: 0.0071507 test: loss0.0001149, mae:0.0078961
training loss 8.145892206812277e-05 mae 0.007130559533834457
training loss 8.005978780405998e-05 mae 0.0069141348038672225
training loss 8.161067844502222e-05 mae 0.006955459040801713
training loss 8.27938458423164e-05 mae 0.007001878265044744
training loss 8.498964408358712e-05 mae 0.007072994316376709
Epoch 83, training: loss: 0.0000853, mae: 0.0070768 test: loss0.0001080, mae:0.0077408
training loss 7.882260251790285e-05 mae 0.00739327073097229
training loss 8.469544106857011e-05 mae 0.0070613429461624105
training loss 9.10163417027452e-05 mae 0.007256541442642413
training loss 8.945379293749623e-05 mae 0.0072067414089691945
training loss 8.688107686680835e-05 mae 0.007134055153499193
Epoch 84, training: loss: 0.0000876, mae: 0.0071650 test: loss0.0001054, mae:0.0076788
training loss 5.252215123618953e-05 mae 0.00596092501655221
training loss 9.0493314072613e-05 mae 0.007252684447403048
training loss 8.757775536326848e-05 mae 0.00713797317043242
training loss 8.353951315504243e-05 mae 0.00700460917112843
training loss 8.423953415638892e-05 mae 0.007017839361742064
Epoch 85, training: loss: 0.0000839, mae: 0.0069960 test: loss0.0001001, mae:0.0074544
training loss 7.550266309408471e-05 mae 0.006455483380705118
training loss 7.607429392473754e-05 mae 0.00673218108896239
training loss 7.772101025243456e-05 mae 0.006769872265913995
training loss 7.98979526734911e-05 mae 0.006849123531989509
training loss 7.961412154882915e-05 mae 0.00683267634658523
Epoch 86, training: loss: 0.0000798, mae: 0.0068428 test: loss0.0001017, mae:0.0075790
training loss 5.7014654885279015e-05 mae 0.0058413478545844555
training loss 8.166692079576738e-05 mae 0.006871304182591392
training loss 8.145020078835403e-05 mae 0.006874559760683833
training loss 8.07158186631231e-05 mae 0.006874747719270305
training loss 8.124492866153927e-05 mae 0.0068781045775174795
Epoch 87, training: loss: 0.0000812, mae: 0.0068776 test: loss0.0001045, mae:0.0076242
training loss 8.828537102090195e-05 mae 0.006603988353163004
training loss 7.786434406286797e-05 mae 0.006803855984745656
training loss 7.811161348138617e-05 mae 0.006736261004784907
training loss 7.775657255081492e-05 mae 0.0067610876505195296
training loss 7.608002938943522e-05 mae 0.00670351644060505
Epoch 88, training: loss: 0.0000763, mae: 0.0067134 test: loss0.0001004, mae:0.0074957
training loss 5.953712752670981e-05 mae 0.0053968545980751514
training loss 7.621208914868333e-05 mae 0.006640558505394295
training loss 7.4968401737117e-05 mae 0.006622254700943975
training loss 7.568809509008276e-05 mae 0.006675507262694522
training loss 7.666101429723807e-05 mae 0.00672587108524952
Epoch 89, training: loss: 0.0000775, mae: 0.0067554 test: loss0.0001007, mae:0.0074044
training loss 4.101088779862039e-05 mae 0.004972278606146574
training loss 7.309581811640266e-05 mae 0.006546392425091243
training loss 7.597481500383901e-05 mae 0.006687438756338145
training loss 7.778000221849839e-05 mae 0.006793610381566927
training loss 7.67035705950493e-05 mae 0.0067493196655602295
Epoch 90, training: loss: 0.0000765, mae: 0.0067386 test: loss0.0001041, mae:0.0075906
training loss 8.0294550571125e-05 mae 0.007125276606529951
training loss 7.610336686804086e-05 mae 0.006772571928141749
training loss 7.812440036456005e-05 mae 0.0068225328266473114
training loss 7.829559616230285e-05 mae 0.0068148027192677894
training loss 7.779751581777538e-05 mae 0.006793911480785011
Epoch 91, training: loss: 0.0000775, mae: 0.0067768 test: loss0.0001051, mae:0.0076307
training loss 7.375202403636649e-05 mae 0.006737848278135061
training loss 7.06978117865647e-05 mae 0.006570242363594326
training loss 7.021779091170028e-05 mae 0.006536035701669384
training loss 7.322170097644544e-05 mae 0.006632291412733447
training loss 7.484609407253459e-05 mae 0.0066874662845103575
Epoch 92, training: loss: 0.0000753, mae: 0.0066933 test: loss0.0000979, mae:0.0074166
training loss 6.894682883284986e-05 mae 0.006754925008863211
training loss 7.082651563013867e-05 mae 0.006496574346195249
training loss 7.340426222216063e-05 mae 0.006641209134739812
training loss 7.220141832783836e-05 mae 0.00657347106800371
training loss 7.28892429915097e-05 mae 0.006586718864145858
Epoch 93, training: loss: 0.0000731, mae: 0.0065982 test: loss0.0001038, mae:0.0075330
training loss 8.191270899260417e-05 mae 0.007547078654170036
training loss 7.073787584850638e-05 mae 0.0065342674086637355
training loss 7.005034227530464e-05 mae 0.006529167294502258
training loss 6.996051151330876e-05 mae 0.006482111857525559
training loss 7.101914365990176e-05 mae 0.00651210182768047
Epoch 94, training: loss: 0.0000715, mae: 0.0065295 test: loss0.0000933, mae:0.0071867
training loss 0.00010555101471254602 mae 0.007300819735974073
training loss 6.619961263198753e-05 mae 0.006314714238339779
training loss 6.727294313161182e-05 mae 0.006345222616512881
training loss 6.836220209694173e-05 mae 0.00642577405180165
training loss 6.895447072407364e-05 mae 0.006418322145809135
Epoch 95, training: loss: 0.0000689, mae: 0.0064144 test: loss0.0000998, mae:0.0074168
training loss 7.723963790340349e-05 mae 0.0071671889163553715
training loss 6.958072802209402e-05 mae 0.006334320569009171
training loss 7.192249198635521e-05 mae 0.006458313356494844
training loss 7.12676786656479e-05 mae 0.006468795665496626
training loss 7.159256901063802e-05 mae 0.006501219974395202
Epoch 96, training: loss: 0.0000720, mae: 0.0065186 test: loss0.0001106, mae:0.0078595
training loss 7.871876005083323e-05 mae 0.006802683230489492
training loss 7.394989256235753e-05 mae 0.006678971726739523
training loss 7.01976410475776e-05 mae 0.006489395546625453
training loss 6.981749131516951e-05 mae 0.006454750250975622
training loss 7.167610196328362e-05 mae 0.00652520239714589
Epoch 97, training: loss: 0.0000717, mae: 0.0065245 test: loss0.0001092, mae:0.0076695
training loss 0.00010571838356554508 mae 0.007917855866253376
training loss 6.94476013236186e-05 mae 0.006522253904418618
training loss 7.490692709919549e-05 mae 0.006681012768888532
training loss 7.72676120812092e-05 mae 0.006774604878702898
training loss 7.578478845917107e-05 mae 0.006720961954222241
Epoch 98, training: loss: 0.0000755, mae: 0.0067012 test: loss0.0000935, mae:0.0071790
training loss 6.598128675250337e-05 mae 0.00656735897064209
training loss 6.718066430824133e-05 mae 0.006363593032766206
training loss 6.46936933113502e-05 mae 0.00621603631257716
training loss 6.909177077522454e-05 mae 0.006374730842669079
training loss 6.93096506311755e-05 mae 0.0064010458734275666
Epoch 99, training: loss: 0.0000693, mae: 0.0064002 test: loss0.0000972, mae:0.0072583
current learning rate: 0.00025
training loss 6.70134904794395e-05 mae 0.006586018949747086
training loss 5.911207114752637e-05 mae 0.005976613583591054
training loss 5.905804245779981e-05 mae 0.005957659038871822
training loss 5.981522987732379e-05 mae 0.005971644770674752
training loss 5.905830946189708e-05 mae 0.005925392444750563
Epoch 100, training: loss: 0.0000588, mae: 0.0059221 test: loss0.0000887, mae:0.0069331
training loss 5.272493581287563e-05 mae 0.0054315864108502865
training loss 5.357352290279688e-05 mae 0.0057163627763443135
training loss 5.439248555979581e-05 mae 0.005691576495508453
training loss 5.4369724130806113e-05 mae 0.005685716370947117
training loss 5.4262665138259625e-05 mae 0.005700323346131178
Epoch 101, training: loss: 0.0000544, mae: 0.0057129 test: loss0.0000897, mae:0.0069449
training loss 7.377764995908365e-05 mae 0.006438319105654955
training loss 5.1870052594298894e-05 mae 0.005614451141845363
training loss 5.048342162727651e-05 mae 0.005554633397184829
training loss 5.067982427087393e-05 mae 0.005562600890939303
training loss 5.1900504811204165e-05 mae 0.005613073320892543
Epoch 102, training: loss: 0.0000525, mae: 0.0056417 test: loss0.0000897, mae:0.0070496
training loss 7.562858081655577e-05 mae 0.006594622042030096
training loss 5.640362695546584e-05 mae 0.005799288258833044
training loss 5.3618793509206694e-05 mae 0.005697672342936885
training loss 5.2638981323739337e-05 mae 0.005646473102098859
training loss 5.220265031755447e-05 mae 0.005630723091852458
Epoch 103, training: loss: 0.0000523, mae: 0.0056366 test: loss0.0000874, mae:0.0069072
training loss 4.065476605319418e-05 mae 0.0048274570144712925
training loss 5.054970082995829e-05 mae 0.005550176201972599
training loss 5.032350479565806e-05 mae 0.005532836585333292
training loss 5.161611039776647e-05 mae 0.005595953488993822
training loss 5.2212643836287867e-05 mae 0.005622679107030159
Epoch 104, training: loss: 0.0000523, mae: 0.0056253 test: loss0.0000832, mae:0.0067347
training loss 4.289971911930479e-05 mae 0.005100665148347616
training loss 4.7594720907749016e-05 mae 0.005353693832077231
training loss 4.9324933804587784e-05 mae 0.005494344735167701
training loss 4.805681185686225e-05 mae 0.005438534988534474
training loss 4.889895503019128e-05 mae 0.0054632749730507985
Epoch 105, training: loss: 0.0000495, mae: 0.0054853 test: loss0.0000864, mae:0.0068913
training loss 5.1830444135703146e-05 mae 0.005814762320369482
training loss 5.049514516477025e-05 mae 0.005538491723949418
training loss 4.9946063936118606e-05 mae 0.005515871781597632
training loss 4.985964122603782e-05 mae 0.005515644015337263
training loss 4.960356760710426e-05 mae 0.0054971055007210735
Epoch 106, training: loss: 0.0000497, mae: 0.0055084 test: loss0.0000848, mae:0.0067863
training loss 5.565964966081083e-05 mae 0.005468316376209259
training loss 4.6285637247766486e-05 mae 0.005329940449811667
training loss 4.74995352549128e-05 mae 0.00539442484202509
training loss 4.8199090569870665e-05 mae 0.005421317833173554
training loss 4.840438446284573e-05 mae 0.005423497105137773
Epoch 107, training: loss: 0.0000482, mae: 0.0054130 test: loss0.0000871, mae:0.0068902
training loss 3.477209611446597e-05 mae 0.004723639693111181
training loss 4.614106190131803e-05 mae 0.005321517906279542
training loss 4.629554416961844e-05 mae 0.005362528528428021
training loss 4.682840189830222e-05 mae 0.005379818796198693
training loss 4.719039652763291e-05 mae 0.005380658599645345
Epoch 108, training: loss: 0.0000470, mae: 0.0053688 test: loss0.0000865, mae:0.0068135
training loss 4.181209078524262e-05 mae 0.005026802886277437
training loss 4.7482048103120185e-05 mae 0.005445873063495931
training loss 4.629762399489853e-05 mae 0.005367196404771641
training loss 4.7801325760418646e-05 mae 0.005443483643505156
training loss 4.773303725645502e-05 mae 0.005435269386206401
Epoch 109, training: loss: 0.0000476, mae: 0.0054275 test: loss0.0000854, mae:0.0068389
training loss 4.299071224522777e-05 mae 0.00503907073289156
training loss 4.70721732234975e-05 mae 0.005313579915274007
training loss 4.774267488593781e-05 mae 0.005397976320790182
training loss 4.644572636486874e-05 mae 0.0053352513967316285
training loss 4.688134629135161e-05 mae 0.00536913722660856
Epoch 110, training: loss: 0.0000471, mae: 0.0053804 test: loss0.0000881, mae:0.0069092
training loss 4.7974142944440246e-05 mae 0.00547705078497529
training loss 4.219617118246342e-05 mae 0.005085034606357418
training loss 4.4788889168233765e-05 mae 0.005219252538028186
training loss 4.464725022281698e-05 mae 0.0052481458186239794
training loss 4.544764703328761e-05 mae 0.005289633330930749
Epoch 111, training: loss: 0.0000457, mae: 0.0053034 test: loss0.0000814, mae:0.0066549
training loss 4.559380977298133e-05 mae 0.0053745754994452
training loss 4.312072326452014e-05 mae 0.0051511446854063106
training loss 4.205583711226796e-05 mae 0.005082052996345234
training loss 4.3271122664515386e-05 mae 0.005160879289780721
training loss 4.4969138060928096e-05 mae 0.005243345544863467
Epoch 112, training: loss: 0.0000453, mae: 0.0052632 test: loss0.0000833, mae:0.0067519
training loss 4.6660661610076204e-05 mae 0.005180298816412687
training loss 4.460251793538119e-05 mae 0.0052465055886583
training loss 4.415460597835528e-05 mae 0.005230767562487485
training loss 4.435209326848114e-05 mae 0.005221884250517515
training loss 4.4894834361876004e-05 mae 0.005257231946710259
Epoch 113, training: loss: 0.0000449, mae: 0.0052557 test: loss0.0000846, mae:0.0067421
training loss 3.2704119803383946e-05 mae 0.004165174905210733
training loss 4.566603235491807e-05 mae 0.005201949080561893
training loss 4.586641845511765e-05 mae 0.005265641192177143
training loss 4.621346336637164e-05 mae 0.005290199472069344
training loss 4.5048041052062804e-05 mae 0.005235986109934193
Epoch 114, training: loss: 0.0000452, mae: 0.0052491 test: loss0.0000837, mae:0.0067489
training loss 3.338708120281808e-05 mae 0.004605167545378208
training loss 4.187152644233578e-05 mae 0.005085668727463366
training loss 4.3652234099042324e-05 mae 0.005205415673369524
training loss 4.331870908969984e-05 mae 0.005179763436995892
training loss 4.341782797966328e-05 mae 0.005170844684109388
Epoch 115, training: loss: 0.0000435, mae: 0.0051755 test: loss0.0000834, mae:0.0067319
training loss 3.236653355997987e-05 mae 0.004627392161637545
training loss 3.8316710618114574e-05 mae 0.004871263894635965
training loss 3.949779606009903e-05 mae 0.004951839383673105
training loss 4.0894836214442734e-05 mae 0.0050276193462484905
training loss 4.1643200008243794e-05 mae 0.0050705831708729426
Epoch 116, training: loss: 0.0000420, mae: 0.0050915 test: loss0.0000846, mae:0.0068346
training loss 3.451427983236499e-05 mae 0.004703947808593512
training loss 3.819874469698727e-05 mae 0.004836973095057059
training loss 4.0819500505262605e-05 mae 0.005021657725556358
training loss 3.972446238970929e-05 mae 0.004962068532180314
training loss 4.0139153841764194e-05 mae 0.004981359125193746
Epoch 117, training: loss: 0.0000401, mae: 0.0049773 test: loss0.0000849, mae:0.0067968
training loss 4.48363134637475e-05 mae 0.004859426058828831
training loss 4.094966710659255e-05 mae 0.005037425072206293
training loss 4.078788376795959e-05 mae 0.005038064570249161
training loss 4.1819762993435896e-05 mae 0.00509323006872063
training loss 4.15349616139168e-05 mae 0.005070608601539363
Epoch 118, training: loss: 0.0000419, mae: 0.0050905 test: loss0.0000812, mae:0.0066207
training loss 3.7532747228397056e-05 mae 0.004813576117157936
training loss 4.0413037698040254e-05 mae 0.004953441995323873
training loss 4.089068444036548e-05 mae 0.004993837412059455
training loss 4.128349910207659e-05 mae 0.005028977009486283
training loss 4.068934332231288e-05 mae 0.005002350973854979
Epoch 119, training: loss: 0.0000406, mae: 0.0049915 test: loss0.0000826, mae:0.0067148
training loss 3.8312409742502496e-05 mae 0.0049751317128539085
training loss 3.977199747739588e-05 mae 0.004991926828070599
training loss 4.06839851103262e-05 mae 0.005048939155986404
training loss 3.941613385603771e-05 mae 0.004963943939841053
training loss 3.9848997312672044e-05 mae 0.004974196046424001
Epoch 120, training: loss: 0.0000398, mae: 0.0049698 test: loss0.0000829, mae:0.0066765
training loss 2.5886576622724533e-05 mae 0.0040413010865449905
training loss 3.761240837133406e-05 mae 0.004837667537560942
training loss 3.7775821609930864e-05 mae 0.004809832347944232
training loss 3.832676921828899e-05 mae 0.004856546313828785
training loss 3.84724675250571e-05 mae 0.00488061938939885
Epoch 121, training: loss: 0.0000382, mae: 0.0048647 test: loss0.0000814, mae:0.0066303
training loss 2.382546335866209e-05 mae 0.004144029226154089
training loss 3.675080805288757e-05 mae 0.004777341903022981
training loss 3.802728207290893e-05 mae 0.004869145728500172
training loss 3.785565373341673e-05 mae 0.004863861629073292
training loss 3.783915073554952e-05 mae 0.004861159235081491
Epoch 122, training: loss: 0.0000380, mae: 0.0048647 test: loss0.0000837, mae:0.0067212
training loss 3.072741310461424e-05 mae 0.004410260356962681
training loss 3.789733576316697e-05 mae 0.004787210365939959
training loss 3.858326601745138e-05 mae 0.004873355859798371
training loss 3.7515787320540735e-05 mae 0.004802895683659526
training loss 3.7584100412355464e-05 mae 0.004803368022470776
Epoch 123, training: loss: 0.0000377, mae: 0.0048155 test: loss0.0000819, mae:0.0066483
training loss 2.264239992655348e-05 mae 0.0037371318321675062
training loss 3.6155605116886485e-05 mae 0.004715842596602205
training loss 3.648860271850666e-05 mae 0.004734027655887426
training loss 3.7353970649608236e-05 mae 0.004796726478029362
training loss 3.848148661750179e-05 mae 0.004864670075615174
Epoch 124, training: loss: 0.0000384, mae: 0.0048563 test: loss0.0000900, mae:0.0069541
training loss 2.3914994017104618e-05 mae 0.0041019609197974205
training loss 4.269700808685692e-05 mae 0.00496837523692817
training loss 4.303666867032223e-05 mae 0.0050119163380880445
training loss 4.166070663290012e-05 mae 0.004978116296404443
training loss 4.137827302507339e-05 mae 0.004987020020034925
Epoch 125, training: loss: 0.0000413, mae: 0.0049863 test: loss0.0000804, mae:0.0065803
training loss 3.398077024030499e-05 mae 0.00476237153634429
training loss 3.494464416529381e-05 mae 0.004606966915376047
training loss 3.632822807787198e-05 mae 0.004691158202338485
training loss 3.7528122441815405e-05 mae 0.0047941727714847435
training loss 3.70654687690126e-05 mae 0.00476823538189652
Epoch 126, training: loss: 0.0000369, mae: 0.0047529 test: loss0.0000815, mae:0.0065983
training loss 3.1395102269016206e-05 mae 0.0043743508867919445
training loss 3.4643449123227487e-05 mae 0.004605029024841153
training loss 3.4347226438953974e-05 mae 0.0045953162364883
training loss 3.5298616932404366e-05 mae 0.004678359506528404
training loss 3.5942843133124355e-05 mae 0.004702796534037411
Epoch 127, training: loss: 0.0000360, mae: 0.0047114 test: loss0.0000807, mae:0.0065429
training loss 3.063971234951168e-05 mae 0.004094310104846954
training loss 3.3463494879006435e-05 mae 0.0045309264358936565
training loss 3.5119857748573896e-05 mae 0.004645394723825524
training loss 3.588091353084028e-05 mae 0.004695134599760963
training loss 3.585093843106894e-05 mae 0.0046899201932238095
Epoch 128, training: loss: 0.0000360, mae: 0.0047006 test: loss0.0000848, mae:0.0067134
training loss 5.014694397686981e-05 mae 0.005630215629935265
training loss 3.339150362972207e-05 mae 0.004533704096341833
training loss 3.344330000593141e-05 mae 0.004554381172200389
training loss 3.387073027155058e-05 mae 0.004573815872288303
training loss 3.476335900157761e-05 mae 0.004626981844541741
Epoch 129, training: loss: 0.0000348, mae: 0.0046280 test: loss0.0000797, mae:0.0065262
training loss 3.9442838897230104e-05 mae 0.004477201960980892
training loss 3.3612675763958804e-05 mae 0.004576968996986453
training loss 3.2716332930474466e-05 mae 0.004518618120230955
training loss 3.277049385938663e-05 mae 0.0044998708705337636
training loss 3.340785673146763e-05 mae 0.004544602965342984
Epoch 130, training: loss: 0.0000334, mae: 0.0045414 test: loss0.0000816, mae:0.0065992
training loss 3.170060881529935e-05 mae 0.004641907289624214
training loss 3.302496624397176e-05 mae 0.00449985938201494
training loss 3.267497042615295e-05 mae 0.0044838072533839105
training loss 3.336305032902096e-05 mae 0.004538359046475777
training loss 3.351221310376742e-05 mae 0.004552274479053507
Epoch 131, training: loss: 0.0000336, mae: 0.0045562 test: loss0.0000827, mae:0.0066304
training loss 4.424993312568404e-05 mae 0.00535757839679718
training loss 3.1128313304518584e-05 mae 0.0044165086594647645
training loss 3.180402533268201e-05 mae 0.004442873968752008
training loss 3.332014195910123e-05 mae 0.004526203385683776
training loss 3.342860884854712e-05 mae 0.004534088499121256
Epoch 132, training: loss: 0.0000335, mae: 0.0045427 test: loss0.0000796, mae:0.0064875
training loss 2.723865873122122e-05 mae 0.004268085118383169
training loss 3.2487913296220134e-05 mae 0.0044376126664015005
training loss 3.214284503099967e-05 mae 0.004438153179915677
training loss 3.253313963001192e-05 mae 0.004470790811931555
training loss 3.316090990199487e-05 mae 0.00450822624808817
Epoch 133, training: loss: 0.0000332, mae: 0.0045082 test: loss0.0000803, mae:0.0065542
training loss 2.7662681532092392e-05 mae 0.004328342620283365
training loss 3.228843303861819e-05 mae 0.004467339898623963
training loss 3.163775239378389e-05 mae 0.004416838850789141
training loss 3.217479481451624e-05 mae 0.0044376301511332705
training loss 3.2226175296920174e-05 mae 0.004451567482027176
Epoch 134, training: loss: 0.0000323, mae: 0.0044538 test: loss0.0000825, mae:0.0066459
training loss 2.2905796868144535e-05 mae 0.0040230038575828075
training loss 3.1221905980297996e-05 mae 0.0043752866651059355
training loss 3.192829741003675e-05 mae 0.004452098854863555
training loss 3.350922618466447e-05 mae 0.004484904035679156
training loss 3.566373149864088e-05 mae 0.0046331532118814205
Epoch 135, training: loss: 0.0000359, mae: 0.0046476 test: loss0.0000824, mae:0.0066897
training loss 2.9737011573161e-05 mae 0.004423829261213541
training loss 3.6529306824234614e-05 mae 0.004772106468604478
training loss 3.472637109724128e-05 mae 0.004640854318801427
training loss 3.495216130871491e-05 mae 0.004594784273167714
training loss 3.551692502674201e-05 mae 0.004641554993693135
Epoch 136, training: loss: 0.0000356, mae: 0.0046548 test: loss0.0000782, mae:0.0064740
training loss 2.6706511562224478e-05 mae 0.004174352157860994
training loss 3.0933423034844497e-05 mae 0.00440563433164475
training loss 3.1481684615782534e-05 mae 0.004427701269396312
training loss 3.116740110320262e-05 mae 0.004402284664626152
training loss 3.135720677408841e-05 mae 0.0044133703059873585
Epoch 137, training: loss: 0.0000314, mae: 0.0044149 test: loss0.0000792, mae:0.0064930
training loss 3.342570926179178e-05 mae 0.004375289659947157
training loss 3.1668712515110046e-05 mae 0.004360529881737689
training loss 3.1667399453719634e-05 mae 0.004372301808844256
training loss 3.1641249192010496e-05 mae 0.004399279275884393
training loss 3.157649718641143e-05 mae 0.004397633712534879
Epoch 138, training: loss: 0.0000316, mae: 0.0043951 test: loss0.0000819, mae:0.0065817
training loss 2.2334488676278852e-05 mae 0.00358242797665298
training loss 3.0075063656184676e-05 mae 0.004267468692406135
training loss 2.95963647550878e-05 mae 0.004246546462694606
training loss 3.040572526226352e-05 mae 0.004314755496649157
training loss 3.068227824000342e-05 mae 0.0043371156395519556
Epoch 139, training: loss: 0.0000308, mae: 0.0043463 test: loss0.0000807, mae:0.0065762
training loss 2.5710534828249365e-05 mae 0.004201202187687159
training loss 2.7961748820220524e-05 mae 0.004162767418094126
training loss 2.8676155107130408e-05 mae 0.004219063774378292
training loss 2.948743716818635e-05 mae 0.004276963840516298
training loss 2.990161849436899e-05 mae 0.004300342497429741
Epoch 140, training: loss: 0.0000300, mae: 0.0043021 test: loss0.0000788, mae:0.0064394
training loss 2.61328968917951e-05 mae 0.004054567310959101
training loss 3.357819821080651e-05 mae 0.004463038334221232
training loss 3.1584108307136996e-05 mae 0.0043888548804022895
training loss 3.087903706240084e-05 mae 0.0043503336565236
training loss 3.0428793410582646e-05 mae 0.004314107409173354
Epoch 141, training: loss: 0.0000303, mae: 0.0043073 test: loss0.0000795, mae:0.0065158
training loss 2.342422703804914e-05 mae 0.00416908785700798
training loss 2.8385500145839834e-05 mae 0.0041408225090480335
training loss 2.863456208642404e-05 mae 0.004199214438714162
training loss 2.9634961659917142e-05 mae 0.0042493085759315674
training loss 3.070512387361603e-05 mae 0.004326740433855806
Epoch 142, training: loss: 0.0000307, mae: 0.0043295 test: loss0.0000812, mae:0.0065606
training loss 3.346654193592258e-05 mae 0.00459479121491313
training loss 2.878737024159408e-05 mae 0.004188335925231085
training loss 2.8091748894853885e-05 mae 0.004168822722841458
training loss 2.8676652624765222e-05 mae 0.004207634004696413
training loss 2.909893117619468e-05 mae 0.004231749996728266
Epoch 143, training: loss: 0.0000292, mae: 0.0042407 test: loss0.0000776, mae:0.0064348
training loss 2.353594572923612e-05 mae 0.0038835660088807344
training loss 2.7059496335591658e-05 mae 0.00410482454497148
training loss 2.6996177214164374e-05 mae 0.004087994409964816
training loss 2.7204103293151748e-05 mae 0.004085706986351225
training loss 2.769949224870767e-05 mae 0.0041347467132023955
Epoch 144, training: loss: 0.0000277, mae: 0.0041356 test: loss0.0000822, mae:0.0065596
training loss 2.526627758925315e-05 mae 0.00391916511580348
training loss 2.812648894819979e-05 mae 0.004171900501401694
training loss 2.758868129711857e-05 mae 0.004155327042675403
training loss 2.829112065156124e-05 mae 0.0041909286326771945
training loss 2.865553542324956e-05 mae 0.004224349676274616
Epoch 145, training: loss: 0.0000288, mae: 0.0042307 test: loss0.0000805, mae:0.0065466
training loss 1.79512699105544e-05 mae 0.0033943476155400276
training loss 2.8608775655990102e-05 mae 0.004187435322605512
training loss 2.867201274597446e-05 mae 0.004184704142539661
training loss 2.8633974676231697e-05 mae 0.0041992331296822295
training loss 2.8410510437528898e-05 mae 0.004194459837365576
Epoch 146, training: loss: 0.0000284, mae: 0.0041914 test: loss0.0000810, mae:0.0065541
training loss 2.2077450921642594e-05 mae 0.0034303709398955107
training loss 2.7590212994255132e-05 mae 0.004105656878000088
training loss 2.7468192717723204e-05 mae 0.004115535211426641
training loss 2.7421628767223594e-05 mae 0.0041109814388763845
training loss 2.7409830380661758e-05 mae 0.004110741197358615
Epoch 147, training: loss: 0.0000272, mae: 0.0041026 test: loss0.0000820, mae:0.0065842
training loss 2.0133700672886334e-05 mae 0.0038213564548641443
training loss 2.5430426046345842e-05 mae 0.003984389972745203
training loss 2.8489538752831197e-05 mae 0.004190596910931242
training loss 2.86638967748554e-05 mae 0.004210368067100152
training loss 2.8872782662512315e-05 mae 0.004230015253212617
Epoch 148, training: loss: 0.0000289, mae: 0.0042290 test: loss0.0000798, mae:0.0065565
training loss 2.0402483642101288e-05 mae 0.0034494083374738693
training loss 2.4266225142020944e-05 mae 0.003888640612108157
training loss 2.6476843096575195e-05 mae 0.004039122149034745
training loss 2.7086167982821844e-05 mae 0.004083453598956496
training loss 2.7230300330116752e-05 mae 0.004090854124309709
Epoch 149, training: loss: 0.0000272, mae: 0.0040910 test: loss0.0000800, mae:0.0065065
training loss 2.187885547755286e-05 mae 0.0037312358617782593
training loss 2.6310742909759837e-05 mae 0.004042601774391881
training loss 2.8024470348010253e-05 mae 0.004158027700637238
training loss 2.8609233456956763e-05 mae 0.004205508370396515
training loss 2.9080546930833698e-05 mae 0.004228077250861781
Epoch 150, training: loss: 0.0000292, mae: 0.0042388 test: loss0.0000789, mae:0.0064478
training loss 1.8424130757921375e-05 mae 0.003442559391260147
training loss 2.66749556983853e-05 mae 0.004074327944430942
training loss 2.708833593408933e-05 mae 0.004097519919626637
training loss 2.691801600669941e-05 mae 0.004078875909503918
training loss 2.7386714731194248e-05 mae 0.004105940630279522
Epoch 151, training: loss: 0.0000275, mae: 0.0041125 test: loss0.0000791, mae:0.0064636
training loss 2.092128670483362e-05 mae 0.003608343191444874
training loss 2.60640046654575e-05 mae 0.003941421004413974
training loss 2.6643413953519987e-05 mae 0.004010715962175537
training loss 2.6543595402544687e-05 mae 0.0040328300508689
training loss 2.691214875773056e-05 mae 0.004060947636272107
Epoch 152, training: loss: 0.0000271, mae: 0.0040742 test: loss0.0000812, mae:0.0064212
training loss 2.1010537238908e-05 mae 0.0036846252623945475
training loss 2.508309929301807e-05 mae 0.003910595149386164
training loss 2.5661026844354275e-05 mae 0.003972244709453517
training loss 2.551726946280151e-05 mae 0.00396510150548786
training loss 2.6345679720206914e-05 mae 0.004025319829787041
Epoch 153, training: loss: 0.0000264, mae: 0.0040352 test: loss0.0000782, mae:0.0064407
training loss 2.478749956935644e-05 mae 0.0040472871623933315
training loss 2.5174772631811835e-05 mae 0.003938192373835573
training loss 2.4149904621112064e-05 mae 0.003820024310878597
training loss 2.4587043847212963e-05 mae 0.00387958006082189
training loss 2.5541337375066928e-05 mae 0.003963043438317023
Epoch 154, training: loss: 0.0000256, mae: 0.0039720 test: loss0.0000794, mae:0.0064644
training loss 1.8354388885200024e-05 mae 0.0034449128434062004
training loss 2.5937038634426654e-05 mae 0.0039765483781914485
training loss 2.758392889914507e-05 mae 0.004097073567145972
training loss 2.7804171159065142e-05 mae 0.004121851088133768
training loss 2.752759560076919e-05 mae 0.004099197736686438
Epoch 155, training: loss: 0.0000277, mae: 0.0041144 test: loss0.0000789, mae:0.0064391
training loss 1.979402259166818e-05 mae 0.0037810252979397774
training loss 2.3299425069411637e-05 mae 0.0038049345762998454
training loss 2.4391681153706444e-05 mae 0.0038895169457439146
training loss 2.437216311145616e-05 mae 0.003893468143504778
training loss 2.481058928238799e-05 mae 0.003928166889441916
Epoch 156, training: loss: 0.0000250, mae: 0.0039416 test: loss0.0000811, mae:0.0065805
training loss 2.3568625692860223e-05 mae 0.004094232339411974
training loss 2.4398038637146416e-05 mae 0.0038581531695729377
training loss 2.365662412287721e-05 mae 0.003798688515227768
training loss 2.4607609347432596e-05 mae 0.0038767917473362576
training loss 2.483981881315725e-05 mae 0.0039000940863599095
Epoch 157, training: loss: 0.0000248, mae: 0.0038959 test: loss0.0000777, mae:0.0064039
training loss 2.132221379724797e-05 mae 0.0034717016387730837
training loss 2.2854226628231686e-05 mae 0.003731694127268651
training loss 2.418290977861898e-05 mae 0.003845556498514396
training loss 2.377003370414161e-05 mae 0.003831497546199044
training loss 2.4309375163935715e-05 mae 0.00386458061362121
Epoch 158, training: loss: 0.0000243, mae: 0.0038647 test: loss0.0000839, mae:0.0066787
training loss 2.319434133823961e-05 mae 0.003710726276040077
training loss 2.7152893844508e-05 mae 0.004078801456546666
training loss 2.5861307420038236e-05 mae 0.003983278602066606
training loss 2.6189719488648072e-05 mae 0.003990229266997894
training loss 2.6105922220109172e-05 mae 0.003995577880037734
Epoch 159, training: loss: 0.0000263, mae: 0.0040141 test: loss0.0000815, mae:0.0065750
training loss 2.6425608666613698e-05 mae 0.0038337267469614744
training loss 2.477623002050677e-05 mae 0.003889425589209972
training loss 2.4843944503469304e-05 mae 0.003923450970081706
training loss 2.439326747825188e-05 mae 0.0038864796638908186
training loss 2.457678618075486e-05 mae 0.0038966120357407418
Epoch 160, training: loss: 0.0000247, mae: 0.0039051 test: loss0.0000782, mae:0.0064043
training loss 3.6397479561856017e-05 mae 0.004661740269511938
training loss 2.1848796410160826e-05 mae 0.003687481309159422
training loss 2.3549464179331437e-05 mae 0.0037938718232187895
training loss 2.4114415360684446e-05 mae 0.0038339678947474586
training loss 2.433344537560367e-05 mae 0.003866600226008551
Epoch 161, training: loss: 0.0000245, mae: 0.0038793 test: loss0.0000802, mae:0.0064627
training loss 2.2401143723982386e-05 mae 0.0037825535982847214
training loss 2.364201993580483e-05 mae 0.0038224579172902837
training loss 2.402496514573929e-05 mae 0.0038522062546545925
training loss 2.3908312182387002e-05 mae 0.0038589520275050053
training loss 2.423049969745907e-05 mae 0.0038666322373261492
Epoch 162, training: loss: 0.0000242, mae: 0.0038648 test: loss0.0000782, mae:0.0064389
training loss 2.4530825612600893e-05 mae 0.003922570496797562
training loss 2.2009981769181596e-05 mae 0.0036703158334335855
training loss 2.3921324667537545e-05 mae 0.0038190223783771
training loss 2.3936858727494175e-05 mae 0.0038339840124873136
training loss 2.390614054771749e-05 mae 0.0038243983582644473
Epoch 163, training: loss: 0.0000239, mae: 0.0038273 test: loss0.0000800, mae:0.0064995
training loss 1.5946679923217744e-05 mae 0.00326547515578568
training loss 2.3000697254639242e-05 mae 0.003755396446578351
training loss 2.352063346743217e-05 mae 0.0037813042336613833
training loss 2.4028789536431528e-05 mae 0.003828961007445065
training loss 2.4212075502203957e-05 mae 0.0038549993372526937
Epoch 164, training: loss: 0.0000242, mae: 0.0038522 test: loss0.0000774, mae:0.0063880
training loss 2.6366096790297888e-05 mae 0.004268476739525795
training loss 2.376326750998166e-05 mae 0.003772957140908521
training loss 2.4286453571230505e-05 mae 0.0038206913061255565
training loss 2.453348690853801e-05 mae 0.0038520719806682193
training loss 2.4539789447586397e-05 mae 0.00385989305396467
Epoch 165, training: loss: 0.0000245, mae: 0.0038559 test: loss0.0000798, mae:0.0065066
training loss 1.578503542987164e-05 mae 0.003310250351205468
training loss 2.2690465491049347e-05 mae 0.0037250757610023597
training loss 2.3787381419739662e-05 mae 0.003826281639037304
training loss 2.3771173066100323e-05 mae 0.0038174691236892476
training loss 2.3662108635189306e-05 mae 0.0038056963526490904
Epoch 166, training: loss: 0.0000236, mae: 0.0037996 test: loss0.0000765, mae:0.0063085
training loss 2.210318052675575e-05 mae 0.0037336600944399834
training loss 2.1596565210190826e-05 mae 0.0036498512433586158
training loss 2.1524703319444987e-05 mae 0.00363405896719582
training loss 2.144167770917639e-05 mae 0.003646435265410814
training loss 2.2295730251873053e-05 mae 0.0037044286171891787
Epoch 167, training: loss: 0.0000223, mae: 0.0037011 test: loss0.0000794, mae:0.0064005
training loss 2.2618260118179023e-05 mae 0.003723457222804427
training loss 2.0665145390975178e-05 mae 0.003575822927386446
training loss 2.1633167469388205e-05 mae 0.0036483147679505367
training loss 2.196094427088744e-05 mae 0.0036876204250910817
training loss 2.226151733266669e-05 mae 0.003712941782400754
Epoch 168, training: loss: 0.0000223, mae: 0.0037214 test: loss0.0000774, mae:0.0063670
training loss 1.3229720025265124e-05 mae 0.002916758181527257
training loss 2.0370204872640045e-05 mae 0.0035196955854474915
training loss 2.1090721965814697e-05 mae 0.0035827040215861976
training loss 2.137451573562792e-05 mae 0.003616082173654971
training loss 2.127817543483407e-05 mae 0.003616618033062983
Epoch 169, training: loss: 0.0000213, mae: 0.0036181 test: loss0.0000771, mae:0.0063686
training loss 1.6385742128477432e-05 mae 0.0033746531698852777
training loss 2.046169650865897e-05 mae 0.003562235990173969
training loss 2.128642574804178e-05 mae 0.003614624252192455
training loss 2.1204808397079426e-05 mae 0.0036108817239076086
training loss 2.112644979115883e-05 mae 0.003608327441204765
Epoch 170, training: loss: 0.0000211, mae: 0.0036068 test: loss0.0000786, mae:0.0063839
training loss 1.8924012692878023e-05 mae 0.0034194467589259148
training loss 2.182245953008533e-05 mae 0.0036702272208297944
training loss 2.1758785184676244e-05 mae 0.0036604491979953364
training loss 2.1829586920836504e-05 mae 0.0036672190917248744
training loss 2.2151707331495023e-05 mae 0.0036869692310357266
Epoch 171, training: loss: 0.0000222, mae: 0.0036938 test: loss0.0000787, mae:0.0064488
training loss 1.9075525415246375e-05 mae 0.0035366166848689318
training loss 2.131534638108718e-05 mae 0.003620960238371409
training loss 2.139908349701078e-05 mae 0.003628517899865119
training loss 2.191743838101808e-05 mae 0.0036708379737934137
training loss 2.215565066522974e-05 mae 0.003689311196870027
Epoch 172, training: loss: 0.0000221, mae: 0.0036903 test: loss0.0000794, mae:0.0064150
training loss 2.9509566957131028e-05 mae 0.004271299112588167
training loss 2.20232660984609e-05 mae 0.003658305524903185
training loss 2.16287430928355e-05 mae 0.003628688437937952
training loss 2.1718225727321903e-05 mae 0.003640631034920941
training loss 2.1670599552685166e-05 mae 0.003643783648030377
Epoch 173, training: loss: 0.0000216, mae: 0.0036408 test: loss0.0000822, mae:0.0066352
training loss 2.871651122404728e-05 mae 0.004374957177788019
training loss 2.2103197789321852e-05 mae 0.003673124912322737
training loss 2.188407642738594e-05 mae 0.003638631097124059
training loss 2.2256610445886004e-05 mae 0.0036734127337791473
training loss 2.247869286664317e-05 mae 0.003701493155618954
Epoch 174, training: loss: 0.0000226, mae: 0.0037131 test: loss0.0000789, mae:0.0064720
training loss 3.035779263882432e-05 mae 0.004254119470715523
training loss 2.2421428879811976e-05 mae 0.0036837676649584482
training loss 2.2745396428476986e-05 mae 0.0037082389946842547
training loss 2.242718923093739e-05 mae 0.0037046979653297474
training loss 2.2211805211989138e-05 mae 0.0036924475669601366
Epoch 175, training: loss: 0.0000223, mae: 0.0036994 test: loss0.0000793, mae:0.0065166
training loss 2.8645017664530315e-05 mae 0.003946535289287567
training loss 2.2552716134169696e-05 mae 0.003685991078907369
training loss 2.260779294176407e-05 mae 0.00372586029323537
training loss 2.2234493998204672e-05 mae 0.003695354534818834
training loss 2.252670506027583e-05 mae 0.003713011876008107
Epoch 176, training: loss: 0.0000226, mae: 0.0037149 test: loss0.0000822, mae:0.0065608
training loss 2.776625296974089e-05 mae 0.004436877090483904
training loss 2.312859503624222e-05 mae 0.0037933923349734036
training loss 2.2978148342307316e-05 mae 0.003749455996134345
training loss 2.276237379999127e-05 mae 0.003726959469055892
training loss 2.295564796051749e-05 mae 0.0037438557691760917
Epoch 177, training: loss: 0.0000230, mae: 0.0037485 test: loss0.0000796, mae:0.0065111
training loss 1.4942852430976927e-05 mae 0.0031168062705546618
training loss 2.1260909396041106e-05 mae 0.0036516243308855625
training loss 2.10960937837095e-05 mae 0.0036170387935667947
training loss 2.046373124388186e-05 mae 0.0035655273614907702
training loss 2.0821159623167422e-05 mae 0.0035814955490015787
Epoch 178, training: loss: 0.0000209, mae: 0.0035919 test: loss0.0000798, mae:0.0064463
training loss 2.136059629265219e-05 mae 0.003881596028804779
training loss 1.9973631549196315e-05 mae 0.0035252623606984515
training loss 2.0821083432232317e-05 mae 0.0035993917752979413
training loss 2.128245960916231e-05 mae 0.0036199627296123298
training loss 2.146958064910375e-05 mae 0.0036289090063626813
Epoch 179, training: loss: 0.0000214, mae: 0.0036307 test: loss0.0000794, mae:0.0065165
training loss 1.631030136195477e-05 mae 0.003314628265798092
training loss 2.003825858421395e-05 mae 0.0035187498228076626
training loss 1.9176255456373046e-05 mae 0.0034394297416847535
training loss 1.9378093402217502e-05 mae 0.0034462709985180013
training loss 1.9745390231245295e-05 mae 0.0034743091392687602
Epoch 180, training: loss: 0.0000199, mae: 0.0034869 test: loss0.0000826, mae:0.0065971
training loss 1.4117237697064411e-05 mae 0.002823867602273822
training loss 2.390648011236871e-05 mae 0.003832858384532087
training loss 2.3693523412520796e-05 mae 0.003822326095369045
training loss 2.256950892541567e-05 mae 0.003731166100474877
training loss 2.1829117180622333e-05 mae 0.003665775989659539
Epoch 181, training: loss: 0.0000219, mae: 0.0036730 test: loss0.0000775, mae:0.0064123
training loss 2.8073793146177195e-05 mae 0.0042889304459095
training loss 1.8998027764039345e-05 mae 0.003437799177881257
training loss 1.8643835682098026e-05 mae 0.0034081039899535987
training loss 1.913563523915503e-05 mae 0.0034449168647460596
training loss 1.9210390723969225e-05 mae 0.0034436043699397094
Epoch 182, training: loss: 0.0000194, mae: 0.0034629 test: loss0.0000765, mae:0.0063247
training loss 1.2185494597360957e-05 mae 0.0028321563731878996
training loss 1.7514590296081687e-05 mae 0.003267109471683701
training loss 1.7933701678659857e-05 mae 0.0033148069022278684
training loss 1.8777604982340168e-05 mae 0.0033916578125455323
training loss 1.919494753108223e-05 mae 0.003422134870950559
Epoch 183, training: loss: 0.0000194, mae: 0.0034390 test: loss0.0000811, mae:0.0065233
training loss 2.0262314137653448e-05 mae 0.003576659830287099
training loss 1.9434443339716877e-05 mae 0.0034222180553364983
training loss 2.0011543341806293e-05 mae 0.0034797306167939214
training loss 2.149524784455923e-05 mae 0.003608438978896828
training loss 2.1571789075458607e-05 mae 0.0036209016095439146
Epoch 184, training: loss: 0.0000216, mae: 0.0036237 test: loss0.0000790, mae:0.0064691
training loss 1.7369400666211732e-05 mae 0.003178577870130539
training loss 2.0078472498782643e-05 mae 0.003502107134052351
training loss 1.989426782709433e-05 mae 0.003491337166748719
training loss 2.0017422654643646e-05 mae 0.003509477430041757
training loss 2.0171700040253786e-05 mae 0.0035190459118405386
Epoch 185, training: loss: 0.0000203, mae: 0.0035224 test: loss0.0000831, mae:0.0065493
training loss 2.0081693946849555e-05 mae 0.003741805674508214
training loss 2.2054877905140852e-05 mae 0.0036639592783781244
training loss 2.098236148926259e-05 mae 0.0035797392483800654
training loss 2.059921800705916e-05 mae 0.0035501903875897476
training loss 2.0158585384570643e-05 mae 0.003511980764419582
Epoch 186, training: loss: 0.0000202, mae: 0.0035150 test: loss0.0000777, mae:0.0063655
training loss 1.7072570699383505e-05 mae 0.003147629089653492
training loss 1.973710842838616e-05 mae 0.0034790438207268127
training loss 1.9102074862296444e-05 mae 0.0034301347090023573
training loss 1.916112356917702e-05 mae 0.0034376009990343985
training loss 1.931174019818855e-05 mae 0.003441495555615172
Epoch 187, training: loss: 0.0000192, mae: 0.0034318 test: loss0.0000769, mae:0.0063059
training loss 1.6154930563061498e-05 mae 0.00306783989071846
training loss 1.8299825747888854e-05 mae 0.003356164398913582
training loss 1.941342696515523e-05 mae 0.0034280394614296093
training loss 1.9673813689139284e-05 mae 0.003465975324826822
training loss 2.0099210498066255e-05 mae 0.003497099509666587
Epoch 188, training: loss: 0.0000200, mae: 0.0034920 test: loss0.0000773, mae:0.0063506
training loss 2.2406848074751906e-05 mae 0.0035812659189105034
training loss 1.9609539117036368e-05 mae 0.0034512248867209637
training loss 1.896548912716951e-05 mae 0.00340428239306306
training loss 1.9474929688280204e-05 mae 0.0034490199420379114
training loss 1.942412415986322e-05 mae 0.00345872516224893
Epoch 189, training: loss: 0.0000194, mae: 0.0034555 test: loss0.0000761, mae:0.0062781
training loss 2.0429512005648576e-05 mae 0.00340382126159966
training loss 1.7511002713981013e-05 mae 0.003260260813084303
training loss 1.8000965565529486e-05 mae 0.0033002753654439556
training loss 1.7920433763732253e-05 mae 0.003300866987926282
training loss 1.7830268278437372e-05 mae 0.00329802931976778
Epoch 190, training: loss: 0.0000178, mae: 0.0032996 test: loss0.0000764, mae:0.0063025
training loss 1.728716779325623e-05 mae 0.0030515699181705713
training loss 1.8490758592116773e-05 mae 0.0033205520252094552
training loss 1.8534650527978757e-05 mae 0.0033373765297012763
training loss 1.922640877932739e-05 mae 0.0034034286459155437
training loss 1.9271981940465465e-05 mae 0.003414358223199994
Epoch 191, training: loss: 0.0000193, mae: 0.0034160 test: loss0.0000781, mae:0.0064209
training loss 1.9150606021867134e-05 mae 0.003582499222829938
training loss 1.6684545449143295e-05 mae 0.0031817983835935593
training loss 1.7925444574370212e-05 mae 0.0033148637267364426
training loss 1.8016784975915417e-05 mae 0.003321040579000648
training loss 1.7985347672513298e-05 mae 0.0033211054857264248
Epoch 192, training: loss: 0.0000181, mae: 0.0033236 test: loss0.0000775, mae:0.0063409
training loss 7.7193726610858e-06 mae 0.0021575698629021645
training loss 1.8984164639465127e-05 mae 0.0033877035465967992
training loss 1.7794188514215268e-05 mae 0.00329504722019307
training loss 1.8007752235926034e-05 mae 0.0033172495502805867
training loss 1.8324066865775127e-05 mae 0.0033636361520403813
Epoch 193, training: loss: 0.0000184, mae: 0.0033688 test: loss0.0000788, mae:0.0064203
training loss 1.6396304999943823e-05 mae 0.0032411664724349976
training loss 1.644328453859963e-05 mae 0.003156179758519227
training loss 2.627525480258356e-05 mae 0.0038088857989278762
training loss 2.7782217637421434e-05 mae 0.003946280281139624
training loss 2.652424865204491e-05 mae 0.0038920741085778573
Epoch 194, training: loss: 0.0000263, mae: 0.0038825 test: loss0.0000784, mae:0.0064305
training loss 1.9200992028345354e-05 mae 0.0033795644994825125
training loss 1.749444896524758e-05 mae 0.0032158351877667737
training loss 1.8469198129432605e-05 mae 0.003336184510573893
training loss 1.8377749804721137e-05 mae 0.003343758443123754
training loss 1.876576601743955e-05 mae 0.0033809560656881156
Epoch 195, training: loss: 0.0000188, mae: 0.0033797 test: loss0.0000860, mae:0.0067150
training loss 3.582179851946421e-05 mae 0.004473559092730284
training loss 1.9135715828109392e-05 mae 0.003416740390307763
training loss 1.8216691954660182e-05 mae 0.003343287348230876
training loss 1.7954589819187927e-05 mae 0.003311906318297448
training loss 1.7758274107697418e-05 mae 0.0032947946685037343
Epoch 196, training: loss: 0.0000178, mae: 0.0032932 test: loss0.0000792, mae:0.0064701
training loss 1.871878521342296e-05 mae 0.003439647378399968
training loss 1.6578227497994502e-05 mae 0.0031911193520999417
training loss 1.7203840663101026e-05 mae 0.0032488802438694056
training loss 2.0120788444096097e-05 mae 0.0034395832142835804
training loss 2.052126590352776e-05 mae 0.003489609625866978
Epoch 197, training: loss: 0.0000206, mae: 0.0034941 test: loss0.0000820, mae:0.0065527
training loss 2.1642559659085236e-05 mae 0.003755829529836774
training loss 1.827847395093961e-05 mae 0.0033306324279264495
training loss 1.947470542236801e-05 mae 0.003409974604365554
training loss 1.931524064468178e-05 mae 0.0034101598893002375
training loss 1.9085116448637873e-05 mae 0.003394245036845259
Epoch 198, training: loss: 0.0000190, mae: 0.0033871 test: loss0.0000771, mae:0.0063054
training loss 2.6748610252980143e-05 mae 0.004020638298243284
training loss 1.6093526384935883e-05 mae 0.003087939890832001
training loss 1.667454420169066e-05 mae 0.003174267453372037
training loss 1.671445311300784e-05 mae 0.003189146944278519
training loss 1.679344261028565e-05 mae 0.0031909821809857943
Epoch 199, training: loss: 0.0000168, mae: 0.0031903 test: loss0.0000783, mae:0.0064108
current learning rate: 0.000125
training loss 1.0949566785711795e-05 mae 0.0027376862708479166
training loss 1.5180182986916732e-05 mae 0.0030120876252505134
training loss 1.4340301799400027e-05 mae 0.0029262684119662438
training loss 1.3909512937898887e-05 mae 0.0028852656921556836
training loss 1.3598602789042804e-05 mae 0.002857878342020644
Epoch 200, training: loss: 0.0000135, mae: 0.0028528 test: loss0.0000749, mae:0.0062465
training loss 1.7938673408934847e-05 mae 0.003282718127593398
training loss 1.2452670649936944e-05 mae 0.002719946401924187
training loss 1.222910526452648e-05 mae 0.0026916806415497956
training loss 1.2600368143539047e-05 mae 0.00273577631173643
training loss 1.256579728882869e-05 mae 0.0027370479389960257
Epoch 201, training: loss: 0.0000125, mae: 0.0027257 test: loss0.0000751, mae:0.0062525
training loss 1.2467713531805202e-05 mae 0.0027799559757113457
training loss 1.0832835512558755e-05 mae 0.0025426784973116776
training loss 1.1850154571742909e-05 mae 0.002644773490846821
training loss 1.185953937575378e-05 mae 0.002650671684035157
training loss 1.2056995952906305e-05 mae 0.0026711376461510863
Epoch 202, training: loss: 0.0000121, mae: 0.0026731 test: loss0.0000749, mae:0.0062338
training loss 1.157826409325935e-05 mae 0.0022059802431613207
training loss 1.182775226612541e-05 mae 0.002661005854058791
training loss 1.1710690592051399e-05 mae 0.0026386448730816043
training loss 1.1939674981031143e-05 mae 0.0026648336318352353
training loss 1.2125873090734792e-05 mae 0.002685535296019333
Epoch 203, training: loss: 0.0000121, mae: 0.0026840 test: loss0.0000748, mae:0.0062252
training loss 9.126014447247144e-06 mae 0.0024385256692767143
training loss 1.1734291065830109e-05 mae 0.002614011992609092
training loss 1.173591264959403e-05 mae 0.0026316133911011394
training loss 1.1675327108521094e-05 mae 0.0026260617101219615
training loss 1.198725705753577e-05 mae 0.002660899260313024
Epoch 204, training: loss: 0.0000121, mae: 0.0026702 test: loss0.0000757, mae:0.0062538
training loss 1.1345092389092315e-05 mae 0.002793170278891921
training loss 1.2136043863517675e-05 mae 0.0026998476116169316
training loss 1.1912577146275823e-05 mae 0.002680933995098612
training loss 1.217724192977265e-05 mae 0.002698729516296887
training loss 1.2271460726612047e-05 mae 0.002704338650149639
Epoch 205, training: loss: 0.0000123, mae: 0.0027071 test: loss0.0000768, mae:0.0063435
training loss 1.3983903954795096e-05 mae 0.0028962772339582443
training loss 1.1565917702682779e-05 mae 0.0026244823980674735
training loss 1.1622130796690776e-05 mae 0.002626651820800462
training loss 1.1916403529286851e-05 mae 0.002652266284062325
training loss 1.2193503132595596e-05 mae 0.0026909727910162883
Epoch 206, training: loss: 0.0000122, mae: 0.0026933 test: loss0.0000771, mae:0.0063565
training loss 9.550222785037477e-06 mae 0.0024825374130159616
training loss 1.1649644224520421e-05 mae 0.0026322515480512497
training loss 1.2030014920245517e-05 mae 0.0026775770255890066
training loss 1.1952813866742717e-05 mae 0.002656755097740829
training loss 1.2177298767303219e-05 mae 0.002690947855549367
Epoch 207, training: loss: 0.0000122, mae: 0.0026913 test: loss0.0000754, mae:0.0062852
training loss 1.4404354260477703e-05 mae 0.0029865142423659563
training loss 1.1341731529754212e-05 mae 0.0026052692681368364
training loss 1.1294082000993981e-05 mae 0.0026031171218353763
training loss 1.1708536120997018e-05 mae 0.002639894939601322
training loss 1.1737005850555027e-05 mae 0.0026440266809375276
Epoch 208, training: loss: 0.0000118, mae: 0.0026539 test: loss0.0000772, mae:0.0063318
training loss 9.796619451662991e-06 mae 0.0024149457458406687
training loss 1.2172226121822626e-05 mae 0.0027124266797567105
training loss 1.1654337440361788e-05 mae 0.002649990150170161
training loss 1.1744693784562248e-05 mae 0.0026604698135190654
training loss 1.1838277499629472e-05 mae 0.002660338949893988
Epoch 209, training: loss: 0.0000119, mae: 0.0026638 test: loss0.0000775, mae:0.0063818
training loss 1.2496949238993693e-05 mae 0.0027138746809214354
training loss 1.1660280622391822e-05 mae 0.0026120361232874435
training loss 1.1849727280326145e-05 mae 0.0026465020220874254
training loss 1.220980496911929e-05 mae 0.0026867750196390783
training loss 1.2117025822653577e-05 mae 0.0026816309879149364
Epoch 210, training: loss: 0.0000121, mae: 0.0026785 test: loss0.0000762, mae:0.0062914
training loss 1.3934909475210588e-05 mae 0.002841401845216751
training loss 1.0629997525350464e-05 mae 0.0025079417334613837
training loss 1.0814203857206498e-05 mae 0.0025435091459681046
training loss 1.1263027968408816e-05 mae 0.0026037608803749475
training loss 1.1762002753614375e-05 mae 0.002652761613282916
Epoch 211, training: loss: 0.0000117, mae: 0.0026477 test: loss0.0000770, mae:0.0062918
training loss 1.0875833140744362e-05 mae 0.0024258894845843315
training loss 1.1974225148436495e-05 mae 0.0026481306162096712
training loss 1.2062482638422879e-05 mae 0.002674954984853469
training loss 1.194008638282825e-05 mae 0.0026635424371204332
training loss 1.1811220134060834e-05 mae 0.002649914305911989
Epoch 212, training: loss: 0.0000118, mae: 0.0026470 test: loss0.0000771, mae:0.0063407
training loss 7.828944944776595e-06 mae 0.0020310773979872465
training loss 1.1936146923593089e-05 mae 0.0026167754229961663
training loss 1.16497613808041e-05 mae 0.002611187307422263
training loss 1.1687929624249648e-05 mae 0.00262846566415978
training loss 1.1836485098240187e-05 mae 0.0026499713650575637
Epoch 213, training: loss: 0.0000118, mae: 0.0026441 test: loss0.0000767, mae:0.0063101
training loss 8.13844508229522e-06 mae 0.002215034095570445
training loss 1.1121289907011895e-05 mae 0.0025783137683117515
training loss 1.1086666115198289e-05 mae 0.002567902924308535
training loss 1.125683746260548e-05 mae 0.002591019659765707
training loss 1.1470501312625628e-05 mae 0.0026088222196390537
Epoch 214, training: loss: 0.0000115, mae: 0.0026080 test: loss0.0000785, mae:0.0063759
training loss 1.1627903404587414e-05 mae 0.0024906108155846596
training loss 1.0667300343544305e-05 mae 0.0025210714392254452
training loss 1.0777466651225129e-05 mae 0.0025207315811352562
training loss 1.0970652462692549e-05 mae 0.0025459684706015495
training loss 1.1074289237576206e-05 mae 0.002560385070806044
Epoch 215, training: loss: 0.0000112, mae: 0.0025785 test: loss0.0000772, mae:0.0063249
training loss 1.5240955690387636e-05 mae 0.002953769639134407
training loss 1.1019219804367432e-05 mae 0.0025571031568973673
training loss 1.1589246791082038e-05 mae 0.0026171399732258654
training loss 1.1518194419626745e-05 mae 0.0026130691519167457
training loss 1.1631417685327795e-05 mae 0.0026324430969427927
Epoch 216, training: loss: 0.0000116, mae: 0.0026266 test: loss0.0000772, mae:0.0063296
training loss 1.375381998514058e-05 mae 0.002988644642755389
training loss 1.139757366628128e-05 mae 0.0026246311859793813
training loss 1.1410791958582244e-05 mae 0.002624070791684385
training loss 1.1120266187227747e-05 mae 0.002588665331035407
training loss 1.1457545818515277e-05 mae 0.0026199702081960894
Epoch 217, training: loss: 0.0000114, mae: 0.0026123 test: loss0.0000785, mae:0.0063938
training loss 1.0624507012835238e-05 mae 0.0026467759162187576
training loss 1.1007726751653105e-05 mae 0.002555491900363681
training loss 1.1279973599506191e-05 mae 0.0025823791972407603
training loss 1.1210247479213989e-05 mae 0.0025820944324098784
training loss 1.1325853978886334e-05 mae 0.002591318174598934
Epoch 218, training: loss: 0.0000113, mae: 0.0025954 test: loss0.0000800, mae:0.0064527
training loss 1.0203366400673985e-05 mae 0.0024593009147793055
training loss 1.1207346135194588e-05 mae 0.0025621057173018068
training loss 1.0873720465596227e-05 mae 0.0025310616255599517
training loss 1.0689596503941978e-05 mae 0.002518481167884456
training loss 1.0850851649321606e-05 mae 0.0025304411526361633
Epoch 219, training: loss: 0.0000109, mae: 0.0025366 test: loss0.0000787, mae:0.0064050
training loss 1.1830846233351622e-05 mae 0.002618789440020919
training loss 1.0515334469899099e-05 mae 0.0024945676650888495
training loss 1.0456428375763072e-05 mae 0.0024844753696573996
training loss 1.0683204002979254e-05 mae 0.0025252610397222914
training loss 1.0881150682512237e-05 mae 0.0025492816641046415
Epoch 220, training: loss: 0.0000109, mae: 0.0025541 test: loss0.0000781, mae:0.0063987
training loss 1.4905122952768579e-05 mae 0.0028249688912183046
training loss 1.0696293670394774e-05 mae 0.002477993015382512
training loss 1.0936092148947882e-05 mae 0.002524909627909708
training loss 1.0793108685902558e-05 mae 0.0025287070242219316
training loss 1.088432535521608e-05 mae 0.002544604078519034
Epoch 221, training: loss: 0.0000109, mae: 0.0025487 test: loss0.0000777, mae:0.0063437
training loss 5.513331871043192e-06 mae 0.001912876614369452
training loss 1.0797001091394158e-05 mae 0.0025285654998037452
training loss 1.1131918947087049e-05 mae 0.002571395279074291
training loss 1.115664449962845e-05 mae 0.002574828969906793
training loss 1.1056203795161123e-05 mae 0.0025747988792367985
Epoch 222, training: loss: 0.0000111, mae: 0.0025815 test: loss0.0000788, mae:0.0063834
training loss 8.758626790950075e-06 mae 0.002398117445409298
training loss 1.0423396949644323e-05 mae 0.002460914332985732
training loss 1.0841821478769106e-05 mae 0.002526984136265766
training loss 1.0774267476209325e-05 mae 0.0025248572258093696
training loss 1.0922596258787783e-05 mae 0.002531615527235528
Epoch 223, training: loss: 0.0000110, mae: 0.0025379 test: loss0.0000784, mae:0.0063764
training loss 1.433445777365705e-05 mae 0.002863098168745637
training loss 1.0883232013892506e-05 mae 0.002528189857234703
training loss 1.0850077065733428e-05 mae 0.0025414656720926412
training loss 1.098889002388741e-05 mae 0.0025568072574632555
training loss 1.1145280262809224e-05 mae 0.002571502476884285
Epoch 224, training: loss: 0.0000111, mae: 0.0025713 test: loss0.0000785, mae:0.0063804
training loss 1.143482222687453e-05 mae 0.0026896269991993904
training loss 9.540014839420146e-06 mae 0.002408340080257724
training loss 9.893066263511271e-06 mae 0.002431347363370259
training loss 1.0402901343755937e-05 mae 0.0024877096224107517
training loss 1.0621485863742045e-05 mae 0.0025206031752703956
Epoch 225, training: loss: 0.0000107, mae: 0.0025308 test: loss0.0000788, mae:0.0063921
training loss 8.2029691839125e-06 mae 0.0022069241385906935
training loss 1.0213993244392709e-05 mae 0.0024556127541205458
training loss 1.0486544575127987e-05 mae 0.002464978221462726
training loss 1.054390722645814e-05 mae 0.0024890110794060076
training loss 1.071169693394909e-05 mae 0.0025152853957669283
Epoch 226, training: loss: 0.0000107, mae: 0.0025159 test: loss0.0000783, mae:0.0063751
training loss 7.092294708854752e-06 mae 0.0021462347358465195
training loss 1.002378897229552e-05 mae 0.002432666098078092
training loss 1.0080816825240332e-05 mae 0.0024437423470420852
training loss 1.0196455602171e-05 mae 0.0024581417913197103
training loss 1.0376861639395714e-05 mae 0.002480285495881049
Epoch 227, training: loss: 0.0000104, mae: 0.0024802 test: loss0.0000777, mae:0.0063562
training loss 1.1178662134625483e-05 mae 0.002515817293897271
training loss 9.364870314967969e-06 mae 0.0023536672359149823
training loss 9.934935977719648e-06 mae 0.0024202033084365403
training loss 1.0499976570941578e-05 mae 0.002493263713380211
training loss 1.0634939016611265e-05 mae 0.002515034222470328
Epoch 228, training: loss: 0.0000106, mae: 0.0025106 test: loss0.0000832, mae:0.0065180
training loss 6.7886844590248074e-06 mae 0.0018106106435880065
training loss 1.044172776185887e-05 mae 0.002484943555649735
training loss 1.0439764321178007e-05 mae 0.002475789999404904
training loss 1.0414508875292328e-05 mae 0.0024784676837025592
training loss 1.0394959034200122e-05 mae 0.00247514345544154
Epoch 229, training: loss: 0.0000104, mae: 0.0024769 test: loss0.0000783, mae:0.0063621
training loss 9.482404493610375e-06 mae 0.002335289726033807
training loss 1.0379825737925107e-05 mae 0.0024950285465000018
training loss 1.0371709693950689e-05 mae 0.002484869639536752
training loss 1.026805377315388e-05 mae 0.00245842391575705
training loss 1.0273188294223332e-05 mae 0.0024619940378754115
Epoch 230, training: loss: 0.0000102, mae: 0.0024601 test: loss0.0000788, mae:0.0063961
training loss 1.392570175084984e-05 mae 0.002804504008963704
training loss 1.0765562641537659e-05 mae 0.0025124453172525943
training loss 1.0274413925078919e-05 mae 0.0024574307210761875
training loss 1.010572916900567e-05 mae 0.002440258868297698
training loss 1.0275856246026431e-05 mae 0.0024691875462202503
Epoch 231, training: loss: 0.0000103, mae: 0.0024725 test: loss0.0000788, mae:0.0064238
training loss 1.4199918041413184e-05 mae 0.0029445576947182417
training loss 1.0616140722656075e-05 mae 0.002512095509754384
training loss 1.0213782397750937e-05 mae 0.002457653903079653
training loss 1.0311577568323708e-05 mae 0.002470942966590653
training loss 1.0138341160688752e-05 mae 0.0024504501729351053
Epoch 232, training: loss: 0.0000102, mae: 0.0024554 test: loss0.0000795, mae:0.0064369
training loss 8.572817023377866e-06 mae 0.002404402010142803
training loss 1.0666964106919705e-05 mae 0.0025241932781486245
training loss 1.0639605025602629e-05 mae 0.0025077015622565063
training loss 1.0465018178534077e-05 mae 0.0024816537035370102
training loss 1.0313058769339358e-05 mae 0.002466982972474573
Epoch 233, training: loss: 0.0000104, mae: 0.0024686 test: loss0.0000783, mae:0.0063673
training loss 5.558892098633805e-06 mae 0.0018632455030456185
training loss 8.875855655787749e-06 mae 0.002295749147842621
training loss 9.454570889568342e-06 mae 0.0023579488019689486
training loss 9.618156519048693e-06 mae 0.002378386614955222
training loss 9.858472546072706e-06 mae 0.002412959628631895
Epoch 234, training: loss: 0.0000098, mae: 0.0024116 test: loss0.0000786, mae:0.0064005
training loss 1.8077927961712703e-05 mae 0.0032413583248853683
training loss 9.305626168925836e-06 mae 0.0023422279605167166
training loss 9.595072215503961e-06 mae 0.0023736504487472
training loss 9.619180712476335e-06 mae 0.0023764477447538786
training loss 9.684977983815954e-06 mae 0.0023945646716133514
Epoch 235, training: loss: 0.0000097, mae: 0.0024012 test: loss0.0000795, mae:0.0063940
training loss 8.835871085466351e-06 mae 0.002427224302664399
training loss 9.910395803279808e-06 mae 0.002421238206272178
training loss 9.859902237090718e-06 mae 0.0024229473247663062
training loss 9.752560128244479e-06 mae 0.002412663996281312
training loss 1.0022540170858923e-05 mae 0.002441804544462716
Epoch 236, training: loss: 0.0000101, mae: 0.0024500 test: loss0.0000785, mae:0.0063654
training loss 7.999632543942425e-06 mae 0.002305743284523487
training loss 1.0013201143224517e-05 mae 0.0024321476915193825
training loss 9.949603295218809e-06 mae 0.002422535194932381
training loss 9.660827616256252e-06 mae 0.002389108144604162
training loss 9.617604935293864e-06 mae 0.0023793100885611323
Epoch 237, training: loss: 0.0000096, mae: 0.0023801 test: loss0.0000802, mae:0.0064074
training loss 7.0070541369204875e-06 mae 0.0021169877145439386
training loss 9.166222619515953e-06 mae 0.0022988890379886415
training loss 9.06917038453612e-06 mae 0.002308880586389194
training loss 9.431469399281172e-06 mae 0.0023536378886266944
training loss 9.552332446232799e-06 mae 0.0023803262970183256
Epoch 238, training: loss: 0.0000096, mae: 0.0023889 test: loss0.0000801, mae:0.0064369
training loss 1.5301355233532377e-05 mae 0.002970937406644225
training loss 9.764753958377549e-06 mae 0.0024158367476261717
training loss 9.515859639379575e-06 mae 0.0023850666734823354
training loss 9.491609411186151e-06 mae 0.0023826779581803755
training loss 9.662333511426987e-06 mae 0.0023998832052100935
Epoch 239, training: loss: 0.0000097, mae: 0.0023995 test: loss0.0000818, mae:0.0064710
training loss 1.042632447934011e-05 mae 0.002667643828317523
training loss 9.044626539452668e-06 mae 0.0023222463068497527
training loss 9.19527851557619e-06 mae 0.0023293811343794704
training loss 9.413564746274454e-06 mae 0.0023522817950683414
training loss 9.480094944936454e-06 mae 0.0023631335343859755
Epoch 240, training: loss: 0.0000095, mae: 0.0023634 test: loss0.0000792, mae:0.0064154
training loss 5.582338872045511e-06 mae 0.0016964497044682503
training loss 9.365977401852005e-06 mae 0.0023667916367926135
training loss 9.35701536315656e-06 mae 0.0023539877279965885
training loss 9.447751860070699e-06 mae 0.002371240377931899
training loss 9.61868326795859e-06 mae 0.0023952533452729903
Epoch 241, training: loss: 0.0000096, mae: 0.0023936 test: loss0.0000814, mae:0.0064535
training loss 1.1045797691622283e-05 mae 0.0026449905708432198
training loss 8.97183682515307e-06 mae 0.0022954139424780124
training loss 9.204617360257258e-06 mae 0.0023308334812189983
training loss 9.236568063674861e-06 mae 0.0023354843385295556
training loss 9.304452748949357e-06 mae 0.0023461631881490134
Epoch 242, training: loss: 0.0000093, mae: 0.0023475 test: loss0.0000795, mae:0.0064299
training loss 7.811185241735075e-06 mae 0.0020831322763115168
training loss 1.0419231811658178e-05 mae 0.0025073485197864604
training loss 9.81904194493764e-06 mae 0.002436324321240703
training loss 9.723904910566752e-06 mae 0.0024223613432479024
training loss 9.94318229323422e-06 mae 0.0024411623216748467
Epoch 243, training: loss: 0.0000099, mae: 0.0024416 test: loss0.0000795, mae:0.0064548
training loss 8.042571607802529e-06 mae 0.0020916128996759653
training loss 8.87104981445066e-06 mae 0.0022988602742735377
training loss 9.203444152614013e-06 mae 0.002320637577213347
training loss 9.340671706302493e-06 mae 0.002347952660037489
training loss 9.470960774262895e-06 mae 0.0023613881038279444
Epoch 244, training: loss: 0.0000095, mae: 0.0023608 test: loss0.0000813, mae:0.0064634
training loss 1.0193812158831861e-05 mae 0.0023590978235006332
training loss 9.006047985523905e-06 mae 0.0023049304225281173
training loss 9.068851239693561e-06 mae 0.0023165958343387243
training loss 9.156603631624684e-06 mae 0.0023262650445288703
training loss 9.161558876877822e-06 mae 0.002328200309782926
Epoch 245, training: loss: 0.0000092, mae: 0.0023311 test: loss0.0000787, mae:0.0063780
training loss 8.277276720036753e-06 mae 0.0021350218448787928
training loss 9.14245903878313e-06 mae 0.0023399992028762615
training loss 9.083638094962947e-06 mae 0.0023164328805512146
training loss 9.175212289244658e-06 mae 0.0023209859990200262
training loss 9.371035564221697e-06 mae 0.0023562764750089285
Epoch 246, training: loss: 0.0000093, mae: 0.0023542 test: loss0.0000799, mae:0.0064503
training loss 7.3476985562592745e-06 mae 0.002142631448805332
training loss 9.346302668549759e-06 mae 0.002380360515934287
training loss 9.421721998559505e-06 mae 0.0023853771519930208
training loss 9.380604692915278e-06 mae 0.0023724311031401153
training loss 9.609416699064394e-06 mae 0.0023960600020966265
Epoch 247, training: loss: 0.0000096, mae: 0.0023959 test: loss0.0000782, mae:0.0063717
training loss 1.1723823263309896e-05 mae 0.002654202049598098
training loss 9.320122417728482e-06 mae 0.0023371460916036184
training loss 8.932120415398224e-06 mae 0.0022953587588808028
training loss 9.0015331361368e-06 mae 0.002305734998990615
training loss 9.109163876919452e-06 mae 0.002323136469743108
Epoch 248, training: loss: 0.0000092, mae: 0.0023368 test: loss0.0000806, mae:0.0064893
training loss 8.528330909030046e-06 mae 0.0024110497906804085
training loss 9.532441646454159e-06 mae 0.002367272973060608
training loss 9.230001384398431e-06 mae 0.002344254742026109
training loss 9.127220443244841e-06 mae 0.0023363368787697905
training loss 9.192633997508218e-06 mae 0.0023419594534658905
Epoch 249, training: loss: 0.0000092, mae: 0.0023401 test: loss0.0000799, mae:0.0064607
training loss 7.368229034909746e-06 mae 0.0019425874343141913
training loss 8.790792791984915e-06 mae 0.002279151178549464
training loss 8.324416735421892e-06 mae 0.0022179554562351783
training loss 8.581775964798816e-06 mae 0.0022546569492963965
training loss 8.77128635086895e-06 mae 0.002278231990764576
Epoch 250, training: loss: 0.0000088, mae: 0.0022816 test: loss0.0000789, mae:0.0064300
training loss 6.391495389834745e-06 mae 0.0021309664007276297
training loss 8.302601341425793e-06 mae 0.0022051583418586087
training loss 8.514412497383507e-06 mae 0.002243934749017036
training loss 8.62026596127216e-06 mae 0.0022527735413588735
training loss 8.763205117143635e-06 mae 0.002274103971795916
Epoch 251, training: loss: 0.0000088, mae: 0.0022771 test: loss0.0000811, mae:0.0065039
training loss 8.350486496055964e-06 mae 0.0020839280914515257
training loss 8.702726518095007e-06 mae 0.0022676537799484592
training loss 8.722970717529525e-06 mae 0.002284294086517674
training loss 8.98061043639041e-06 mae 0.0023131745455737237
training loss 9.041214536808012e-06 mae 0.002315655291029507
Epoch 252, training: loss: 0.0000091, mae: 0.0023250 test: loss0.0000798, mae:0.0064389
training loss 9.556105396768544e-06 mae 0.0022068852558732033
training loss 9.387038936242168e-06 mae 0.002368965489771582
training loss 9.070316856637102e-06 mae 0.002334081436424415
training loss 9.069206759180678e-06 mae 0.0023350177166231024
training loss 9.2850857934319e-06 mae 0.002354727499071156
Epoch 253, training: loss: 0.0000093, mae: 0.0023575 test: loss0.0000796, mae:0.0064406
training loss 9.004477760754526e-06 mae 0.002361177233979106
training loss 9.195278318086411e-06 mae 0.002354224134857456
training loss 9.18003322319894e-06 mae 0.0023488767023225957
training loss 8.953556872009789e-06 mae 0.0023105239452461132
training loss 8.834735325677205e-06 mae 0.002298499933624667
Epoch 254, training: loss: 0.0000088, mae: 0.0023017 test: loss0.0000811, mae:0.0064824
training loss 7.245653705467703e-06 mae 0.002103501232340932
training loss 8.604198210734609e-06 mae 0.002255665462500617
training loss 8.719889155874604e-06 mae 0.0022644852633855435
training loss 8.447975805337658e-06 mae 0.0022395490560779318
training loss 8.706016490745312e-06 mae 0.002264235864870658
Epoch 255, training: loss: 0.0000087, mae: 0.0022668 test: loss0.0000812, mae:0.0065013
training loss 7.279055807885015e-06 mae 0.0020796179305762053
training loss 8.66973519520138e-06 mae 0.0022757573500640834
training loss 8.465804859213788e-06 mae 0.0022664244302852752
training loss 8.674942533098572e-06 mae 0.002278493973272328
training loss 8.66010313486856e-06 mae 0.0022753852173982565
Epoch 256, training: loss: 0.0000087, mae: 0.0022778 test: loss0.0000799, mae:0.0064453
training loss 9.243412023351993e-06 mae 0.002135438611730933
training loss 8.10404097529838e-06 mae 0.0021621765975164717
training loss 8.304159150206448e-06 mae 0.002199981629037031
training loss 8.276019057512228e-06 mae 0.0022100535589917904
training loss 8.336939576997905e-06 mae 0.0022171122637063036
Epoch 257, training: loss: 0.0000084, mae: 0.0022182 test: loss0.0000801, mae:0.0064483
training loss 5.28265945831663e-06 mae 0.0017904993146657944
training loss 8.118685159655903e-06 mae 0.002192244447274682
training loss 8.350620831124885e-06 mae 0.0022154371715196876
training loss 8.20772965686432e-06 mae 0.0022091153748518494
training loss 8.507771876318308e-06 mae 0.0022448627052328264
Epoch 258, training: loss: 0.0000085, mae: 0.0022482 test: loss0.0000791, mae:0.0064018
training loss 8.321610948769376e-06 mae 0.0022174043115228415
training loss 8.361912611743557e-06 mae 0.002221019641843204
training loss 8.470917484625918e-06 mae 0.0022289188161473917
training loss 8.404628140442275e-06 mae 0.0022315719189578737
training loss 8.485162842548759e-06 mae 0.002240694537457653
Epoch 259, training: loss: 0.0000085, mae: 0.0022437 test: loss0.0000804, mae:0.0064605
training loss 6.121822025306756e-06 mae 0.0019689283799380064
training loss 8.82594418262848e-06 mae 0.002287319140033979
training loss 8.562919492310309e-06 mae 0.0022585020633414383
training loss 8.518124996164673e-06 mae 0.0022434616062037695
training loss 8.51261039265659e-06 mae 0.002248308310906093
Epoch 260, training: loss: 0.0000085, mae: 0.0022505 test: loss0.0000801, mae:0.0064611
training loss 8.128227818815503e-06 mae 0.0023428741842508316
training loss 8.119602269146653e-06 mae 0.0022033631650950104
training loss 8.41681926070997e-06 mae 0.002229224791344586
training loss 8.370509375775195e-06 mae 0.002220835321619859
training loss 8.241789792190511e-06 mae 0.0022099290404524363
Epoch 261, training: loss: 0.0000082, mae: 0.0022073 test: loss0.0000809, mae:0.0064799
training loss 6.525317530758912e-06 mae 0.002014550380408764
training loss 8.08776361487153e-06 mae 0.002171393063868963
training loss 8.216171863750793e-06 mae 0.0021843360114547593
training loss 8.39339236637103e-06 mae 0.0022104403161326593
training loss 8.427044726139792e-06 mae 0.0022159118757847317
Epoch 262, training: loss: 0.0000084, mae: 0.0022164 test: loss0.0000805, mae:0.0064659
training loss 4.408314907777822e-06 mae 0.001577115966938436
training loss 8.497736265174652e-06 mae 0.0022291298179576784
training loss 8.445896056211567e-06 mae 0.002235174385397372
training loss 8.74383616347821e-06 mae 0.0022719863074406096
training loss 8.585190973680075e-06 mae 0.0022508601706697426
Epoch 263, training: loss: 0.0000085, mae: 0.0022463 test: loss0.0000801, mae:0.0064669
training loss 8.787520528130699e-06 mae 0.0022340111900120974
training loss 8.299850503367362e-06 mae 0.0022191534060802248
training loss 8.12585267340182e-06 mae 0.0022039003278322444
training loss 8.242180120573445e-06 mae 0.0022178664086509904
training loss 8.239657659462528e-06 mae 0.0022168555152162297
Epoch 264, training: loss: 0.0000083, mae: 0.0022177 test: loss0.0000809, mae:0.0064760
training loss 1.1121202078356873e-05 mae 0.002332861302420497
training loss 8.757969966359717e-06 mae 0.002262247342835455
training loss 8.28905616223289e-06 mae 0.002206888477465525
training loss 8.390054195565766e-06 mae 0.002214414253368333
training loss 8.37592629588598e-06 mae 0.0022146883223724744
Epoch 265, training: loss: 0.0000084, mae: 0.0022228 test: loss0.0000798, mae:0.0064308
training loss 6.449497504945612e-06 mae 0.001819566241465509
training loss 7.319304243127608e-06 mae 0.0020924209247288462
training loss 7.745330812762373e-06 mae 0.002143177073587212
training loss 7.97215431329645e-06 mae 0.0021728591651143814
training loss 8.158014162829247e-06 mae 0.002192430955760958
Epoch 266, training: loss: 0.0000082, mae: 0.0021993 test: loss0.0000820, mae:0.0065373
training loss 8.575356332585216e-06 mae 0.00235443445853889
training loss 8.26258466237478e-06 mae 0.0022587675792986855
training loss 8.18278666802954e-06 mae 0.002221585268680338
training loss 8.274587407446615e-06 mae 0.0022338665135217123
training loss 8.423591010506782e-06 mae 0.0022508413003831736
Epoch 267, training: loss: 0.0000084, mae: 0.0022468 test: loss0.0000828, mae:0.0065418
training loss 9.902048077492509e-06 mae 0.0024039652198553085
training loss 7.73405588238258e-06 mae 0.002108238386355487
training loss 8.00116007856487e-06 mae 0.0021544338796416866
training loss 7.93166062169423e-06 mae 0.0021560436164010433
training loss 7.774993426083429e-06 mae 0.0021382864977033855
Epoch 268, training: loss: 0.0000078, mae: 0.0021415 test: loss0.0000794, mae:0.0064270
training loss 5.795014658360742e-06 mae 0.0018509350484237075
training loss 7.502558696169039e-06 mae 0.0021109188057701375
training loss 7.890220153472403e-06 mae 0.002168800491646007
training loss 8.153640916760863e-06 mae 0.0021883146968417317
training loss 8.24400664140879e-06 mae 0.002206477491805951
Epoch 269, training: loss: 0.0000083, mae: 0.0022104 test: loss0.0000811, mae:0.0065252
training loss 8.186032573576085e-06 mae 0.0022017688024789095
training loss 7.988273311781868e-06 mae 0.002180408974013784
training loss 7.994680975634131e-06 mae 0.0021821029396672346
training loss 8.05376477415511e-06 mae 0.0021895142827207674
training loss 8.010464700500561e-06 mae 0.0021837539345359617
Epoch 270, training: loss: 0.0000081, mae: 0.0021900 test: loss0.0000814, mae:0.0065021
training loss 6.470516836998286e-06 mae 0.001962179085239768
training loss 7.5750779026694705e-06 mae 0.0021278547150466376
training loss 7.650303170593552e-06 mae 0.0021193290237650868
training loss 7.675667667166066e-06 mae 0.00212102096422964
training loss 7.699871461209199e-06 mae 0.0021248378458465283
Epoch 271, training: loss: 0.0000077, mae: 0.0021253 test: loss0.0000805, mae:0.0064530
training loss 5.617846909444779e-06 mae 0.001883233548142016
training loss 7.084623156711756e-06 mae 0.002010541328904676
training loss 7.557358786822203e-06 mae 0.0021046663802762574
training loss 7.57756613544092e-06 mae 0.002108144236185791
training loss 7.816464675222475e-06 mae 0.002151902186434105
Epoch 272, training: loss: 0.0000078, mae: 0.0021503 test: loss0.0000803, mae:0.0064567
training loss 5.665645858243806e-06 mae 0.0018574375426396728
training loss 7.875200333942572e-06 mae 0.0021430358413935583
training loss 7.674060282057544e-06 mae 0.002108502516403131
training loss 7.735560237786514e-06 mae 0.0021270768854162645
training loss 7.679394198179699e-06 mae 0.0021178940269373256
Epoch 273, training: loss: 0.0000077, mae: 0.0021186 test: loss0.0000867, mae:0.0065610
training loss 5.789703664049739e-06 mae 0.0018303189426660538
training loss 7.501355269721218e-06 mae 0.0021160296050320355
training loss 7.705592402584944e-06 mae 0.0021264778339479223
training loss 7.723778602763516e-06 mae 0.0021372217822215518
training loss 7.600660132111306e-06 mae 0.0021250113602773986
Epoch 274, training: loss: 0.0000076, mae: 0.0021270 test: loss0.0000808, mae:0.0064614
training loss 5.580284778261557e-06 mae 0.0018538683652877808
training loss 7.452878310455251e-06 mae 0.0020813023278891452
training loss 7.508374068547169e-06 mae 0.0020822698462496293
training loss 7.62228630614214e-06 mae 0.002102020332306031
training loss 7.684602589271245e-06 mae 0.002111773560893958
Epoch 275, training: loss: 0.0000077, mae: 0.0021150 test: loss0.0000813, mae:0.0064912
training loss 1.1120824638055637e-05 mae 0.0023937236983329058
training loss 7.429218483451348e-06 mae 0.0020960107897682223
training loss 7.697335708018172e-06 mae 0.0021228897950086416
training loss 7.855014812336374e-06 mae 0.002157287628988162
training loss 7.768148359683616e-06 mae 0.0021480408691759425
Epoch 276, training: loss: 0.0000078, mae: 0.0021494 test: loss0.0000822, mae:0.0065087
training loss 5.427002179203555e-06 mae 0.0016633110353723168
training loss 7.508344513588978e-06 mae 0.002105343954491557
training loss 7.592199786399356e-06 mae 0.0021154182815537
training loss 7.735683978756887e-06 mae 0.0021468205038236063
training loss 7.720160131915087e-06 mae 0.0021454899621645516
Epoch 277, training: loss: 0.0000077, mae: 0.0021463 test: loss0.0000816, mae:0.0065231
training loss 4.985288796888199e-06 mae 0.0017414981266483665
training loss 7.294406032087863e-06 mae 0.0020743539036415957
training loss 7.369858441943685e-06 mae 0.0020782971503397465
training loss 7.4673699820524995e-06 mae 0.0021002961211646627
training loss 7.726124879841177e-06 mae 0.0021306321955058935
Epoch 278, training: loss: 0.0000077, mae: 0.0021303 test: loss0.0000802, mae:0.0064732
training loss 5.458266969071701e-06 mae 0.0019832777325063944
training loss 6.898642314969466e-06 mae 0.002018464747451099
training loss 7.154258065104905e-06 mae 0.002060849850047565
training loss 7.486026288762445e-06 mae 0.0021049328647948676
training loss 7.49481614661252e-06 mae 0.0021132676264465763
Epoch 279, training: loss: 0.0000076, mae: 0.0021215 test: loss0.0000806, mae:0.0064656
training loss 6.7941473389510065e-06 mae 0.0020574929658323526
training loss 6.886268211554339e-06 mae 0.0020259224453612287
training loss 7.162054532840341e-06 mae 0.0020676775325993357
training loss 7.2533310393562656e-06 mae 0.0020739025565863455
training loss 7.514202192469019e-06 mae 0.00211282451440168
Epoch 280, training: loss: 0.0000076, mae: 0.0021183 test: loss0.0000811, mae:0.0065009
training loss 7.101167739165248e-06 mae 0.0021155260037630796
training loss 7.610190376748075e-06 mae 0.0021311001619324075
training loss 7.730500610165052e-06 mae 0.002141794664693055
training loss 7.752136411156413e-06 mae 0.00214488470513645
training loss 7.822495723502389e-06 mae 0.0021563674437938564
Epoch 281, training: loss: 0.0000078, mae: 0.0021540 test: loss0.0000829, mae:0.0065641
training loss 4.42941336586955e-06 mae 0.0016030125552788377
training loss 7.256133162003359e-06 mae 0.002071545696726033
training loss 7.357210871296137e-06 mae 0.0020773203992791997
training loss 7.541201004690248e-06 mae 0.002100129580180722
training loss 7.58074391306035e-06 mae 0.0021152780426026735
Epoch 282, training: loss: 0.0000076, mae: 0.0021166 test: loss0.0000819, mae:0.0065128
training loss 6.077988928154809e-06 mae 0.0019511599093675613
training loss 6.936850419222929e-06 mae 0.0020220084161515907
training loss 7.106057074623206e-06 mae 0.0020370390946229944
training loss 7.45941061295345e-06 mae 0.002085591841408896
training loss 7.5751187052980685e-06 mae 0.0021089052976067386
Epoch 283, training: loss: 0.0000075, mae: 0.0021038 test: loss0.0000798, mae:0.0064438
training loss 5.410933681559982e-06 mae 0.001850223750807345
training loss 6.662886303898879e-06 mae 0.0020036745757120197
training loss 6.9432851657915976e-06 mae 0.002031353296536842
training loss 7.0689804852611104e-06 mae 0.0020374846382572374
training loss 7.140073220040871e-06 mae 0.0020449843978166448
Epoch 284, training: loss: 0.0000071, mae: 0.0020449 test: loss0.0000809, mae:0.0064868
training loss 4.894016001344426e-06 mae 0.0016988329589366913
training loss 8.229603121007504e-06 mae 0.0022173239392977127
training loss 8.007958418638374e-06 mae 0.0021919572315556865
training loss 8.01086485771761e-06 mae 0.002187509368796716
training loss 8.026719843651354e-06 mae 0.0021866026487368266
Epoch 285, training: loss: 0.0000080, mae: 0.0021920 test: loss0.0000827, mae:0.0065635
training loss 5.139363111084094e-06 mae 0.001816925941966474
training loss 6.90774228649959e-06 mae 0.002048362918453767
training loss 7.061899762871094e-06 mae 0.0020608821766229706
training loss 7.1615097236428105e-06 mae 0.002071786668793057
training loss 7.3353050461649095e-06 mae 0.0020884975891754917
Epoch 286, training: loss: 0.0000073, mae: 0.0020871 test: loss0.0000816, mae:0.0065386
training loss 6.933912573003909e-06 mae 0.0021452708169817924
training loss 6.854669436506809e-06 mae 0.0020029615455617504
training loss 6.8313559535958505e-06 mae 0.0020042853821806692
training loss 7.118213836702424e-06 mae 0.0020503772303943116
training loss 7.202642208951815e-06 mae 0.0020642235083968854
Epoch 287, training: loss: 0.0000072, mae: 0.0020591 test: loss0.0000810, mae:0.0064775
training loss 4.945713953929953e-06 mae 0.001725182868540287
training loss 6.637499573829984e-06 mae 0.00198221142294214
training loss 6.849233816658193e-06 mae 0.0020112094243453565
training loss 7.025520047540072e-06 mae 0.0020388725024049737
training loss 7.029909398281133e-06 mae 0.0020378520735423646
Epoch 288, training: loss: 0.0000070, mae: 0.0020384 test: loss0.0000809, mae:0.0065126
training loss 5.839532605023123e-06 mae 0.0018916632980108261
training loss 6.537053761848273e-06 mae 0.001967941819434511
training loss 6.815964610777341e-06 mae 0.0020053865189783946
training loss 7.164356056578164e-06 mae 0.0020556309359111152
training loss 7.240632318477693e-06 mae 0.00206368621519827
Epoch 289, training: loss: 0.0000072, mae: 0.0020634 test: loss0.0000833, mae:0.0065739
training loss 8.689490641700104e-06 mae 0.0019962310325354338
training loss 7.507368955688721e-06 mae 0.0020950163038922294
training loss 7.452964328507787e-06 mae 0.0020903540212697906
training loss 7.340243437223557e-06 mae 0.0020857781221140293
training loss 7.400623466970834e-06 mae 0.002098648256358149
Epoch 290, training: loss: 0.0000074, mae: 0.0021001 test: loss0.0000810, mae:0.0064992
training loss 6.249551461223746e-06 mae 0.0019405210623517632
training loss 6.948055340449085e-06 mae 0.0020239370048740035
training loss 7.159701719573792e-06 mae 0.002060664928343037
training loss 7.13141435272633e-06 mae 0.0020542523094348075
training loss 7.041539113819976e-06 mae 0.002040146098272941
Epoch 291, training: loss: 0.0000070, mae: 0.0020367 test: loss0.0000825, mae:0.0065685
training loss 5.244146450422704e-06 mae 0.0017401505028828979
training loss 6.618527723891449e-06 mae 0.0019737246934799293
training loss 6.6390980351786895e-06 mae 0.001977183132220318
training loss 6.7191443842048785e-06 mae 0.0019871371455909993
training loss 6.8959881948320615e-06 mae 0.0020117069228519843
Epoch 292, training: loss: 0.0000070, mae: 0.0020196 test: loss0.0000812, mae:0.0065111
training loss 5.8743139561556745e-06 mae 0.0018179978942498565
training loss 7.656294780876943e-06 mae 0.0021086435870030056
training loss 7.599157596463429e-06 mae 0.002105891263103204
training loss 7.4921907444691905e-06 mae 0.0021065249568813565
training loss 7.441116824917858e-06 mae 0.0020998333996643673
Epoch 293, training: loss: 0.0000075, mae: 0.0021013 test: loss0.0000823, mae:0.0065379
training loss 7.3579763011366595e-06 mae 0.0021508957725018263
training loss 7.009102510696097e-06 mae 0.0020382662267223292
training loss 6.884169766131473e-06 mae 0.0020189580120293796
training loss 6.910020080933751e-06 mae 0.002014987811366009
training loss 7.075969627371321e-06 mae 0.0020383145041596965
Epoch 294, training: loss: 0.0000071, mae: 0.0020446 test: loss0.0000826, mae:0.0065688
training loss 7.386779088847106e-06 mae 0.002219013636931777
training loss 7.153032530528864e-06 mae 0.0020359901907652913
training loss 6.774118606465503e-06 mae 0.001989255886349716
training loss 6.778201386683781e-06 mae 0.001991625118920492
training loss 6.90945644938569e-06 mae 0.0020172265885323648
Epoch 295, training: loss: 0.0000069, mae: 0.0020133 test: loss0.0000817, mae:0.0065204
training loss 4.987506599718472e-06 mae 0.0016685988521203399
training loss 6.6026068363463136e-06 mae 0.0019629932780220517
training loss 6.7147770375350505e-06 mae 0.0019911622160603054
training loss 6.7851777651756005e-06 mae 0.0020071892684559943
training loss 6.734596823816129e-06 mae 0.0019977214237656524
Epoch 296, training: loss: 0.0000067, mae: 0.0019934 test: loss0.0000820, mae:0.0065778
training loss 7.437558451783843e-06 mae 0.002187400823459029
training loss 6.7308366236401904e-06 mae 0.0019635827367321832
training loss 6.602549198156915e-06 mae 0.0019545217579470414
training loss 6.790147697467656e-06 mae 0.0019820222626471063
training loss 6.97161099490934e-06 mae 0.0020168698479685544
Epoch 297, training: loss: 0.0000070, mae: 0.0020237 test: loss0.0000813, mae:0.0065170
training loss 6.739956006640568e-06 mae 0.0018979063024744391
training loss 6.9092411961844175e-06 mae 0.0020189825217148253
training loss 6.781872949287922e-06 mae 0.0020109644023664657
training loss 6.875481018286283e-06 mae 0.0020242574538369435
training loss 6.869904946731155e-06 mae 0.0020169885690087693
Epoch 298, training: loss: 0.0000069, mae: 0.0020184 test: loss0.0000850, mae:0.0066419
training loss 7.5969896897731815e-06 mae 0.002254694467410445
training loss 6.667430460427067e-06 mae 0.001986315368912091
training loss 6.7308710505572105e-06 mae 0.0019944394064956516
training loss 6.892334546884004e-06 mae 0.0020194333910152616
training loss 7.105503006293396e-06 mae 0.0020503839315726097
Epoch 299, training: loss: 0.0000071, mae: 0.0020509 test: loss0.0000814, mae:0.0065274
current learning rate: 6.25e-05
training loss 7.431417088810122e-06 mae 0.0020781320054084063
training loss 5.9685475359098445e-06 mae 0.0018559615858191366
training loss 5.5316361925692355e-06 mae 0.0017804114579103218
training loss 5.45712752730633e-06 mae 0.0017690222058136824
training loss 5.396363651097592e-06 mae 0.001747368672742178
Epoch 300, training: loss: 0.0000054, mae: 0.0017442 test: loss0.0000814, mae:0.0065185
training loss 4.697485110227717e-06 mae 0.0015682732919231057
training loss 4.794142765318861e-06 mae 0.0016300672569366942
training loss 4.699097549782532e-06 mae 0.0016135974155536094
training loss 4.943874130472279e-06 mae 0.0016480482897307997
training loss 4.9252904381912e-06 mae 0.001650412993816038
Epoch 301, training: loss: 0.0000049, mae: 0.0016503 test: loss0.0000804, mae:0.0064523
training loss 5.698224867956014e-06 mae 0.0018858866533264518
training loss 4.837595782851613e-06 mae 0.001622226148588108
training loss 4.936836835395748e-06 mae 0.0016383484506168136
training loss 4.917743817653697e-06 mae 0.0016358182503416253
training loss 4.928348885616867e-06 mae 0.001642823939220934
Epoch 302, training: loss: 0.0000049, mae: 0.0016422 test: loss0.0000829, mae:0.0065577
training loss 7.523176464019343e-06 mae 0.0020563791040331125
training loss 4.927950051162898e-06 mae 0.001629477250389755
training loss 4.857786261889504e-06 mae 0.0016243517776895876
training loss 4.84233047601018e-06 mae 0.0016359503794190113
training loss 4.942898803124285e-06 mae 0.0016514886134716487
Epoch 303, training: loss: 0.0000049, mae: 0.0016505 test: loss0.0000811, mae:0.0064939
training loss 3.564484359230846e-06 mae 0.0014673707773908973
training loss 4.510004698971115e-06 mae 0.0015663556850460522
training loss 4.646092187307084e-06 mae 0.0015954472145323854
training loss 4.873637212556981e-06 mae 0.0016325883473275805
training loss 4.878723714442749e-06 mae 0.0016384296581399304
Epoch 304, training: loss: 0.0000049, mae: 0.0016405 test: loss0.0000821, mae:0.0065365
training loss 3.6812728012591833e-06 mae 0.0013788469368591905
training loss 4.864655541799341e-06 mae 0.001628465345585901
training loss 4.890087992781347e-06 mae 0.001637079696502942
training loss 4.786281038495089e-06 mae 0.0016213046195639284
training loss 4.890168296905227e-06 mae 0.0016436262747773255
Epoch 305, training: loss: 0.0000049, mae: 0.0016463 test: loss0.0000816, mae:0.0065084
training loss 6.280062279984122e-06 mae 0.0018635295564308763
training loss 4.675420938178386e-06 mae 0.001609449366144106
training loss 4.822948940813202e-06 mae 0.001637455289215219
training loss 4.911242404700445e-06 mae 0.0016453467318726495
training loss 4.924022654612697e-06 mae 0.0016464858885798879
Epoch 306, training: loss: 0.0000049, mae: 0.0016470 test: loss0.0000826, mae:0.0065658
training loss 4.984859060641611e-06 mae 0.0015680048381909728
training loss 4.999687431051164e-06 mae 0.001675703088004215
training loss 4.90956267415163e-06 mae 0.001659231200482291
training loss 4.898284168126906e-06 mae 0.0016529828021672886
training loss 4.998657720819609e-06 mae 0.0016737523464374802
Epoch 307, training: loss: 0.0000050, mae: 0.0016763 test: loss0.0000835, mae:0.0065798
training loss 4.824678399018012e-06 mae 0.0017505952855572104
training loss 5.050112572551405e-06 mae 0.0017053223164825168
training loss 4.8170140736588226e-06 mae 0.0016517943223145336
training loss 4.963616409995002e-06 mae 0.0016681587613428268
training loss 4.943797007195934e-06 mae 0.00166076690245846
Epoch 308, training: loss: 0.0000049, mae: 0.0016609 test: loss0.0000826, mae:0.0065679
training loss 4.514511601882987e-06 mae 0.001702535548247397
training loss 4.557925306061424e-06 mae 0.0016033829933981979
training loss 4.572936089485073e-06 mae 0.0016047999142936552
training loss 4.756707991590467e-06 mae 0.001628992900973963
training loss 4.784382120880617e-06 mae 0.0016292350486022845
Epoch 309, training: loss: 0.0000048, mae: 0.0016261 test: loss0.0000826, mae:0.0065679
training loss 3.6196227028995054e-06 mae 0.0014647748321294785
training loss 4.745770241902835e-06 mae 0.0015856892043980312
training loss 4.793292653108199e-06 mae 0.0016078942888739089
training loss 4.816803671651871e-06 mae 0.0016211889528465885
training loss 4.808145395054906e-06 mae 0.0016249883786742735
Epoch 310, training: loss: 0.0000048, mae: 0.0016279 test: loss0.0000830, mae:0.0065902
training loss 5.669423899234971e-06 mae 0.0018192523857578635
training loss 4.461429362694903e-06 mae 0.0015778018266656526
training loss 4.820660800724361e-06 mae 0.0016423074095187214
training loss 4.829703903845599e-06 mae 0.0016365851513079263
training loss 4.8518347694197236e-06 mae 0.0016370112677591283
Epoch 311, training: loss: 0.0000049, mae: 0.0016396 test: loss0.0000824, mae:0.0065560
training loss 2.6446405172464438e-06 mae 0.001224795705638826
training loss 4.555332425832815e-06 mae 0.0015920710307089426
training loss 4.671302169281441e-06 mae 0.0016148776798001906
training loss 4.707345235917893e-06 mae 0.0016227010768585354
training loss 4.807594033122563e-06 mae 0.0016374985631833323
Epoch 312, training: loss: 0.0000048, mae: 0.0016335 test: loss0.0000837, mae:0.0066146
training loss 7.778146937198471e-06 mae 0.0021754615008831024
training loss 4.959787711235478e-06 mae 0.0016567854224430287
training loss 4.839787170387337e-06 mae 0.0016417642340691074
training loss 4.7971526005945074e-06 mae 0.0016355214498641074
training loss 4.7985216843332695e-06 mae 0.001634275551134748
Epoch 313, training: loss: 0.0000048, mae: 0.0016336 test: loss0.0000888, mae:0.0066553
training loss 3.95423558074981e-06 mae 0.0014847656711935997
training loss 4.7630499011575e-06 mae 0.0016191077519062105
training loss 4.687567392429988e-06 mae 0.001609429998304879
training loss 4.677207511760559e-06 mae 0.0016070253898802873
training loss 4.654532687881378e-06 mae 0.0016095363343857337
Epoch 314, training: loss: 0.0000047, mae: 0.0016207 test: loss0.0000839, mae:0.0066186
training loss 3.2656107578077354e-06 mae 0.0014306107768788934
training loss 4.86368184747156e-06 mae 0.0016319149324451297
training loss 4.6444324022496125e-06 mae 0.0016046644691009037
training loss 4.65070793505834e-06 mae 0.0016085335318714582
training loss 4.702972750129085e-06 mae 0.0016162547147346303
Epoch 315, training: loss: 0.0000047, mae: 0.0016217 test: loss0.0000834, mae:0.0065946
training loss 5.3665717132389545e-06 mae 0.0016514398157596588
training loss 4.6156008769078945e-06 mae 0.0016076110358185624
training loss 4.515928308722835e-06 mae 0.0015949528574445608
training loss 4.608042317405782e-06 mae 0.001610174153727905
training loss 4.6844816233036636e-06 mae 0.001617555099942569
Epoch 316, training: loss: 0.0000047, mae: 0.0016183 test: loss0.0000838, mae:0.0066125
training loss 4.939471637044335e-06 mae 0.0015172235434874892
training loss 4.695215394866526e-06 mae 0.0016186298761407243
training loss 4.7218196607783555e-06 mae 0.0016195706061852895
training loss 4.600789320768355e-06 mae 0.0016042753778064982
training loss 4.73169326985827e-06 mae 0.00162252407307871
Epoch 317, training: loss: 0.0000047, mae: 0.0016262 test: loss0.0000837, mae:0.0066144
training loss 4.43882163381204e-06 mae 0.0016721241408959031
training loss 4.390468909739009e-06 mae 0.0015722117911768604
training loss 4.487570114215127e-06 mae 0.0015825984174256567
training loss 4.566602786699691e-06 mae 0.0015905266450347974
training loss 4.63496509658837e-06 mae 0.0016069411698599052
Epoch 318, training: loss: 0.0000047, mae: 0.0016123 test: loss0.0000834, mae:0.0066090
training loss 4.768523467646446e-06 mae 0.001506136148236692
training loss 4.530376490858507e-06 mae 0.001590716992687507
training loss 4.527079570357499e-06 mae 0.0016021794445886472
training loss 4.640895296586146e-06 mae 0.0016197756346043766
training loss 4.713203541211273e-06 mae 0.001631129698118946
Epoch 319, training: loss: 0.0000047, mae: 0.0016298 test: loss0.0000837, mae:0.0066141
training loss 9.077298273041379e-06 mae 0.0020084830466657877
training loss 4.702994686465222e-06 mae 0.0016068287773569135
training loss 4.670370560801313e-06 mae 0.0016116784173202249
training loss 4.562690996531964e-06 mae 0.0015954792750875976
training loss 4.615854111228083e-06 mae 0.0016061140413150485
Epoch 320, training: loss: 0.0000046, mae: 0.0016046 test: loss0.0000843, mae:0.0066340
training loss 3.950794962293003e-06 mae 0.0014412343734875321
training loss 4.515233580177131e-06 mae 0.001571484423680779
training loss 4.399555832200764e-06 mae 0.0015626352667661
training loss 4.482312687172611e-06 mae 0.0015770053214991821
training loss 4.558804660143383e-06 mae 0.0015856489476594913
Epoch 321, training: loss: 0.0000046, mae: 0.0015873 test: loss0.0000845, mae:0.0066467
training loss 3.838943939626915e-06 mae 0.001387985423207283
training loss 4.41493011303286e-06 mae 0.0015474218948214658
training loss 4.479119795205134e-06 mae 0.0015659326408519452
training loss 4.513341862198913e-06 mae 0.0015876164292879644
training loss 4.640511060099909e-06 mae 0.001614652768087884
Epoch 322, training: loss: 0.0000046, mae: 0.0016100 test: loss0.0000842, mae:0.0066411
training loss 3.533634071573033e-06 mae 0.0014752320712432265
training loss 4.567378221722987e-06 mae 0.0016054845478970046
training loss 4.5849363486652665e-06 mae 0.0016085823345468335
training loss 4.610180489005822e-06 mae 0.0016180247959637687
training loss 4.674623435458846e-06 mae 0.0016266167712912195
Epoch 323, training: loss: 0.0000047, mae: 0.0016255 test: loss0.0000844, mae:0.0066341
training loss 5.915742804063484e-06 mae 0.0019504619995132089
training loss 4.919470314894617e-06 mae 0.0016483404883640069
training loss 4.648489649109323e-06 mae 0.0016152053578102053
training loss 4.732837345462584e-06 mae 0.0016293780444905355
training loss 4.5899962560498564e-06 mae 0.0016093287977218555
Epoch 324, training: loss: 0.0000046, mae: 0.0016121 test: loss0.0000843, mae:0.0066232
training loss 3.623948032327462e-06 mae 0.0013730274513363838
training loss 4.547110656145094e-06 mae 0.0015799521158138912
training loss 4.485240521203781e-06 mae 0.0015786905706131663
training loss 4.433251791888087e-06 mae 0.0015767788543791475
training loss 4.462888916771305e-06 mae 0.001584048150107265
Epoch 325, training: loss: 0.0000045, mae: 0.0015853 test: loss0.0000844, mae:0.0066374
training loss 4.7532134885841515e-06 mae 0.0017708163941279054
training loss 4.728252484514372e-06 mae 0.0015967956666524214
training loss 4.501595407247696e-06 mae 0.0015805800696650507
training loss 4.454416636317003e-06 mae 0.0015860835185878917
training loss 4.519097413874046e-06 mae 0.0015930845599102584
Epoch 326, training: loss: 0.0000045, mae: 0.0015890 test: loss0.0000846, mae:0.0066422
training loss 4.454403097042814e-06 mae 0.001413563615642488
training loss 4.216251129179784e-06 mae 0.0015260585122650453
training loss 4.309598870629027e-06 mae 0.0015298928512204994
training loss 4.336210922220014e-06 mae 0.0015388371550866613
training loss 4.419275773673221e-06 mae 0.0015575544247218409
Epoch 327, training: loss: 0.0000044, mae: 0.0015542 test: loss0.0000841, mae:0.0066138
training loss 5.0962407840415835e-06 mae 0.0015845628222450614
training loss 4.267365640914125e-06 mae 0.0015525094406934929
training loss 4.244315001000865e-06 mae 0.00154373115089971
training loss 4.323769479659503e-06 mae 0.0015513218341037536
training loss 4.370049710606639e-06 mae 0.00155829033578065
Epoch 328, training: loss: 0.0000044, mae: 0.0015614 test: loss0.0000846, mae:0.0066551
training loss 3.925545115635032e-06 mae 0.0014922888949513435
training loss 4.237644457491258e-06 mae 0.0015272025142193716
training loss 4.217665620969213e-06 mae 0.0015328512054471539
training loss 4.268081411192419e-06 mae 0.0015462998590672643
training loss 4.31491263955184e-06 mae 0.0015554387152510046
Epoch 329, training: loss: 0.0000044, mae: 0.0015586 test: loss0.0000843, mae:0.0066237
training loss 2.8408512662281282e-06 mae 0.0012678835773840547
training loss 3.936352703592502e-06 mae 0.0014810856367808343
training loss 4.220143090164904e-06 mae 0.0015276682938882467
training loss 4.341576525363558e-06 mae 0.0015505969821975026
training loss 4.4048909464223585e-06 mae 0.0015654782347603515
Epoch 330, training: loss: 0.0000044, mae: 0.0015664 test: loss0.0000845, mae:0.0066546
training loss 3.5210243822803022e-06 mae 0.0014710534596815705
training loss 4.2852369309305995e-06 mae 0.0015596383064072216
training loss 4.296206250476437e-06 mae 0.0015433087500070436
training loss 4.334248142306432e-06 mae 0.0015586136966721702
training loss 4.3342355880821875e-06 mae 0.0015551853526513371
Epoch 331, training: loss: 0.0000043, mae: 0.0015527 test: loss0.0000856, mae:0.0066876
training loss 3.498875457808026e-06 mae 0.0014253411209210753
training loss 4.043764337724248e-06 mae 0.001494775670032729
training loss 4.302886424901061e-06 mae 0.0015382870160375196
training loss 4.313048860553293e-06 mae 0.0015431477068399162
training loss 4.275994532084347e-06 mae 0.0015420540802375366
Epoch 332, training: loss: 0.0000043, mae: 0.0015464 test: loss0.0000842, mae:0.0066185
training loss 3.7783527204737766e-06 mae 0.0015065952902659774
training loss 4.686714905177474e-06 mae 0.001621107146253481
training loss 4.386833379072359e-06 mae 0.0015674053188782225
training loss 4.404779725964604e-06 mae 0.001571194900181268
training loss 4.352725687766822e-06 mae 0.0015641180995332811
Epoch 333, training: loss: 0.0000044, mae: 0.0015654 test: loss0.0000846, mae:0.0066486
training loss 3.862332505377708e-06 mae 0.00147283636033535
training loss 4.220493273279834e-06 mae 0.0015365451451062279
training loss 4.243200432128023e-06 mae 0.001538179838444374
training loss 4.2753556484461736e-06 mae 0.001551062657747615
training loss 4.2894099245208575e-06 mae 0.0015533578171468894
Epoch 334, training: loss: 0.0000043, mae: 0.0015581 test: loss0.0000846, mae:0.0066342
training loss 4.345446086517768e-06 mae 0.0015498045831918716
training loss 4.1687709538154215e-06 mae 0.0015184569318650988
training loss 4.4099770993942915e-06 mae 0.0015630297844602477
training loss 4.391486816421814e-06 mae 0.0015687473212362614
training loss 4.358919240351936e-06 mae 0.0015659561845246899
Epoch 335, training: loss: 0.0000043, mae: 0.0015630 test: loss0.0000855, mae:0.0066943
training loss 3.3385933875251794e-06 mae 0.0013478932669386268
training loss 4.305526560671626e-06 mae 0.0015530985387443917
training loss 4.248332183943595e-06 mae 0.0015343507592759959
training loss 4.2298812754990454e-06 mae 0.0015329867308428943
training loss 4.1973923314355885e-06 mae 0.0015291348852643483
Epoch 336, training: loss: 0.0000042, mae: 0.0015335 test: loss0.0000853, mae:0.0066823
training loss 2.765450744846021e-06 mae 0.0012328015873208642
training loss 3.983223526423294e-06 mae 0.0014881390836272463
training loss 3.909199084782725e-06 mae 0.0014827761310383235
training loss 4.128257515484144e-06 mae 0.0015188696299254009
training loss 4.282430565119772e-06 mae 0.0015476682734216992
Epoch 337, training: loss: 0.0000043, mae: 0.0015430 test: loss0.0000846, mae:0.0066615
training loss 2.34695562539855e-06 mae 0.0011701866751536727
training loss 4.049629198704554e-06 mae 0.0015194045891072237
training loss 4.081860388756504e-06 mae 0.001528890139543184
training loss 4.184089973764067e-06 mae 0.00154151246167522
training loss 4.294416601225921e-06 mae 0.0015546516486355541
Epoch 338, training: loss: 0.0000043, mae: 0.0015547 test: loss0.0000854, mae:0.0066652
training loss 2.675725227163639e-06 mae 0.0013045495143160224
training loss 3.91095773902991e-06 mae 0.0014800013969743656
training loss 4.007747712997718e-06 mae 0.0014920911456615031
training loss 4.086415912830571e-06 mae 0.0015100802577082187
training loss 4.2001424092527985e-06 mae 0.001531174758551132
Epoch 339, training: loss: 0.0000042, mae: 0.0015297 test: loss0.0000852, mae:0.0066906
training loss 3.097995659118169e-06 mae 0.0012962399050593376
training loss 4.014481490622869e-06 mae 0.001493276838742781
training loss 3.984643804455621e-06 mae 0.0015076950842533082
training loss 4.0034757789997436e-06 mae 0.0015100428220871471
training loss 4.101186931534787e-06 mae 0.001518612765748777
Epoch 340, training: loss: 0.0000041, mae: 0.0015252 test: loss0.0000848, mae:0.0066727
training loss 5.274371233099373e-06 mae 0.001518354401923716
training loss 4.120296509034804e-06 mae 0.0015073839447223673
training loss 4.092100689016626e-06 mae 0.0015018738571111812
training loss 4.014325459018028e-06 mae 0.0014960677748459656
training loss 4.116338808970213e-06 mae 0.001520650670400343
Epoch 341, training: loss: 0.0000041, mae: 0.0015255 test: loss0.0000861, mae:0.0067201
training loss 4.962809271091828e-06 mae 0.0017217807471752167
training loss 4.169377965737628e-06 mae 0.0015299262195898624
training loss 4.147160525732968e-06 mae 0.00152279805428911
training loss 4.18040810129711e-06 mae 0.0015264078034149693
training loss 4.194861903941729e-06 mae 0.0015279756017974506
Epoch 342, training: loss: 0.0000042, mae: 0.0015267 test: loss0.0000845, mae:0.0066482
training loss 4.061892923346022e-06 mae 0.0014534955844283104
training loss 4.119098343389673e-06 mae 0.001512164218059065
training loss 4.078218625641009e-06 mae 0.0015077012254496901
training loss 4.046445089581826e-06 mae 0.0015046771049857233
training loss 4.08340840851397e-06 mae 0.0015115535363962702
Epoch 343, training: loss: 0.0000041, mae: 0.0015155 test: loss0.0000852, mae:0.0066888
training loss 4.661456841859035e-06 mae 0.0015681920340284705
training loss 3.939135010613445e-06 mae 0.0014995454249027021
training loss 3.991088424254934e-06 mae 0.0015049495441507143
training loss 3.963265046407688e-06 mae 0.0014981841429488251
training loss 4.049101478211293e-06 mae 0.0015075290726210492
Epoch 344, training: loss: 0.0000041, mae: 0.0015156 test: loss0.0000855, mae:0.0066889
training loss 1.8531351315687061e-06 mae 0.000991961918771267
training loss 4.0898218449179764e-06 mae 0.001502447414631937
training loss 4.186766306144035e-06 mae 0.001520942981898932
training loss 4.069781957937174e-06 mae 0.001503367542862793
training loss 4.11174779453371e-06 mae 0.0015147141926917268
Epoch 345, training: loss: 0.0000041, mae: 0.0015149 test: loss0.0000871, mae:0.0067297
training loss 5.397147106123157e-06 mae 0.0016995851183310151
training loss 3.5347448900603352e-06 mae 0.0014087542019528795
training loss 3.739246957287257e-06 mae 0.0014552960223011154
training loss 3.899445747470205e-06 mae 0.001474688334594055
training loss 4.024018486501092e-06 mae 0.0014957305564040045
Epoch 346, training: loss: 0.0000040, mae: 0.0014965 test: loss0.0000868, mae:0.0067271
training loss 3.418733513171901e-06 mae 0.0014254298293963075
training loss 3.8812123626021455e-06 mae 0.0014855645610676966
training loss 4.061875411070563e-06 mae 0.0015095615445977384
training loss 4.035118143184077e-06 mae 0.0014978997202725792
training loss 4.049747500555813e-06 mae 0.0015025351784975069
Epoch 347, training: loss: 0.0000040, mae: 0.0014973 test: loss0.0000858, mae:0.0066930
training loss 4.355444161774358e-06 mae 0.0015840543201193213
training loss 3.9628027444254555e-06 mae 0.0014892430638200516
training loss 3.995036485600332e-06 mae 0.0014956309425524707
training loss 4.029175930940112e-06 mae 0.001496313409633096
training loss 4.018212958405246e-06 mae 0.0014964097042785799
Epoch 348, training: loss: 0.0000040, mae: 0.0014976 test: loss0.0000856, mae:0.0066814
training loss 3.0357339255715488e-06 mae 0.0013674363726750016
training loss 3.718903416501787e-06 mae 0.0014653765190137072
training loss 3.991592753700027e-06 mae 0.0014963713774254718
training loss 3.935609760181803e-06 mae 0.0014796183638842103
training loss 3.963653508679865e-06 mae 0.001482467368055153
Epoch 349, training: loss: 0.0000040, mae: 0.0014903 test: loss0.0000853, mae:0.0066870
training loss 2.785952574413386e-06 mae 0.0013105431571602821
training loss 3.879015177214266e-06 mae 0.0014771542902214123
training loss 3.946465025734087e-06 mae 0.001490169311389511
training loss 3.961844581319973e-06 mae 0.0014922723712997863
training loss 3.9846018198297305e-06 mae 0.0014974156881691844
Epoch 350, training: loss: 0.0000040, mae: 0.0014982 test: loss0.0000849, mae:0.0066649
training loss 2.415056997051579e-06 mae 0.001204674132168293
training loss 3.7458158573702114e-06 mae 0.0014397955976663997
training loss 3.804805284951475e-06 mae 0.001452453242554929
training loss 3.877081388699123e-06 mae 0.001471095065823726
training loss 3.974795023526014e-06 mae 0.0014911343703227155
Epoch 351, training: loss: 0.0000040, mae: 0.0015004 test: loss0.0000856, mae:0.0066918
training loss 3.850866050925106e-06 mae 0.001561606302857399
training loss 3.720331956263217e-06 mae 0.0014668132986544685
training loss 3.839614250729469e-06 mae 0.0014910900649513204
training loss 3.9557334522690514e-06 mae 0.0015019757730195564
training loss 3.993003532225737e-06 mae 0.0015046922443666274
Epoch 352, training: loss: 0.0000040, mae: 0.0015107 test: loss0.0000860, mae:0.0067291
training loss 3.670203795991256e-06 mae 0.0016415361315011978
training loss 3.8595893712744005e-06 mae 0.0015012044916112048
training loss 3.924199988729683e-06 mae 0.0015006448225219652
training loss 3.922448974319665e-06 mae 0.001493178129146825
training loss 3.991681840724327e-06 mae 0.001499373223099727
Epoch 353, training: loss: 0.0000040, mae: 0.0015022 test: loss0.0000866, mae:0.0067439
training loss 4.913676548312651e-06 mae 0.0016751004150137305
training loss 3.574293188256549e-06 mae 0.0014420076337296006
training loss 3.880889049595329e-06 mae 0.001485027310595211
training loss 3.798266576774306e-06 mae 0.0014625483944962748
training loss 3.876159079350801e-06 mae 0.0014761853702505352
Epoch 354, training: loss: 0.0000039, mae: 0.0014800 test: loss0.0000852, mae:0.0066750
training loss 3.841974375973223e-06 mae 0.0014536011731252074
training loss 3.721264391185519e-06 mae 0.0014406782780390457
training loss 3.802059952681929e-06 mae 0.0014544542622983022
training loss 3.8778151687233606e-06 mae 0.0014734526314396442
training loss 3.959617037144744e-06 mae 0.0014886107099988488
Epoch 355, training: loss: 0.0000039, mae: 0.0014826 test: loss0.0000857, mae:0.0067034
training loss 3.015181164300884e-06 mae 0.0012695245677605271
training loss 3.5919292869556213e-06 mae 0.0014302307344954827
training loss 3.6445234307861796e-06 mae 0.0014382394585103755
training loss 3.732525975549575e-06 mae 0.0014496274421219738
training loss 3.865842290114737e-06 mae 0.001475169464994444
Epoch 356, training: loss: 0.0000039, mae: 0.0014792 test: loss0.0000869, mae:0.0067711
training loss 3.1416705041920068e-06 mae 0.001435373560525477
training loss 3.965043886494765e-06 mae 0.0014925243095586114
training loss 3.95833086429883e-06 mae 0.001492510894935873
training loss 3.905324452439674e-06 mae 0.0014758446436554241
training loss 3.985690314174693e-06 mae 0.001493619762729408
Epoch 357, training: loss: 0.0000040, mae: 0.0014933 test: loss0.0000859, mae:0.0067226
training loss 3.6094854749535443e-06 mae 0.001431043609045446
training loss 3.895614799701292e-06 mae 0.0014782348035962559
training loss 3.983434833859819e-06 mae 0.0014945408951786189
training loss 3.975082030508277e-06 mae 0.0014931831519668364
training loss 3.90539984192713e-06 mae 0.0014776875964592929
Epoch 358, training: loss: 0.0000039, mae: 0.0014750 test: loss0.0000852, mae:0.0066745
training loss 1.5791487157912343e-06 mae 0.0009565837681293488
training loss 3.5990339185053587e-06 mae 0.0014181662369154248
training loss 3.907163939555463e-06 mae 0.001479201895959381
training loss 3.930359449043783e-06 mae 0.001485733856564228
training loss 3.973934284640655e-06 mae 0.0014944517201933999
Epoch 359, training: loss: 0.0000040, mae: 0.0014923 test: loss0.0000862, mae:0.0067165
training loss 2.3517577574239112e-06 mae 0.0012140799080953002
training loss 3.5731986403619635e-06 mae 0.001428850243945478
training loss 3.5872504512742967e-06 mae 0.0014241129773124906
training loss 3.797864571834299e-06 mae 0.0014585938269198434
training loss 3.7988052817855523e-06 mae 0.001458530543622241
Epoch 360, training: loss: 0.0000038, mae: 0.0014641 test: loss0.0000866, mae:0.0067288
training loss 3.888105766236549e-06 mae 0.0016321642324328423
training loss 4.007193043060612e-06 mae 0.0015070582009559749
training loss 3.825188257744034e-06 mae 0.0014696565933875962
training loss 3.804561657904701e-06 mae 0.0014632132138736199
training loss 3.797097172681975e-06 mae 0.0014632281254217101
Epoch 361, training: loss: 0.0000038, mae: 0.0014699 test: loss0.0000863, mae:0.0067347
training loss 3.2168870802706806e-06 mae 0.001429870375432074
training loss 3.712610615972759e-06 mae 0.0014446312972508813
training loss 3.803163989200899e-06 mae 0.0014632919890861407
training loss 3.7435571426757584e-06 mae 0.0014512460772873256
training loss 3.795746406196025e-06 mae 0.0014577549372554125
Epoch 362, training: loss: 0.0000038, mae: 0.0014565 test: loss0.0000854, mae:0.0066863
training loss 3.5957625641458435e-06 mae 0.0014628268545493484
training loss 3.6879553603926874e-06 mae 0.0014244788668721041
training loss 3.761863940600941e-06 mae 0.0014426122675419298
training loss 3.7320220386897103e-06 mae 0.0014386751113878871
training loss 3.8626346097667534e-06 mae 0.0014639790036683137
Epoch 363, training: loss: 0.0000038, mae: 0.0014596 test: loss0.0000873, mae:0.0067794
training loss 3.9452688724850304e-06 mae 0.0015931468224152923
training loss 3.750160067729845e-06 mae 0.0014403889357459314
training loss 3.735780256143284e-06 mae 0.0014356257230283159
training loss 3.7554255215088193e-06 mae 0.0014430712353413473
training loss 3.770744597068867e-06 mae 0.001451884649476195
Epoch 364, training: loss: 0.0000038, mae: 0.0014521 test: loss0.0000866, mae:0.0067379
training loss 4.093535153515404e-06 mae 0.0014980180421844125
training loss 3.7691732662400073e-06 mae 0.0014121144138020919
training loss 3.867330480428032e-06 mae 0.001441613044866388
training loss 3.832260036712332e-06 mae 0.0014535715203695245
training loss 3.7535986417532876e-06 mae 0.0014465102324707074
Epoch 365, training: loss: 0.0000037, mae: 0.0014438 test: loss0.0000875, mae:0.0067851
training loss 4.420188361109467e-06 mae 0.0016594467451795936
training loss 3.585767473726032e-06 mae 0.001417136331107102
training loss 3.734044150072497e-06 mae 0.0014561875965354023
training loss 3.7386470254710984e-06 mae 0.0014499091567928026
training loss 3.773460690440084e-06 mae 0.0014547266868818827
Epoch 366, training: loss: 0.0000038, mae: 0.0014519 test: loss0.0000863, mae:0.0067119
training loss 3.6997034840169363e-06 mae 0.0013265522429719567
training loss 3.391149635610739e-06 mae 0.0013540888835182959
training loss 3.706672901394581e-06 mae 0.0014268074779411647
training loss 3.6980711653893575e-06 mae 0.001437224534845954
training loss 3.744175541920521e-06 mae 0.0014477019570072855
Epoch 367, training: loss: 0.0000037, mae: 0.0014494 test: loss0.0000867, mae:0.0067557
training loss 3.5142741126037436e-06 mae 0.0014446746790781617
training loss 3.907825009050345e-06 mae 0.0014753399722679866
training loss 3.6820025615729686e-06 mae 0.001427663402576553
training loss 3.6834382238200935e-06 mae 0.0014329320976251602
training loss 3.7397195733070818e-06 mae 0.0014436296992402396
Epoch 368, training: loss: 0.0000037, mae: 0.0014448 test: loss0.0000922, mae:0.0068108
training loss 2.3549348497908795e-06 mae 0.0012595088919624686
training loss 3.567590702718292e-06 mae 0.0014022594881609226
training loss 3.738236220516849e-06 mae 0.0014426591304720997
training loss 3.7679736339446164e-06 mae 0.0014469387932581862
training loss 3.760601407068393e-06 mae 0.0014478490628251127
Epoch 369, training: loss: 0.0000038, mae: 0.0014492 test: loss0.0000870, mae:0.0067227
training loss 3.2623681818222394e-06 mae 0.0014555106172338128
training loss 3.815582426885193e-06 mae 0.0014723729817014107
training loss 3.878025564058634e-06 mae 0.0014869445372409746
training loss 3.873900290188686e-06 mae 0.0014780899409562436
training loss 3.820292204408138e-06 mae 0.0014620593497622287
Epoch 370, training: loss: 0.0000038, mae: 0.0014593 test: loss0.0000869, mae:0.0067531
training loss 4.1292250898550265e-06 mae 0.0015474216779693961
training loss 3.687241703529235e-06 mae 0.001438236452967805
training loss 3.5162391411924343e-06 mae 0.0014074472116887343
training loss 3.557580118425578e-06 mae 0.0014096738879309833
training loss 3.5718718311894983e-06 mae 0.0014096264520646138
Epoch 371, training: loss: 0.0000036, mae: 0.0014103 test: loss0.0000865, mae:0.0067318
training loss 4.724157861346612e-06 mae 0.001560566364787519
training loss 3.7380807097018562e-06 mae 0.001428042732489606
training loss 3.7176810380542245e-06 mae 0.0014318288776488735
training loss 3.686964693155917e-06 mae 0.0014287376142606534
training loss 3.549568499629775e-06 mae 0.0014049575353662172
Epoch 372, training: loss: 0.0000036, mae: 0.0014091 test: loss0.0000858, mae:0.0067136
training loss 3.9829546949476935e-06 mae 0.0015836367383599281
training loss 3.5613372155435793e-06 mae 0.001417359090684091
training loss 3.59320429174884e-06 mae 0.0014243989901635613
training loss 3.6782385650620822e-06 mae 0.0014295497707196123
training loss 3.6437878447737715e-06 mae 0.0014269556557708679
Epoch 373, training: loss: 0.0000037, mae: 0.0014309 test: loss0.0000875, mae:0.0067806
training loss 2.943607114502811e-06 mae 0.0012851295759901404
training loss 3.459800431229786e-06 mae 0.0014049267378069604
training loss 3.586205548795123e-06 mae 0.0014141457798512707
training loss 3.628363436363969e-06 mae 0.0014247675017694268
training loss 3.602040295895427e-06 mae 0.0014189777421459214
Epoch 374, training: loss: 0.0000036, mae: 0.0014162 test: loss0.0000882, mae:0.0068204
training loss 2.8332997317193076e-06 mae 0.0013180685928091407
training loss 3.5652708180243605e-06 mae 0.0014084692286583142
training loss 3.599497285949387e-06 mae 0.0014215853724554924
training loss 3.565343949928216e-06 mae 0.001409004092376852
training loss 3.5964116724375603e-06 mae 0.001411707140505314
Epoch 375, training: loss: 0.0000036, mae: 0.0014090 test: loss0.0000872, mae:0.0067568
training loss 2.711364686547313e-06 mae 0.0012194175506010652
training loss 3.5717654951690302e-06 mae 0.0014136727960944616
training loss 3.5817306029862765e-06 mae 0.0014068959943609
training loss 3.593191192991576e-06 mae 0.001411675930020109
training loss 3.543279744216174e-06 mae 0.001406826660678773
Epoch 376, training: loss: 0.0000035, mae: 0.0014072 test: loss0.0000876, mae:0.0067875
training loss 2.0970705918443855e-06 mae 0.001130033633671701
training loss 3.435631206943357e-06 mae 0.0013968368773074709
training loss 3.4675196149133756e-06 mae 0.0014046704659404436
training loss 3.5244034916405443e-06 mae 0.0014163439500528016
training loss 3.5258463093899764e-06 mae 0.0014126711164193404
Epoch 377, training: loss: 0.0000035, mae: 0.0014167 test: loss0.0000881, mae:0.0068147
training loss 2.5490455755061703e-06 mae 0.0012869093334302306
training loss 3.3864256487968125e-06 mae 0.001393772711904318
training loss 3.4157257646661987e-06 mae 0.001391030481490906
training loss 3.4865069306725e-06 mae 0.0013937503824146184
training loss 3.525611022884383e-06 mae 0.001402388247475955
Epoch 378, training: loss: 0.0000036, mae: 0.0014092 test: loss0.0000867, mae:0.0067506
training loss 3.983849637734238e-06 mae 0.001423155888915062
training loss 3.265352505895505e-06 mae 0.0013641924117965734
training loss 3.452211217336297e-06 mae 0.0013995740221499809
training loss 3.4803165280154996e-06 mae 0.001393564287896903
training loss 3.494904920113927e-06 mae 0.0013967704051637566
Epoch 379, training: loss: 0.0000035, mae: 0.0013962 test: loss0.0000875, mae:0.0067875
training loss 3.3949152111745207e-06 mae 0.0014051860198378563
training loss 3.3139071957503248e-06 mae 0.0013498039037336176
training loss 3.3897902681874806e-06 mae 0.0013725967918716293
training loss 3.429033809452269e-06 mae 0.001391182427713955
training loss 3.493628479367571e-06 mae 0.0013973533294038534
Epoch 380, training: loss: 0.0000035, mae: 0.0014017 test: loss0.0000874, mae:0.0067991
training loss 3.5718776416615583e-06 mae 0.001489281072281301
training loss 3.6362488367112554e-06 mae 0.001407825321817369
training loss 3.4616362884185253e-06 mae 0.0013792581215390175
training loss 3.4531770457483954e-06 mae 0.0013873624401043684
training loss 3.458792227673312e-06 mae 0.0013909019768441945
Epoch 381, training: loss: 0.0000035, mae: 0.0013909 test: loss0.0000873, mae:0.0067660
training loss 3.972989816247718e-06 mae 0.0013973867753520608
training loss 3.6498753299348014e-06 mae 0.0014216888287360327
training loss 3.448498370681313e-06 mae 0.0013877356910183662
training loss 3.4831993243100425e-06 mae 0.001390788447426398
training loss 3.4973404677404134e-06 mae 0.0013957748181459984
Epoch 382, training: loss: 0.0000035, mae: 0.0013956 test: loss0.0000874, mae:0.0067765
training loss 3.6281962820794433e-06 mae 0.001566533581353724
training loss 3.1662705302604835e-06 mae 0.001361790941754246
training loss 3.2142115505938817e-06 mae 0.0013576496304888827
training loss 3.390126144853675e-06 mae 0.001384901394759187
training loss 3.4965546043638292e-06 mae 0.0014024960144714846
Epoch 383, training: loss: 0.0000035, mae: 0.0014017 test: loss0.0000876, mae:0.0067768
training loss 3.872139131999575e-06 mae 0.00139559805393219
training loss 3.3421428399521453e-06 mae 0.0013638731073953357
training loss 3.408879186479518e-06 mae 0.0013879885520131354
training loss 3.4936318361240003e-06 mae 0.001406557475207435
training loss 3.495588016245274e-06 mae 0.0014087999554059066
Epoch 384, training: loss: 0.0000035, mae: 0.0014172 test: loss0.0000874, mae:0.0067888
training loss 3.3584567518119e-06 mae 0.0013247547904029489
training loss 3.41886226017189e-06 mae 0.0013871519283081093
training loss 3.365003861983837e-06 mae 0.0013874288899639597
training loss 3.483713464375369e-06 mae 0.001403955042682933
training loss 3.4736856920985634e-06 mae 0.001399721018390831
Epoch 385, training: loss: 0.0000035, mae: 0.0014007 test: loss0.0000879, mae:0.0068032
training loss 3.2847663078428013e-06 mae 0.001526754698716104
training loss 3.2928586996800443e-06 mae 0.001353251140601203
training loss 3.3229083904792205e-06 mae 0.00137533894094574
training loss 3.4118198679806057e-06 mae 0.0013872374944691084
training loss 3.406080105351201e-06 mae 0.0013841239463848372
Epoch 386, training: loss: 0.0000034, mae: 0.0013847 test: loss0.0000879, mae:0.0068076
training loss 3.183617764079827e-06 mae 0.0013342559104785323
training loss 3.194337414153744e-06 mae 0.0013232081523621639
training loss 3.3756724520788e-06 mae 0.00135393249099189
training loss 3.4321451930857118e-06 mae 0.0013759884661809852
training loss 3.4100810800094704e-06 mae 0.0013742636476842625
Epoch 387, training: loss: 0.0000034, mae: 0.0013743 test: loss0.0000880, mae:0.0067990
training loss 2.19781895793858e-06 mae 0.001046017394401133
training loss 3.6279590065033506e-06 mae 0.0014215085870020233
training loss 3.590450670470649e-06 mae 0.0014282225361462715
training loss 3.5668024948363208e-06 mae 0.0014167936080881688
training loss 3.568763001264112e-06 mae 0.0014262710963213686
Epoch 388, training: loss: 0.0000036, mae: 0.0014246 test: loss0.0000870, mae:0.0067709
training loss 5.357652753446018e-06 mae 0.0016423072665929794
training loss 3.7034214760662477e-06 mae 0.0014704869005500393
training loss 3.546163257077309e-06 mae 0.0014204327881539075
training loss 3.627036317957629e-06 mae 0.0014354598551174465
training loss 3.555341689759155e-06 mae 0.0014196422990105711
Epoch 389, training: loss: 0.0000036, mae: 0.0014183 test: loss0.0000882, mae:0.0068102
training loss 3.1224692520481767e-06 mae 0.0012686395784839988
training loss 3.2794828368922955e-06 mae 0.0013548103469314384
training loss 3.5068356019470264e-06 mae 0.0014086161033563904
training loss 3.4572162156181983e-06 mae 0.0013958505525975332
training loss 3.4312420228758783e-06 mae 0.0013929157470357581
Epoch 390, training: loss: 0.0000034, mae: 0.0013931 test: loss0.0000872, mae:0.0067741
training loss 2.262666612296016e-06 mae 0.0011095963418483734
training loss 3.1866736551320158e-06 mae 0.0013352358706898112
training loss 3.183991800529604e-06 mae 0.0013382730534125008
training loss 3.2879987679584195e-06 mae 0.0013599602944242295
training loss 3.341618150841723e-06 mae 0.0013701257244359462
Epoch 391, training: loss: 0.0000033, mae: 0.0013700 test: loss0.0000871, mae:0.0067753
training loss 4.198118404019624e-06 mae 0.0014928216114640236
training loss 3.019077377186976e-06 mae 0.001296141837705292
training loss 3.070255377336945e-06 mae 0.0013045692403736236
training loss 3.1976226325500437e-06 mae 0.0013364775257419476
training loss 3.290859707979946e-06 mae 0.001354483455484409
Epoch 392, training: loss: 0.0000033, mae: 0.0013536 test: loss0.0000881, mae:0.0068073
training loss 3.113684897471103e-06 mae 0.001229279674589634
training loss 3.3524493587452424e-06 mae 0.0013704433484806441
training loss 3.305272501408879e-06 mae 0.0013626336435888808
training loss 3.3548344361900535e-06 mae 0.0013749553970005726
training loss 3.3474363807550597e-06 mae 0.001371836601362669
Epoch 393, training: loss: 0.0000033, mae: 0.0013740 test: loss0.0000874, mae:0.0067810
training loss 3.2852015010575997e-06 mae 0.0013577267527580261
training loss 3.2453558805725912e-06 mae 0.0013372063666454278
training loss 3.272554543868728e-06 mae 0.001356088356368502
training loss 3.2360007001907357e-06 mae 0.0013427016986780707
training loss 3.3022397491237557e-06 mae 0.001352151046710932
Epoch 394, training: loss: 0.0000033, mae: 0.0013554 test: loss0.0000883, mae:0.0068210
training loss 3.722610927070491e-06 mae 0.0014557763934135437
training loss 3.446427477487591e-06 mae 0.0013773069265024629
training loss 3.3660915478591982e-06 mae 0.0013636260784443209
training loss 3.344654441664998e-06 mae 0.0013611805918225584
training loss 3.3171724236302935e-06 mae 0.0013631654779347987
Epoch 395, training: loss: 0.0000033, mae: 0.0013623 test: loss0.0000900, mae:0.0068921
training loss 1.7700002672427217e-06 mae 0.0010229836916550994
training loss 2.978346595441775e-06 mae 0.0012968341698961366
training loss 3.0525450143829974e-06 mae 0.001301976966229037
training loss 3.137904535086644e-06 mae 0.0013190768188827769
training loss 3.2135730976389718e-06 mae 0.00133640617126395
Epoch 396, training: loss: 0.0000032, mae: 0.0013387 test: loss0.0000873, mae:0.0067889
training loss 3.3034909847629024e-06 mae 0.0014292551204562187
training loss 3.010595164328954e-06 mae 0.0012939167140489992
training loss 3.276424970611308e-06 mae 0.0013595371977380005
training loss 3.247041935754542e-06 mae 0.0013457828737943381
training loss 3.300534493503244e-06 mae 0.001356868087244567
Epoch 397, training: loss: 0.0000033, mae: 0.0013579 test: loss0.0000888, mae:0.0068460
training loss 2.110672312483075e-06 mae 0.0011908604064956307
training loss 3.2288323814546123e-06 mae 0.0013334932440307505
training loss 3.171943877298484e-06 mae 0.0013268915567510196
training loss 3.283961819122863e-06 mae 0.0013513350026645012
training loss 3.2878751671797096e-06 mae 0.0013539448052654
Epoch 398, training: loss: 0.0000033, mae: 0.0013536 test: loss0.0000873, mae:0.0067788
training loss 2.72490365205158e-06 mae 0.0011482065310701728
training loss 2.955479245946324e-06 mae 0.0012916231792712326
training loss 3.104392327835434e-06 mae 0.0013159163023050092
training loss 3.228109835316787e-06 mae 0.0013373608054093617
training loss 3.274541826662906e-06 mae 0.0013499679909764193
Epoch 399, training: loss: 0.0000033, mae: 0.0013505 test: loss0.0000873, mae:0.0067773
current learning rate: 3.125e-05
training loss 3.4691129258135334e-06 mae 0.001428381190635264
training loss 2.8458157809533678e-06 mae 0.0012274176172236459
training loss 2.7675847942452597e-06 mae 0.0012009994813821992
training loss 2.7231011133593194e-06 mae 0.0012010280316578383
training loss 2.7491484103454392e-06 mae 0.001206857374688583
Epoch 400, training: loss: 0.0000027, mae: 0.0012052 test: loss0.0000881, mae:0.0068167
training loss 2.585133870525169e-06 mae 0.001233881339430809
training loss 2.724937905227039e-06 mae 0.0011681310305207528
training loss 2.6131741887553224e-06 mae 0.0011592154531730435
training loss 2.6075788317584064e-06 mae 0.0011575883878888382
training loss 2.579002369460414e-06 mae 0.0011542317861765257
Epoch 401, training: loss: 0.0000026, mae: 0.0011543 test: loss0.0000879, mae:0.0068115
training loss 2.6251407234667568e-06 mae 0.0011691624531522393
training loss 2.4051887744552036e-06 mae 0.0011176156169990551
training loss 2.5116198437647444e-06 mae 0.0011359382089974348
training loss 2.5352296470313177e-06 mae 0.0011473696132264545
training loss 2.558756336409137e-06 mae 0.0011458252385763731
Epoch 402, training: loss: 0.0000026, mae: 0.0011458 test: loss0.0000876, mae:0.0067959
training loss 2.3654738470213488e-06 mae 0.0011740656336769462
training loss 2.39232072848002e-06 mae 0.0011131744489402454
training loss 2.4512903924951035e-06 mae 0.0011149607610307856
training loss 2.5151269395767967e-06 mae 0.001130929322749845
training loss 2.5480791089929397e-06 mae 0.0011351959981413472
Epoch 403, training: loss: 0.0000026, mae: 0.0011403 test: loss0.0000891, mae:0.0068533
training loss 3.4701722597674234e-06 mae 0.0013184566050767899
training loss 2.603522235029445e-06 mae 0.0011618611124325907
training loss 2.542369322714642e-06 mae 0.0011450070363358761
training loss 2.558957351159535e-06 mae 0.001148086159169328
training loss 2.5863206355464156e-06 mae 0.0011536040686798367
Epoch 404, training: loss: 0.0000026, mae: 0.0011528 test: loss0.0000881, mae:0.0068106
training loss 2.419327074676403e-06 mae 0.0011644978076219559
training loss 2.5463551898664104e-06 mae 0.0011527002389615801
training loss 2.5398105854397765e-06 mae 0.0011458148468645422
training loss 2.5788984945597954e-06 mae 0.0011533199896610383
training loss 2.5722210372592748e-06 mae 0.0011529386540485625
Epoch 405, training: loss: 0.0000026, mae: 0.0011532 test: loss0.0000887, mae:0.0068442
training loss 3.102820301137399e-06 mae 0.0009497485007159412
training loss 2.4014455717923403e-06 mae 0.001101759237273797
training loss 2.4452764966671303e-06 mae 0.001116235163593019
training loss 2.4965356432921484e-06 mae 0.001135792295658183
training loss 2.5454250402757884e-06 mae 0.0011471643845842618
Epoch 406, training: loss: 0.0000026, mae: 0.0011494 test: loss0.0000881, mae:0.0068208
training loss 3.3314772736048326e-06 mae 0.0014030135935172439
training loss 2.483432104318305e-06 mae 0.001128831072527842
training loss 2.5129297615187695e-06 mae 0.0011321538275495028
training loss 2.53492972157819e-06 mae 0.0011359093343031078
training loss 2.542493203505745e-06 mae 0.001139350180150431
Epoch 407, training: loss: 0.0000026, mae: 0.0011430 test: loss0.0000884, mae:0.0068136
training loss 1.6639374962323927e-06 mae 0.000984936603344977
training loss 2.4747776670469023e-06 mae 0.0011206302602373647
training loss 2.5314839003030026e-06 mae 0.0011382077101359865
training loss 2.5185610831541035e-06 mae 0.0011366101722726414
training loss 2.5448902288280362e-06 mae 0.001146521584353229
Epoch 408, training: loss: 0.0000026, mae: 0.0011490 test: loss0.0000906, mae:0.0068973
training loss 3.0131780022202292e-06 mae 0.0011735716834664345
training loss 2.6947668852788446e-06 mae 0.0011691600449529346
training loss 2.5261218560994132e-06 mae 0.0011489449655591036
training loss 2.53121641432416e-06 mae 0.0011455409147079715
training loss 2.5548125343525123e-06 mae 0.001147476632196678
Epoch 409, training: loss: 0.0000026, mae: 0.0011492 test: loss0.0000889, mae:0.0068494
training loss 2.932432153102127e-06 mae 0.001256095594726503
training loss 2.5279317618893125e-06 mae 0.0011353892714753022
training loss 2.530030814237888e-06 mae 0.001139421578459811
training loss 2.5402841310855768e-06 mae 0.0011498393604242
training loss 2.5870469717029245e-06 mae 0.0011598800088334549
Epoch 410, training: loss: 0.0000026, mae: 0.0011604 test: loss0.0000886, mae:0.0068429
training loss 3.0748112749279244e-06 mae 0.001207678229548037
training loss 2.4883393721478674e-06 mae 0.001128132881907125
training loss 2.4721332713195103e-06 mae 0.001134543753999036
training loss 2.530644918119863e-06 mae 0.0011455461641909272
training loss 2.5564545885753438e-06 mae 0.0011514079779043644
Epoch 411, training: loss: 0.0000026, mae: 0.0011537 test: loss0.0000889, mae:0.0068419
training loss 2.266357569169486e-06 mae 0.0010836822912096977
training loss 2.5927525917259293e-06 mae 0.001154736872515915
training loss 2.493432996761658e-06 mae 0.0011307453957967244
training loss 2.4779759323731707e-06 mae 0.0011347376534080022
training loss 2.516892569605515e-06 mae 0.0011420297822729683
Epoch 412, training: loss: 0.0000025, mae: 0.0011394 test: loss0.0000894, mae:0.0068650
training loss 3.083310730289668e-06 mae 0.0012748850276693702
training loss 2.470436256504293e-06 mae 0.0011424669315673267
training loss 2.5223116349794988e-06 mae 0.0011484536969351893
training loss 2.5274025710138293e-06 mae 0.0011441584927837888
training loss 2.5335534214877183e-06 mae 0.0011431156502415738
Epoch 413, training: loss: 0.0000025, mae: 0.0011437 test: loss0.0000895, mae:0.0068519
training loss 2.5926283342414536e-06 mae 0.001250272965990007
training loss 2.4238871228411802e-06 mae 0.0011235801366103048
training loss 2.371162013737872e-06 mae 0.0011094382607912884
training loss 2.4640580930594657e-06 mae 0.0011284057966846701
training loss 2.5146517352924976e-06 mae 0.0011437200953887399
Epoch 414, training: loss: 0.0000025, mae: 0.0011457 test: loss0.0000897, mae:0.0068910
training loss 1.735601813379617e-06 mae 0.000916777818929404
training loss 2.4262535411182012e-06 mae 0.0011255316431725434
training loss 2.479847880029338e-06 mae 0.0011387045211665849
training loss 2.53156913112621e-06 mae 0.00114647603504335
training loss 2.539222450718287e-06 mae 0.0011543059320216842
Epoch 415, training: loss: 0.0000026, mae: 0.0011580 test: loss0.0000902, mae:0.0069141
training loss 2.9768932563456474e-06 mae 0.0013091039145365357
training loss 2.4256186625458283e-06 mae 0.0011080344077473618
training loss 2.4729408598414186e-06 mae 0.0011160206095785787
training loss 2.4719903662935988e-06 mae 0.0011237338732693668
training loss 2.4883821154377367e-06 mae 0.001130979775743031
Epoch 416, training: loss: 0.0000025, mae: 0.0011312 test: loss0.0000895, mae:0.0068742
training loss 2.7352853066986427e-06 mae 0.0011374428868293762
training loss 2.296843708969462e-06 mae 0.0010948192440064662
training loss 2.4134049449502987e-06 mae 0.0011113047545619021
training loss 2.4980232916542617e-06 mae 0.0011285944215009697
training loss 2.5256982910864127e-06 mae 0.0011360337676837192
Epoch 417, training: loss: 0.0000025, mae: 0.0011401 test: loss0.0000892, mae:0.0068518
training loss 2.4954624677775428e-06 mae 0.001097963540814817
training loss 2.337933270770438e-06 mae 0.0011078693696261184
training loss 2.464796415831125e-06 mae 0.0011251251625892988
training loss 2.472308733483458e-06 mae 0.0011283303819140359
training loss 2.4842897436320147e-06 mae 0.0011340651724288308
Epoch 418, training: loss: 0.0000025, mae: 0.0011368 test: loss0.0000893, mae:0.0068686
training loss 5.194209279579809e-06 mae 0.0011522602289915085
training loss 2.5751862677662194e-06 mae 0.0011292983019980144
training loss 2.490597967828374e-06 mae 0.0011237938289004974
training loss 2.4430332283728342e-06 mae 0.0011225012080922775
training loss 2.4654386325254117e-06 mae 0.001127465713227085
Epoch 419, training: loss: 0.0000025, mae: 0.0011294 test: loss0.0000892, mae:0.0068568
training loss 3.136218538202229e-06 mae 0.0011302527273073792
training loss 2.6303463501415755e-06 mae 0.0011580308737215018
training loss 2.5237813862968502e-06 mae 0.0011319303902391682
training loss 2.4570974304553953e-06 mae 0.001126119645700994
training loss 2.484383183183304e-06 mae 0.0011347381885172753
Epoch 420, training: loss: 0.0000025, mae: 0.0011331 test: loss0.0000893, mae:0.0068518
training loss 1.956243750100839e-06 mae 0.0010261976858600974
training loss 2.4101063454330804e-06 mae 0.001114145230602327
training loss 2.4751262101447785e-06 mae 0.0011357049294055828
training loss 2.4265127551538265e-06 mae 0.0011294722566319408
training loss 2.467362829764766e-06 mae 0.0011333739367633381
Epoch 421, training: loss: 0.0000025, mae: 0.0011348 test: loss0.0000894, mae:0.0068734
training loss 2.179628381782095e-06 mae 0.0010888358810916543
training loss 2.3033226482891453e-06 mae 0.0010985889741420452
training loss 2.347333244672135e-06 mae 0.0011000658712801658
training loss 2.382304805393766e-06 mae 0.0011123757800149008
training loss 2.482328674244431e-06 mae 0.001137766220250088
Epoch 422, training: loss: 0.0000025, mae: 0.0011360 test: loss0.0000893, mae:0.0068620
training loss 2.236932459709351e-06 mae 0.001025967882014811
training loss 2.390275128886167e-06 mae 0.0011251548284609962
training loss 2.3510730615595494e-06 mae 0.0011082612845982132
training loss 2.397428490879468e-06 mae 0.0011242690776776987
training loss 2.476662578812999e-06 mae 0.0011422553462258989
Epoch 423, training: loss: 0.0000025, mae: 0.0011381 test: loss0.0000911, mae:0.0069215
training loss 1.7008727581924177e-06 mae 0.0009589179535396397
training loss 2.3583564268169458e-06 mae 0.0010839718081257943
training loss 2.3538647549838363e-06 mae 0.001101550311288141
training loss 2.4194134363932987e-06 mae 0.001116113376800612
training loss 2.3996595614199355e-06 mae 0.0011166875865727436
Epoch 424, training: loss: 0.0000024, mae: 0.0011205 test: loss0.0000907, mae:0.0068957
training loss 2.0052832496730844e-06 mae 0.0010627222945913672
training loss 2.3345005323718903e-06 mae 0.0011098338542140872
training loss 2.4520971671993515e-06 mae 0.0011303930267885251
training loss 2.487839110291429e-06 mae 0.0011346273024763838
training loss 2.4722503745416274e-06 mae 0.0011292164580576792
Epoch 425, training: loss: 0.0000025, mae: 0.0011283 test: loss0.0000900, mae:0.0068922
training loss 1.650254944252083e-06 mae 0.0009203183581121266
training loss 2.3335813653722964e-06 mae 0.001082489041008932
training loss 2.301333513897399e-06 mae 0.0010892860470385764
training loss 2.3352592502336037e-06 mae 0.0010982878966448182
training loss 2.4093081540048655e-06 mae 0.00111352240948229
Epoch 426, training: loss: 0.0000024, mae: 0.0011151 test: loss0.0000893, mae:0.0068620
training loss 2.6624672955222195e-06 mae 0.001238485798239708
training loss 2.3059482168273286e-06 mae 0.0010965051330333831
training loss 2.3163926018674135e-06 mae 0.0010968506858352169
training loss 2.337213405005376e-06 mae 0.0010972265701506615
training loss 2.3640908794456223e-06 mae 0.0011042378902486273
Epoch 427, training: loss: 0.0000024, mae: 0.0011098 test: loss0.0000897, mae:0.0068894
training loss 1.7943901866601664e-06 mae 0.000974311784375459
training loss 2.388963861556156e-06 mae 0.0011112920576980443
training loss 2.37734295999214e-06 mae 0.0011069428078867125
training loss 2.41388834228627e-06 mae 0.0011187629579159353
training loss 2.4221586563409543e-06 mae 0.0011250365114844963
Epoch 428, training: loss: 0.0000024, mae: 0.0011283 test: loss0.0000897, mae:0.0068893
training loss 3.912944066541968e-06 mae 0.0013580633094534278
training loss 2.227060445322565e-06 mae 0.0010850645976104574
training loss 2.2639672550050625e-06 mae 0.0010983420427575117
training loss 2.347565131404188e-06 mae 0.0011142947894539562
training loss 2.4260683681832373e-06 mae 0.0011272001235435414
Epoch 429, training: loss: 0.0000024, mae: 0.0011294 test: loss0.0000898, mae:0.0068801
training loss 2.047340558419819e-06 mae 0.0010785771301016212
training loss 2.2629828645578668e-06 mae 0.001086690983570674
training loss 2.2827109086221647e-06 mae 0.0010909712348953996
training loss 2.3206418528924954e-06 mae 0.0011027058804660588
training loss 2.394658897391133e-06 mae 0.0011136517257647434
Epoch 430, training: loss: 0.0000024, mae: 0.0011138 test: loss0.0000898, mae:0.0068990
training loss 2.8291331091168104e-06 mae 0.0011094823712483048
training loss 2.3242957031114057e-06 mae 0.0011082148218216992
training loss 2.4618929969484724e-06 mae 0.0011224449153454732
training loss 2.429295038355303e-06 mae 0.0011189491711618587
training loss 2.4192173154078416e-06 mae 0.0011187226428135998
Epoch 431, training: loss: 0.0000024, mae: 0.0011195 test: loss0.0000899, mae:0.0068937
training loss 2.831630581567879e-06 mae 0.0012514634290710092
training loss 2.2903082762080456e-06 mae 0.001097665299597543
training loss 2.3621517082592435e-06 mae 0.0011011874735456805
training loss 2.326064757966716e-06 mae 0.0010945215044136074
training loss 2.3362838147925263e-06 mae 0.0010978761740116888
Epoch 432, training: loss: 0.0000023, mae: 0.0011022 test: loss0.0000897, mae:0.0068771
training loss 2.0777340523636667e-06 mae 0.001143429079093039
training loss 2.2805008025077803e-06 mae 0.00109230358308802
training loss 2.4165826203844157e-06 mae 0.0011221187870423916
training loss 2.3938073938148862e-06 mae 0.0011128539324349524
training loss 2.356676136503266e-06 mae 0.001106977864430374
Epoch 433, training: loss: 0.0000024, mae: 0.0011091 test: loss0.0000894, mae:0.0068653
training loss 2.0854749891441315e-06 mae 0.0010957792401313782
training loss 2.3724259977718568e-06 mae 0.0011119077749112073
training loss 2.330529998564071e-06 mae 0.0010994207436716794
training loss 2.2988644370542298e-06 mae 0.001091915090639292
training loss 2.3381158292574767e-06 mae 0.001103569273906412
Epoch 434, training: loss: 0.0000023, mae: 0.0011041 test: loss0.0000904, mae:0.0069072
training loss 3.5262482924736105e-06 mae 0.0013678312534466386
training loss 2.230554786383079e-06 mae 0.0010859855569387766
training loss 2.3076123253835385e-06 mae 0.0010904208854956568
training loss 2.30692415662147e-06 mae 0.0010961446759216985
training loss 2.349406516788885e-06 mae 0.0011038094033853187
Epoch 435, training: loss: 0.0000024, mae: 0.0011056 test: loss0.0000899, mae:0.0068865
training loss 3.599588126235176e-06 mae 0.0012732684845104814
training loss 2.3468395711996188e-06 mae 0.0010887723194672634
training loss 2.3010522706706724e-06 mae 0.001085324563883651
training loss 2.3175870009802233e-06 mae 0.0010914919895601046
training loss 2.35740720650051e-06 mae 0.0011041463921495263
Epoch 436, training: loss: 0.0000024, mae: 0.0011046 test: loss0.0000904, mae:0.0069182
training loss 2.4056146230577724e-06 mae 0.0011636344715952873
training loss 2.248333521938726e-06 mae 0.0010681746016238244
training loss 2.322475793199815e-06 mae 0.0010855395293859111
training loss 2.332691055534755e-06 mae 0.0010954825579965887
training loss 2.3457620558524202e-06 mae 0.0010995444692833462
Epoch 437, training: loss: 0.0000023, mae: 0.0010982 test: loss0.0000901, mae:0.0069017
training loss 1.4738207028131e-06 mae 0.0009767137235030532
training loss 2.0965719457700624e-06 mae 0.0010571502795095977
training loss 2.149160721246786e-06 mae 0.001054088169556701
training loss 2.220182200169391e-06 mae 0.0010716501077550728
training loss 2.328983987750072e-06 mae 0.0010943783337681256
Epoch 438, training: loss: 0.0000023, mae: 0.0010947 test: loss0.0000901, mae:0.0068955
training loss 2.920207180068246e-06 mae 0.0011837618658319116
training loss 2.317549778055371e-06 mae 0.0010912221132814155
training loss 2.2968554591475056e-06 mae 0.001087222318049741
training loss 2.3089437626373755e-06 mae 0.001096957983812996
training loss 2.3446001619479493e-06 mae 0.00110558582155905
Epoch 439, training: loss: 0.0000024, mae: 0.0011080 test: loss0.0000899, mae:0.0068915
training loss 3.038557451873203e-06 mae 0.0012361668050289154
training loss 2.3963311776902164e-06 mae 0.0010950014031693048
training loss 2.3179135848637297e-06 mae 0.0010966245715487934
training loss 2.3738790601355305e-06 mae 0.001113481487730358
training loss 2.380330597102485e-06 mae 0.0011162882507431203
Epoch 440, training: loss: 0.0000024, mae: 0.0011133 test: loss0.0000898, mae:0.0068815
training loss 2.549737473600544e-06 mae 0.001262233010493219
training loss 2.428373684725718e-06 mae 0.0011128771949705538
training loss 2.3404847219902847e-06 mae 0.0010958645091677952
training loss 2.3156133986067183e-06 mae 0.0010919736313559606
training loss 2.321230226415586e-06 mae 0.0010980693244060215
Epoch 441, training: loss: 0.0000023, mae: 0.0011048 test: loss0.0000901, mae:0.0068944
training loss 1.5016524912425666e-06 mae 0.000966150953900069
training loss 2.371428567415682e-06 mae 0.001106708666638416
training loss 2.363064147562673e-06 mae 0.001104865549939066
training loss 2.3445686806463207e-06 mae 0.0010995807327385623
training loss 2.3111497386653613e-06 mae 0.0010930123311509174
Epoch 442, training: loss: 0.0000023, mae: 0.0010930 test: loss0.0000904, mae:0.0069026
training loss 9.455629879084881e-07 mae 0.0008226819336414337
training loss 2.2470122958646454e-06 mae 0.0010828868652620885
training loss 2.2475556684950416e-06 mae 0.0010731576827797335
training loss 2.2874205742843847e-06 mae 0.001080562518246546
training loss 2.30740631390227e-06 mae 0.0010930368365761611
Epoch 443, training: loss: 0.0000023, mae: 0.0010950 test: loss0.0000908, mae:0.0069264
training loss 2.4862204099918017e-06 mae 0.0011001717066392303
training loss 2.347017642683104e-06 mae 0.0011090513492258744
training loss 2.3010224126800057e-06 mae 0.0010933500955499116
training loss 2.2911583987449285e-06 mae 0.0010914040096443366
training loss 2.2875673272409505e-06 mae 0.0010905403951391463
Epoch 444, training: loss: 0.0000023, mae: 0.0010892 test: loss0.0000901, mae:0.0069052
training loss 3.079851239817799e-06 mae 0.0012224424863234162
training loss 2.2499001354791445e-06 mae 0.001066942482401489
training loss 2.1773697000915684e-06 mae 0.0010583781986497345
training loss 2.2675860269759006e-06 mae 0.0010827713686855668
training loss 2.28714780555877e-06 mae 0.0010916211891951809
Epoch 445, training: loss: 0.0000023, mae: 0.0010890 test: loss0.0000901, mae:0.0069092
training loss 2.683947968762368e-06 mae 0.0012599859619513154
training loss 2.155820131038663e-06 mae 0.001074093439634524
training loss 2.2152545758253483e-06 mae 0.0010765232912283208
training loss 2.255118882908242e-06 mae 0.0010834660065755098
training loss 2.2880619549164215e-06 mae 0.0010899220881705623
Epoch 446, training: loss: 0.0000023, mae: 0.0010908 test: loss0.0000901, mae:0.0068972
training loss 1.41617601912003e-06 mae 0.0009151755948550999
training loss 2.245651361876048e-06 mae 0.0010733835892203973
training loss 2.3091816753756324e-06 mae 0.0010877584613223396
training loss 2.265695809756951e-06 mae 0.0010768074271676207
training loss 2.281458407719244e-06 mae 0.0010833816935275841
Epoch 447, training: loss: 0.0000023, mae: 0.0010805 test: loss0.0000910, mae:0.0069388
training loss 3.274338041592273e-06 mae 0.0013302341103553772
training loss 2.189970500159474e-06 mae 0.0010562285600577061
training loss 2.2504172663799847e-06 mae 0.0010781056134694135
training loss 2.2172986698042096e-06 mae 0.0010725571488690198
training loss 2.2407940101872924e-06 mae 0.0010783306985574573
Epoch 448, training: loss: 0.0000023, mae: 0.0010848 test: loss0.0000907, mae:0.0069269
training loss 2.6277148208464496e-06 mae 0.0010725650936365128
training loss 2.2298647440591454e-06 mae 0.0010726608018683014
training loss 2.1857057801510276e-06 mae 0.0010665057977375632
training loss 2.2847288595892458e-06 mae 0.0010898553038521709
training loss 2.282145685376916e-06 mae 0.0010887963279724963
Epoch 449, training: loss: 0.0000023, mae: 0.0010881 test: loss0.0000905, mae:0.0069214
training loss 3.424388296480174e-06 mae 0.0013827010989189148
training loss 1.9692178402603496e-06 mae 0.0010098496956440308
training loss 2.072532507523052e-06 mae 0.0010343616088335909
training loss 2.18404196569986e-06 mae 0.0010556041695553788
training loss 2.2353134892878355e-06 mae 0.0010714723699058952
Epoch 450, training: loss: 0.0000022, mae: 0.0010739 test: loss0.0000911, mae:0.0069393
training loss 2.205082410000614e-06 mae 0.0010561399394646287
training loss 2.2571644078697338e-06 mae 0.0010877873670930664
training loss 2.2143747505741897e-06 mae 0.0010761277753592352
training loss 2.24294927875235e-06 mae 0.0010828655637146914
training loss 2.2485007331684695e-06 mae 0.0010829712391659536
Epoch 451, training: loss: 0.0000023, mae: 0.0010835 test: loss0.0000906, mae:0.0069305
training loss 2.0517954908427782e-06 mae 0.001011220389045775
training loss 2.0617904408259066e-06 mae 0.0010243305495903625
training loss 2.135873446019499e-06 mae 0.0010467384691755885
training loss 2.1638289846478722e-06 mae 0.0010598746755943692
training loss 2.2059851530616796e-06 mae 0.0010648679708719117
Epoch 452, training: loss: 0.0000022, mae: 0.0010649 test: loss0.0000912, mae:0.0069533
training loss 1.9568451534723863e-06 mae 0.000990103930234909
training loss 2.2867855624794814e-06 mae 0.0010966324320464745
training loss 2.2397015756030318e-06 mae 0.0010776941100882345
training loss 2.288613663061532e-06 mae 0.001081514519574552
training loss 2.2829364661334353e-06 mae 0.001084835264213216
Epoch 453, training: loss: 0.0000023, mae: 0.0010879 test: loss0.0000908, mae:0.0069231
training loss 2.1081466456962517e-06 mae 0.0011146562173962593
training loss 2.2607123032328515e-06 mae 0.0010758584602644631
training loss 2.2052065746000064e-06 mae 0.001066462896386217
training loss 2.1563410933107267e-06 mae 0.0010588206786397126
training loss 2.2063283159329334e-06 mae 0.0010687957404283289
Epoch 454, training: loss: 0.0000022, mae: 0.0010743 test: loss0.0000911, mae:0.0069410
training loss 2.2497599729831563e-06 mae 0.0010567127028480172
training loss 2.05524817570846e-06 mae 0.001032602594361878
training loss 2.152384221481403e-06 mae 0.0010523728209105095
training loss 2.1662980820688743e-06 mae 0.0010595834811699169
training loss 2.21523913495644e-06 mae 0.0010719154897697305
Epoch 455, training: loss: 0.0000022, mae: 0.0010706 test: loss0.0000914, mae:0.0069646
training loss 1.8145971125704818e-06 mae 0.0010780220618471503
training loss 2.1855358307434745e-06 mae 0.0010588433611772813
training loss 2.1644821725491818e-06 mae 0.0010522409056377886
training loss 2.206418011251991e-06 mae 0.0010654486246277507
training loss 2.210920972041308e-06 mae 0.0010693511813855841
Epoch 456, training: loss: 0.0000022, mae: 0.0010671 test: loss0.0000912, mae:0.0069533
training loss 1.8252453628520016e-06 mae 0.0010077854385599494
training loss 2.1572030957530866e-06 mae 0.001066185618458571
training loss 2.0858813182838835e-06 mae 0.0010567057340654855
training loss 2.2073358638185678e-06 mae 0.001070640049489302
training loss 2.1950722878984698e-06 mae 0.00106772947019726
Epoch 457, training: loss: 0.0000022, mae: 0.0010665 test: loss0.0000912, mae:0.0069486
training loss 2.0719689928228036e-06 mae 0.0010735945543274283
training loss 2.2139007324214758e-06 mae 0.001077447128801734
training loss 2.1751099119241974e-06 mae 0.0010598364382945365
training loss 2.242434179877832e-06 mae 0.0010749319030915621
training loss 2.2178621897911393e-06 mae 0.0010718225581983843
Epoch 458, training: loss: 0.0000022, mae: 0.0010709 test: loss0.0000915, mae:0.0069696
training loss 2.8846632176282583e-06 mae 0.0011908890446648002
training loss 2.1653352434316496e-06 mae 0.0010509780703989024
training loss 2.1295240473888713e-06 mae 0.0010524225965289791
training loss 2.1945837399760294e-06 mae 0.0010650697436387223
training loss 2.2133313074905034e-06 mae 0.0010721531910335281
Epoch 459, training: loss: 0.0000022, mae: 0.0010698 test: loss0.0000913, mae:0.0069576
training loss 1.1982897376583423e-06 mae 0.0008248249068856239
training loss 1.957675220084781e-06 mae 0.0010080903040373
training loss 2.049796164285882e-06 mae 0.001043905172667623
training loss 2.1369831234846527e-06 mae 0.00105348097111117
training loss 2.170554432610173e-06 mae 0.0010622889382662763
Epoch 460, training: loss: 0.0000022, mae: 0.0010651 test: loss0.0000911, mae:0.0069451
training loss 1.490214344812557e-06 mae 0.0009439624845981598
training loss 2.0592926183977116e-06 mae 0.001025139862059743
training loss 2.1180420853762355e-06 mae 0.0010453884018409886
training loss 2.1650595023694394e-06 mae 0.0010574498824429824
training loss 2.157658285008735e-06 mae 0.0010530415940357926
Epoch 461, training: loss: 0.0000022, mae: 0.0010524 test: loss0.0000907, mae:0.0069320
training loss 2.540815557949827e-06 mae 0.0011368952691555023
training loss 2.045778240458543e-06 mae 0.0010229979399774298
training loss 2.1511329098550305e-06 mae 0.0010440782834038064
training loss 2.1965093036374575e-06 mae 0.0010620825750512742
training loss 2.162159326211287e-06 mae 0.0010566283547347264
Epoch 462, training: loss: 0.0000022, mae: 0.0010605 test: loss0.0000917, mae:0.0069712
training loss 1.6649532881274354e-06 mae 0.0008930964395403862
training loss 2.1112641438687166e-06 mae 0.0010265314348918552
training loss 2.168312846043764e-06 mae 0.0010520873659308815
training loss 2.173529408564539e-06 mae 0.001059699148272738
training loss 2.1949186175130445e-06 mae 0.001064778124306592
Epoch 463, training: loss: 0.0000022, mae: 0.0010634 test: loss0.0000919, mae:0.0069830
training loss 2.6040188458864577e-06 mae 0.001168277463875711
training loss 2.112179807753302e-06 mae 0.0010390480096890207
training loss 2.1486222671777753e-06 mae 0.0010501829893466562
training loss 2.169004991273389e-06 mae 0.0010497861191790722
training loss 2.1815751899760192e-06 mae 0.001057542554700552
Epoch 464, training: loss: 0.0000022, mae: 0.0010615 test: loss0.0000914, mae:0.0069519
training loss 1.4769813105885987e-06 mae 0.0008560757269151509
training loss 2.243981571957514e-06 mae 0.0010845215463846484
training loss 2.207380245550612e-06 mae 0.0010813587762433846
training loss 2.1927920500949513e-06 mae 0.0010745635337893239
training loss 2.1968273852992157e-06 mae 0.0010684565261622606
Epoch 465, training: loss: 0.0000022, mae: 0.0010693 test: loss0.0000912, mae:0.0069528
training loss 2.678678583833971e-06 mae 0.0012057684361934662
training loss 2.2135947699330164e-06 mae 0.0010616252856219515
training loss 2.158689957612674e-06 mae 0.0010469127100624952
training loss 2.1020599329641828e-06 mae 0.0010389527591216747
training loss 2.1312072843088644e-06 mae 0.0010464815732754258
Epoch 466, training: loss: 0.0000021, mae: 0.0010462 test: loss0.0000926, mae:0.0069725
training loss 2.314207677045488e-06 mae 0.0011407770216464996
training loss 2.1204862106042488e-06 mae 0.0010409252592545077
training loss 2.044687822953985e-06 mae 0.0010244791807450062
training loss 2.122381513932574e-06 mae 0.0010409622880266302
training loss 2.133154196291517e-06 mae 0.0010464085365613155
Epoch 467, training: loss: 0.0000021, mae: 0.0010473 test: loss0.0000912, mae:0.0069449
training loss 1.733917656565609e-06 mae 0.0010025579249486327
training loss 2.0935860858099104e-06 mae 0.0010365504970061864
training loss 2.106002383617026e-06 mae 0.0010484407389637932
training loss 2.0788417924414065e-06 mae 0.0010366791848475674
training loss 2.11469468260828e-06 mae 0.001040692946790204
Epoch 468, training: loss: 0.0000021, mae: 0.0010390 test: loss0.0000907, mae:0.0069123
training loss 2.3331131160375662e-06 mae 0.0010695162927731872
training loss 1.9325691059124184e-06 mae 0.0010024498173437431
training loss 1.982649476164201e-06 mae 0.0010120588755880551
training loss 2.076623217142949e-06 mae 0.0010316104659755154
training loss 2.1266123934905605e-06 mae 0.0010491011827021134
Epoch 469, training: loss: 0.0000021, mae: 0.0010485 test: loss0.0000920, mae:0.0069662
training loss 1.171310145764437e-06 mae 0.0007933508604764938
training loss 2.0928543739162193e-06 mae 0.0010170475305879814
training loss 2.029020126568379e-06 mae 0.0010143238163580843
training loss 2.084741622834582e-06 mae 0.0010318956561448263
training loss 2.1178140144260018e-06 mae 0.0010449106434930067
Epoch 470, training: loss: 0.0000021, mae: 0.0010465 test: loss0.0000917, mae:0.0069642
training loss 1.4612447785111726e-06 mae 0.0009176659514196217
training loss 2.0151432355466084e-06 mae 0.001023954391141659
training loss 2.0340152138604677e-06 mae 0.0010261434034702578
training loss 2.0820630050890184e-06 mae 0.0010419644863307715
training loss 2.1182345061227426e-06 mae 0.0010495677570678034
Epoch 471, training: loss: 0.0000021, mae: 0.0010524 test: loss0.0000916, mae:0.0069424
training loss 2.0880595457128948e-06 mae 0.00104567501693964
training loss 2.086093864622229e-06 mae 0.0010281839024494674
training loss 2.0838198562421107e-06 mae 0.001042623044046951
training loss 2.0870633267762647e-06 mae 0.001042185463813066
training loss 2.126948619485092e-06 mae 0.0010518297846707056
Epoch 472, training: loss: 0.0000021, mae: 0.0010476 test: loss0.0000924, mae:0.0069837
training loss 2.1204521090112394e-06 mae 0.001119556836783886
training loss 2.1200301636567663e-06 mae 0.0010326558281210996
training loss 2.064703458378098e-06 mae 0.0010268361120955036
training loss 2.0831108081107007e-06 mae 0.0010286189256457992
training loss 2.091887641152208e-06 mae 0.001035503147399303
Epoch 473, training: loss: 0.0000021, mae: 0.0010372 test: loss0.0000922, mae:0.0069808
training loss 2.488121253918507e-06 mae 0.0011872906470671296
training loss 2.0919181193390868e-06 mae 0.0010500623645973116
training loss 2.069405619151062e-06 mae 0.0010406375075413812
training loss 2.1108138653952213e-06 mae 0.001054094250881366
training loss 2.1190700746856747e-06 mae 0.0010517202016540725
Epoch 474, training: loss: 0.0000021, mae: 0.0010529 test: loss0.0000917, mae:0.0069660
training loss 3.336124564157217e-06 mae 0.0012899547582492232
training loss 2.1795555062899623e-06 mae 0.0010580401981760768
training loss 2.1075783713445835e-06 mae 0.001045937949588688
training loss 2.152581549393928e-06 mae 0.0010487117098385327
training loss 2.129646578674483e-06 mae 0.0010451241677270197
Epoch 475, training: loss: 0.0000021, mae: 0.0010435 test: loss0.0000920, mae:0.0069790
training loss 1.2243509672771324e-06 mae 0.0007914311136119068
training loss 2.0122056278470323e-06 mae 0.001024240269508286
training loss 2.0752177484195787e-06 mae 0.0010406951527203452
training loss 2.035160684803884e-06 mae 0.0010346045400731467
training loss 2.076122506088577e-06 mae 0.0010420777032786608
Epoch 476, training: loss: 0.0000021, mae: 0.0010420 test: loss0.0000925, mae:0.0069741
training loss 1.5297546269721352e-06 mae 0.0008524631266482174
training loss 2.0156606577823856e-06 mae 0.0010215693185398097
training loss 1.9501087974889076e-06 mae 0.0010108300265717772
training loss 2.0252788618677927e-06 mae 0.001026711920091253
training loss 2.074276168213285e-06 mae 0.0010376832359910604
Epoch 477, training: loss: 0.0000021, mae: 0.0010363 test: loss0.0000916, mae:0.0069532
training loss 2.977449184982106e-06 mae 0.0010618368396535516
training loss 1.9602205480070692e-06 mae 0.0009951801393089778
training loss 2.0510699510121436e-06 mae 0.0010109599322394125
training loss 2.097015291479657e-06 mae 0.0010337851462046924
training loss 2.073176519832089e-06 mae 0.0010306214466598346
Epoch 478, training: loss: 0.0000021, mae: 0.0010294 test: loss0.0000919, mae:0.0069656
training loss 3.2982668471959187e-06 mae 0.001273887581191957
training loss 2.140153875260176e-06 mae 0.0010536060515590304
training loss 2.1073735813906106e-06 mae 0.0010482311769766683
training loss 2.069000644924196e-06 mae 0.001039963755648682
training loss 2.0589778765538324e-06 mae 0.0010391121037508278
Epoch 479, training: loss: 0.0000021, mae: 0.0010407 test: loss0.0000916, mae:0.0069623
training loss 3.0309977319120662e-06 mae 0.001323168515227735
training loss 2.108484215354752e-06 mae 0.001039797603380958
training loss 2.007158210708801e-06 mae 0.0010200160109474891
training loss 2.0623152377294722e-06 mae 0.001031689290712828
training loss 2.068028211385867e-06 mae 0.0010347187496485438
Epoch 480, training: loss: 0.0000021, mae: 0.0010328 test: loss0.0000923, mae:0.0069911
training loss 2.7702963052433915e-06 mae 0.0012892009690403938
training loss 2.1884388196863876e-06 mae 0.0010578216261266933
training loss 2.031229948244962e-06 mae 0.0010353540834546601
training loss 2.031865873857366e-06 mae 0.0010336273213361214
training loss 2.0691603150968063e-06 mae 0.001037629945863927
Epoch 481, training: loss: 0.0000021, mae: 0.0010394 test: loss0.0000920, mae:0.0069753
training loss 1.3584603948402219e-06 mae 0.0009249874274246395
training loss 2.054269523742893e-06 mae 0.001019365913188998
training loss 2.0072947840576893e-06 mae 0.0010243981672072838
training loss 2.0450541798396104e-06 mae 0.0010289153483517422
training loss 2.0601746324422512e-06 mae 0.001034335129887367
Epoch 482, training: loss: 0.0000021, mae: 0.0010371 test: loss0.0000925, mae:0.0069887
training loss 1.2105668929507374e-06 mae 0.0008662131731398404
training loss 1.9927919685655722e-06 mae 0.00101985307821237
training loss 1.9649052756174597e-06 mae 0.001007837102181361
training loss 1.973177664703995e-06 mae 0.001009457755836948
training loss 2.0223131570280354e-06 mae 0.0010222533705588704
Epoch 483, training: loss: 0.0000020, mae: 0.0010218 test: loss0.0000938, mae:0.0070155
training loss 3.306709913886152e-06 mae 0.00129004439804703
training loss 1.8952671414844387e-06 mae 0.0009904269816116522
training loss 1.9525403512723125e-06 mae 0.0010060530241191535
training loss 1.9873743819715438e-06 mae 0.0010111923441321695
training loss 2.016646491465599e-06 mae 0.0010197516649484224
Epoch 484, training: loss: 0.0000020, mae: 0.0010220 test: loss0.0000916, mae:0.0069676
training loss 3.2936343359324383e-06 mae 0.001367544406093657
training loss 2.040700906036364e-06 mae 0.0010107601456362382
training loss 2.0127251461714634e-06 mae 0.0010134513561364886
training loss 2.049908501652415e-06 mae 0.001025083973242081
training loss 2.0796228573633028e-06 mae 0.0010360393534757342
Epoch 485, training: loss: 0.0000021, mae: 0.0010350 test: loss0.0000917, mae:0.0069813
training loss 1.1298012623228715e-06 mae 0.0007787657086737454
training loss 2.048911171061941e-06 mae 0.0010151434860959207
training loss 1.992552462511548e-06 mae 0.0010071344924906245
training loss 1.9821135404960687e-06 mae 0.001009448405605006
training loss 2.0039949099871833e-06 mae 0.0010170822879716525
Epoch 486, training: loss: 0.0000020, mae: 0.0010212 test: loss0.0000923, mae:0.0069850
training loss 1.7228462638740893e-06 mae 0.0009620816563256085
training loss 1.936734519199044e-06 mae 0.0009993331292278919
training loss 1.907575640663959e-06 mae 0.0010005678177008978
training loss 1.9563236749593283e-06 mae 0.0010046611134493298
training loss 2.009573954954484e-06 mae 0.0010216670720119832
Epoch 487, training: loss: 0.0000020, mae: 0.0010246 test: loss0.0000918, mae:0.0069565
training loss 3.123124542980804e-06 mae 0.0011730637634173036
training loss 1.985014783517103e-06 mae 0.0010094231658834309
training loss 2.0175592011231722e-06 mae 0.0010227142668347631
training loss 2.020222680416345e-06 mae 0.001024220173867013
training loss 2.051386820597539e-06 mae 0.001031175231160271
Epoch 488, training: loss: 0.0000020, mae: 0.0010301 test: loss0.0000925, mae:0.0070122
training loss 1.2883092495030724e-06 mae 0.0009031339432112873
training loss 1.9622923891980537e-06 mae 0.0010027223935478607
training loss 2.0333796594131135e-06 mae 0.0010214553227288522
training loss 2.01366906487647e-06 mae 0.001022582135120074
training loss 2.042912532164591e-06 mae 0.0010316811067015122
Epoch 489, training: loss: 0.0000020, mae: 0.0010298 test: loss0.0000921, mae:0.0069776
training loss 2.1017474409745773e-06 mae 0.0010332940146327019
training loss 1.9413251179150174e-06 mae 0.0009855833315454859
training loss 1.9139507325199163e-06 mae 0.0009908001342013766
training loss 1.9568664306040262e-06 mae 0.001003784530746769
training loss 1.969306247280943e-06 mae 0.001006590597162403
Epoch 490, training: loss: 0.0000020, mae: 0.0010077 test: loss0.0000918, mae:0.0069738
training loss 2.6250011160300346e-06 mae 0.0011673519620671868
training loss 1.806616362322458e-06 mae 0.0009626430041594981
training loss 1.8247909317981593e-06 mae 0.0009638469967078233
training loss 1.921314782256204e-06 mae 0.0009883978563693925
training loss 1.9873631039608067e-06 mae 0.0010076269251989458
Epoch 491, training: loss: 0.0000020, mae: 0.0010075 test: loss0.0000926, mae:0.0070018
training loss 3.2646094041410834e-06 mae 0.0012513721594586968
training loss 2.035023037612571e-06 mae 0.001010937120893276
training loss 1.990035299725307e-06 mae 0.001001207525081158
training loss 1.9896994600030476e-06 mae 0.0010092939652613553
training loss 1.962320969942626e-06 mae 0.0010042095076714392
Epoch 492, training: loss: 0.0000020, mae: 0.0010044 test: loss0.0000926, mae:0.0070081
training loss 1.5368456161013455e-06 mae 0.0009221701766364276
training loss 1.9006235414267663e-06 mae 0.0009864522098545349
training loss 1.9383601666770204e-06 mae 0.0010027373411305395
training loss 1.942701992493012e-06 mae 0.0010085910892527275
training loss 1.9974844522310853e-06 mae 0.0010148659675724956
Epoch 493, training: loss: 0.0000020, mae: 0.0010157 test: loss0.0000929, mae:0.0070095
training loss 1.1931915651075542e-06 mae 0.0008904030546545982
training loss 1.957314719122366e-06 mae 0.0009964164277976929
training loss 1.9549428434357186e-06 mae 0.0009949320240296648
training loss 1.9489098396800094e-06 mae 0.000999429356075503
training loss 1.9763501256592505e-06 mae 0.0010060623293367235
Epoch 494, training: loss: 0.0000020, mae: 0.0010088 test: loss0.0000919, mae:0.0069710
training loss 1.4395494645214058e-06 mae 0.0009516958962194622
training loss 1.8937436420053608e-06 mae 0.0009668919591087045
training loss 1.9286227629575905e-06 mae 0.000976749143729189
training loss 1.9160225274511014e-06 mae 0.0009840424639012484
training loss 1.964922302390032e-06 mae 0.0010050818632792366
Epoch 495, training: loss: 0.0000020, mae: 0.0010034 test: loss0.0000927, mae:0.0070128
training loss 1.7569612964507542e-06 mae 0.0010309228673577309
training loss 1.911204662974342e-06 mae 0.000996551585986334
training loss 1.9983554484317204e-06 mae 0.0010235982210522905
training loss 1.959496819155589e-06 mae 0.00101538704760644
training loss 1.995101172269644e-06 mae 0.0010183042998944607
Epoch 496, training: loss: 0.0000020, mae: 0.0010201 test: loss0.0000921, mae:0.0069913
training loss 1.2048354847138398e-06 mae 0.0008206606726162136
training loss 1.8503470075712338e-06 mae 0.0009683903544118592
training loss 1.8576549076996262e-06 mae 0.0009759287649562746
training loss 1.9170492148366865e-06 mae 0.00099099419058134
training loss 1.9128239427824263e-06 mae 0.0009892956867457285
Epoch 497, training: loss: 0.0000019, mae: 0.0009936 test: loss0.0000921, mae:0.0069808
training loss 1.4354810673467e-06 mae 0.0009021979640237987
training loss 1.809954528876094e-06 mae 0.0009809571316045727
training loss 1.8890578698488693e-06 mae 0.000989816565928741
training loss 1.9296130190676627e-06 mae 0.001001847186280007
training loss 1.990968487167827e-06 mae 0.001016594265096483
Epoch 498, training: loss: 0.0000020, mae: 0.0010157 test: loss0.0000926, mae:0.0070002
training loss 1.5334007912315428e-06 mae 0.0008714680443517864
training loss 1.9308432026228526e-06 mae 0.001001000347291576
training loss 1.950178408727573e-06 mae 0.001003953121086159
training loss 1.9550807932294903e-06 mae 0.0010051825056808538
training loss 1.9731436837059507e-06 mae 0.0010138928924399947
Epoch 499, training: loss: 0.0000020, mae: 0.0010159 test: loss0.0000923, mae:0.0069949
current learning rate: 1.5625e-05
training loss 1.5851873058636556e-06 mae 0.0009208545088768005
training loss 1.7526623457128793e-06 mae 0.0009337101307441937
training loss 1.7741434844268013e-06 mae 0.0009293981279145076
training loss 1.762301416049663e-06 mae 0.0009225900123043859
training loss 1.7436639055785372e-06 mae 0.0009186843097955908
Epoch 500, training: loss: 0.0000018, mae: 0.0009211 test: loss0.0000923, mae:0.0069916
training loss 1.0355530548622482e-06 mae 0.0008241422474384308
training loss 1.622899970680366e-06 mae 0.0008795635464290777
training loss 1.6641107876158357e-06 mae 0.0008919105080122332
training loss 1.6861036453899234e-06 mae 0.0008944247217867404
training loss 1.6974907741463705e-06 mae 0.0008964948909269156
Epoch 501, training: loss: 0.0000017, mae: 0.0008983 test: loss0.0000921, mae:0.0069791
training loss 2.0721433884318685e-06 mae 0.0009697449277155101
training loss 1.6626076678542577e-06 mae 0.0008892557588314598
training loss 1.6369484238692726e-06 mae 0.0008837520634746267
training loss 1.6684309831747523e-06 mae 0.0008860647787080937
training loss 1.6780340566488594e-06 mae 0.0008892353942075656
Epoch 502, training: loss: 0.0000017, mae: 0.0008911 test: loss0.0000928, mae:0.0070163
training loss 8.053885380832071e-07 mae 0.0005890115280635655
training loss 1.5490449850760485e-06 mae 0.0008577459398200554
training loss 1.580856104679373e-06 mae 0.000871040624345072
training loss 1.6014106069591108e-06 mae 0.0008774918449320985
training loss 1.7053687372198559e-06 mae 0.0008974528410448808
Epoch 503, training: loss: 0.0000017, mae: 0.0008960 test: loss0.0000925, mae:0.0069964
training loss 9.512850738246925e-07 mae 0.000730498053599149
training loss 1.6358765318567319e-06 mae 0.0008805022705072428
training loss 1.5907792040292915e-06 mae 0.000869740846412404
training loss 1.6252930874350642e-06 mae 0.0008777529034001866
training loss 1.6780834663059884e-06 mae 0.0008926000308525748
Epoch 504, training: loss: 0.0000017, mae: 0.0008918 test: loss0.0000937, mae:0.0070119
training loss 1.0231957503492595e-06 mae 0.0007190101896412671
training loss 1.65629450957924e-06 mae 0.0008831382478021231
training loss 1.678197788400626e-06 mae 0.0008909510175402432
training loss 1.680375608231276e-06 mae 0.0008884923631288354
training loss 1.6911692859732045e-06 mae 0.0008914076566200384
Epoch 505, training: loss: 0.0000017, mae: 0.0008918 test: loss0.0000926, mae:0.0070097
training loss 1.8333538491788204e-06 mae 0.0008817852358333766
training loss 1.7758308717038391e-06 mae 0.0009047637930979917
training loss 1.6590662701871976e-06 mae 0.0008813336884630568
training loss 1.717971152804288e-06 mae 0.0009035589042372558
training loss 1.6983649266645687e-06 mae 0.0008971762709639302
Epoch 506, training: loss: 0.0000017, mae: 0.0008969 test: loss0.0000924, mae:0.0069960
training loss 1.4455976042881957e-06 mae 0.0008203154429793358
training loss 1.7840569169083225e-06 mae 0.0009147997034768409
training loss 1.667564151943008e-06 mae 0.0008942872760725201
training loss 1.6813494270964107e-06 mae 0.0008944230561224832
training loss 1.6829645453560546e-06 mae 0.0008965379576458231
Epoch 507, training: loss: 0.0000017, mae: 0.0008924 test: loss0.0000928, mae:0.0070211
training loss 2.5324402486148756e-06 mae 0.0009254462202079594
training loss 1.673119454920322e-06 mae 0.0008821442549792573
training loss 1.6284842311254371e-06 mae 0.0008824298724227833
training loss 1.701133656309362e-06 mae 0.0008951418898962685
training loss 1.6860785041980808e-06 mae 0.0008952980151571407
Epoch 508, training: loss: 0.0000017, mae: 0.0008953 test: loss0.0000929, mae:0.0070179
training loss 2.0111053800064838e-06 mae 0.0009668137063272297
training loss 1.6583063809633256e-06 mae 0.0008881250168105551
training loss 1.606273055793029e-06 mae 0.0008765367444115924
training loss 1.6373294758390605e-06 mae 0.0008849114100973047
training loss 1.682078432247239e-06 mae 0.0008932236041665192
Epoch 509, training: loss: 0.0000017, mae: 0.0008946 test: loss0.0000927, mae:0.0070016
training loss 1.7126857301263954e-06 mae 0.0008795655448921025
training loss 1.5324219180224735e-06 mae 0.0008509675743898338
training loss 1.600367915214087e-06 mae 0.0008708592786656523
training loss 1.6151174376670868e-06 mae 0.0008793431735097964
training loss 1.6660665688896502e-06 mae 0.0008874420978393014
Epoch 510, training: loss: 0.0000017, mae: 0.0008897 test: loss0.0000931, mae:0.0070309
training loss 1.9250664990977384e-06 mae 0.0009577960590831935
training loss 1.6851997848593158e-06 mae 0.0008925179352837746
training loss 1.6734424114942932e-06 mae 0.000892471231054515
training loss 1.6894417439174667e-06 mae 0.0008929367385348244
training loss 1.6757183083658433e-06 mae 0.0008949090243636895
Epoch 511, training: loss: 0.0000017, mae: 0.0008975 test: loss0.0000934, mae:0.0070343
training loss 2.1742873741459334e-06 mae 0.0009877901757135987
training loss 1.6711111490675488e-06 mae 0.0008886130926126214
training loss 1.6234260943513236e-06 mae 0.0008760994446629862
training loss 1.6432727054296467e-06 mae 0.0008853361891761474
training loss 1.6784590829515707e-06 mae 0.0008918271563363388
Epoch 512, training: loss: 0.0000017, mae: 0.0008877 test: loss0.0000935, mae:0.0070409
training loss 1.7149873201560695e-06 mae 0.0009641063516028225
training loss 1.6377679821298266e-06 mae 0.0008804007164001757
training loss 1.72480116758055e-06 mae 0.0009027722021933683
training loss 1.7077112289033855e-06 mae 0.0008947607391939457
training loss 1.6759792052354573e-06 mae 0.0008916620689847358
Epoch 513, training: loss: 0.0000017, mae: 0.0008904 test: loss0.0000933, mae:0.0070258
training loss 1.2991386029170826e-06 mae 0.0009134206920862198
training loss 1.6872080372124009e-06 mae 0.0009102790340251636
training loss 1.6886476776625658e-06 mae 0.0009031793146421856
training loss 1.6664394683482663e-06 mae 0.0008914619745896352
training loss 1.6842968399932984e-06 mae 0.0008965889157018445
Epoch 514, training: loss: 0.0000017, mae: 0.0008949 test: loss0.0000933, mae:0.0070287
training loss 1.4147167348710354e-06 mae 0.0007860682089813054
training loss 1.6511036532767108e-06 mae 0.0008867906312933009
training loss 1.6022749502895966e-06 mae 0.0008766488195166433
training loss 1.69158503820124e-06 mae 0.0008962217406894
training loss 1.65670903878895e-06 mae 0.000886734773112989
Epoch 515, training: loss: 0.0000017, mae: 0.0008879 test: loss0.0000935, mae:0.0070280
training loss 7.3186316740248e-07 mae 0.000635667413007468
training loss 1.5964185021242458e-06 mae 0.0008728542164260265
training loss 1.6207128178303708e-06 mae 0.0008774917217727642
training loss 1.6218590010776216e-06 mae 0.0008756491534461251
training loss 1.6687659125415137e-06 mae 0.0008904684719569354
Epoch 516, training: loss: 0.0000017, mae: 0.0008877 test: loss0.0000931, mae:0.0070209
training loss 1.1773014421123662e-06 mae 0.000817789405118674
training loss 1.4959156672768016e-06 mae 0.00083696596922499
training loss 1.576590358410504e-06 mae 0.0008629926504558046
training loss 1.5870815536950938e-06 mae 0.0008632825588844469
training loss 1.633565917264736e-06 mae 0.0008781156841484802
Epoch 517, training: loss: 0.0000016, mae: 0.0008814 test: loss0.0000930, mae:0.0070275
training loss 1.862561816778907e-06 mae 0.0009761781548149884
training loss 1.5695772838305e-06 mae 0.0008781342892724949
training loss 1.6849277475296617e-06 mae 0.0008968441127015667
training loss 1.6315333301005644e-06 mae 0.0008887208320527332
training loss 1.6499124145769237e-06 mae 0.0008917555687323547
Epoch 518, training: loss: 0.0000017, mae: 0.0008934 test: loss0.0000932, mae:0.0070357
training loss 1.2133928066759836e-06 mae 0.0007873913273215294
training loss 1.67335772358438e-06 mae 0.0009013023250279765
training loss 1.6640507283227637e-06 mae 0.0009008216398473057
training loss 1.6828367253632955e-06 mae 0.0008990969269887996
training loss 1.6709399385863837e-06 mae 0.0008944288774773438
Epoch 519, training: loss: 0.0000017, mae: 0.0008946 test: loss0.0000930, mae:0.0070166
training loss 1.5664381862734444e-06 mae 0.0008363763918168843
training loss 1.5917167132226732e-06 mae 0.0008765336299570752
training loss 1.6438752396371089e-06 mae 0.0008868717820162303
training loss 1.6282755389011377e-06 mae 0.0008863697617950029
training loss 1.645216106049675e-06 mae 0.0008865859401782982
Epoch 520, training: loss: 0.0000016, mae: 0.0008849 test: loss0.0000932, mae:0.0070286
training loss 1.200431029246829e-06 mae 0.0008225385099649429
training loss 1.592797811518525e-06 mae 0.000871658074103442
training loss 1.6082603867001278e-06 mae 0.0008728572491344323
training loss 1.6357347344758136e-06 mae 0.0008814405558926872
training loss 1.6447123562780934e-06 mae 0.0008856789840255576
Epoch 521, training: loss: 0.0000016, mae: 0.0008849 test: loss0.0000931, mae:0.0070279
training loss 2.2640851966571063e-06 mae 0.0010499147465452552
training loss 1.5654962763334222e-06 mae 0.0008662167454923631
training loss 1.6056536178619377e-06 mae 0.0008722928411437956
training loss 1.596903209942776e-06 mae 0.0008712180262579529
training loss 1.63419932602633e-06 mae 0.0008823539533499463
Epoch 522, training: loss: 0.0000016, mae: 0.0008855 test: loss0.0000939, mae:0.0070526
training loss 1.7480997485108674e-06 mae 0.000886045687366277
training loss 1.466725667769959e-06 mae 0.0008331718500794881
training loss 1.5160928613919292e-06 mae 0.0008511672494933008
training loss 1.6236020689427958e-06 mae 0.0008762943410370128
training loss 1.637927387936366e-06 mae 0.0008828772292171245
Epoch 523, training: loss: 0.0000016, mae: 0.0008840 test: loss0.0000941, mae:0.0070621
training loss 1.6445752635263489e-06 mae 0.0009650969877839088
training loss 1.6342406752301035e-06 mae 0.0008863831525577196
training loss 1.6245068976705526e-06 mae 0.0008740701129438707
training loss 1.624534228922612e-06 mae 0.0008756831821843685
training loss 1.633369050405824e-06 mae 0.0008810964020302712
Epoch 524, training: loss: 0.0000016, mae: 0.0008826 test: loss0.0000929, mae:0.0070061
training loss 8.687716217536945e-07 mae 0.0006569325923919678
training loss 1.587127698320389e-06 mae 0.0008791636951797293
training loss 1.663570741368291e-06 mae 0.0008925872429569095
training loss 1.6849643097824617e-06 mae 0.0008967750939380204
training loss 1.6523213187597404e-06 mae 0.0008895096096293584
Epoch 525, training: loss: 0.0000016, mae: 0.0008880 test: loss0.0000929, mae:0.0070113
training loss 1.510228344159259e-06 mae 0.0009507124195806682
training loss 1.6618982151746477e-06 mae 0.0008812279328612574
training loss 1.6669381241149715e-06 mae 0.0008872351349829372
training loss 1.6320211298290613e-06 mae 0.0008808134126766824
training loss 1.638404492426057e-06 mae 0.0008816938393804899
Epoch 526, training: loss: 0.0000016, mae: 0.0008815 test: loss0.0000959, mae:0.0070798
training loss 2.1922903670201777e-06 mae 0.0010403416818007827
training loss 1.510308310589251e-06 mae 0.0008553625073046512
training loss 1.6218702078074084e-06 mae 0.0008732443404531493
training loss 1.6458948959179364e-06 mae 0.0008795890989497503
training loss 1.6457743669631253e-06 mae 0.0008882598925974983
Epoch 527, training: loss: 0.0000017, mae: 0.0008892 test: loss0.0000937, mae:0.0070513
training loss 9.147877904069901e-07 mae 0.0007069014827720821
training loss 1.5959437113380378e-06 mae 0.0008763268786281639
training loss 1.6758168640936303e-06 mae 0.0008896842021448996
training loss 1.6523859400292731e-06 mae 0.0008899025373863953
training loss 1.6476464894091367e-06 mae 0.0008868218926059553
Epoch 528, training: loss: 0.0000016, mae: 0.0008846 test: loss0.0000938, mae:0.0070637
training loss 1.0895593050008756e-06 mae 0.0007742978632450104
training loss 1.5069008563663297e-06 mae 0.0008522755758581206
training loss 1.5406005347655746e-06 mae 0.0008629015422112649
training loss 1.5628016436096831e-06 mae 0.0008659592659261993
training loss 1.6232553154772334e-06 mae 0.0008789978490502975
Epoch 529, training: loss: 0.0000016, mae: 0.0008778 test: loss0.0000937, mae:0.0070440
training loss 1.3118677770762588e-06 mae 0.0008038316736929119
training loss 1.6647064060135543e-06 mae 0.0008759692553248181
training loss 1.6207332944057011e-06 mae 0.0008688956950929493
training loss 1.655771292377979e-06 mae 0.0008788192702537486
training loss 1.626841797540473e-06 mae 0.0008745752170737554
Epoch 530, training: loss: 0.0000016, mae: 0.0008751 test: loss0.0000935, mae:0.0070395
training loss 2.4960183964140015e-06 mae 0.0010520328069105744
training loss 1.4532951996336446e-06 mae 0.0008413664832272948
training loss 1.5500746037492477e-06 mae 0.0008601248408391111
training loss 1.5744417904693214e-06 mae 0.0008642210622028206
training loss 1.6261390873187887e-06 mae 0.000879969390512291
Epoch 531, training: loss: 0.0000016, mae: 0.0008766 test: loss0.0000940, mae:0.0070573
training loss 1.7528964235680178e-06 mae 0.0009037898853421211
training loss 1.6182754275267176e-06 mae 0.000861686945427209
training loss 1.6092386107233647e-06 mae 0.0008807142085331206
training loss 1.6130783074425904e-06 mae 0.0008758038728781634
training loss 1.6024385637540752e-06 mae 0.0008796109419445785
Epoch 532, training: loss: 0.0000016, mae: 0.0008823 test: loss0.0000935, mae:0.0070481
training loss 3.2657092106092023e-06 mae 0.001257963478565216
training loss 1.6874044390266032e-06 mae 0.0008952103272153464
training loss 1.6104570696676677e-06 mae 0.0008753510722245022
training loss 1.5950654629638988e-06 mae 0.0008715785170954632
training loss 1.6030443616026659e-06 mae 0.0008775812037409612
Epoch 533, training: loss: 0.0000016, mae: 0.0008753 test: loss0.0000935, mae:0.0070442
training loss 6.742878326804203e-07 mae 0.0006362643907777965
training loss 1.5706047798969439e-06 mae 0.0008511575835976092
training loss 1.6045500895568983e-06 mae 0.0008608662116451945
training loss 1.5544499611853155e-06 mae 0.0008574517695109874
training loss 1.5984945863595425e-06 mae 0.0008703463911241041
Epoch 534, training: loss: 0.0000016, mae: 0.0008725 test: loss0.0000936, mae:0.0070459
training loss 8.584297574998345e-07 mae 0.0006948849186301231
training loss 1.5571981090907485e-06 mae 0.0008623768374616024
training loss 1.5965441284966837e-06 mae 0.0008644315086784632
training loss 1.6062484697788071e-06 mae 0.0008681169109702699
training loss 1.6039339661982931e-06 mae 0.0008763613979174608
Epoch 535, training: loss: 0.0000016, mae: 0.0008814 test: loss0.0000929, mae:0.0070151
training loss 1.432781232324487e-06 mae 0.0008850430021993816
training loss 1.5452743861144472e-06 mae 0.0008597573728811946
training loss 1.5768279667153739e-06 mae 0.0008663529573967405
training loss 1.609112379376874e-06 mae 0.0008748166593846356
training loss 1.59554338917352e-06 mae 0.0008716663404770979
Epoch 536, training: loss: 0.0000016, mae: 0.0008733 test: loss0.0000938, mae:0.0070536
training loss 1.00213526366133e-06 mae 0.0006881288136355579
training loss 1.5316679307533397e-06 mae 0.0008494403217827864
training loss 1.5679892878396645e-06 mae 0.0008592692032078334
training loss 1.600404047414052e-06 mae 0.0008707349978862267
training loss 1.59315455693923e-06 mae 0.0008665797774406584
Epoch 537, training: loss: 0.0000016, mae: 0.0008708 test: loss0.0000939, mae:0.0070612
training loss 1.2390755728119984e-06 mae 0.000770693935919553
training loss 1.570577719085835e-06 mae 0.0008684900011775979
training loss 1.5827194077489661e-06 mae 0.000872363100095772
training loss 1.5868016465365679e-06 mae 0.0008794307394187579
training loss 1.5928227457835642e-06 mae 0.0008729276941181046
Epoch 538, training: loss: 0.0000016, mae: 0.0008751 test: loss0.0000941, mae:0.0070710
training loss 1.0553476386121474e-06 mae 0.000781198323238641
training loss 1.4949134063446016e-06 mae 0.0008294709704761559
training loss 1.5385973231579809e-06 mae 0.000848700786309114
training loss 1.5917576009942785e-06 mae 0.0008607456685716153
training loss 1.5862354371391811e-06 mae 0.0008638552824424497
Epoch 539, training: loss: 0.0000016, mae: 0.0008635 test: loss0.0000945, mae:0.0070768
training loss 1.1851810768348514e-06 mae 0.0007615734939463437
training loss 1.4979273937658982e-06 mae 0.0008404175157421358
training loss 1.5261079786994587e-06 mae 0.0008493126485275456
training loss 1.5525348729001574e-06 mae 0.0008594012670434816
training loss 1.6017701784732889e-06 mae 0.0008722367764228207
Epoch 540, training: loss: 0.0000016, mae: 0.0008691 test: loss0.0000943, mae:0.0070781
training loss 1.7328007970718318e-06 mae 0.0009775640210136771
training loss 1.6802125539434913e-06 mae 0.0008721247694327258
training loss 1.6260791534306814e-06 mae 0.0008646307638754127
training loss 1.5863082776588083e-06 mae 0.0008640920933002962
training loss 1.5708408730900002e-06 mae 0.0008623491942567462
Epoch 541, training: loss: 0.0000016, mae: 0.0008653 test: loss0.0000940, mae:0.0070641
training loss 2.7048884021496633e-06 mae 0.0011403122916817665
training loss 1.460755278654402e-06 mae 0.0008357003942935491
training loss 1.514202884407334e-06 mae 0.0008491136578584808
training loss 1.5591324329201695e-06 mae 0.0008585878169096214
training loss 1.5705654240223547e-06 mae 0.0008610164183678465
Epoch 542, training: loss: 0.0000016, mae: 0.0008634 test: loss0.0000934, mae:0.0070321
training loss 1.6017320376704447e-06 mae 0.000882049382198602
training loss 1.5811800755513746e-06 mae 0.0008685295253206847
training loss 1.5582410889359072e-06 mae 0.0008617720081434023
training loss 1.514142369289751e-06 mae 0.0008485980018887006
training loss 1.563508694789335e-06 mae 0.0008612182660292447
Epoch 543, training: loss: 0.0000016, mae: 0.0008644 test: loss0.0000945, mae:0.0070772
training loss 1.2027173852402484e-06 mae 0.0008345057140104473
training loss 1.456892875342187e-06 mae 0.0008446624069976821
training loss 1.547101645104308e-06 mae 0.0008665131589992544
training loss 1.595404763589758e-06 mae 0.0008704732363004334
training loss 1.5937538276927592e-06 mae 0.0008709162260250499
Epoch 544, training: loss: 0.0000016, mae: 0.0008720 test: loss0.0000940, mae:0.0070603
training loss 1.549128342048789e-06 mae 0.0008524448494426906
training loss 1.436580228474829e-06 mae 0.0008325064680356458
training loss 1.489806373967332e-06 mae 0.0008404854682762874
training loss 1.5081614248436457e-06 mae 0.0008453754257157887
training loss 1.5630451819570183e-06 mae 0.0008577031189974268
Epoch 545, training: loss: 0.0000016, mae: 0.0008597 test: loss0.0000943, mae:0.0070735
training loss 1.1676660278681084e-06 mae 0.0007183619891293347
training loss 1.5561759618785624e-06 mae 0.0008508775762154484
training loss 1.5398620869764096e-06 mae 0.0008532204993742306
training loss 1.5770749361149238e-06 mae 0.0008625597457882536
training loss 1.576669324920623e-06 mae 0.0008638359126125221
Epoch 546, training: loss: 0.0000016, mae: 0.0008638 test: loss0.0000936, mae:0.0070412
training loss 2.657246341186692e-06 mae 0.0011293893912807107
training loss 1.4885588393108087e-06 mae 0.0008264674551730208
training loss 1.5718075779110259e-06 mae 0.0008464371929599877
training loss 1.5677910155980937e-06 mae 0.000851885254867992
training loss 1.5661039698775927e-06 mae 0.0008587146983528283
Epoch 547, training: loss: 0.0000016, mae: 0.0008618 test: loss0.0000942, mae:0.0070668
training loss 1.6672541960360832e-06 mae 0.000866525515448302
training loss 1.5433985276567736e-06 mae 0.0008451938126966651
training loss 1.5761492033904e-06 mae 0.0008533821547274839
training loss 1.574645122766605e-06 mae 0.0008640442678044913
training loss 1.565672969784045e-06 mae 0.0008604215954393337
Epoch 548, training: loss: 0.0000016, mae: 0.0008580 test: loss0.0000939, mae:0.0070573
training loss 1.599039478605846e-06 mae 0.0008063086424954236
training loss 1.5195171779485492e-06 mae 0.0008520838700682291
training loss 1.543325864061147e-06 mae 0.0008556029057973941
training loss 1.5484913356864881e-06 mae 0.0008594585485858168
training loss 1.553892136581766e-06 mae 0.000858822792151072
Epoch 549, training: loss: 0.0000016, mae: 0.0008590 test: loss0.0000940, mae:0.0070628
training loss 2.581959961389657e-06 mae 0.0010265171295031905
training loss 1.673082212449777e-06 mae 0.0008775572253701587
training loss 1.6036163767446596e-06 mae 0.0008701634337424789
training loss 1.5314656247967938e-06 mae 0.0008583518765891429
training loss 1.5480755193529881e-06 mae 0.0008576369913299545
Epoch 550, training: loss: 0.0000016, mae: 0.0008612 test: loss0.0000945, mae:0.0070655
training loss 2.3714505914540496e-06 mae 0.0010067675029858947
training loss 1.59656054268221e-06 mae 0.0008470834821791333
training loss 1.5361376566457095e-06 mae 0.000852224259697233
training loss 1.5809765283474372e-06 mae 0.0008612840894350646
training loss 1.5474798498138643e-06 mae 0.0008550022704994758
Epoch 551, training: loss: 0.0000016, mae: 0.0008563 test: loss0.0000945, mae:0.0070689
training loss 1.8095566929332563e-06 mae 0.0008728585089556873
training loss 1.5890408148526848e-06 mae 0.0008595603467969624
training loss 1.556526574359391e-06 mae 0.000856805611217376
training loss 1.5537526034876055e-06 mae 0.0008620985302556036
training loss 1.5801450861848334e-06 mae 0.0008688689712249668
Epoch 552, training: loss: 0.0000016, mae: 0.0008648 test: loss0.0000940, mae:0.0070513
training loss 1.4446150089497678e-06 mae 0.000834838196169585
training loss 1.4885959591779014e-06 mae 0.0008396915041421556
training loss 1.5673465373531667e-06 mae 0.0008558050555287683
training loss 1.5608391957218988e-06 mae 0.0008582554951106625
training loss 1.5652939291001858e-06 mae 0.00086164415642545
Epoch 553, training: loss: 0.0000016, mae: 0.0008602 test: loss0.0000938, mae:0.0070457
training loss 1.5775372048665304e-06 mae 0.0007423413917422295
training loss 1.4937276258773209e-06 mae 0.0008405509699300369
training loss 1.5076569921478551e-06 mae 0.0008413381401984264
training loss 1.529585881448326e-06 mae 0.000850629744533178
training loss 1.5416014297900333e-06 mae 0.0008593905683541993
Epoch 554, training: loss: 0.0000015, mae: 0.0008614 test: loss0.0000941, mae:0.0070609
training loss 1.7858141063697985e-06 mae 0.0008791837026365101
training loss 1.5017091006002905e-06 mae 0.0008360460564932405
training loss 1.524198397770432e-06 mae 0.0008516226506718246
training loss 1.5591541147317768e-06 mae 0.0008577715782047749
training loss 1.5426119493872756e-06 mae 0.0008573498045770794
Epoch 555, training: loss: 0.0000015, mae: 0.0008597 test: loss0.0000944, mae:0.0070702
training loss 2.0968504941265564e-06 mae 0.0010123070096597075
training loss 1.4387594390820026e-06 mae 0.0008235027632383884
training loss 1.496620795100314e-06 mae 0.0008406991457179334
training loss 1.4957887997117102e-06 mae 0.0008420876484820678
training loss 1.5209149117127423e-06 mae 0.0008486150920307676
Epoch 556, training: loss: 0.0000015, mae: 0.0008527 test: loss0.0000964, mae:0.0071097
training loss 1.7280264046348748e-06 mae 0.0009666699916124344
training loss 1.5422798658922452e-06 mae 0.0008560035094691843
training loss 1.5102347843497746e-06 mae 0.0008453439303332628
training loss 1.507388000487445e-06 mae 0.0008479673862580632
training loss 1.5368158366543333e-06 mae 0.0008578207361192183
Epoch 557, training: loss: 0.0000015, mae: 0.0008571 test: loss0.0000948, mae:0.0070914
training loss 1.3234139260021038e-06 mae 0.000797485641669482
training loss 1.5615697469572708e-06 mae 0.0008591196931186406
training loss 1.5762655140302868e-06 mae 0.0008591533567074061
training loss 1.5546566965588e-06 mae 0.0008527600539490482
training loss 1.5356294265509507e-06 mae 0.000850093923972466
Epoch 558, training: loss: 0.0000015, mae: 0.0008497 test: loss0.0000943, mae:0.0070757
training loss 1.1482053423605976e-06 mae 0.0007284432649612427
training loss 1.5038910530664925e-06 mae 0.0008415377666881565
training loss 1.5289187580051338e-06 mae 0.0008545549716662138
training loss 1.5270665423504272e-06 mae 0.0008526195159848062
training loss 1.5266756248530863e-06 mae 0.0008505518317676671
Epoch 559, training: loss: 0.0000015, mae: 0.0008506 test: loss0.0000951, mae:0.0071060
training loss 4.337518930697115e-07 mae 0.0004840344190597534
training loss 1.4207543162514162e-06 mae 0.0008196967560341399
training loss 1.482526388912501e-06 mae 0.0008400740386632338
training loss 1.4935035162143193e-06 mae 0.0008430972141028683
training loss 1.5206364236432105e-06 mae 0.000852594655400967
Epoch 560, training: loss: 0.0000015, mae: 0.0008543 test: loss0.0000944, mae:0.0070856
training loss 1.109645836550044e-06 mae 0.0007287782500497997
training loss 1.3884891123048946e-06 mae 0.0008132674585224365
training loss 1.4625326680333728e-06 mae 0.0008370208641569505
training loss 1.5184496319216727e-06 mae 0.0008522035131564003
training loss 1.539780651091408e-06 mae 0.0008558209991865602
Epoch 561, training: loss: 0.0000015, mae: 0.0008540 test: loss0.0000978, mae:0.0071098
training loss 9.284688076149905e-07 mae 0.0006490458617918193
training loss 1.401184168152409e-06 mae 0.0008185342634461967
training loss 1.415913399148056e-06 mae 0.000828630769842
training loss 1.4986082445806e-06 mae 0.0008408940803500153
training loss 1.5119777263925826e-06 mae 0.0008449565233495924
Epoch 562, training: loss: 0.0000015, mae: 0.0008479 test: loss0.0000939, mae:0.0070578
training loss 1.264153183910821e-06 mae 0.0007871656562201679
training loss 1.4318203634324091e-06 mae 0.0008248473271070157
training loss 1.487797043531521e-06 mae 0.0008415503169197844
training loss 1.4768449025705314e-06 mae 0.0008431811884835065
training loss 1.5076956064470531e-06 mae 0.0008471585733153447
Epoch 563, training: loss: 0.0000015, mae: 0.0008469 test: loss0.0000944, mae:0.0070775
training loss 9.569324674885138e-07 mae 0.000746624544262886
training loss 1.4824063517456434e-06 mae 0.0008471780730520977
training loss 1.5177447533748673e-06 mae 0.0008443895707801353
training loss 1.4845495608270298e-06 mae 0.0008386448249282527
training loss 1.5092090474856599e-06 mae 0.0008457904245440307
Epoch 564, training: loss: 0.0000015, mae: 0.0008452 test: loss0.0000949, mae:0.0070920
training loss 1.5241763549056486e-06 mae 0.0008781105279922485
training loss 1.5317962051727485e-06 mae 0.0008627097293570201
training loss 1.5475347244192914e-06 mae 0.0008503114931414475
training loss 1.5007115173330324e-06 mae 0.000840859589805296
training loss 1.506939152328251e-06 mae 0.0008438485144609724
Epoch 565, training: loss: 0.0000015, mae: 0.0008456 test: loss0.0000953, mae:0.0071083
training loss 1.5801821291461238e-06 mae 0.0008568065240979195
training loss 1.5965908235058105e-06 mae 0.0008633327430716772
training loss 1.5206274294748911e-06 mae 0.0008452671665311009
training loss 1.5074489554547876e-06 mae 0.0008436681661101071
training loss 1.5089953032218016e-06 mae 0.000844545790691633
Epoch 566, training: loss: 0.0000015, mae: 0.0008442 test: loss0.0000948, mae:0.0070979
training loss 1.500444341218099e-06 mae 0.0008169887587428093
training loss 1.4018025476438123e-06 mae 0.0008049989039279228
training loss 1.5210000622861878e-06 mae 0.0008396872879457799
training loss 1.4912661246148826e-06 mae 0.0008378745001179494
training loss 1.5151535919597955e-06 mae 0.0008451491658024451
Epoch 567, training: loss: 0.0000015, mae: 0.0008423 test: loss0.0000944, mae:0.0070736
training loss 1.5413716027978808e-06 mae 0.0009846457978710532
training loss 1.4569621931050456e-06 mae 0.0008284439647789387
training loss 1.4376139140292456e-06 mae 0.0008285305178078759
training loss 1.4955144202933346e-06 mae 0.0008441836427471643
training loss 1.5125312961685618e-06 mae 0.0008492674777024794
Epoch 568, training: loss: 0.0000015, mae: 0.0008512 test: loss0.0000944, mae:0.0070719
training loss 1.0310226343790418e-06 mae 0.0006590106640942395
training loss 1.629828728692148e-06 mae 0.0008564561676215747
training loss 1.5061803025565663e-06 mae 0.0008445939655406493
training loss 1.5083573354985259e-06 mae 0.0008473152176026774
training loss 1.505102228967456e-06 mae 0.0008461622235165973
Epoch 569, training: loss: 0.0000015, mae: 0.0008472 test: loss0.0000946, mae:0.0070875
training loss 1.286648512177635e-06 mae 0.0008332350407727063
training loss 1.4669645572533168e-06 mae 0.0008121460205966642
training loss 1.3986651056527107e-06 mae 0.0008054652353886992
training loss 1.4289432139974943e-06 mae 0.0008159842090033585
training loss 1.4736084585367728e-06 mae 0.0008271991985326455
Epoch 570, training: loss: 0.0000015, mae: 0.0008302 test: loss0.0000953, mae:0.0071115
training loss 1.361753106721153e-06 mae 0.0009010524372570217
training loss 1.489376380308671e-06 mae 0.0008262481629464993
training loss 1.4524283518641268e-06 mae 0.0008192377949898859
training loss 1.4885688434056066e-06 mae 0.0008363898081045031
training loss 1.4689971577870139e-06 mae 0.0008349249664165857
Epoch 571, training: loss: 0.0000015, mae: 0.0008369 test: loss0.0000948, mae:0.0070939
training loss 6.896065656292194e-07 mae 0.000688754313159734
training loss 1.4248372575393123e-06 mae 0.0008281220216304064
training loss 1.4498753000567595e-06 mae 0.0008326125075339829
training loss 1.4880053609940306e-06 mae 0.0008397884532001292
training loss 1.50554717266681e-06 mae 0.0008478422975862649
Epoch 572, training: loss: 0.0000015, mae: 0.0008471 test: loss0.0000947, mae:0.0070913
training loss 1.1845485232697683e-06 mae 0.0007738170097582042
training loss 1.4947038569819901e-06 mae 0.0008327495249663934
training loss 1.506362263124192e-06 mae 0.0008434160683986428
training loss 1.5200118063350467e-06 mae 0.0008462538047607323
training loss 1.4798269706146251e-06 mae 0.0008398950110370322
Epoch 573, training: loss: 0.0000015, mae: 0.0008431 test: loss0.0000948, mae:0.0070957
training loss 7.017037546575011e-07 mae 0.0006005761097185314
training loss 1.3314965307401113e-06 mae 0.0008017775504484626
training loss 1.39197504553846e-06 mae 0.0008132641986335194
training loss 1.4959727220519086e-06 mae 0.0008388197906037777
training loss 1.5092833322188055e-06 mae 0.0008474183726397839
Epoch 574, training: loss: 0.0000015, mae: 0.0008452 test: loss0.0000948, mae:0.0071016
training loss 2.289337089678156e-06 mae 0.0010200230171903968
training loss 1.4173854263871737e-06 mae 0.0008172871346366316
training loss 1.4012560883418358e-06 mae 0.0008056065238237675
training loss 1.4415746521093065e-06 mae 0.0008225390754658605
training loss 1.4746845547947561e-06 mae 0.0008358977649903009
Epoch 575, training: loss: 0.0000015, mae: 0.0008393 test: loss0.0000947, mae:0.0070927
training loss 1.6093541717054904e-06 mae 0.0009559439495205879
training loss 1.4564737866481563e-06 mae 0.0008305035846527007
training loss 1.4394262403710509e-06 mae 0.0008236007358058176
training loss 1.4686950054366273e-06 mae 0.0008340177707534454
training loss 1.4951446037422522e-06 mae 0.0008410288389327368
Epoch 576, training: loss: 0.0000015, mae: 0.0008392 test: loss0.0000952, mae:0.0071060
training loss 2.061223085547681e-06 mae 0.0009549219976179302
training loss 1.3684506479965458e-06 mae 0.0008161434807869443
training loss 1.5069989952433678e-06 mae 0.0008459478889887064
training loss 1.4989078651120773e-06 mae 0.0008416159806267316
training loss 1.4800903896634987e-06 mae 0.0008386826618755267
Epoch 577, training: loss: 0.0000015, mae: 0.0008358 test: loss0.0000947, mae:0.0070928
training loss 3.523295163176954e-06 mae 0.0009511178359389305
training loss 1.4690940488167941e-06 mae 0.0008346601503481175
training loss 1.4042147730977749e-06 mae 0.0008236824703116965
training loss 1.4869026348069924e-06 mae 0.0008428660645427196
training loss 1.4777900923266984e-06 mae 0.0008381409289672121
Epoch 578, training: loss: 0.0000015, mae: 0.0008382 test: loss0.0000950, mae:0.0070939
training loss 9.28776955788635e-07 mae 0.0006843879818916321
training loss 1.4682904428158791e-06 mae 0.0008199849644191932
training loss 1.4170687070238252e-06 mae 0.0008129764519433613
training loss 1.4562042446224393e-06 mae 0.0008228970099848059
training loss 1.475644199673518e-06 mae 0.000832996617415364
Epoch 579, training: loss: 0.0000015, mae: 0.0008290 test: loss0.0000952, mae:0.0071065
training loss 1.5994145314834896e-06 mae 0.0008916596998460591
training loss 1.4842392243183125e-06 mae 0.0008400528789863136
training loss 1.4436617678905232e-06 mae 0.0008203133377062137
training loss 1.4218274659831284e-06 mae 0.0008181054310854649
training loss 1.4597610289678212e-06 mae 0.0008288202899976157
Epoch 580, training: loss: 0.0000015, mae: 0.0008311 test: loss0.0000950, mae:0.0070985
training loss 8.47464946218679e-07 mae 0.0006923948531039059
training loss 1.4188789916760486e-06 mae 0.0008140916824249513
training loss 1.4009287315950651e-06 mae 0.0008115467130169637
training loss 1.4766302140122097e-06 mae 0.0008294496813246249
training loss 1.4593173711740678e-06 mae 0.0008268521644582791
Epoch 581, training: loss: 0.0000015, mae: 0.0008255 test: loss0.0000949, mae:0.0070848
training loss 1.94414792531461e-06 mae 0.0009236422483809292
training loss 1.372940136759865e-06 mae 0.000812040027870121
training loss 1.3963370004193629e-06 mae 0.0008174164346327065
training loss 1.4285006072338926e-06 mae 0.0008289634711910902
training loss 1.4623269172811411e-06 mae 0.0008359036995654005
Epoch 582, training: loss: 0.0000015, mae: 0.0008350 test: loss0.0000949, mae:0.0070853
training loss 1.782579943210294e-06 mae 0.0008904971182346344
training loss 1.4423980242188953e-06 mae 0.0008351389848280187
training loss 1.4199025926164626e-06 mae 0.0008284366490217941
training loss 1.4492909369154164e-06 mae 0.0008363019947166133
training loss 1.4742532680802878e-06 mae 0.0008420737877844915
Epoch 583, training: loss: 0.0000015, mae: 0.0008383 test: loss0.0000949, mae:0.0070973
training loss 2.0774020867975196e-06 mae 0.0009394651278853416
training loss 1.3733931464854574e-06 mae 0.0008254856414472065
training loss 1.4422512355966964e-06 mae 0.0008384059604008378
training loss 1.466868958979095e-06 mae 0.0008430958965281332
training loss 1.4708109764190636e-06 mae 0.0008393788735720503
Epoch 584, training: loss: 0.0000015, mae: 0.0008403 test: loss0.0000950, mae:0.0071001
training loss 1.1085278401878895e-06 mae 0.0007722172886133194
training loss 1.3817824716241772e-06 mae 0.0008096453578521808
training loss 1.4136966493280477e-06 mae 0.0008196387276293173
training loss 1.4193142296473824e-06 mae 0.0008210610218241266
training loss 1.4484540784211573e-06 mae 0.0008250270783090362
Epoch 585, training: loss: 0.0000014, mae: 0.0008251 test: loss0.0000945, mae:0.0070761
training loss 1.0929901463896385e-06 mae 0.0007318987627513707
training loss 1.3679010445836568e-06 mae 0.0008117983450967016
training loss 1.3801500301161641e-06 mae 0.000813622424715558
training loss 1.4148315551891162e-06 mae 0.0008204413365128618
training loss 1.4431751962583624e-06 mae 0.0008251310707839089
Epoch 586, training: loss: 0.0000015, mae: 0.0008280 test: loss0.0000954, mae:0.0071072
training loss 2.339841330467607e-06 mae 0.0010742032900452614
training loss 1.4228135571512192e-06 mae 0.0008299233889499424
training loss 1.4264310851617073e-06 mae 0.0008242516680436189
training loss 1.455185645377753e-06 mae 0.0008289070375146502
training loss 1.4501121108765448e-06 mae 0.000826879513660788
Epoch 587, training: loss: 0.0000014, mae: 0.0008265 test: loss0.0000957, mae:0.0071248
training loss 1.3752538734479458e-06 mae 0.0008342883666045964
training loss 1.3257758625656484e-06 mae 0.0007998360008202203
training loss 1.3522040494490737e-06 mae 0.0008021073489643563
training loss 1.3984014502069153e-06 mae 0.0008158551221435881
training loss 1.4327589598852053e-06 mae 0.000820396592567179
Epoch 588, training: loss: 0.0000014, mae: 0.0008201 test: loss0.0000951, mae:0.0071081
training loss 8.90372746198409e-07 mae 0.0006500433082692325
training loss 1.3396671335488233e-06 mae 0.0008031389408964004
training loss 1.368128979189207e-06 mae 0.000803765107181366
training loss 1.4273987929884597e-06 mae 0.0008234575507231057
training loss 1.44092175967454e-06 mae 0.0008258342705852355
Epoch 589, training: loss: 0.0000014, mae: 0.0008262 test: loss0.0000947, mae:0.0070804
training loss 2.5284132334491005e-06 mae 0.0009684348478913307
training loss 1.4298237049076185e-06 mae 0.0008173912510221057
training loss 1.428818081021205e-06 mae 0.0008112354824171947
training loss 1.4333395747357911e-06 mae 0.0008212425360542458
training loss 1.4317353357588906e-06 mae 0.0008240673286766185
Epoch 590, training: loss: 0.0000014, mae: 0.0008253 test: loss0.0000964, mae:0.0071377
training loss 1.5354220295193954e-06 mae 0.0009443384478799999
training loss 1.186817270063686e-06 mae 0.0007503436063416302
training loss 1.355319171233361e-06 mae 0.0007914540238014541
training loss 1.4228067685441741e-06 mae 0.0008075710201777814
training loss 1.4398544027387033e-06 mae 0.0008198252641039889
Epoch 591, training: loss: 0.0000014, mae: 0.0008216 test: loss0.0000959, mae:0.0071301
training loss 1.478005856370146e-06 mae 0.0007641520351171494
training loss 1.3346221697818194e-06 mae 0.00078076898124909
training loss 1.401231192612793e-06 mae 0.0008091959747310619
training loss 1.4299014668005762e-06 mae 0.0008173880108712328
training loss 1.4229628856360228e-06 mae 0.0008164946217465202
Epoch 592, training: loss: 0.0000014, mae: 0.0008210 test: loss0.0000951, mae:0.0070932
training loss 2.063790589090786e-06 mae 0.0009721365640871227
training loss 1.4981349374474853e-06 mae 0.000839193761303071
training loss 1.4928360418018722e-06 mae 0.0008318663452576735
training loss 1.4588477594864026e-06 mae 0.0008302229616192742
training loss 1.4373117768168442e-06 mae 0.0008285059040500339
Epoch 593, training: loss: 0.0000014, mae: 0.0008282 test: loss0.0000951, mae:0.0070974
training loss 2.4346416012122063e-06 mae 0.00093049556016922
training loss 1.3177757521179064e-06 mae 0.0007878724866838869
training loss 1.4027626792598633e-06 mae 0.0008098072410413608
training loss 1.4142827658601978e-06 mae 0.0008157362732789571
training loss 1.4228112001040054e-06 mae 0.0008164215042367019
Epoch 594, training: loss: 0.0000014, mae: 0.0008157 test: loss0.0000955, mae:0.0071151
training loss 7.018709879957896e-07 mae 0.0006501541356556118
training loss 1.4030178565953262e-06 mae 0.0008074871593100182
training loss 1.4571094147237933e-06 mae 0.0008253104757692776
training loss 1.4123496875740873e-06 mae 0.0008116892403275369
training loss 1.4040474446060516e-06 mae 0.0008133172957615846
Epoch 595, training: loss: 0.0000014, mae: 0.0008192 test: loss0.0000955, mae:0.0071188
training loss 1.1827919479401316e-06 mae 0.0008196597918868065
training loss 1.4086520336069738e-06 mae 0.0008085881994452838
training loss 1.4452513096780465e-06 mae 0.0008208786845115004
training loss 1.439934538592637e-06 mae 0.0008229515939297163
training loss 1.4400706034301047e-06 mae 0.0008234883231731748
Epoch 596, training: loss: 0.0000014, mae: 0.0008231 test: loss0.0000950, mae:0.0070970
training loss 1.1343869346092106e-06 mae 0.000723357021342963
training loss 1.330897518135887e-06 mae 0.0008027400472201407
training loss 1.3711204445880326e-06 mae 0.0008118864356904372
training loss 1.3778786559584923e-06 mae 0.0008114565999188542
training loss 1.402196505524666e-06 mae 0.0008146414003017551
Epoch 597, training: loss: 0.0000014, mae: 0.0008176 test: loss0.0000952, mae:0.0071005
training loss 1.081167283700779e-06 mae 0.0007017118041403592
training loss 1.3473185185384684e-06 mae 0.0007889470126589434
training loss 1.3968893690256623e-06 mae 0.000813097254451512
training loss 1.3852092796250806e-06 mae 0.0008101658760693345
training loss 1.4032808785803582e-06 mae 0.0008141215976364727
Epoch 598, training: loss: 0.0000014, mae: 0.0008164 test: loss0.0000948, mae:0.0070820
training loss 1.0231096894131042e-06 mae 0.000679846212733537
training loss 1.368922870797713e-06 mae 0.000798291471951148
training loss 1.374265904718017e-06 mae 0.0007968427213416021
training loss 1.3952563220990012e-06 mae 0.0008078658500754135
training loss 1.417962820584142e-06 mae 0.0008160878889105138
Epoch 599, training: loss: 0.0000014, mae: 0.0008150 test: loss0.0000953, mae:0.0071132
current learning rate: 7.8125e-06
training loss 8.864819278642244e-07 mae 0.0007374060223810375
training loss 1.3246008009838416e-06 mae 0.0007915954886223463
training loss 1.3057177311296044e-06 mae 0.0007835155131645721
training loss 1.3357722476734882e-06 mae 0.0007796296664815864
training loss 1.3220778870520946e-06 mae 0.0007753486282294107
Epoch 600, training: loss: 0.0000013, mae: 0.0007769 test: loss0.0000951, mae:0.0070990
training loss 1.629414896342496e-06 mae 0.0008891820907592773
training loss 1.4365317778195756e-06 mae 0.0007904055779434592
training loss 1.3218040823905523e-06 mae 0.0007664378148060992
training loss 1.3333669263097694e-06 mae 0.0007695247897560655
training loss 1.318744621209648e-06 mae 0.0007680562505526331
Epoch 601, training: loss: 0.0000013, mae: 0.0007668 test: loss0.0000952, mae:0.0071001
training loss 6.805760790484783e-07 mae 0.000621529936324805
training loss 1.325461435980662e-06 mae 0.0007544878285889533
training loss 1.3315936528913274e-06 mae 0.0007685448315644652
training loss 1.3452926710694095e-06 mae 0.0007693825980774834
training loss 1.325237593534181e-06 mae 0.0007685040271741367
Epoch 602, training: loss: 0.0000013, mae: 0.0007659 test: loss0.0000953, mae:0.0071044
training loss 1.2151826922490727e-06 mae 0.0007400928880088031
training loss 1.2514875200118767e-06 mae 0.0007470508834219299
training loss 1.3346851821241597e-06 mae 0.0007658015419659896
training loss 1.309566434560147e-06 mae 0.0007552794207408653
training loss 1.3121159179715973e-06 mae 0.0007587485294846993
Epoch 603, training: loss: 0.0000013, mae: 0.0007607 test: loss0.0000957, mae:0.0071231
training loss 1.0608226830299827e-06 mae 0.0007047231192700565
training loss 1.2108411891965846e-06 mae 0.0007335144864377
training loss 1.2784115653522522e-06 mae 0.0007473739534893104
training loss 1.3221617004941237e-06 mae 0.0007619781952369838
training loss 1.3165809058822321e-06 mae 0.0007641274047336678
Epoch 604, training: loss: 0.0000013, mae: 0.0007632 test: loss0.0000956, mae:0.0071219
training loss 1.572182213749329e-06 mae 0.0008468252490274608
training loss 1.1623957089243198e-06 mae 0.0007286316283759388
training loss 1.2671605657483315e-06 mae 0.0007473784081043105
training loss 1.3001032575690462e-06 mae 0.000761896532779831
training loss 1.3070078179015266e-06 mae 0.0007615453120267753
Epoch 605, training: loss: 0.0000013, mae: 0.0007619 test: loss0.0000951, mae:0.0070980
training loss 1.1733519613699173e-06 mae 0.0007192601333372295
training loss 1.2827281909944849e-06 mae 0.0007525948389871594
training loss 1.2901545011289383e-06 mae 0.0007547970762070598
training loss 1.2950224444713535e-06 mae 0.0007589388895465294
training loss 1.3150876510445947e-06 mae 0.0007643251328275012
Epoch 606, training: loss: 0.0000013, mae: 0.0007635 test: loss0.0000953, mae:0.0071075
training loss 1.2679596466114162e-06 mae 0.0007941843941807747
training loss 1.203060604131465e-06 mae 0.000730190020369585
training loss 1.2487024799602383e-06 mae 0.0007437056035384315
training loss 1.275015246759697e-06 mae 0.0007532073546429196
training loss 1.3087294768076244e-06 mae 0.0007624658700935914
Epoch 607, training: loss: 0.0000013, mae: 0.0007633 test: loss0.0000951, mae:0.0070970
training loss 1.8349725223743008e-06 mae 0.000805094838142395
training loss 1.235799827019732e-06 mae 0.0007428915237587895
training loss 1.2635765254362742e-06 mae 0.0007550041401634304
training loss 1.2948179308850077e-06 mae 0.0007636775927608895
training loss 1.302968816268654e-06 mae 0.000761537289215641
Epoch 608, training: loss: 0.0000013, mae: 0.0007608 test: loss0.0000955, mae:0.0071164
training loss 1.3497588042810094e-06 mae 0.0007958064670674503
training loss 1.3104520020060948e-06 mae 0.0007575370106553915
training loss 1.2263402559053455e-06 mae 0.0007411243891375745
training loss 1.2655771384692074e-06 mae 0.0007543143408067694
training loss 1.2956084540959473e-06 mae 0.0007614860506913625
Epoch 609, training: loss: 0.0000013, mae: 0.0007652 test: loss0.0000959, mae:0.0071360
training loss 5.466271773002518e-07 mae 0.0005124686285853386
training loss 1.2214485517840026e-06 mae 0.0007408031898176336
training loss 1.2951150785563121e-06 mae 0.0007538034826733798
training loss 1.3065173550782399e-06 mae 0.0007622478067618949
training loss 1.310252261094352e-06 mae 0.0007643230938218277
Epoch 610, training: loss: 0.0000013, mae: 0.0007650 test: loss0.0000960, mae:0.0071283
training loss 2.1256687432469334e-06 mae 0.0008836978231556714
training loss 1.3780389964472753e-06 mae 0.000770603065200917
training loss 1.3133537403055105e-06 mae 0.0007647314119858129
training loss 1.2739239918380981e-06 mae 0.0007571503976807119
training loss 1.290401044189681e-06 mae 0.0007601249405583689
Epoch 611, training: loss: 0.0000013, mae: 0.0007633 test: loss0.0000959, mae:0.0071280
training loss 1.25659119021293e-06 mae 0.0007674296502955258
training loss 1.2784545088281588e-06 mae 0.0007456300047445386
training loss 1.3179787552727783e-06 mae 0.0007647716447176172
training loss 1.3044720255330686e-06 mae 0.0007644897925864831
training loss 1.3074228527714603e-06 mae 0.0007619513016291407
Epoch 612, training: loss: 0.0000013, mae: 0.0007618 test: loss0.0000959, mae:0.0071337
training loss 1.097729182220064e-06 mae 0.0006478379364125431
training loss 1.2874887192101241e-06 mae 0.0007562774995926257
training loss 1.2644470674817653e-06 mae 0.0007481879527602198
training loss 1.2642526544353537e-06 mae 0.000751804988885249
training loss 1.2940897488194304e-06 mae 0.0007583946182186348
Epoch 613, training: loss: 0.0000013, mae: 0.0007612 test: loss0.0000962, mae:0.0071470
training loss 6.63026014535717e-07 mae 0.0006259481306187809
training loss 1.1584016016397572e-06 mae 0.0007343116380712567
training loss 1.2911558311577326e-06 mae 0.000758359823053205
training loss 1.2677590491978283e-06 mae 0.0007522457721797343
training loss 1.3137649711464278e-06 mae 0.0007655326413414654
Epoch 614, training: loss: 0.0000013, mae: 0.0007618 test: loss0.0000962, mae:0.0071431
training loss 1.4399314522961504e-06 mae 0.0007603680714964867
training loss 1.293975966028287e-06 mae 0.0007424048951589595
training loss 1.2654460387900805e-06 mae 0.0007502468963301197
training loss 1.3080490461730216e-06 mae 0.0007586148424869689
training loss 1.3061750708649152e-06 mae 0.0007610459532820392
Epoch 615, training: loss: 0.0000013, mae: 0.0007594 test: loss0.0000959, mae:0.0071367
training loss 1.0051934395960416e-06 mae 0.0007064917008392513
training loss 1.2159949501527116e-06 mae 0.0007408144221841996
training loss 1.2104272213541172e-06 mae 0.0007455156963908735
training loss 1.273966391193649e-06 mae 0.0007562829398963329
training loss 1.281500243889449e-06 mae 0.0007559224613942206
Epoch 616, training: loss: 0.0000013, mae: 0.0007601 test: loss0.0000954, mae:0.0071152
training loss 9.387105706082366e-07 mae 0.0007059471681714058
training loss 1.2722423022358834e-06 mae 0.0007501775204824902
training loss 1.2761405309114396e-06 mae 0.0007536741829993626
training loss 1.2821680083097877e-06 mae 0.0007591483893261492
training loss 1.2938576189618055e-06 mae 0.0007601389238629395
Epoch 617, training: loss: 0.0000013, mae: 0.0007633 test: loss0.0000960, mae:0.0071291
training loss 2.7213343400944723e-06 mae 0.0009684006799943745
training loss 1.222228380517182e-06 mae 0.0007322629852056066
training loss 1.2100723619603224e-06 mae 0.0007362256632278682
training loss 1.2494613401368829e-06 mae 0.0007449559584438853
training loss 1.3026326753978498e-06 mae 0.0007591237683562722
Epoch 618, training: loss: 0.0000013, mae: 0.0007571 test: loss0.0000956, mae:0.0071199
training loss 8.35189382542012e-07 mae 0.0006549898535013199
training loss 1.2301960099666405e-06 mae 0.0007349184004352519
training loss 1.2426694053878647e-06 mae 0.0007412395518420652
training loss 1.3029887290805157e-06 mae 0.000753771635512544
training loss 1.2973732390575632e-06 mae 0.0007558701135940963
Epoch 619, training: loss: 0.0000013, mae: 0.0007566 test: loss0.0000960, mae:0.0071325
training loss 1.638077833376883e-06 mae 0.0008677151054143906
training loss 1.3071621802229105e-06 mae 0.0007645389003514805
training loss 1.2852353448255203e-06 mae 0.0007610468707960281
training loss 1.2713142835622753e-06 mae 0.0007579858173517546
training loss 1.2784004387759157e-06 mae 0.0007585441671654728
Epoch 620, training: loss: 0.0000013, mae: 0.0007621 test: loss0.0000956, mae:0.0071210
training loss 9.74440922618669e-07 mae 0.0006910215015523136
training loss 1.271091532856315e-06 mae 0.000730272926910616
training loss 1.2989964206423276e-06 mae 0.0007454357138779567
training loss 1.2570412790385945e-06 mae 0.0007453361061241305
training loss 1.2918432602830516e-06 mae 0.0007545887808250588
Epoch 621, training: loss: 0.0000013, mae: 0.0007547 test: loss0.0000960, mae:0.0071315
training loss 8.426282533946505e-07 mae 0.0006578353350050747
training loss 1.2726774530967627e-06 mae 0.0007626780333892243
training loss 1.2633606780303188e-06 mae 0.0007474914424831397
training loss 1.2707510665454686e-06 mae 0.0007500413362713938
training loss 1.288176668308845e-06 mae 0.0007557290060612469
Epoch 622, training: loss: 0.0000013, mae: 0.0007564 test: loss0.0000963, mae:0.0071478
training loss 1.2829141269321553e-06 mae 0.0007698684930801392
training loss 1.2094567721885372e-06 mae 0.0007320513273132782
training loss 1.2478234755618165e-06 mae 0.0007383279680472683
training loss 1.2770222828511044e-06 mae 0.0007512972584928676
training loss 1.2931654407509823e-06 mae 0.0007571088848345966
Epoch 623, training: loss: 0.0000013, mae: 0.0007593 test: loss0.0000961, mae:0.0071365
training loss 8.455661486550525e-07 mae 0.0006252620369195938
training loss 1.2064979696957632e-06 mae 0.000737826624090838
training loss 1.2646919511814417e-06 mae 0.0007472057515031444
training loss 1.2698701544839976e-06 mae 0.0007502822915095193
training loss 1.2927633608072919e-06 mae 0.0007576200957884964
Epoch 624, training: loss: 0.0000013, mae: 0.0007580 test: loss0.0000960, mae:0.0071376
training loss 2.2165193058754085e-06 mae 0.0010578896617516875
training loss 1.2038409450127015e-06 mae 0.0007261917327174587
training loss 1.224297320034269e-06 mae 0.0007350820112088232
training loss 1.2497772018871773e-06 mae 0.0007462880662056487
training loss 1.2874387006031073e-06 mae 0.0007576773140639698
Epoch 625, training: loss: 0.0000013, mae: 0.0007589 test: loss0.0000957, mae:0.0071149
training loss 1.0040180313808378e-06 mae 0.0007546401466242969
training loss 1.3019128153237212e-06 mae 0.0007621393169221631
training loss 1.2445615764721487e-06 mae 0.0007476597375439313
training loss 1.2470613993227383e-06 mae 0.0007488501022279957
training loss 1.2858333735919279e-06 mae 0.0007565158670675478
Epoch 626, training: loss: 0.0000013, mae: 0.0007560 test: loss0.0000972, mae:0.0071690
training loss 1.8641103451955132e-06 mae 0.0008078077808022499
training loss 1.332250359182046e-06 mae 0.0007635889479013925
training loss 1.2425216578047203e-06 mae 0.0007446016277772103
training loss 1.2353953461690664e-06 mae 0.0007422078624043906
training loss 1.2758120598407386e-06 mae 0.0007521234673102713
Epoch 627, training: loss: 0.0000013, mae: 0.0007539 test: loss0.0000959, mae:0.0071388
training loss 8.501957040607522e-07 mae 0.0006360110710375011
training loss 1.317562581495141e-06 mae 0.0007588114080421043
training loss 1.2936141069760633e-06 mae 0.0007493117942917394
training loss 1.2762499731529386e-06 mae 0.0007477108117760413
training loss 1.2744879113461816e-06 mae 0.0007484140008078678
Epoch 628, training: loss: 0.0000013, mae: 0.0007474 test: loss0.0000960, mae:0.0071430
training loss 1.3907683751313016e-06 mae 0.000819015025626868
training loss 1.220021488279599e-06 mae 0.0007351007342210732
training loss 1.2271527197353324e-06 mae 0.0007401182544463783
training loss 1.2462218534594455e-06 mae 0.000746390122352846
training loss 1.2690039601867773e-06 mae 0.0007489155547922725
Epoch 629, training: loss: 0.0000013, mae: 0.0007521 test: loss0.0000956, mae:0.0071227
training loss 8.618652032055252e-07 mae 0.0006655243341811001
training loss 1.1971672618118666e-06 mae 0.0007303492234581533
training loss 1.2319705056503187e-06 mae 0.0007405123041739855
training loss 1.2383305415552953e-06 mae 0.0007437660232793836
training loss 1.2544983597600455e-06 mae 0.0007457321944683605
Epoch 630, training: loss: 0.0000013, mae: 0.0007497 test: loss0.0000963, mae:0.0071369
training loss 7.535060717600572e-07 mae 0.0005844021216034889
training loss 1.1346757900150363e-06 mae 0.0007097328061183147
training loss 1.2276799573596285e-06 mae 0.0007390344589210973
training loss 1.2761092433410283e-06 mae 0.0007537197158209338
training loss 1.2769287898107045e-06 mae 0.0007512271706888273
Epoch 631, training: loss: 0.0000013, mae: 0.0007512 test: loss0.0000966, mae:0.0071508
training loss 1.7890350818561274e-06 mae 0.0008065930451266468
training loss 1.1970410794531867e-06 mae 0.0007193482494639123
training loss 1.2292736450584885e-06 mae 0.0007377551012156103
training loss 1.2395780803045692e-06 mae 0.0007456177109328177
training loss 1.2726489973345143e-06 mae 0.00075416961037299
Epoch 632, training: loss: 0.0000013, mae: 0.0007554 test: loss0.0000960, mae:0.0071413
training loss 8.058957519097021e-07 mae 0.000657317228615284
training loss 1.2539353453094546e-06 mae 0.0007402004357929541
training loss 1.2816788558432175e-06 mae 0.0007496338178021001
training loss 1.2523285003503257e-06 mae 0.0007406756351624816
training loss 1.2633802020730788e-06 mae 0.0007456901310352303
Epoch 633, training: loss: 0.0000013, mae: 0.0007494 test: loss0.0000963, mae:0.0071535
training loss 8.770273893787817e-07 mae 0.0006664947723038495
training loss 1.1987638296048705e-06 mae 0.00073567805606324
training loss 1.227375270162264e-06 mae 0.0007428190700299485
training loss 1.2270877738612871e-06 mae 0.0007404009732556533
training loss 1.274831916970611e-06 mae 0.0007531945441620172
Epoch 634, training: loss: 0.0000013, mae: 0.0007505 test: loss0.0000962, mae:0.0071433
training loss 5.904793738409353e-07 mae 0.0005632626707665622
training loss 1.2478365480252678e-06 mae 0.0007387826951932818
training loss 1.2619132110215798e-06 mae 0.000749380471263228
training loss 1.2841570579764828e-06 mae 0.0007563289235629731
training loss 1.259888368120084e-06 mae 0.0007478262480830225
Epoch 635, training: loss: 0.0000013, mae: 0.0007520 test: loss0.0000959, mae:0.0071248
training loss 6.070872018426599e-07 mae 0.0005383326788432896
training loss 1.223816026679867e-06 mae 0.0007314303394534861
training loss 1.2848750304333362e-06 mae 0.0007482452342853795
training loss 1.2676330026809698e-06 mae 0.0007470415075176971
training loss 1.2659511046263895e-06 mae 0.0007499808001630843
Epoch 636, training: loss: 0.0000013, mae: 0.0007512 test: loss0.0000962, mae:0.0071458
training loss 1.3460670515996753e-06 mae 0.0008212585817091167
training loss 1.3166915480536884e-06 mae 0.0007669740714881496
training loss 1.251933346764256e-06 mae 0.0007453919152065301
training loss 1.2477350742075634e-06 mae 0.0007448380887014976
training loss 1.2758533842992426e-06 mae 0.0007515166436127431
Epoch 637, training: loss: 0.0000013, mae: 0.0007473 test: loss0.0000963, mae:0.0071481
training loss 5.598188295152795e-07 mae 0.000551663339138031
training loss 1.26909224727391e-06 mae 0.0007352032847063359
training loss 1.2405004496269952e-06 mae 0.0007362222367561061
training loss 1.2518377174716764e-06 mae 0.0007372183410299707
training loss 1.2749911278925578e-06 mae 0.0007502434261789574
Epoch 638, training: loss: 0.0000013, mae: 0.0007496 test: loss0.0000962, mae:0.0071442
training loss 1.302282612414274e-06 mae 0.0008023691480048001
training loss 1.2141566027786014e-06 mae 0.0007387136055530943
training loss 1.1935940502814562e-06 mae 0.0007341452179785523
training loss 1.2243798806256689e-06 mae 0.0007437985941277361
training loss 1.2632524036445223e-06 mae 0.0007537016653761598
Epoch 639, training: loss: 0.0000013, mae: 0.0007542 test: loss0.0000959, mae:0.0071338
training loss 8.373681339435279e-07 mae 0.0006380916456691921
training loss 1.1703533528798644e-06 mae 0.0007239776896312833
training loss 1.2122610443573576e-06 mae 0.0007311909944787252
training loss 1.252132367403797e-06 mae 0.0007423431670758871
training loss 1.259713548447802e-06 mae 0.0007455571369552958
Epoch 640, training: loss: 0.0000013, mae: 0.0007468 test: loss0.0000960, mae:0.0071403
training loss 2.2721731056662975e-06 mae 0.0009364879806526005
training loss 1.2613498994391658e-06 mae 0.0007383940357934027
training loss 1.2741014432272596e-06 mae 0.0007481336395376097
training loss 1.2930297344272862e-06 mae 0.0007562122578711085
training loss 1.275263946043161e-06 mae 0.0007542678638910461
Epoch 641, training: loss: 0.0000013, mae: 0.0007504 test: loss0.0000963, mae:0.0071505
training loss 9.141115810962219e-07 mae 0.000670075707603246
training loss 1.180748999924582e-06 mae 0.0007234360856077106
training loss 1.2837987934416214e-06 mae 0.0007521292052950968
training loss 1.2691259237407398e-06 mae 0.0007509694662907269
training loss 1.2703901750488573e-06 mae 0.0007484470038276296
Epoch 642, training: loss: 0.0000013, mae: 0.0007469 test: loss0.0000964, mae:0.0071491
training loss 2.399576487732702e-06 mae 0.000992228277027607
training loss 1.165365612602424e-06 mae 0.0007203049015016385
training loss 1.2268204457514275e-06 mae 0.0007401914958205849
training loss 1.2436367869939219e-06 mae 0.0007438534768205752
training loss 1.2471604316485831e-06 mae 0.0007449402620110066
Epoch 643, training: loss: 0.0000013, mae: 0.0007483 test: loss0.0000970, mae:0.0071736
training loss 1.2250185363882338e-06 mae 0.0006644849781878293
training loss 1.3154057181536163e-06 mae 0.0007695725527774616
training loss 1.261715259162055e-06 mae 0.0007461334162594587
training loss 1.2423028624351782e-06 mae 0.0007423330549403581
training loss 1.2597257498452388e-06 mae 0.0007504377318940019
Epoch 644, training: loss: 0.0000013, mae: 0.0007521 test: loss0.0000966, mae:0.0071616
training loss 1.1317985126879648e-06 mae 0.0005840662051923573
training loss 1.260433478756907e-06 mae 0.0007488301021558251
training loss 1.25575112186801e-06 mae 0.0007522465265817055
training loss 1.2412054776472824e-06 mae 0.0007486059457612601
training loss 1.2628272041249283e-06 mae 0.0007500428072494265
Epoch 645, training: loss: 0.0000013, mae: 0.0007481 test: loss0.0000975, mae:0.0071892
training loss 1.5499799701501615e-06 mae 0.0008371922303922474
training loss 1.2961320764731554e-06 mae 0.0007488672163876177
training loss 1.2378595758631004e-06 mae 0.0007357969652460245
training loss 1.2360238711921168e-06 mae 0.0007391240135905343
training loss 1.2499613016057738e-06 mae 0.0007425588784410404
Epoch 646, training: loss: 0.0000013, mae: 0.0007436 test: loss0.0000960, mae:0.0071320
training loss 6.265041747610667e-07 mae 0.0005822035600431263
training loss 1.2336080484441056e-06 mae 0.0007334255736649913
training loss 1.2711673860232429e-06 mae 0.0007407987152576668
training loss 1.2485095456496552e-06 mae 0.0007388791052427223
training loss 1.2505237926876794e-06 mae 0.0007437950773952322
Epoch 647, training: loss: 0.0000013, mae: 0.0007463 test: loss0.0000964, mae:0.0071455
training loss 1.0376006684964523e-06 mae 0.0007248042966239154
training loss 1.2510025481235204e-06 mae 0.0007483988153456035
training loss 1.2683714946280528e-06 mae 0.0007497505492318678
training loss 1.2552622106551806e-06 mae 0.0007455028386922269
training loss 1.2466805012491238e-06 mae 0.0007437232920010364
Epoch 648, training: loss: 0.0000012, mae: 0.0007431 test: loss0.0001012, mae:0.0071912
training loss 1.8392960328128538e-06 mae 0.0007501120562665164
training loss 1.1783665378712325e-06 mae 0.0007299912691700694
training loss 1.2748843324394633e-06 mae 0.0007464951301906442
training loss 1.2540534573752259e-06 mae 0.0007438258728974603
training loss 1.2473874949476472e-06 mae 0.0007393008890904635
Epoch 649, training: loss: 0.0000012, mae: 0.0007411 test: loss0.0000959, mae:0.0071304
training loss 1.4648472870248952e-06 mae 0.0008090436458587646
training loss 1.1314386916174217e-06 mae 0.0007186680997940984
training loss 1.20872381713336e-06 mae 0.0007352764806986814
training loss 1.2245472880704555e-06 mae 0.0007381089577537698
training loss 1.2592046895557962e-06 mae 0.0007468216843435089
Epoch 650, training: loss: 0.0000013, mae: 0.0007458 test: loss0.0000961, mae:0.0071474
training loss 8.150540793394612e-07 mae 0.0006407409091480076
training loss 1.296815891656283e-06 mae 0.0007599339701746608
training loss 1.2301892459253404e-06 mae 0.0007452376562604074
training loss 1.2374631569099507e-06 mae 0.0007416583637750112
training loss 1.2520365405759375e-06 mae 0.000741299350377617
Epoch 651, training: loss: 0.0000012, mae: 0.0007409 test: loss0.0000966, mae:0.0071526
training loss 6.55393762372114e-07 mae 0.0005807606503367424
training loss 1.1857721569957033e-06 mae 0.0007181846007120375
training loss 1.1900073419869527e-06 mae 0.0007267421397315983
training loss 1.1969201063978393e-06 mae 0.0007336780472060309
training loss 1.2484191875872016e-06 mae 0.0007442364689838419
Epoch 652, training: loss: 0.0000012, mae: 0.0007453 test: loss0.0000971, mae:0.0071799
training loss 1.1854831427626777e-06 mae 0.0007375932182185352
training loss 1.2719598222095858e-06 mae 0.0007434754213546495
training loss 1.209016690467306e-06 mae 0.0007309424088654651
training loss 1.2492761391962067e-06 mae 0.000739212893219418
training loss 1.2445231619999245e-06 mae 0.0007406563108001569
Epoch 653, training: loss: 0.0000012, mae: 0.0007413 test: loss0.0000973, mae:0.0071665
training loss 6.353469643727294e-07 mae 0.0005784078384749591
training loss 1.1406242804821207e-06 mae 0.0007129248246486645
training loss 1.213072465870396e-06 mae 0.0007340516056519667
training loss 1.2534225995133228e-06 mae 0.0007443873511260343
training loss 1.2502979085360517e-06 mae 0.0007429323586304006
Epoch 654, training: loss: 0.0000012, mae: 0.0007402 test: loss0.0000964, mae:0.0071574
training loss 1.3370122360356618e-06 mae 0.0008291273261420429
training loss 1.1701444182063809e-06 mae 0.0007282258422278308
training loss 1.2185309573158169e-06 mae 0.00073724098579663
training loss 1.2272421728214229e-06 mae 0.000740003388526152
training loss 1.2474597064514204e-06 mae 0.0007434231188583801
Epoch 655, training: loss: 0.0000012, mae: 0.0007423 test: loss0.0000962, mae:0.0071394
training loss 1.2770082093993551e-06 mae 0.0007898525218479335
training loss 1.2625460298669863e-06 mae 0.0007282156946922778
training loss 1.2661351090650666e-06 mae 0.0007424912395850191
training loss 1.235115664125537e-06 mae 0.0007354696768888007
training loss 1.2401605603987823e-06 mae 0.0007400782606552881
Epoch 656, training: loss: 0.0000012, mae: 0.0007391 test: loss0.0000970, mae:0.0071670
training loss 9.98526047624182e-07 mae 0.0006712538306601346
training loss 1.1482529626561036e-06 mae 0.0007141690139713535
training loss 1.169111507889815e-06 mae 0.0007202990859110685
training loss 1.221547154536652e-06 mae 0.0007351201986708979
training loss 1.2283880435866443e-06 mae 0.0007386959342642768
Epoch 657, training: loss: 0.0000012, mae: 0.0007437 test: loss0.0000962, mae:0.0071390
training loss 1.0735643627413083e-06 mae 0.0007459878106601536
training loss 1.1683251094765702e-06 mae 0.0007211489833471383
training loss 1.2223009183808153e-06 mae 0.000736144246270714
training loss 1.2303992440704417e-06 mae 0.0007375290510182989
training loss 1.2407560717095816e-06 mae 0.0007414272963185213
Epoch 658, training: loss: 0.0000012, mae: 0.0007405 test: loss0.0000965, mae:0.0071587
training loss 1.4834122339379974e-06 mae 0.0008822381496429443
training loss 1.2712653711769594e-06 mae 0.0007351190970261015
training loss 1.228377365607392e-06 mae 0.0007333114453923362
training loss 1.2239698229307324e-06 mae 0.0007356674300926114
training loss 1.226759254666433e-06 mae 0.0007380444658700078
Epoch 659, training: loss: 0.0000012, mae: 0.0007383 test: loss0.0000962, mae:0.0071355
training loss 1.1046613508369774e-06 mae 0.0007099009235389531
training loss 1.119932023232418e-06 mae 0.0007041355694288056
training loss 1.2108158389850417e-06 mae 0.0007261855647055484
training loss 1.2259044760807027e-06 mae 0.0007351713273550893
training loss 1.232615469133911e-06 mae 0.0007363388360308862
Epoch 660, training: loss: 0.0000012, mae: 0.0007369 test: loss0.0000963, mae:0.0071454
training loss 1.0045470162367565e-06 mae 0.000681367062497884
training loss 1.185639824402018e-06 mae 0.0007089285012197628
training loss 1.183660361734252e-06 mae 0.0007173178926695802
training loss 1.1848715711951902e-06 mae 0.0007207090027756768
training loss 1.227051379500429e-06 mae 0.0007376854140855912
Epoch 661, training: loss: 0.0000012, mae: 0.0007403 test: loss0.0000966, mae:0.0071561
training loss 9.157197951026319e-07 mae 0.0006987964734435081
training loss 1.155337118314706e-06 mae 0.0007158753390232212
training loss 1.2399552033322364e-06 mae 0.0007406270928510308
training loss 1.2349758044696714e-06 mae 0.0007395538810211918
training loss 1.2544377462933401e-06 mae 0.0007471100359735776
Epoch 662, training: loss: 0.0000012, mae: 0.0007433 test: loss0.0000968, mae:0.0071587
training loss 2.0737470549647696e-06 mae 0.0009158216416835785
training loss 1.3111353585408519e-06 mae 0.0007572529858033006
training loss 1.2485513164253037e-06 mae 0.0007396966491544639
training loss 1.2373865092063136e-06 mae 0.0007382039204687003
training loss 1.2306777171429818e-06 mae 0.000737152023840955
Epoch 663, training: loss: 0.0000012, mae: 0.0007374 test: loss0.0000964, mae:0.0071517
training loss 1.0496369213797152e-06 mae 0.0007310158689506352
training loss 1.132136272939105e-06 mae 0.000722525289793517
training loss 1.170285841426719e-06 mae 0.0007298787661579133
training loss 1.1710094655361128e-06 mae 0.000725524012378682
training loss 1.2036479784657235e-06 mae 0.0007349388460763055
Epoch 664, training: loss: 0.0000012, mae: 0.0007372 test: loss0.0000961, mae:0.0071384
training loss 1.5380859395008883e-06 mae 0.0007624260033480823
training loss 1.2312764768547128e-06 mae 0.0007295083715671712
training loss 1.2419524241500086e-06 mae 0.0007333119012562948
training loss 1.2385480538386833e-06 mae 0.0007337254910754497
training loss 1.2367762674069978e-06 mae 0.0007371884404070009
Epoch 665, training: loss: 0.0000012, mae: 0.0007353 test: loss0.0000964, mae:0.0071510
training loss 9.269603538086812e-07 mae 0.0006456403061747551
training loss 1.1739238942152274e-06 mae 0.0007225878955568095
training loss 1.2538658797713632e-06 mae 0.0007385151483243015
training loss 1.237178671973974e-06 mae 0.0007349748331328717
training loss 1.221917869997924e-06 mae 0.0007339759433841266
Epoch 666, training: loss: 0.0000012, mae: 0.0007342 test: loss0.0000972, mae:0.0071896
training loss 7.720279882050818e-07 mae 0.0006251561571843922
training loss 1.128877864389591e-06 mae 0.0007008671002718163
training loss 1.1877557899244474e-06 mae 0.0007171980796916649
training loss 1.1920326787520212e-06 mae 0.0007248092122801535
training loss 1.2102576848311533e-06 mae 0.0007327234598956259
Epoch 667, training: loss: 0.0000012, mae: 0.0007364 test: loss0.0000969, mae:0.0071677
training loss 8.73246847277187e-07 mae 0.0006007663905620575
training loss 1.2619065012800358e-06 mae 0.0007298468297128293
training loss 1.214190515699133e-06 mae 0.0007312447466601683
training loss 1.2318384629690252e-06 mae 0.0007394841889116837
training loss 1.2267631779935468e-06 mae 0.0007359703989127251
Epoch 668, training: loss: 0.0000012, mae: 0.0007353 test: loss0.0000966, mae:0.0071543
training loss 7.250957310134254e-07 mae 0.0005926390294916928
training loss 1.1877044264164233e-06 mae 0.0007252326262566972
training loss 1.2131320695719089e-06 mae 0.0007349136360404587
training loss 1.242802277231672e-06 mae 0.0007418022746997361
training loss 1.2238737458483685e-06 mae 0.0007367267479554784
Epoch 669, training: loss: 0.0000012, mae: 0.0007332 test: loss0.0000968, mae:0.0071709
training loss 9.464642403145263e-07 mae 0.0007173381745815277
training loss 1.1500688106248254e-06 mae 0.0007179419238430758
training loss 1.1733799646329922e-06 mae 0.0007270667782643496
training loss 1.184832625736119e-06 mae 0.0007275420989662362
training loss 1.2196280434451206e-06 mae 0.0007329330176105776
Epoch 670, training: loss: 0.0000012, mae: 0.0007322 test: loss0.0000968, mae:0.0071614
training loss 7.669279966648901e-07 mae 0.0005788070266135037
training loss 1.168848988455983e-06 mae 0.0007314755416968293
training loss 1.1543270439836639e-06 mae 0.0007259866146665843
training loss 1.1921458048727083e-06 mae 0.0007274653766433779
training loss 1.2169177529102768e-06 mae 0.0007334048186477499
Epoch 671, training: loss: 0.0000012, mae: 0.0007355 test: loss0.0000971, mae:0.0071643
training loss 1.3493475989889703e-06 mae 0.0007658188114874065
training loss 1.1719381708038175e-06 mae 0.0007257016453271112
training loss 1.2114017781326274e-06 mae 0.0007299396611685578
training loss 1.1811000909133238e-06 mae 0.0007241344000737099
training loss 1.2144409167412783e-06 mae 0.0007325255283645693
Epoch 672, training: loss: 0.0000012, mae: 0.0007331 test: loss0.0000972, mae:0.0071812
training loss 6.043384814802266e-07 mae 0.0005399255896918476
training loss 1.1797557408722333e-06 mae 0.0007191395966847445
training loss 1.2157817619450131e-06 mae 0.0007274331274848098
training loss 1.178012658875553e-06 mae 0.0007223008776435146
training loss 1.2166979170680984e-06 mae 0.0007303583779457994
Epoch 673, training: loss: 0.0000012, mae: 0.0007303 test: loss0.0000967, mae:0.0071655
training loss 9.403237868355063e-07 mae 0.0006809653714299202
training loss 1.2209967998583745e-06 mae 0.0007288825924179573
training loss 1.217941189960953e-06 mae 0.0007335418488917925
training loss 1.222557833535569e-06 mae 0.0007346955121075426
training loss 1.224199000073248e-06 mae 0.0007356410407095184
Epoch 674, training: loss: 0.0000012, mae: 0.0007333 test: loss0.0000969, mae:0.0071712
training loss 1.0645411521181813e-06 mae 0.0007063904777169228
training loss 1.2008834281843131e-06 mae 0.0007131656862375347
training loss 1.1941259929649774e-06 mae 0.000722372371906249
training loss 1.1859190401206217e-06 mae 0.0007213218930171225
training loss 1.2133003095659566e-06 mae 0.0007316972410239151
Epoch 675, training: loss: 0.0000012, mae: 0.0007319 test: loss0.0000971, mae:0.0071891
training loss 5.20620972110919e-07 mae 0.0005578041891567409
training loss 1.2018866387359811e-06 mae 0.000719312358734326
training loss 1.150220130890231e-06 mae 0.0007143764762959382
training loss 1.1886629188166197e-06 mae 0.0007282514828611906
training loss 1.1977968684524188e-06 mae 0.0007306758041576999
Epoch 676, training: loss: 0.0000012, mae: 0.0007318 test: loss0.0000969, mae:0.0071671
training loss 1.7381861425747047e-06 mae 0.0009532111580483615
training loss 1.2374845751167538e-06 mae 0.0007348728381033401
training loss 1.1255637624493248e-06 mae 0.0007077176541684764
training loss 1.1845446396596308e-06 mae 0.0007245712333423583
training loss 1.2023629293350458e-06 mae 0.0007301687744541084
Epoch 677, training: loss: 0.0000012, mae: 0.0007306 test: loss0.0000968, mae:0.0071783
training loss 8.588829700784117e-07 mae 0.000660894438624382
training loss 1.160617393216122e-06 mae 0.0007069473880726625
training loss 1.1940054446763535e-06 mae 0.0007191208769834721
training loss 1.2059460178362546e-06 mae 0.0007268308385488092
training loss 1.2128236479458867e-06 mae 0.0007300382323356916
Epoch 678, training: loss: 0.0000012, mae: 0.0007296 test: loss0.0000967, mae:0.0071575
training loss 9.470603004047007e-07 mae 0.0007044452358968556
training loss 1.1647543229343025e-06 mae 0.0007130687967326273
training loss 1.2207855741390894e-06 mae 0.0007300820471926382
training loss 1.2077054599940795e-06 mae 0.0007280791396142558
training loss 1.218098581187889e-06 mae 0.0007345974869592885
Epoch 679, training: loss: 0.0000012, mae: 0.0007319 test: loss0.0000967, mae:0.0071628
training loss 1.2552968655654695e-06 mae 0.0006828407640568912
training loss 1.140497458356335e-06 mae 0.000706763371971308
training loss 1.1710866427881342e-06 mae 0.0007198738449456004
training loss 1.2064567115340335e-06 mae 0.0007248381048669137
training loss 1.197862405944933e-06 mae 0.0007254515220851298
Epoch 680, training: loss: 0.0000012, mae: 0.0007270 test: loss0.0000970, mae:0.0071697
training loss 9.94301331047609e-07 mae 0.0006838326225988567
training loss 1.1693885105213505e-06 mae 0.0007175338847160921
training loss 1.1791849639920846e-06 mae 0.0007285602041520175
training loss 1.1867321918152393e-06 mae 0.0007285220833381152
training loss 1.2105695996590057e-06 mae 0.0007294076435506542
Epoch 681, training: loss: 0.0000012, mae: 0.0007291 test: loss0.0000974, mae:0.0071702
training loss 2.1356488559831632e-06 mae 0.0008952651987783611
training loss 1.1231376298186947e-06 mae 0.0007042911248372905
training loss 1.1832942355245943e-06 mae 0.0007243875683847778
training loss 1.1776305197693232e-06 mae 0.0007224238521206681
training loss 1.2023634564799855e-06 mae 0.0007245758160617466
Epoch 682, training: loss: 0.0000012, mae: 0.0007249 test: loss0.0000968, mae:0.0071687
training loss 1.664875867390947e-06 mae 0.0009175765444524586
training loss 1.1482269523336679e-06 mae 0.0007138713839345191
training loss 1.2128403046846317e-06 mae 0.0007249465584524416
training loss 1.2055374501164959e-06 mae 0.000726513710298532
training loss 1.185746879258008e-06 mae 0.0007236330818896881
Epoch 683, training: loss: 0.0000012, mae: 0.0007264 test: loss0.0000967, mae:0.0071588
training loss 6.789118742744904e-07 mae 0.0006252887542359531
training loss 1.1171428376005054e-06 mae 0.000708874882833448
training loss 1.1435125801685588e-06 mae 0.0007174465316001994
training loss 1.191544087168348e-06 mae 0.0007222759491526555
training loss 1.2103062583809795e-06 mae 0.0007318560726447175
Epoch 684, training: loss: 0.0000012, mae: 0.0007299 test: loss0.0000970, mae:0.0071678
training loss 1.753911533342034e-06 mae 0.0008512853528372943
training loss 1.2300712988493906e-06 mae 0.0007331446794719967
training loss 1.2192508975068893e-06 mae 0.0007277990075861021
training loss 1.1975759092976837e-06 mae 0.0007277226094033149
training loss 1.2072189122984894e-06 mae 0.0007307082982224164
Epoch 685, training: loss: 0.0000012, mae: 0.0007290 test: loss0.0000969, mae:0.0071741
training loss 6.823104286013404e-07 mae 0.0006262874230742455
training loss 1.2750703358862808e-06 mae 0.0007351600466862175
training loss 1.2404757950603784e-06 mae 0.0007322455080926862
training loss 1.2072006366027474e-06 mae 0.0007254907758043009
training loss 1.207294605532369e-06 mae 0.0007285725333164936
Epoch 686, training: loss: 0.0000012, mae: 0.0007271 test: loss0.0000967, mae:0.0071542
training loss 1.1159644373037736e-06 mae 0.0007008857210166752
training loss 1.119642175853205e-06 mae 0.0007103826130475557
training loss 1.146309964125598e-06 mae 0.0007098807113609358
training loss 1.2125163622813667e-06 mae 0.0007266736778643427
training loss 1.2014897120466296e-06 mae 0.0007285780529086176
Epoch 687, training: loss: 0.0000012, mae: 0.0007278 test: loss0.0000966, mae:0.0071569
training loss 9.283263011639065e-07 mae 0.0006172126159071922
training loss 1.1527345317936838e-06 mae 0.0007150232285151587
training loss 1.14542965486117e-06 mae 0.0007122231164421555
training loss 1.1695359201767972e-06 mae 0.0007204851600116378
training loss 1.191501912157109e-06 mae 0.0007236854372191617
Epoch 688, training: loss: 0.0000012, mae: 0.0007256 test: loss0.0000970, mae:0.0071777
training loss 1.0533252634559176e-06 mae 0.000705492973793298
training loss 1.1374309186370135e-06 mae 0.0006998848591191584
training loss 1.1482789056238884e-06 mae 0.0007184358693470273
training loss 1.18775783249138e-06 mae 0.0007297190760851325
training loss 1.2002646089673926e-06 mae 0.0007297147984271267
Epoch 689, training: loss: 0.0000012, mae: 0.0007303 test: loss0.0000968, mae:0.0071708
training loss 1.1909631894013728e-06 mae 0.0007225405424833298
training loss 1.2238888408702747e-06 mae 0.0007241111928962316
training loss 1.1486219998369958e-06 mae 0.0007085141824964101
training loss 1.1648229233377915e-06 mae 0.0007151952075050367
training loss 1.1898604530933566e-06 mae 0.0007222517415538278
Epoch 690, training: loss: 0.0000012, mae: 0.0007255 test: loss0.0000967, mae:0.0071583
training loss 1.3076828508928884e-06 mae 0.0008446599240414798
training loss 1.2127403880078738e-06 mae 0.0007338095724573541
training loss 1.1493624121009485e-06 mae 0.0007180136211774322
training loss 1.2157644549018412e-06 mae 0.0007324754485986906
training loss 1.1904192295568198e-06 mae 0.0007257748229269276
Epoch 691, training: loss: 0.0000012, mae: 0.0007247 test: loss0.0000971, mae:0.0071848
training loss 5.54134771846293e-07 mae 0.0005671304534189403
training loss 1.1442868529844847e-06 mae 0.0006931343580386144
training loss 1.1580343529077613e-06 mae 0.0007112757559280178
training loss 1.178365208911811e-06 mae 0.0007191452219702967
training loss 1.1917562358347602e-06 mae 0.0007259541491465873
Epoch 692, training: loss: 0.0000012, mae: 0.0007270 test: loss0.0000972, mae:0.0071842
training loss 8.738738870306406e-07 mae 0.0006813236395828426
training loss 1.2261191091279305e-06 mae 0.0007307256461239842
training loss 1.2102586862452556e-06 mae 0.0007235412860501302
training loss 1.192977822182796e-06 mae 0.0007227460760469028
training loss 1.1862725831740057e-06 mae 0.0007231184679595526
Epoch 693, training: loss: 0.0000012, mae: 0.0007250 test: loss0.0000974, mae:0.0071886
training loss 1.2509667612903286e-06 mae 0.000714143447112292
training loss 1.2086348252684245e-06 mae 0.0007245839075824501
training loss 1.1758505895433677e-06 mae 0.0007149332435225312
training loss 1.1751264406824851e-06 mae 0.0007186204729685003
training loss 1.1985220893749813e-06 mae 0.0007265218840830901
Epoch 694, training: loss: 0.0000012, mae: 0.0007238 test: loss0.0000978, mae:0.0071758
training loss 6.261655585149128e-07 mae 0.0005193967372179031
training loss 1.1087468305736278e-06 mae 0.0007078947268846428
training loss 1.1263799407997522e-06 mae 0.000706356408081414
training loss 1.149800142261272e-06 mae 0.0007112609357064259
training loss 1.1856748511520283e-06 mae 0.0007215915382457483
Epoch 695, training: loss: 0.0000012, mae: 0.0007246 test: loss0.0000975, mae:0.0071921
training loss 1.7349093468510546e-06 mae 0.0006879779393784702
training loss 1.1240300402866578e-06 mae 0.0007058231441952363
training loss 1.1936470137148751e-06 mae 0.0007236771971923252
training loss 1.1920644113082662e-06 mae 0.0007239588004743543
training loss 1.1923283613105558e-06 mae 0.0007249290189898076
Epoch 696, training: loss: 0.0000012, mae: 0.0007232 test: loss0.0000972, mae:0.0071836
training loss 1.217990984514472e-06 mae 0.0007008693064562976
training loss 1.150199615133662e-06 mae 0.0007124353050683425
training loss 1.143554116949174e-06 mae 0.0007128382506803381
training loss 1.1490915931860834e-06 mae 0.0007146202000428535
training loss 1.1805377202146957e-06 mae 0.0007206962760410442
Epoch 697, training: loss: 0.0000012, mae: 0.0007229 test: loss0.0000974, mae:0.0071905
training loss 9.72840894064575e-07 mae 0.0006578334723599255
training loss 1.1245101136098006e-06 mae 0.0007066511630089771
training loss 1.1631256555655547e-06 mae 0.0007153306704470979
training loss 1.1594380813320884e-06 mae 0.0007161440893264477
training loss 1.1841753817167326e-06 mae 0.0007213225395045714
Epoch 698, training: loss: 0.0000012, mae: 0.0007207 test: loss0.0000972, mae:0.0071857
training loss 8.677323535266623e-07 mae 0.0006673115422017872
training loss 1.0771478576887107e-06 mae 0.000689888347908124
training loss 1.125450877328932e-06 mae 0.0007048906357313445
training loss 1.184572322216393e-06 mae 0.0007189613626653096
training loss 1.176664528006609e-06 mae 0.000718556658859453
Epoch 699, training: loss: 0.0000012, mae: 0.0007202 test: loss0.0000972, mae:0.0071726
current learning rate: 3.90625e-06
training loss 8.833803804009221e-07 mae 0.0006324875284917653
training loss 1.1432763670324908e-06 mae 0.0006928727368586787
training loss 1.102795587178995e-06 mae 0.0006880247642719508
training loss 1.1294774758094506e-06 mae 0.0006907733360811178
training loss 1.1354340763567626e-06 mae 0.0006965186353657037
Epoch 700, training: loss: 0.0000011, mae: 0.0006992 test: loss0.0000974, mae:0.0071848
training loss 9.50724995618657e-07 mae 0.0007033047149889171
training loss 1.043970710315097e-06 mae 0.0006585062992777313
training loss 1.1245129496616067e-06 mae 0.0006866269522475653
training loss 1.1190736808389648e-06 mae 0.0006852628817760217
training loss 1.1322110233990167e-06 mae 0.0006927751855801938
Epoch 701, training: loss: 0.0000011, mae: 0.0006958 test: loss0.0000971, mae:0.0071854
training loss 1.240211418007675e-06 mae 0.000731268897652626
training loss 1.0861543713934797e-06 mae 0.0006701448748536482
training loss 1.1520629503494172e-06 mae 0.000697070089835065
training loss 1.1386864900640714e-06 mae 0.0006967397566286488
training loss 1.1355904838517891e-06 mae 0.000697442594526419
Epoch 702, training: loss: 0.0000011, mae: 0.0006979 test: loss0.0000974, mae:0.0071940
training loss 7.752675514893781e-07 mae 0.0006050591473467648
training loss 1.1627444878821042e-06 mae 0.0006940856481235767
training loss 1.1576962212376826e-06 mae 0.0006972862180497605
training loss 1.150730986372409e-06 mae 0.0006952127075556737
training loss 1.1334581523132515e-06 mae 0.00069336095287358
Epoch 703, training: loss: 0.0000011, mae: 0.0006934 test: loss0.0000993, mae:0.0072383
training loss 1.3101285958327935e-06 mae 0.0007392903789877892
training loss 1.1087712108274348e-06 mae 0.0006810480285910706
training loss 1.1303675662902876e-06 mae 0.0006946363696127684
training loss 1.127521321542184e-06 mae 0.0006926662528760952
training loss 1.1400492318538891e-06 mae 0.0006954289408949718
Epoch 704, training: loss: 0.0000011, mae: 0.0006933 test: loss0.0000979, mae:0.0072152
training loss 6.377848080774129e-07 mae 0.0005701028858311474
training loss 1.1535634637668547e-06 mae 0.0006957319907515367
training loss 1.1737932055947934e-06 mae 0.0007069603998748828
training loss 1.1162734539394522e-06 mae 0.0006923567428907368
training loss 1.1280734630549219e-06 mae 0.0006923672190892846
Epoch 705, training: loss: 0.0000011, mae: 0.0006931 test: loss0.0000970, mae:0.0071740
training loss 7.225365266094741e-07 mae 0.0006469019572250545
training loss 1.084840052352079e-06 mae 0.0006872834586401416
training loss 1.063918173855146e-06 mae 0.0006779909487191295
training loss 1.112810958671761e-06 mae 0.0006853936632111398
training loss 1.1358660748865802e-06 mae 0.000694091040270394
Epoch 706, training: loss: 0.0000011, mae: 0.0006933 test: loss0.0000971, mae:0.0071739
training loss 1.6881945157365408e-06 mae 0.0007247657631523907
training loss 1.0697440932201024e-06 mae 0.0006869154218721258
training loss 1.1283296866694582e-06 mae 0.0006946176604602925
training loss 1.1500396318667558e-06 mae 0.0007001187034423104
training loss 1.1345818494198698e-06 mae 0.0006977046328186707
Epoch 707, training: loss: 0.0000011, mae: 0.0006951 test: loss0.0000988, mae:0.0072285
training loss 9.379401149089972e-07 mae 0.0006520279566757381
training loss 9.747039393468917e-07 mae 0.0006484892710075513
training loss 1.0761516027696867e-06 mae 0.0006682982343816117
training loss 1.0908688921063003e-06 mae 0.0006763559730649487
training loss 1.11544793466875e-06 mae 0.000687436145221796
Epoch 708, training: loss: 0.0000011, mae: 0.0006915 test: loss0.0000974, mae:0.0071881
training loss 7.735234248684719e-07 mae 0.0006248389254324138
training loss 1.0830788916533208e-06 mae 0.0006831574610973177
training loss 1.0695042903134354e-06 mae 0.0006829609879463136
training loss 1.1109045343023992e-06 mae 0.0006926526155150092
training loss 1.1113767716799337e-06 mae 0.0006932660261694967
Epoch 709, training: loss: 0.0000011, mae: 0.0006953 test: loss0.0000970, mae:0.0071741
training loss 1.3367745168579859e-06 mae 0.0008188768406398594
training loss 1.0498533019837813e-06 mae 0.0006693218676053789
training loss 1.08649675381133e-06 mae 0.0006731173443137712
training loss 1.0968863947761098e-06 mae 0.0006810093106970265
training loss 1.1393999951695744e-06 mae 0.0006948892912996553
Epoch 710, training: loss: 0.0000011, mae: 0.0006926 test: loss0.0000983, mae:0.0072076
training loss 4.1052763322113606e-07 mae 0.0004724320024251938
training loss 1.1564195769515923e-06 mae 0.0007010165096361045
training loss 1.1716637005736285e-06 mae 0.0007007784249911101
training loss 1.1301845017821703e-06 mae 0.0006918959311496418
training loss 1.1274918438797682e-06 mae 0.0006925327396680559
Epoch 711, training: loss: 0.0000011, mae: 0.0006940 test: loss0.0000976, mae:0.0071958
training loss 1.0567447361609084e-06 mae 0.0005982685834169388
training loss 1.1170993250779445e-06 mae 0.0006823221286621821
training loss 1.1270611112465336e-06 mae 0.0006936269464768876
training loss 1.1459122615272145e-06 mae 0.0006980243285837286
training loss 1.1359160887524877e-06 mae 0.000694701395522275
Epoch 712, training: loss: 0.0000011, mae: 0.0006920 test: loss0.0000974, mae:0.0071973
training loss 6.757986170669028e-07 mae 0.000559208623599261
training loss 1.178597050752347e-06 mae 0.0007026383405615229
training loss 1.104313936805441e-06 mae 0.0006796478802144602
training loss 1.0971355140708398e-06 mae 0.0006843443542860745
training loss 1.1267370140404494e-06 mae 0.0006912114679007163
Epoch 713, training: loss: 0.0000011, mae: 0.0006938 test: loss0.0000973, mae:0.0071828
training loss 7.455575996573316e-07 mae 0.0006000890280120075
training loss 1.1550509273048172e-06 mae 0.0007003537783234873
training loss 1.1370207704824831e-06 mae 0.0006928698552800065
training loss 1.1119387387595705e-06 mae 0.0006863472112824534
training loss 1.132996260932387e-06 mae 0.0006924632974942005
Epoch 714, training: loss: 0.0000011, mae: 0.0006911 test: loss0.0000970, mae:0.0071733
training loss 1.4134948287392035e-06 mae 0.0008006294374354184
training loss 1.1280440282783224e-06 mae 0.0006921633538178297
training loss 1.1055459942713915e-06 mae 0.000687671778608335
training loss 1.1189916395983494e-06 mae 0.0006896725526952266
training loss 1.1273507682708616e-06 mae 0.0006904187670153619
Epoch 715, training: loss: 0.0000011, mae: 0.0006886 test: loss0.0000975, mae:0.0071903
training loss 1.456970949220704e-06 mae 0.0008063409477472305
training loss 1.165910968920263e-06 mae 0.0006934283478139883
training loss 1.1211413360198625e-06 mae 0.0006843099660484472
training loss 1.1243512512196779e-06 mae 0.0006892883530293671
training loss 1.126595279076165e-06 mae 0.0006894134384677258
Epoch 716, training: loss: 0.0000011, mae: 0.0006906 test: loss0.0000976, mae:0.0071940
training loss 9.416100397174887e-07 mae 0.0006307422299869359
training loss 1.0748352222025477e-06 mae 0.000666463057688602
training loss 1.0828722291661148e-06 mae 0.00067620705450092
training loss 1.1279367609160354e-06 mae 0.0006879195538179837
training loss 1.1218275910518e-06 mae 0.0006910349440469005
Epoch 717, training: loss: 0.0000011, mae: 0.0006920 test: loss0.0000979, mae:0.0072090
training loss 2.4831253995216684e-06 mae 0.0010428806999698281
training loss 1.1642816849532617e-06 mae 0.0006928419119075818
training loss 1.0475164159099299e-06 mae 0.000667884339299514
training loss 1.0642755431499554e-06 mae 0.0006743522924879898
training loss 1.1130340507479202e-06 mae 0.0006874596249180111
Epoch 718, training: loss: 0.0000011, mae: 0.0006895 test: loss0.0000974, mae:0.0071941
training loss 1.0419685168017168e-06 mae 0.0006336753140203655
training loss 1.162370455399558e-06 mae 0.000697155008275135
training loss 1.0627910909880664e-06 mae 0.0006716650409859117
training loss 1.10810792825338e-06 mae 0.0006822693766486389
training loss 1.1225128436980243e-06 mae 0.0006885910075127761
Epoch 719, training: loss: 0.0000011, mae: 0.0006907 test: loss0.0000976, mae:0.0072054
training loss 1.0847852536244318e-06 mae 0.0006283503025770187
training loss 1.136025406033107e-06 mae 0.0006799836803967244
training loss 1.1327856276409982e-06 mae 0.0006814841809221497
training loss 1.1407465928750026e-06 mae 0.000687411583656297
training loss 1.1306435416614632e-06 mae 0.0006908775414096942
Epoch 720, training: loss: 0.0000011, mae: 0.0006893 test: loss0.0000971, mae:0.0071796
training loss 1.3823269000567961e-06 mae 0.0007767537608742714
training loss 1.1484905592303276e-06 mae 0.0006889751404706462
training loss 1.0712617028171863e-06 mae 0.0006729674665026811
training loss 1.1003036850326192e-06 mae 0.0006857559789166423
training loss 1.1113154533265531e-06 mae 0.0006866455041926209
Epoch 721, training: loss: 0.0000011, mae: 0.0006916 test: loss0.0000982, mae:0.0072167
training loss 1.5329713960454683e-06 mae 0.0007596925715915859
training loss 1.1212470399989212e-06 mae 0.0006914200813637353
training loss 1.111906068771451e-06 mae 0.0006892848344943902
training loss 1.1485050125830088e-06 mae 0.0006967017111763212
training loss 1.1144367468078318e-06 mae 0.0006884703046609815
Epoch 722, training: loss: 0.0000011, mae: 0.0006905 test: loss0.0000979, mae:0.0072170
training loss 5.122371931065572e-07 mae 0.0005514901131391525
training loss 1.0954076602127528e-06 mae 0.0006683334849738315
training loss 1.076871568568437e-06 mae 0.000675024972458619
training loss 1.1183319988266175e-06 mae 0.0006889521180180254
training loss 1.1283076623240777e-06 mae 0.0006920234704174713
Epoch 723, training: loss: 0.0000011, mae: 0.0006911 test: loss0.0000971, mae:0.0071797
training loss 7.766937528685958e-07 mae 0.0005926148151047528
training loss 1.0986003549836212e-06 mae 0.0006877797263144861
training loss 1.0943747870689796e-06 mae 0.0006863296577720767
training loss 1.085229718527871e-06 mae 0.0006858111825225791
training loss 1.1093199315952788e-06 mae 0.0006890701166520926
Epoch 724, training: loss: 0.0000011, mae: 0.0006920 test: loss0.0000973, mae:0.0071758
training loss 1.1132447070849594e-06 mae 0.0006997722084634006
training loss 1.1005036604532787e-06 mae 0.0006898270683664391
training loss 1.1146030641553477e-06 mae 0.0006883666458156996
training loss 1.1289010705547974e-06 mae 0.0006924073014482362
training loss 1.1217990543829169e-06 mae 0.000689679991328783
Epoch 725, training: loss: 0.0000011, mae: 0.0006901 test: loss0.0000980, mae:0.0072185
training loss 1.2571399565786123e-06 mae 0.0007220925763249397
training loss 9.8254196134304e-07 mae 0.000655147737216241
training loss 1.0580023951147863e-06 mae 0.000672308230452184
training loss 1.0705020906924212e-06 mae 0.0006806198948923345
training loss 1.1162300369057875e-06 mae 0.0006902254932555387
Epoch 726, training: loss: 0.0000011, mae: 0.0006914 test: loss0.0000971, mae:0.0071745
training loss 1.4994038792792708e-06 mae 0.0007399243186227977
training loss 1.1571357984563542e-06 mae 0.000697665736558573
training loss 1.1343139217703115e-06 mae 0.000691660481624969
training loss 1.1109233940829497e-06 mae 0.0006860978494703597
training loss 1.1135545850236355e-06 mae 0.0006872904420348432
Epoch 727, training: loss: 0.0000011, mae: 0.0006904 test: loss0.0000972, mae:0.0071796
training loss 6.481170089500665e-07 mae 0.0005180069128982723
training loss 1.0818559489650274e-06 mae 0.0006723436367168438
training loss 1.1098395963481293e-06 mae 0.0006847443265983335
training loss 1.1272135731547046e-06 mae 0.0006899565898902738
training loss 1.1242604482311945e-06 mae 0.0006894276488111328
Epoch 728, training: loss: 0.0000011, mae: 0.0006888 test: loss0.0000980, mae:0.0072178
training loss 1.8465356106389663e-06 mae 0.0008247283403761685
training loss 1.057588325934108e-06 mae 0.0006833877306038915
training loss 1.1322613168899678e-06 mae 0.0007002329601362198
training loss 1.1153742958934207e-06 mae 0.0006906120478567227
training loss 1.1308283288696636e-06 mae 0.000692265889397706
Epoch 729, training: loss: 0.0000011, mae: 0.0006884 test: loss0.0000979, mae:0.0072113
training loss 1.7636015172683983e-06 mae 0.000804973125923425
training loss 1.09979360757404e-06 mae 0.0006870930870611437
training loss 1.1171908470102362e-06 mae 0.0006927412883885719
training loss 1.1137863271788399e-06 mae 0.0006860875752399866
training loss 1.124957596281925e-06 mae 0.0006913921709777092
Epoch 730, training: loss: 0.0000011, mae: 0.0006895 test: loss0.0000974, mae:0.0071924
training loss 9.257964279640873e-07 mae 0.0006364081054925919
training loss 1.0930120138299587e-06 mae 0.0006772982802324215
training loss 1.1222830889914577e-06 mae 0.0006842558306185437
training loss 1.105980823427033e-06 mae 0.0006821999863709094
training loss 1.110929469968508e-06 mae 0.0006852358199113897
Epoch 731, training: loss: 0.0000011, mae: 0.0006874 test: loss0.0000972, mae:0.0071827
training loss 1.1732068969649845e-06 mae 0.0006473222747445107
training loss 1.181108000109281e-06 mae 0.0006918790442047314
training loss 1.1460059469489277e-06 mae 0.0006879782920823167
training loss 1.1241937612574723e-06 mae 0.0006864525966380188
training loss 1.1154662139285583e-06 mae 0.0006874745509839749
Epoch 732, training: loss: 0.0000011, mae: 0.0006870 test: loss0.0000977, mae:0.0072082
training loss 6.639827461185632e-07 mae 0.0006147821550257504
training loss 1.0417487161186158e-06 mae 0.0006717665671078742
training loss 1.0823543059832273e-06 mae 0.0006744533576679847
training loss 1.1348870290798286e-06 mae 0.0006893578626135276
training loss 1.1202912170873207e-06 mae 0.0006890376196914716
Epoch 733, training: loss: 0.0000011, mae: 0.0006884 test: loss0.0000987, mae:0.0072325
training loss 8.306876679853303e-07 mae 0.0006593841244466603
training loss 1.0828331480796823e-06 mae 0.0006843558193056606
training loss 1.0915714812658693e-06 mae 0.0006827794728307329
training loss 1.1004784205728577e-06 mae 0.0006812481521490023
training loss 1.1096415913930264e-06 mae 0.0006865889661648862
Epoch 734, training: loss: 0.0000011, mae: 0.0006866 test: loss0.0000977, mae:0.0072043
training loss 6.597909418815107e-07 mae 0.000565103895496577
training loss 1.150862812317107e-06 mae 0.0007015711584530185
training loss 1.1418917469231617e-06 mae 0.0006923889104475144
training loss 1.1047944197747442e-06 mae 0.0006827831165001582
training loss 1.1054394645607911e-06 mae 0.0006842105032584222
Epoch 735, training: loss: 0.0000011, mae: 0.0006874 test: loss0.0000980, mae:0.0072162
training loss 1.438927824892744e-06 mae 0.0007800462772138417
training loss 1.1284530924682504e-06 mae 0.0006849087258873909
training loss 1.1424302097156667e-06 mae 0.0006954085789528669
training loss 1.1432764656983748e-06 mae 0.0006905046809350593
training loss 1.1180674434356414e-06 mae 0.0006877232183104575
Epoch 736, training: loss: 0.0000011, mae: 0.0006881 test: loss0.0000980, mae:0.0071969
training loss 6.369000402628444e-07 mae 0.0005979612469673157
training loss 9.749331041473172e-07 mae 0.0006493866100094703
training loss 1.0990182466000554e-06 mae 0.0006697191960537936
training loss 1.1090002206684632e-06 mae 0.0006780422611442693
training loss 1.1119046347974385e-06 mae 0.000684932526867067
Epoch 737, training: loss: 0.0000011, mae: 0.0006868 test: loss0.0000979, mae:0.0072050
training loss 7.114264803931292e-07 mae 0.0005534759839065373
training loss 9.852881555170645e-07 mae 0.0006549753421031888
training loss 1.067029587872666e-06 mae 0.0006714579715545631
training loss 1.088234537126374e-06 mae 0.0006763168316895747
training loss 1.11852321410947e-06 mae 0.000688134770219525
Epoch 738, training: loss: 0.0000011, mae: 0.0006840 test: loss0.0000974, mae:0.0071862
training loss 1.0350811407988658e-06 mae 0.0006563237984664738
training loss 1.1161455716444155e-06 mae 0.0006899939194851207
training loss 1.1109402385833472e-06 mae 0.0006850880886529487
training loss 1.1045167483318789e-06 mae 0.0006831916647004148
training loss 1.1120672539206918e-06 mae 0.0006863361139682155
Epoch 739, training: loss: 0.0000011, mae: 0.0006853 test: loss0.0000976, mae:0.0071904
training loss 6.252045068322332e-07 mae 0.0005767873371951282
training loss 1.1997198885405939e-06 mae 0.0006984510600530342
training loss 1.1464296836821986e-06 mae 0.0006939925617621384
training loss 1.138598326109642e-06 mae 0.0006908767300408138
training loss 1.1143940163938218e-06 mae 0.0006886135540040798
Epoch 740, training: loss: 0.0000011, mae: 0.0006863 test: loss0.0000979, mae:0.0072127
training loss 9.596698191671749e-07 mae 0.0006503909826278687
training loss 1.109463230410684e-06 mae 0.0006742952094760303
training loss 1.1170255393131126e-06 mae 0.0006825694206889978
training loss 1.0884401922866778e-06 mae 0.0006783964031064478
training loss 1.1171968273699451e-06 mae 0.0006872068863855539
Epoch 741, training: loss: 0.0000011, mae: 0.0006842 test: loss0.0000974, mae:0.0071835
training loss 1.5261412045219913e-06 mae 0.0007174123893491924
training loss 1.212779043761853e-06 mae 0.0007074652867409966
training loss 1.1373408664674052e-06 mae 0.0006934606307647236
training loss 1.1196730427412367e-06 mae 0.0006889127004453726
training loss 1.1156980886978057e-06 mae 0.0006887760262100014
Epoch 742, training: loss: 0.0000011, mae: 0.0006884 test: loss0.0000978, mae:0.0071950
training loss 1.1775582606787793e-06 mae 0.0008066396112553775
training loss 1.0540937714321284e-06 mae 0.0006891344770944365
training loss 1.0830118402610778e-06 mae 0.0006836041255443736
training loss 1.0856120498100312e-06 mae 0.0006818091760879842
training loss 1.1165979038985969e-06 mae 0.000688119924370088
Epoch 743, training: loss: 0.0000011, mae: 0.0006870 test: loss0.0000973, mae:0.0071786
training loss 2.0092502381885424e-06 mae 0.0008404608815908432
training loss 1.0039128749569644e-06 mae 0.0006562107945701073
training loss 1.1144421075123946e-06 mae 0.000683963648043573
training loss 1.1064625392157042e-06 mae 0.0006837339750691532
training loss 1.1044694748868016e-06 mae 0.0006857280060822672
Epoch 744, training: loss: 0.0000011, mae: 0.0006848 test: loss0.0000975, mae:0.0071968
training loss 6.157342795631848e-07 mae 0.0005585731123574078
training loss 1.0143624491302388e-06 mae 0.0006663584687938806
training loss 1.0991977330719454e-06 mae 0.0006808013428177795
training loss 1.098245525626653e-06 mae 0.0006823390832136918
training loss 1.1000434969742737e-06 mae 0.0006830589650303068
Epoch 745, training: loss: 0.0000011, mae: 0.0006839 test: loss0.0000975, mae:0.0071947
training loss 1.6979187194010592e-06 mae 0.0008215720881707966
training loss 1.1092603490040204e-06 mae 0.0006877770738712712
training loss 1.0869037951822364e-06 mae 0.0006835842824951369
training loss 1.1152043997609385e-06 mae 0.0006880343616832149
training loss 1.1145729726267384e-06 mae 0.0006873573452252804
Epoch 746, training: loss: 0.0000011, mae: 0.0006851 test: loss0.0000978, mae:0.0071997
training loss 1.0082504786623758e-06 mae 0.000681395351421088
training loss 1.0996570942026273e-06 mae 0.000683373708834908
training loss 1.1333844879730596e-06 mae 0.0006887678948512281
training loss 1.1226312011746775e-06 mae 0.0006873004652316134
training loss 1.1090253783312441e-06 mae 0.0006840638843867744
Epoch 747, training: loss: 0.0000011, mae: 0.0006848 test: loss0.0000976, mae:0.0071928
training loss 1.0655850246621412e-06 mae 0.0006895636324770749
training loss 1.084055551312785e-06 mae 0.0006852686672252328
training loss 1.0700171960796989e-06 mae 0.0006814937520481756
training loss 1.0708764665310655e-06 mae 0.0006776784594087487
training loss 1.097640590054951e-06 mae 0.0006832881765185271
Epoch 748, training: loss: 0.0000011, mae: 0.0006849 test: loss0.0000978, mae:0.0072053
training loss 9.196319297188893e-07 mae 0.0006633363664150238
training loss 1.0173447045488894e-06 mae 0.000647994731301807
training loss 1.051010772784193e-06 mae 0.0006636699130251485
training loss 1.0650940776528263e-06 mae 0.0006710203599249509
training loss 1.1048304412016838e-06 mae 0.0006826440318642804
Epoch 749, training: loss: 0.0000011, mae: 0.0006825 test: loss0.0000976, mae:0.0072004
training loss 8.393188295485743e-07 mae 0.0006221123039722443
training loss 1.0794490486998754e-06 mae 0.0006818895173423431
training loss 1.0884240138878313e-06 mae 0.0006820234524242876
training loss 1.1064641466421838e-06 mae 0.0006854433386312142
training loss 1.1003744191384606e-06 mae 0.0006821768510963917
Epoch 750, training: loss: 0.0000011, mae: 0.0006839 test: loss0.0000976, mae:0.0071951
training loss 1.2709475640804158e-06 mae 0.0007534551550634205
training loss 1.073130942818263e-06 mae 0.0006665597851251197
training loss 1.0630390957291214e-06 mae 0.0006715088712724644
training loss 1.0920264578875722e-06 mae 0.0006789676758340429
training loss 1.1075273500604395e-06 mae 0.0006826939754852959
Epoch 751, training: loss: 0.0000011, mae: 0.0006805 test: loss0.0000974, mae:0.0071887
training loss 1.5070799008753966e-06 mae 0.0006898197461850941
training loss 1.0728187420432274e-06 mae 0.0006737156072631479
training loss 1.0788285476224918e-06 mae 0.000676641241292294
training loss 1.101882565952664e-06 mae 0.0006776294221527972
training loss 1.1089805466903569e-06 mae 0.0006838788868296441
Epoch 752, training: loss: 0.0000011, mae: 0.0006840 test: loss0.0000985, mae:0.0072221
training loss 8.230610433201946e-07 mae 0.0005861654062755406
training loss 1.1419793038697337e-06 mae 0.0006942334778917334
training loss 1.1107963445337562e-06 mae 0.0006839955164497662
training loss 1.0924085345036864e-06 mae 0.0006768092374696658
training loss 1.1040803323355622e-06 mae 0.000682430026620691
Epoch 753, training: loss: 0.0000011, mae: 0.0006802 test: loss0.0000976, mae:0.0071986
training loss 1.8092764548782725e-06 mae 0.0008263463969342411
training loss 1.107432426824387e-06 mae 0.0006634838457958883
training loss 1.1024177841492093e-06 mae 0.0006750769553576956
training loss 1.0946420248000026e-06 mae 0.0006820213570820773
training loss 1.1028899844821093e-06 mae 0.0006828214753122621
Epoch 754, training: loss: 0.0000011, mae: 0.0006827 test: loss0.0000978, mae:0.0072077
training loss 7.575295057904441e-07 mae 0.0006420292775146663
training loss 1.0572001611745134e-06 mae 0.0006639997020601203
training loss 1.0763792553155666e-06 mae 0.0006753843768135563
training loss 1.0907162368462713e-06 mae 0.0006765967255399645
training loss 1.095176997129933e-06 mae 0.0006810118596588459
Epoch 755, training: loss: 0.0000011, mae: 0.0006803 test: loss0.0000977, mae:0.0072021
training loss 1.063263084688515e-06 mae 0.0007281508296728134
training loss 1.078030019622626e-06 mae 0.0006743160809171113
training loss 1.0747350129935116e-06 mae 0.0006684966608397849
training loss 1.0741631206351996e-06 mae 0.0006739909790336026
training loss 1.0959641902465947e-06 mae 0.0006809332073534104
Epoch 756, training: loss: 0.0000011, mae: 0.0006822 test: loss0.0000974, mae:0.0071836
training loss 1.4408193464987562e-06 mae 0.0008103434811346233
training loss 1.1214489026823527e-06 mae 0.0006774123963516425
training loss 1.1529642643827265e-06 mae 0.0006942356513619499
training loss 1.120872147133176e-06 mae 0.0006855856442194903
training loss 1.0980230826331912e-06 mae 0.0006805319314132178
Epoch 757, training: loss: 0.0000011, mae: 0.0006803 test: loss0.0000974, mae:0.0071913
training loss 7.152027023948904e-07 mae 0.0005573121015913785
training loss 1.0568627224787914e-06 mae 0.0006634274705349667
training loss 1.1089385319193374e-06 mae 0.0006816689922626469
training loss 1.1156172830631724e-06 mae 0.0006862852569370354
training loss 1.1052540027258459e-06 mae 0.0006839209116024629
Epoch 758, training: loss: 0.0000011, mae: 0.0006819 test: loss0.0000976, mae:0.0071989
training loss 1.2822000599044259e-06 mae 0.0007642243872396648
training loss 1.216600820748235e-06 mae 0.0007031940839107278
training loss 1.1288755737189928e-06 mae 0.0006855398581921134
training loss 1.1294516928016388e-06 mae 0.0006840082302933177
training loss 1.0936277031653925e-06 mae 0.0006798129254808439
Epoch 759, training: loss: 0.0000011, mae: 0.0006803 test: loss0.0000980, mae:0.0072070
training loss 1.146022782450018e-06 mae 0.0006971647962927818
training loss 1.049150965320009e-06 mae 0.0006718831553406942
training loss 1.057946341031518e-06 mae 0.0006746040754559645
training loss 1.0539695523636468e-06 mae 0.0006677035918067648
training loss 1.100024282343282e-06 mae 0.000681010514077272
Epoch 760, training: loss: 0.0000011, mae: 0.0006805 test: loss0.0000980, mae:0.0072158
training loss 9.454500400352117e-07 mae 0.0007475015590898693
training loss 1.02744516817777e-06 mae 0.000666926267406191
training loss 1.0833743822399135e-06 mae 0.0006698052808619334
training loss 1.080853955435312e-06 mae 0.0006723133850972237
training loss 1.091719865659947e-06 mae 0.0006770077758047045
Epoch 761, training: loss: 0.0000011, mae: 0.0006788 test: loss0.0000980, mae:0.0072096
training loss 7.302086828531174e-07 mae 0.0005604680627584457
training loss 1.0383083016293705e-06 mae 0.0006547563300723686
training loss 1.0105206086737413e-06 mae 0.0006561122286032037
training loss 1.0760824396141509e-06 mae 0.0006710353456992679
training loss 1.0877557521205883e-06 mae 0.0006774833703307034
Epoch 762, training: loss: 0.0000011, mae: 0.0006808 test: loss0.0000981, mae:0.0072262
training loss 8.995535267786181e-07 mae 0.0006193027948029339
training loss 1.1104052133554895e-06 mae 0.0006773802303416909
training loss 1.082654874283948e-06 mae 0.0006669854865142021
training loss 1.100472435826678e-06 mae 0.0006803322702165173
training loss 1.0949661726664629e-06 mae 0.0006809684486170088
Epoch 763, training: loss: 0.0000011, mae: 0.0006812 test: loss0.0000974, mae:0.0071849
training loss 1.1268156185906264e-06 mae 0.0006070708041079342
training loss 1.13570702883051e-06 mae 0.0006847347751460676
training loss 1.1368437713322736e-06 mae 0.0006837160871156703
training loss 1.1084292321022487e-06 mae 0.0006797417382700432
training loss 1.0867225135380029e-06 mae 0.0006784727027760216
Epoch 764, training: loss: 0.0000011, mae: 0.0006809 test: loss0.0000981, mae:0.0072057
training loss 7.560473136436485e-07 mae 0.0005799851496703923
training loss 1.0038927005588647e-06 mae 0.000651330643735242
training loss 1.0339754096706443e-06 mae 0.0006656430789619909
training loss 1.048596337284003e-06 mae 0.0006751793206188309
training loss 1.0983512931400562e-06 mae 0.0006815083255407527
Epoch 765, training: loss: 0.0000011, mae: 0.0006795 test: loss0.0000977, mae:0.0072027
training loss 5.221155561230262e-07 mae 0.0004883470828644931
training loss 1.0358588144977233e-06 mae 0.0006580820533118264
training loss 1.0194238805272807e-06 mae 0.0006608973959769756
training loss 1.0726641611856256e-06 mae 0.0006726074719798459
training loss 1.0962317713083796e-06 mae 0.0006813894873612385
Epoch 766, training: loss: 0.0000011, mae: 0.0006808 test: loss0.0000984, mae:0.0072224
training loss 1.0040497500085621e-06 mae 0.0005927812308073044
training loss 1.097617986461581e-06 mae 0.0006800149664438021
training loss 1.0989333163414107e-06 mae 0.0006771321644162817
training loss 1.1020456804017643e-06 mae 0.0006804448902832085
training loss 1.0850520794605953e-06 mae 0.0006792701626444393
Epoch 767, training: loss: 0.0000011, mae: 0.0006790 test: loss0.0000980, mae:0.0072145
training loss 1.219604541802255e-06 mae 0.0006775828078389168
training loss 1.1168857565512463e-06 mae 0.0006790691153511553
training loss 1.0771096223343557e-06 mae 0.0006691983904998452
training loss 1.0958415777279262e-06 mae 0.0006798979215669318
training loss 1.1006819595682896e-06 mae 0.0006817528285579616
Epoch 768, training: loss: 0.0000011, mae: 0.0006803 test: loss0.0000980, mae:0.0072124
training loss 1.3151452549209353e-06 mae 0.0006636853213422
training loss 1.086970834595538e-06 mae 0.0006740765683763822
training loss 1.1313849945441457e-06 mae 0.0006816990104144312
training loss 1.1093723528860733e-06 mae 0.0006814517912652269
training loss 1.0938922139755516e-06 mae 0.0006801421071787307
Epoch 769, training: loss: 0.0000011, mae: 0.0006783 test: loss0.0000977, mae:0.0071981
training loss 1.6952058103925083e-06 mae 0.000835322483908385
training loss 1.1571855771299126e-06 mae 0.0007025850714211738
training loss 1.1004508040302723e-06 mae 0.0006889864657705855
training loss 1.1237952935968397e-06 mae 0.0006878754231594627
training loss 1.0921472817900755e-06 mae 0.0006795130328064441
Epoch 770, training: loss: 0.0000011, mae: 0.0006789 test: loss0.0000976, mae:0.0071916
training loss 1.8944193698189338e-06 mae 0.0008143826271407306
training loss 1.1026200434785357e-06 mae 0.0006850365613240237
training loss 1.033525953541564e-06 mae 0.0006647640672088188
training loss 1.0808410999761568e-06 mae 0.0006763177323663942
training loss 1.089480277801497e-06 mae 0.0006781766806214827
Epoch 771, training: loss: 0.0000011, mae: 0.0006772 test: loss0.0000978, mae:0.0072036
training loss 5.3153411272433e-07 mae 0.0005074292421340942
training loss 1.0264317765239527e-06 mae 0.000669037561986924
training loss 1.0976433022260789e-06 mae 0.0006798158649304456
training loss 1.0753150991977779e-06 mae 0.0006780284383667276
training loss 1.088770527762768e-06 mae 0.0006785947193544523
Epoch 772, training: loss: 0.0000011, mae: 0.0006774 test: loss0.0000978, mae:0.0072074
training loss 1.0478894409970962e-06 mae 0.0006503972108475864
training loss 1.1267502949138756e-06 mae 0.0006912899381645464
training loss 1.066052201304947e-06 mae 0.0006739550202711495
training loss 1.07363652414399e-06 mae 0.0006775001597063403
training loss 1.0797076400090715e-06 mae 0.0006774514147592473
Epoch 773, training: loss: 0.0000011, mae: 0.0006789 test: loss0.0000977, mae:0.0071995
training loss 8.825545592117123e-07 mae 0.000582442618906498
training loss 1.0765601062115928e-06 mae 0.0006623743786750475
training loss 1.0562903703925766e-06 mae 0.0006697150419862703
training loss 1.094544808248785e-06 mae 0.0006764466428685174
training loss 1.0891096739818926e-06 mae 0.0006778744008538056
Epoch 774, training: loss: 0.0000011, mae: 0.0006769 test: loss0.0000978, mae:0.0072013
training loss 1.943625193234766e-06 mae 0.000929451547563076
training loss 1.017640887740152e-06 mae 0.0006544973350851339
training loss 1.04592548567969e-06 mae 0.0006677904053900094
training loss 1.0723262116261926e-06 mae 0.0006752211243200995
training loss 1.084826380262181e-06 mae 0.0006781532621736268
Epoch 775, training: loss: 0.0000011, mae: 0.0006791 test: loss0.0000987, mae:0.0072129
training loss 1.1139882190036587e-06 mae 0.0006908408249728382
training loss 9.393304882156796e-07 mae 0.0006406092223044776
training loss 9.72888329053404e-07 mae 0.0006535758185407866
training loss 1.0598177130301954e-06 mae 0.0006735248158497747
training loss 1.079215360164763e-06 mae 0.0006770242579844287
Epoch 776, training: loss: 0.0000011, mae: 0.0006771 test: loss0.0000979, mae:0.0072066
training loss 7.052263413243054e-07 mae 0.0006196501781232655
training loss 1.1243327119869966e-06 mae 0.0006701629443377183
training loss 1.118252153881006e-06 mae 0.0006806003797158489
training loss 1.0975295946241664e-06 mae 0.0006763042742540655
training loss 1.0847408121103925e-06 mae 0.0006770370751086498
Epoch 777, training: loss: 0.0000011, mae: 0.0006775 test: loss0.0000976, mae:0.0071952
training loss 6.400277925422415e-07 mae 0.0005619370494969189
training loss 1.0454656181117768e-06 mae 0.0006659025219086487
training loss 1.072641846956973e-06 mae 0.000668948558896854
training loss 1.080410739393077e-06 mae 0.000672598837971971
training loss 1.087033714429482e-06 mae 0.0006777004048341784
Epoch 778, training: loss: 0.0000011, mae: 0.0006773 test: loss0.0000977, mae:0.0072005
training loss 1.8596909967527608e-06 mae 0.0007660590927116573
training loss 1.1046640208057984e-06 mae 0.0006736136160279607
training loss 1.0693494674130595e-06 mae 0.0006707266551133538
training loss 1.0961430132961162e-06 mae 0.0006774211470469437
training loss 1.0878616443398495e-06 mae 0.0006781766377621107
Epoch 779, training: loss: 0.0000011, mae: 0.0006780 test: loss0.0000978, mae:0.0072111
training loss 1.4549013940268196e-06 mae 0.0006564827635884285
training loss 1.040053850450325e-06 mae 0.0006721929764585096
training loss 1.0752554026742536e-06 mae 0.0006767701496494622
training loss 1.049692856056767e-06 mae 0.0006676394772247125
training loss 1.0781365555016934e-06 mae 0.0006746102548123279
Epoch 780, training: loss: 0.0000011, mae: 0.0006762 test: loss0.0000978, mae:0.0072098
training loss 1.3956669135950506e-06 mae 0.0007676773821003735
training loss 1.0961298413598807e-06 mae 0.0006743918826995309
training loss 1.069110980567827e-06 mae 0.0006669932955617805
training loss 1.075126598703683e-06 mae 0.0006745586436571836
training loss 1.080336122906637e-06 mae 0.0006767334264473739
Epoch 781, training: loss: 0.0000011, mae: 0.0006755 test: loss0.0000978, mae:0.0072092
training loss 1.1062221574320574e-06 mae 0.0006910369847901165
training loss 1.0965732428758201e-06 mae 0.0006829405736461209
training loss 1.062301029803913e-06 mae 0.0006723933983455865
training loss 1.0843560026173602e-06 mae 0.0006742061376772108
training loss 1.087270304798668e-06 mae 0.0006762920908944972
Epoch 782, training: loss: 0.0000011, mae: 0.0006755 test: loss0.0000985, mae:0.0072239
training loss 9.891726904243114e-07 mae 0.0006573038990609348
training loss 1.0188354627759916e-06 mae 0.0006597468009739017
training loss 1.0336080298103382e-06 mae 0.0006615460987461009
training loss 1.0530207939173014e-06 mae 0.0006655377901732817
training loss 1.0669323369912853e-06 mae 0.0006708836636740007
Epoch 783, training: loss: 0.0000011, mae: 0.0006742 test: loss0.0000980, mae:0.0072141
training loss 1.2678889333983534e-06 mae 0.0007764215697534382
training loss 1.0232219306446638e-06 mae 0.0006618289848762181
training loss 1.0411769693800492e-06 mae 0.0006679205747210596
training loss 1.070178038596005e-06 mae 0.00067167205019817
training loss 1.0869591463276497e-06 mae 0.000677487051168883
Epoch 784, training: loss: 0.0000011, mae: 0.0006769 test: loss0.0000975, mae:0.0071903
training loss 2.4600074084446533e-06 mae 0.0009662974625825882
training loss 1.0170626252129653e-06 mae 0.00065748882365833
training loss 1.0293682668933533e-06 mae 0.0006629850537894241
training loss 1.0364295566649533e-06 mae 0.0006680843044097063
training loss 1.0713501288858914e-06 mae 0.0006742734733948124
Epoch 785, training: loss: 0.0000011, mae: 0.0006748 test: loss0.0000978, mae:0.0072061
training loss 1.517906298431626e-06 mae 0.0007560107042081654
training loss 1.0927781728223989e-06 mae 0.0006805654568369407
training loss 1.120263443444945e-06 mae 0.0006808229286389628
training loss 1.0849400228196843e-06 mae 0.0006736045339397642
training loss 1.0790648253584747e-06 mae 0.0006762693157716106
Epoch 786, training: loss: 0.0000011, mae: 0.0006757 test: loss0.0000980, mae:0.0072200
training loss 1.1603948451011092e-06 mae 0.0007511728326790035
training loss 1.050224353530568e-06 mae 0.0006681863839427629
training loss 1.0734652295072333e-06 mae 0.0006677380331911811
training loss 1.079040648679643e-06 mae 0.0006707997486497957
training loss 1.0694268155475588e-06 mae 0.0006713147564160994
Epoch 787, training: loss: 0.0000011, mae: 0.0006740 test: loss0.0000984, mae:0.0072274
training loss 4.0246732169180177e-07 mae 0.0004314711841288954
training loss 1.0504212775240194e-06 mae 0.0006679716804201771
training loss 1.0391630315318772e-06 mae 0.0006668006429001505
training loss 1.0484245694596097e-06 mae 0.0006680259506514186
training loss 1.0691291798713643e-06 mae 0.0006722157620662701
Epoch 788, training: loss: 0.0000011, mae: 0.0006737 test: loss0.0000978, mae:0.0072083
training loss 1.25459177979792e-06 mae 0.0006460631266236305
training loss 1.0113830960696812e-06 mae 0.0006667754827432479
training loss 1.0653911165646412e-06 mae 0.0006723069867677065
training loss 1.0881848879762395e-06 mae 0.0006783078939249949
training loss 1.080747997452161e-06 mae 0.0006746461016093544
Epoch 789, training: loss: 0.0000011, mae: 0.0006749 test: loss0.0000990, mae:0.0072209
training loss 1.2401088724800502e-06 mae 0.0007889075204730034
training loss 1.109920416455697e-06 mae 0.0006751448729796808
training loss 1.1158857451882442e-06 mae 0.0006736324988259465
training loss 1.1053713106453603e-06 mae 0.0006774860408792383
training loss 1.0816609441897182e-06 mae 0.0006744265266745092
Epoch 790, training: loss: 0.0000011, mae: 0.0006718 test: loss0.0000986, mae:0.0072302
training loss 5.547369141822855e-07 mae 0.0005028086598031223
training loss 9.8708789840773e-07 mae 0.000649768404900005
training loss 1.0412524723546587e-06 mae 0.0006640929132838284
training loss 1.0670556501143147e-06 mae 0.0006685818153100448
training loss 1.0800292163799094e-06 mae 0.0006742689601291641
Epoch 791, training: loss: 0.0000011, mae: 0.0006731 test: loss0.0000984, mae:0.0072286
training loss 1.6708551129340776e-06 mae 0.0008216965943574905
training loss 1.1867829856800754e-06 mae 0.0007045410548373331
training loss 1.1095588907419554e-06 mae 0.000686255424274224
training loss 1.089973920427489e-06 mae 0.0006783070034634274
training loss 1.0844941705662777e-06 mae 0.000676514937910738
Epoch 792, training: loss: 0.0000011, mae: 0.0006755 test: loss0.0000977, mae:0.0071991
training loss 1.3285289242048748e-06 mae 0.0007835884462110698
training loss 1.0174032387808177e-06 mae 0.0006630423435411762
training loss 1.0273830971617842e-06 mae 0.0006632282681611027
training loss 1.0465277738383783e-06 mae 0.0006654471022520982
training loss 1.074357585219613e-06 mae 0.0006739973529028841
Epoch 793, training: loss: 0.0000011, mae: 0.0006723 test: loss0.0000980, mae:0.0072145
training loss 1.2223983958392637e-06 mae 0.0006766468286514282
training loss 9.728184297683562e-07 mae 0.0006290715890845248
training loss 1.0387232483517067e-06 mae 0.0006628457821633595
training loss 1.0525058958769806e-06 mae 0.000663898610787479
training loss 1.0567565201410016e-06 mae 0.0006665441047444258
Epoch 794, training: loss: 0.0000011, mae: 0.0006722 test: loss0.0000977, mae:0.0072046
training loss 8.886821092346509e-07 mae 0.0005996490945108235
training loss 1.1202665328202034e-06 mae 0.000677143674978402
training loss 1.141251805402045e-06 mae 0.000684138170459612
training loss 1.0764882523103287e-06 mae 0.0006691830414686576
training loss 1.0678400108654348e-06 mae 0.0006706679764896427
Epoch 795, training: loss: 0.0000011, mae: 0.0006722 test: loss0.0000978, mae:0.0072004
training loss 1.3054790315436549e-06 mae 0.0007341147284023464
training loss 1.1255326018825004e-06 mae 0.0006832065900751189
training loss 1.0944494165684804e-06 mae 0.000684043576959336
training loss 1.0797698840537161e-06 mae 0.0006784232896490529
training loss 1.0836476044279147e-06 mae 0.0006763877727054246
Epoch 796, training: loss: 0.0000011, mae: 0.0006749 test: loss0.0000982, mae:0.0072239
training loss 6.281740070335218e-07 mae 0.0005303376237861812
training loss 1.0964279639148455e-06 mae 0.0006799498948442072
training loss 1.093606948418233e-06 mae 0.0006744787791434967
training loss 1.0706043486149329e-06 mae 0.0006718255430290364
training loss 1.0593188823241318e-06 mae 0.0006696124216808071
Epoch 797, training: loss: 0.0000011, mae: 0.0006737 test: loss0.0000986, mae:0.0072319
training loss 9.233017408405431e-07 mae 0.0006070096860639751
training loss 9.80715844777888e-07 mae 0.000651451979316406
training loss 1.0357560117566629e-06 mae 0.0006668054922898809
training loss 1.0536747207960976e-06 mae 0.0006684260855317859
training loss 1.080590625740037e-06 mae 0.0006757831782744319
Epoch 798, training: loss: 0.0000011, mae: 0.0006726 test: loss0.0000977, mae:0.0071995
training loss 6.961920462345006e-07 mae 0.0005915897781960666
training loss 9.967914985620654e-07 mae 0.0006457531605573261
training loss 1.0529238712515148e-06 mae 0.000664641750148857
training loss 1.036064423528571e-06 mae 0.0006628280092302139
training loss 1.075150373170985e-06 mae 0.0006727312919706916
Epoch 799, training: loss: 0.0000011, mae: 0.0006709 test: loss0.0000981, mae:0.0072127
current learning rate: 1.953125e-06

Process finished with exit code 0


























ssh://jeffzhu@172.16.46.217:22/home/jeffzhu/anaconda3/bin/python3.6 -u /home/jeffzhu/AL/pre_training/train_part.py
Namespace(al_method='msg_mask', bald_ft_epochs=5, batch_data_num=100, batchsize=48, data_mix=False, data_mixing_rate=0.5, dataset='qm9', device=1, epochs=800, ft_epochs=5, ft_method='by_valid', init_data_num=5000, k_center_ft_epochs=10, lr=0.0005, mc_sampling_num=80, model_num=4, multi_gpu=False, prop_name='homo', qbc_ft_epochs=5, re_init=False, save_model=True, shuffle=True, test_freq=5, test_use_all=False, use_default=False, use_tb=True, workers=0)
1119_22_51  model SchNetModel(
  (activation): ShiftSoftplus(
    beta=1, threshold=20
    (softplus): Softplus(beta=1, threshold=20)
  )
  (embedding_layer): AtomEmbedding(
    (embedding): Embedding(100, 48, padding_idx=0)
  )
  (rbf_layer): RBFLayer()
  (conv_layers): ModuleList(
    (0): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (1): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (2): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (3): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
  )
  (atom_dense_layer1): Linear(in_features=48, out_features=64, bias=True)
  (atom_dense_layer2): Linear(in_features=64, out_features=1, bias=True)
)  optimizer Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
start
SchNetModel(
  (activation): ShiftSoftplus(
    beta=1, threshold=20
    (softplus): Softplus(beta=1, threshold=20)
  )
  (embedding_layer): AtomEmbedding(
    (embedding): Embedding(100, 48, padding_idx=0)
  )
  (rbf_layer): RBFLayer()
  (conv_layers): ModuleList(
    (0): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (1): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (2): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (3): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
  )
  (atom_dense_layer1): Linear(in_features=48, out_features=64, bias=True)
  (atom_dense_layer2): Linear(in_features=64, out_features=1, bias=True)
)
-0.23983721435070038 0.02228054590523243
training loss 0.025324473157525063 mae 0.12759414315223694
training loss 0.0013293022416559427 mae 0.022844889639493295
training loss 0.0008996854856665622 mae 0.019268537124637327
training loss 0.0007569799217047008 mae 0.018111468971219676
training loss 0.00067807266324184 mae 0.017451268291814405
Epoch  0, training: loss: 0.0006711, mae: 0.0174287 test: loss0.0004230, mae:0.0150426
training loss 0.00042806603596545756 mae 0.014697541482746601
training loss 0.0004106381327123838 mae 0.01495217564789688
training loss 0.00042732326990445275 mae 0.015138049978799747
training loss 0.0004302915925813826 mae 0.015266927711616285
training loss 0.0004372697479794709 mae 0.015375034420273785
Epoch  1, training: loss: 0.0004332, mae: 0.0152979 test: loss0.0004190, mae:0.0148473
training loss 0.0005263933562673628 mae 0.01700647734105587
training loss 0.00045630395442119564 mae 0.015423719077279756
training loss 0.00043587232401045856 mae 0.015223293260920168
training loss 0.00043808602963478624 mae 0.015385619142641692
training loss 0.0004350209229455481 mae 0.01532324933127236
Epoch  2, training: loss: 0.0004347, mae: 0.0153116 test: loss0.0004245, mae:0.0152709
training loss 0.0005957321845926344 mae 0.016240132972598076
training loss 0.00046262157597012964 mae 0.01570117731085595
training loss 0.00043074289131633 mae 0.015250367342983136
training loss 0.0004220161796508925 mae 0.015111696073748418
training loss 0.00042417083294374113 mae 0.015164719736991239
Epoch  3, training: loss: 0.0004247, mae: 0.0151661 test: loss0.0004075, mae:0.0146715
training loss 0.0004326222406234592 mae 0.01585039496421814
training loss 0.0004095560086402568 mae 0.014903157657268
training loss 0.00041774038953296306 mae 0.015034547315375638
training loss 0.0004100857069523118 mae 0.01491800317577771
training loss 0.00040296123992067916 mae 0.014766044553313679
Epoch  4, training: loss: 0.0004004, mae: 0.0146910 test: loss0.0004505, mae:0.0165150
training loss 0.00030717800837010145 mae 0.01343514397740364
training loss 0.00037302899399233584 mae 0.014426555886280303
training loss 0.00034979101337289596 mae 0.01398088950848226
training loss 0.0003353128159983046 mae 0.013659474520099087
training loss 0.0003253940435095147 mae 0.01343742001857331
Epoch  5, training: loss: 0.0003225, mae: 0.0133660 test: loss0.0002745, mae:0.0121189
training loss 0.0003908121434506029 mae 0.013747629709541798
training loss 0.0003148763610036385 mae 0.013143750656323108
training loss 0.0003089756623606687 mae 0.012924841374601462
training loss 0.0003000464001655061 mae 0.012817574695788859
training loss 0.00029754002579425324 mae 0.012747753638570282
Epoch  6, training: loss: 0.0002953, mae: 0.0126914 test: loss0.0002733, mae:0.0123386
training loss 0.00024130976817104965 mae 0.011909780092537403
training loss 0.0002934110750941853 mae 0.012779324333749566
training loss 0.00029161167444861826 mae 0.012773566575038553
training loss 0.00028689353165350896 mae 0.012665963953743312
training loss 0.00028706516948759104 mae 0.012590184585372012
Epoch  7, training: loss: 0.0002868, mae: 0.0125756 test: loss0.0003073, mae:0.0136288
training loss 0.00035491326707415283 mae 0.014683482237160206
training loss 0.000287254631984979 mae 0.012550052821490111
training loss 0.0002738500052416413 mae 0.012280935200281668
training loss 0.0002828925592722279 mae 0.01251112080691074
training loss 0.00028411429801831887 mae 0.012530841985115063
Epoch  8, training: loss: 0.0002841, mae: 0.0125261 test: loss0.0002670, mae:0.0121643
training loss 0.00023211732332129031 mae 0.012107324786484241
training loss 0.0002554494447608972 mae 0.011841529633338546
training loss 0.00026682105320528847 mae 0.012089526419076
training loss 0.0002676081936767892 mae 0.012116447966915881
training loss 0.00027359561105339854 mae 0.012238175691619734
Epoch  9, training: loss: 0.0002727, mae: 0.0122290 test: loss0.0002901, mae:0.0125918
training loss 0.0003538437595125288 mae 0.014573824591934681
training loss 0.0002674729460291108 mae 0.012073801538231326
training loss 0.00026601760601797756 mae 0.012026531206868075
training loss 0.0002658803354496206 mae 0.012073636338706844
training loss 0.00026384061938690363 mae 0.012024583827833935
Epoch 10, training: loss: 0.0002643, mae: 0.0120504 test: loss0.0002626, mae:0.0117265
training loss 0.00016588163271080703 mae 0.009249363094568253
training loss 0.0002533347424640156 mae 0.011749856685306512
training loss 0.000263624797496797 mae 0.012068084339694222
training loss 0.000267353561276644 mae 0.012093399447814516
training loss 0.0002639651033837367 mae 0.012076506984248678
Epoch 11, training: loss: 0.0002648, mae: 0.0120954 test: loss0.0002758, mae:0.0124165
training loss 0.00040347353206016123 mae 0.01438774075359106
training loss 0.0002620269475372362 mae 0.012116648895921662
training loss 0.00026801456412010004 mae 0.012151601982515046
training loss 0.0002661179816689565 mae 0.01209593902878611
training loss 0.000260366656772899 mae 0.012025205858644857
Epoch 12, training: loss: 0.0002618, mae: 0.0120716 test: loss0.0002763, mae:0.0123330
training loss 0.0001691095530986786 mae 0.010323199443519115
training loss 0.00025351759384148844 mae 0.011987283734568192
training loss 0.00026205772854443084 mae 0.012098903611007305
training loss 0.00025533898164007785 mae 0.011945483737335297
training loss 0.0002592950156905493 mae 0.012026738435660123
Epoch 13, training: loss: 0.0002594, mae: 0.0120334 test: loss0.0002533, mae:0.0118542
training loss 0.000203850693651475 mae 0.011415340006351471
training loss 0.00023855926273801527 mae 0.011702411771551064
training loss 0.0002447773346857231 mae 0.01178865337578377
training loss 0.00025093092024943087 mae 0.011873679463782457
training loss 0.00024907227920655586 mae 0.011798813308945935
Epoch 14, training: loss: 0.0002513, mae: 0.0118331 test: loss0.0002664, mae:0.0119536
training loss 0.0002051767223747447 mae 0.011324763298034668
training loss 0.00026617554263508525 mae 0.012485680809500172
training loss 0.00026561776376363925 mae 0.012245564149821754
training loss 0.00026308783043417993 mae 0.012176331126482686
training loss 0.00025405226412445847 mae 0.011980225023957188
Epoch 15, training: loss: 0.0002531, mae: 0.0119491 test: loss0.0002561, mae:0.0117557
training loss 0.00017233757534995675 mae 0.010094396770000458
training loss 0.00024700894416538157 mae 0.011685366486655732
training loss 0.00024156967070792802 mae 0.011726261116564279
training loss 0.0002475200312108801 mae 0.011773676852023365
training loss 0.00024221117530204587 mae 0.011635830130112998
Epoch 16, training: loss: 0.0002397, mae: 0.0115817 test: loss0.0002537, mae:0.0116511
training loss 0.00017283146735280752 mae 0.010301009751856327
training loss 0.00022431893597183494 mae 0.011216960902161456
training loss 0.00023080348117867172 mae 0.011427512779684353
training loss 0.00023305067453889596 mae 0.011466078216592408
training loss 0.00022946648830802764 mae 0.011384245812133032
Epoch 17, training: loss: 0.0002291, mae: 0.0113761 test: loss0.0002393, mae:0.0116288
training loss 0.00021249351266305894 mae 0.012323450297117233
training loss 0.00023116799089692384 mae 0.011294222637718801
training loss 0.00023560939598160033 mae 0.011504452251414263
training loss 0.00022888299507946273 mae 0.01139553783720495
training loss 0.0002247663793621459 mae 0.011299800493786878
Epoch 18, training: loss: 0.0002255, mae: 0.0113106 test: loss0.0002114, mae:0.0108490
training loss 0.00017555808881297708 mae 0.010505951941013336
training loss 0.000223425644685003 mae 0.011393674320596108
training loss 0.00022027555404756548 mae 0.01120693688880246
training loss 0.00022101456139256732 mae 0.011155747583395
training loss 0.0002203219498111519 mae 0.01112919884840425
Epoch 19, training: loss: 0.0002197, mae: 0.0111299 test: loss0.0002259, mae:0.0110321
training loss 0.0002395806513959542 mae 0.01217122096568346
training loss 0.00021464175683921023 mae 0.011090177987866542
training loss 0.00021106473891765326 mae 0.011012650685071353
training loss 0.00020922529219151692 mae 0.010944086799262372
training loss 0.0002109240820536979 mae 0.010961467974740466
Epoch 20, training: loss: 0.0002113, mae: 0.0109646 test: loss0.0002305, mae:0.0114249
training loss 0.00020609970670193434 mae 0.0106581412255764
training loss 0.00021601414796439748 mae 0.011060675040033514
training loss 0.00021389120467016384 mae 0.011006013402659998
training loss 0.00021625585936546472 mae 0.011068044893369574
training loss 0.00020910096928171483 mae 0.010884295231593195
Epoch 21, training: loss: 0.0002088, mae: 0.0108631 test: loss0.0002335, mae:0.0113072
training loss 0.00010762080637505278 mae 0.008144441060721874
training loss 0.00023505442040200875 mae 0.011542488820850851
training loss 0.0002186951785213499 mae 0.011165816705710822
training loss 0.00021209486198167486 mae 0.01101732576598987
training loss 0.00021588079230535538 mae 0.011050071896271626
Epoch 22, training: loss: 0.0002137, mae: 0.0109974 test: loss0.0002150, mae:0.0110583
training loss 0.0002434301859466359 mae 0.011332673020660877
training loss 0.00018881924274062516 mae 0.010598672599549972
training loss 0.000193063139749488 mae 0.010477541069878213
training loss 0.0001936432419692486 mae 0.010476490932929993
training loss 0.0001961854038892119 mae 0.010561622183454869
Epoch 23, training: loss: 0.0001957, mae: 0.0105507 test: loss0.0001926, mae:0.0103378
training loss 0.00010288650082657114 mae 0.00835255067795515
training loss 0.00019949412072652623 mae 0.010590751107562992
training loss 0.00019601047594310013 mae 0.010467367352788698
training loss 0.00019315360784715672 mae 0.010405212585253036
training loss 0.00019162531397844075 mae 0.01038343388012689
Epoch 24, training: loss: 0.0001911, mae: 0.0103577 test: loss0.0001826, mae:0.0101056
training loss 0.00011299190373392776 mae 0.008858907036483288
training loss 0.00019198110042934244 mae 0.010534453370115333
training loss 0.000193655200413253 mae 0.010453041541473107
training loss 0.00018721138602639198 mae 0.010325356306421837
training loss 0.0001863041508288947 mae 0.01030030192353239
Epoch 25, training: loss: 0.0001873, mae: 0.0103140 test: loss0.0001833, mae:0.0100900
training loss 0.00012994371354579926 mae 0.009063982404768467
training loss 0.00018725065556103728 mae 0.010145819170729202
training loss 0.00018342714042098515 mae 0.010181827890216415
training loss 0.00018324498981484114 mae 0.010150913176873068
training loss 0.00018488602980331917 mae 0.010193281846389691
Epoch 26, training: loss: 0.0001845, mae: 0.0102023 test: loss0.0001910, mae:0.0103724
training loss 0.0001691460347501561 mae 0.01057906448841095
training loss 0.00019404937541135138 mae 0.010414097293773121
training loss 0.00019895203581879765 mae 0.010532224845915743
training loss 0.00019272519898098206 mae 0.01045630100053667
training loss 0.0001925250208158792 mae 0.010432437161882564
Epoch 27, training: loss: 0.0001929, mae: 0.0104299 test: loss0.0001776, mae:0.0099309
training loss 0.0001555754424771294 mae 0.009393027983605862
training loss 0.0001713423801394289 mae 0.010008194374249263
training loss 0.00017855736446603762 mae 0.010070876725534405
training loss 0.00017872115295297097 mae 0.01002948641752368
training loss 0.00017560299023156476 mae 0.009945405336717773
Epoch 28, training: loss: 0.0001744, mae: 0.0099220 test: loss0.0001827, mae:0.0102871
training loss 0.00017464219126850367 mae 0.010251838713884354
training loss 0.00016788639186415819 mae 0.009932187385857105
training loss 0.00018377524586210008 mae 0.010245862762711134
training loss 0.00017860120667155448 mae 0.010079477636131236
training loss 0.00017425263740281015 mae 0.009944349221551598
Epoch 29, training: loss: 0.0001748, mae: 0.0099694 test: loss0.0002070, mae:0.0106753
training loss 0.00014604291936848313 mae 0.00864101480692625
training loss 0.00017519510098888236 mae 0.009832778945565224
training loss 0.00018058592133168692 mae 0.010058651042014064
training loss 0.00017725861729053316 mae 0.009971675025114158
training loss 0.0001731002629825507 mae 0.00986738453864755
Epoch 30, training: loss: 0.0001734, mae: 0.0098642 test: loss0.0001660, mae:0.0096092
training loss 0.00010207187006017193 mae 0.008097310550510883
training loss 0.0001601493804984927 mae 0.009495179451929008
training loss 0.0001613459472618259 mae 0.009567255417050998
training loss 0.0001615520492207647 mae 0.009630622209623355
training loss 0.0001641057308404752 mae 0.009678849454654092
Epoch 31, training: loss: 0.0001652, mae: 0.0096895 test: loss0.0001574, mae:0.0094483
training loss 0.00015592867566738278 mae 0.009544980712234974
training loss 0.00016122645505603546 mae 0.0095073165207663
training loss 0.00016884186881707317 mae 0.009733037912462016
training loss 0.00017048789007860596 mae 0.009806439783177434
training loss 0.00016709926776502246 mae 0.009743607352807453
Epoch 32, training: loss: 0.0001675, mae: 0.0097607 test: loss0.0001814, mae:0.0100547
training loss 0.00015941036690492183 mae 0.009608526714146137
training loss 0.0001569700944659702 mae 0.009558429950665607
training loss 0.00016421711494907576 mae 0.009672319470434496
training loss 0.00016926898308686577 mae 0.009788592700655293
training loss 0.00016706385913629928 mae 0.009755200727272836
Epoch 33, training: loss: 0.0001674, mae: 0.0097647 test: loss0.0001649, mae:0.0095817
training loss 0.00012477693962864578 mae 0.008462649770081043
training loss 0.00015134066123602118 mae 0.009373134111656863
training loss 0.00015488900587142513 mae 0.0094333909737813
training loss 0.00016108362374415573 mae 0.00955419449925126
training loss 0.00016579425052319193 mae 0.009672893412800409
Epoch 34, training: loss: 0.0001656, mae: 0.0096763 test: loss0.0001817, mae:0.0100800
training loss 0.00017541406850796193 mae 0.010365903377532959
training loss 0.00017518935012905037 mae 0.009973068849421014
training loss 0.00017027982964761332 mae 0.00984964834175783
training loss 0.0001695652097033411 mae 0.00980355721953886
training loss 0.00016643704565019534 mae 0.009701664566493297
Epoch 35, training: loss: 0.0001658, mae: 0.0096918 test: loss0.0001679, mae:0.0095683
training loss 0.00013974652392789721 mae 0.009814382530748844
training loss 0.0001499143485596184 mae 0.009356596625830028
training loss 0.00015828221044238044 mae 0.009462896771360153
training loss 0.0001575584209308996 mae 0.00945514152504948
training loss 0.00015496053772460697 mae 0.009388225310625725
Epoch 36, training: loss: 0.0001548, mae: 0.0093908 test: loss0.0001571, mae:0.0093107
training loss 0.00015707832062616944 mae 0.009592247195541859
training loss 0.0001509950750772202 mae 0.009237184645790679
training loss 0.00015227254946937428 mae 0.009274230246422904
training loss 0.00015029782093583372 mae 0.009265473624382984
training loss 0.00015498525100332723 mae 0.009373511291984743
Epoch 37, training: loss: 0.0001560, mae: 0.0094060 test: loss0.0001839, mae:0.0100232
training loss 0.00020472968753892928 mae 0.01001740898936987
training loss 0.000165870077348789 mae 0.00958538828782883
training loss 0.0001576497783353622 mae 0.009417870939515606
training loss 0.00015387234585155396 mae 0.009320151995226057
training loss 0.0001511565872112728 mae 0.009247993174662343
Epoch 38, training: loss: 0.0001510, mae: 0.0092421 test: loss0.0001516, mae:0.0092012
training loss 0.000155166897457093 mae 0.009372946806252003
training loss 0.00015493317800528348 mae 0.0092185971728873
training loss 0.00015023847551881524 mae 0.00925220408947161
training loss 0.00015323531807848208 mae 0.009310857161558819
training loss 0.0001558927275318321 mae 0.009398867123625919
Epoch 39, training: loss: 0.0001565, mae: 0.0094099 test: loss0.0001830, mae:0.0102861
training loss 0.00013616392971016467 mae 0.008606046438217163
training loss 0.0001595082723332441 mae 0.009577229272062875
training loss 0.00016302348189182702 mae 0.00960734901986648
training loss 0.00016254211089031045 mae 0.009608122174757603
training loss 0.00015780178686535556 mae 0.009488952478199308
Epoch 40, training: loss: 0.0001570, mae: 0.0094760 test: loss0.0001650, mae:0.0094801
training loss 0.00018849170010071248 mae 0.00995423924177885
training loss 0.0001439943225705064 mae 0.009164173393418974
training loss 0.00014769712792672582 mae 0.009255357630698393
training loss 0.0001530668218616266 mae 0.009352978895609543
training loss 0.00015038281668438598 mae 0.009266901216863545
Epoch 41, training: loss: 0.0001513, mae: 0.0092849 test: loss0.0001614, mae:0.0094490
training loss 0.00033078473643399775 mae 0.012337875552475452
training loss 0.0001469649155920956 mae 0.009268221253638758
training loss 0.00015252366200110205 mae 0.009346896796348956
training loss 0.00015356863439484 mae 0.009338960480191645
training loss 0.00015455972082132195 mae 0.009377911473758774
Epoch 42, training: loss: 0.0001550, mae: 0.0093903 test: loss0.0001636, mae:0.0096458
training loss 0.00014345173258334398 mae 0.009974677115678787
training loss 0.00013766734628006819 mae 0.008884849602027851
training loss 0.00013644880340561407 mae 0.008840072711026018
training loss 0.00013842817760398004 mae 0.008893267266970399
training loss 0.0001404693613578312 mae 0.00896963667337648
Epoch 43, training: loss: 0.0001400, mae: 0.0089570 test: loss0.0001601, mae:0.0094979
training loss 0.00010734932584455237 mae 0.0083928182721138
training loss 0.00015240209647933678 mae 0.009354608023867885
training loss 0.00014688943258871657 mae 0.00911484155711709
training loss 0.00014672368494655 mae 0.009174361985349499
training loss 0.000145152050140835 mae 0.009103439477230633
Epoch 44, training: loss: 0.0001446, mae: 0.0090856 test: loss0.0001473, mae:0.0090867
training loss 0.00013713205407839268 mae 0.009324057959020138
training loss 0.00015329595151021804 mae 0.009405812805555028
training loss 0.00014721034745205115 mae 0.009246749466856812
training loss 0.0001488282659257969 mae 0.009256941966391754
training loss 0.00014983372486892052 mae 0.009248573226455727
Epoch 45, training: loss: 0.0001494, mae: 0.0092408 test: loss0.0001528, mae:0.0094180
training loss 0.00011043826816603541 mae 0.008024916052818298
training loss 0.00013953261405535443 mae 0.008999584388791348
training loss 0.00014678007393722692 mae 0.009188873993029981
training loss 0.00014257611387288996 mae 0.009027421798260988
training loss 0.00014784083901053124 mae 0.009209595284707358
Epoch 46, training: loss: 0.0001475, mae: 0.0092019 test: loss0.0001627, mae:0.0094502
training loss 0.00010783624748000875 mae 0.007708611432462931
training loss 0.00013261371635713194 mae 0.008688120737525763
training loss 0.0001318387114206596 mae 0.008754396064225399
training loss 0.00013546695329853342 mae 0.008793905398974946
training loss 0.00013714052687673626 mae 0.008866760539196763
Epoch 47, training: loss: 0.0001390, mae: 0.0089010 test: loss0.0001567, mae:0.0093401
training loss 0.00012971436080988497 mae 0.009491528384387493
training loss 0.00012967116986949213 mae 0.008728444311475635
training loss 0.00013299802864848704 mae 0.008794909767290153
training loss 0.00013160171621943222 mae 0.008735226044764387
training loss 0.0001318851929356053 mae 0.008715476370439753
Epoch 48, training: loss: 0.0001329, mae: 0.0087217 test: loss0.0001454, mae:0.0090853
training loss 0.00011488387826830149 mae 0.008308774791657925
training loss 0.0001226330329218934 mae 0.008429159318991736
training loss 0.00012240768611975164 mae 0.008438959261310278
training loss 0.000127167578808584 mae 0.008561872448525484
training loss 0.00013103152357970844 mae 0.0086835868892941
Epoch 49, training: loss: 0.0001321, mae: 0.0087025 test: loss0.0001433, mae:0.0089109
training loss 0.0002385900734225288 mae 0.011407795362174511
training loss 0.00013021977942758792 mae 0.008576756370637343
training loss 0.00014459841640747958 mae 0.008925599437944663
training loss 0.0001398818294908189 mae 0.00888788099986631
training loss 0.00013836495356680015 mae 0.008881279793266786
Epoch 50, training: loss: 0.0001376, mae: 0.0088727 test: loss0.0001359, mae:0.0086290
training loss 0.00010493477020645514 mae 0.00834654364734888
training loss 0.00011718169237913874 mae 0.008243290159632178
training loss 0.00012181685804642998 mae 0.008335400262091421
training loss 0.00012892571817397372 mae 0.008525188250578197
training loss 0.0001275520835994726 mae 0.008517484641657094
Epoch 51, training: loss: 0.0001278, mae: 0.0085398 test: loss0.0001482, mae:0.0089524
training loss 0.0001342303294222802 mae 0.008949597366154194
training loss 0.00013033713584459914 mae 0.008705456640717448
training loss 0.00012794646397313694 mae 0.008604141528542974
training loss 0.00012816049655651167 mae 0.008584638237213062
training loss 0.00012715695966243645 mae 0.008544442324959373
Epoch 52, training: loss: 0.0001264, mae: 0.0085147 test: loss0.0001329, mae:0.0085840
training loss 0.00015281433297786862 mae 0.009757631458342075
training loss 0.00013006253849358896 mae 0.008621170587253334
training loss 0.00012594777635427753 mae 0.008493168597653653
training loss 0.00012910799406675488 mae 0.008623538551148986
training loss 0.00013284403260273683 mae 0.008710751583364174
Epoch 53, training: loss: 0.0001326, mae: 0.0086979 test: loss0.0001342, mae:0.0087417
training loss 0.00010706097236834466 mae 0.00807975884526968
training loss 0.00012148345608604324 mae 0.008289932026801741
training loss 0.00012099194948562644 mae 0.00825117018055355
training loss 0.0001243152999944001 mae 0.008386575228331108
training loss 0.00012452153586947585 mae 0.008412937890849457
Epoch 54, training: loss: 0.0001252, mae: 0.0084417 test: loss0.0001522, mae:0.0093059
training loss 0.00015532691031694412 mae 0.009647744707763195
training loss 0.00012792383110054822 mae 0.008596578804666508
training loss 0.0001238753430869218 mae 0.00842471013196034
training loss 0.0001212255408141657 mae 0.008320673368871217
training loss 0.00012440929059443105 mae 0.008421334827235385
Epoch 55, training: loss: 0.0001235, mae: 0.0084005 test: loss0.0001292, mae:0.0085158
training loss 8.676413563080132e-05 mae 0.007404051721096039
training loss 0.00011632726490657375 mae 0.008174730729603886
training loss 0.00011885633824985601 mae 0.008210866979443197
training loss 0.00012467729153430759 mae 0.008408405336964606
training loss 0.00012318179603846774 mae 0.008380579451719924
Epoch 56, training: loss: 0.0001225, mae: 0.0083633 test: loss0.0001407, mae:0.0088404
training loss 0.00012257556954864413 mae 0.009250719100236893
training loss 0.0001223098361686639 mae 0.008276341236470376
training loss 0.00011767629722172468 mae 0.008208427366127473
training loss 0.00011694018028129899 mae 0.008171634076298867
training loss 0.00011832140697703803 mae 0.00821167619352169
Epoch 57, training: loss: 0.0001186, mae: 0.0082234 test: loss0.0001329, mae:0.0085874
training loss 0.00018239494238514453 mae 0.008455462753772736
training loss 0.00011160710403369284 mae 0.008085321507179274
training loss 0.00010966343656360949 mae 0.008005574705208292
training loss 0.00011183158800507811 mae 0.00804733952495041
training loss 0.00011470990118064422 mae 0.008125957474575854
Epoch 58, training: loss: 0.0001143, mae: 0.0081207 test: loss0.0001281, mae:0.0084877
training loss 8.792828157311305e-05 mae 0.00754565978422761
training loss 0.000123879129401472 mae 0.008479983016264205
training loss 0.00011973287178304895 mae 0.00826955582522372
training loss 0.00012330189510132954 mae 0.008344908479280419
training loss 0.0001244118172702592 mae 0.008398101616428412
Epoch 59, training: loss: 0.0001229, mae: 0.0083489 test: loss0.0001314, mae:0.0085410
training loss 0.00010396856669103727 mae 0.008055738173425198
training loss 0.00011323714338062678 mae 0.007932012973755013
training loss 0.00012184315728338313 mae 0.008266779469639655
training loss 0.00012109751660559509 mae 0.00828082094413062
training loss 0.00012113090566679754 mae 0.008318210880165522
Epoch 60, training: loss: 0.0001210, mae: 0.0083075 test: loss0.0001240, mae:0.0082418
training loss 0.00014730704424437135 mae 0.00842174794524908
training loss 0.000134647266046765 mae 0.008634602377081618
training loss 0.0001271181413673307 mae 0.008439077299530848
training loss 0.00012045552773768883 mae 0.008243171424167046
training loss 0.00011720390296373186 mae 0.00820204767106629
Epoch 61, training: loss: 0.0001162, mae: 0.0081810 test: loss0.0001411, mae:0.0088423
training loss 0.00017902282706927508 mae 0.01029945071786642
training loss 0.00011603159352899623 mae 0.008259376310104248
training loss 0.00011648703899607983 mae 0.008205380728744932
training loss 0.00011512305605262274 mae 0.00817095507738113
training loss 0.00011489592951653524 mae 0.008109351006146548
Epoch 62, training: loss: 0.0001143, mae: 0.0080973 test: loss0.0001225, mae:0.0083043
training loss 9.909585787681863e-05 mae 0.007843549363315105
training loss 0.00010351204343582957 mae 0.007847206648804393
training loss 0.0001056407295391632 mae 0.007845666075106894
training loss 0.00011109832581460304 mae 0.007982947641601233
training loss 0.0001149586657370876 mae 0.008087879257619533
Epoch 63, training: loss: 0.0001157, mae: 0.0081032 test: loss0.0001225, mae:0.0082311
training loss 9.797781967790797e-05 mae 0.007816831581294537
training loss 0.00011307029247594379 mae 0.008126416106653565
training loss 0.00011261712333549386 mae 0.008092961914948016
training loss 0.00011026032826214074 mae 0.008017334985293893
training loss 0.00011140683656677127 mae 0.008026675411513931
Epoch 64, training: loss: 0.0001112, mae: 0.0080267 test: loss0.0001278, mae:0.0085408
training loss 0.00010171544272452593 mae 0.008142375387251377
training loss 0.00011917278193412162 mae 0.00811224731196668
training loss 0.00011503823007293282 mae 0.008013172176583573
training loss 0.00011200605469928522 mae 0.007974569658002514
training loss 0.00010996056764849358 mae 0.007901411763026346
Epoch 65, training: loss: 0.0001104, mae: 0.0079221 test: loss0.0001211, mae:0.0081772
training loss 0.00011635166447376832 mae 0.008813567459583282
training loss 9.999087984290193e-05 mae 0.0076860927100128994
training loss 0.00010322590285363506 mae 0.007756927138249767
training loss 0.00010571391203183221 mae 0.007768167657715987
training loss 0.0001077350997778677 mae 0.007855707432018286
Epoch 66, training: loss: 0.0001073, mae: 0.0078549 test: loss0.0001286, mae:0.0084995
training loss 8.973345393314958e-05 mae 0.007444539573043585
training loss 0.00010304263410302265 mae 0.007734559321155151
training loss 0.00011494119260731009 mae 0.008030387065788304
training loss 0.00011331979941367786 mae 0.007997639067100966
training loss 0.00011367298341144817 mae 0.008042696566872336
Epoch 67, training: loss: 0.0001143, mae: 0.0080625 test: loss0.0001130, mae:0.0078954
training loss 5.752111610490829e-05 mae 0.00582194933667779
training loss 0.00011662813352920825 mae 0.008127485201054932
training loss 0.0001148698944901001 mae 0.008063729617302078
training loss 0.00011431781124745323 mae 0.008089331271657294
training loss 0.00011363120792162562 mae 0.00807432248604609
Epoch 68, training: loss: 0.0001138, mae: 0.0080788 test: loss0.0001295, mae:0.0085410
training loss 0.0001423136709490791 mae 0.008813783526420593
training loss 0.00010899483237938281 mae 0.007711248499724797
training loss 0.0001131821661714423 mae 0.008010192368511517
training loss 0.00011126254003403232 mae 0.007948514425645996
training loss 0.00011023278632670388 mae 0.007941679452170634
Epoch 69, training: loss: 0.0001104, mae: 0.0079536 test: loss0.0001254, mae:0.0085613
training loss 8.535225788364187e-05 mae 0.007083000149577856
training loss 0.00010968617268600595 mae 0.007895249791224215
training loss 0.00010800639740571769 mae 0.00780889783331221
training loss 0.0001039882822955623 mae 0.007711585438409389
training loss 0.00010370678267721438 mae 0.007697021224957643
Epoch 70, training: loss: 0.0001039, mae: 0.0077097 test: loss0.0001230, mae:0.0082495
training loss 7.345368794631213e-05 mae 0.006986323278397322
training loss 0.00011522967328988564 mae 0.008162348951194802
training loss 0.0001144667864113811 mae 0.008133380139006838
training loss 0.00011174503111724574 mae 0.008021397799845568
training loss 0.00010880490572615155 mae 0.007910931580213469
Epoch 71, training: loss: 0.0001081, mae: 0.0078921 test: loss0.0001139, mae:0.0078816
training loss 0.00016364124894607812 mae 0.007979238405823708
training loss 0.00010835808058675634 mae 0.007840723182786911
training loss 0.00010759160096934617 mae 0.007851482816224931
training loss 0.00010614649004581234 mae 0.007804546647069858
training loss 0.00010489419392838297 mae 0.007757501136882232
Epoch 72, training: loss: 0.0001047, mae: 0.0077614 test: loss0.0001204, mae:0.0081780
training loss 0.00022450248070526868 mae 0.009972686879336834
training loss 9.934805326766389e-05 mae 0.007630056080718836
training loss 9.815129795800987e-05 mae 0.007585974972658227
training loss 9.851884911671771e-05 mae 0.007582092114927754
training loss 0.00010215798273797852 mae 0.007670542125622581
Epoch 73, training: loss: 0.0001023, mae: 0.0076849 test: loss0.0001263, mae:0.0082618
training loss 0.00012945283378940076 mae 0.008078197948634624
training loss 0.00010355564232917922 mae 0.0076978181434028325
training loss 0.00010299799403511128 mae 0.007718934082785753
training loss 0.00010177229419030677 mae 0.007675737605140306
training loss 0.00010190304140030382 mae 0.00769481463218803
Epoch 74, training: loss: 0.0001014, mae: 0.0076757 test: loss0.0001148, mae:0.0079806
training loss 7.439405453624204e-05 mae 0.006506431847810745
training loss 9.132126219543244e-05 mae 0.0072704604461643054
training loss 9.68375075755452e-05 mae 0.007470540974362945
training loss 9.807969402253559e-05 mae 0.007513503809243636
training loss 9.716888084609195e-05 mae 0.007479523266874143
Epoch 75, training: loss: 0.0000965, mae: 0.0074546 test: loss0.0001052, mae:0.0075712
training loss 7.21126634743996e-05 mae 0.0066958279348909855
training loss 9.42645412721379e-05 mae 0.007228137980050903
training loss 9.579679137441594e-05 mae 0.007358784234095921
training loss 9.867318995803473e-05 mae 0.007505304007390086
training loss 0.00010061287431556623 mae 0.007570231419093721
Epoch 76, training: loss: 0.0001003, mae: 0.0075708 test: loss0.0001167, mae:0.0080464
training loss 6.838428816990927e-05 mae 0.006405834574252367
training loss 9.105229771960817e-05 mae 0.007230590828055262
training loss 9.286425642364908e-05 mae 0.007328461500640852
training loss 9.227587252821382e-05 mae 0.007348548608603857
training loss 9.317233296550244e-05 mae 0.007364222589898762
Epoch 77, training: loss: 0.0000940, mae: 0.0073764 test: loss0.0001146, mae:0.0080466
training loss 9.751386096468195e-05 mae 0.007249786052852869
training loss 9.386263149744813e-05 mae 0.007366522772274182
training loss 9.155361307295047e-05 mae 0.0073419384575347515
training loss 9.308756197670113e-05 mae 0.007371845753826451
training loss 9.659956119878131e-05 mae 0.0074815887828074915
Epoch 78, training: loss: 0.0000961, mae: 0.0074546 test: loss0.0001117, mae:0.0079156
training loss 5.4843014368088916e-05 mae 0.006002751644700766
training loss 9.532903170432239e-05 mae 0.007376488690794099
training loss 9.251858529463032e-05 mae 0.007288153467718327
training loss 9.06771057865903e-05 mae 0.0072497845880243154
training loss 9.26965651558758e-05 mae 0.00731108899677719
Epoch 79, training: loss: 0.0000928, mae: 0.0073243 test: loss0.0001114, mae:0.0078411
training loss 5.3125404519960284e-05 mae 0.00591607391834259
training loss 9.647017691795732e-05 mae 0.007415685510518506
training loss 9.26839149344394e-05 mae 0.007391250567824241
training loss 8.85257152954212e-05 mae 0.007243115420989841
training loss 8.998309198459401e-05 mae 0.007276303859992851
Epoch 80, training: loss: 0.0000905, mae: 0.0072950 test: loss0.0001154, mae:0.0081529
training loss 0.00011770849232561886 mae 0.008679691702127457
training loss 8.494890297537069e-05 mae 0.0070507774841697774
training loss 8.238793456221533e-05 mae 0.006959489878672772
training loss 8.707913347105155e-05 mae 0.00710075087721971
training loss 8.97078369515346e-05 mae 0.007177131658243892
Epoch 81, training: loss: 0.0000900, mae: 0.0071928 test: loss0.0001120, mae:0.0078518
training loss 9.369706822326407e-05 mae 0.007976546883583069
training loss 8.864389197768498e-05 mae 0.0071972357657025835
training loss 8.809116051191152e-05 mae 0.007184304596653374
training loss 8.754771036949373e-05 mae 0.007128856502572037
training loss 8.82891976678239e-05 mae 0.00715004021431602
Epoch 82, training: loss: 0.0000899, mae: 0.0072062 test: loss0.0001161, mae:0.0080512
training loss 4.451796485227533e-05 mae 0.005102530587464571
training loss 8.990563190879993e-05 mae 0.0073642074769618435
training loss 9.016569750354681e-05 mae 0.007278582975637201
training loss 8.906824962873575e-05 mae 0.00723412799161692
training loss 8.847406069213524e-05 mae 0.0071856591489110385
Epoch 83, training: loss: 0.0000879, mae: 0.0071615 test: loss0.0001049, mae:0.0077450
training loss 7.595201896037906e-05 mae 0.006550755817443132
training loss 8.231641921013885e-05 mae 0.006948876569011048
training loss 8.044968554974844e-05 mae 0.006869366590745082
training loss 8.007750176864722e-05 mae 0.006849991425945861
training loss 8.304687938319906e-05 mae 0.006967853655139752
Epoch 84, training: loss: 0.0000834, mae: 0.0069768 test: loss0.0001108, mae:0.0077740
training loss 7.913646550150588e-05 mae 0.007014595437794924
training loss 8.794038091996212e-05 mae 0.00701218223491428
training loss 8.65005137468573e-05 mae 0.007052965531365412
training loss 8.870131267826013e-05 mae 0.007153386675709528
training loss 9.131686630863255e-05 mae 0.007290472396978394
Epoch 85, training: loss: 0.0000915, mae: 0.0072855 test: loss0.0001108, mae:0.0078932
training loss 0.00010299260611645877 mae 0.007974154315888882
training loss 9.873290664651064e-05 mae 0.007535301377156784
training loss 9.151860660386944e-05 mae 0.007271870601885391
training loss 8.808629626708124e-05 mae 0.0071513079421360376
training loss 8.710739684965354e-05 mae 0.007119787173503224
Epoch 86, training: loss: 0.0000871, mae: 0.0071284 test: loss0.0001097, mae:0.0077601
training loss 8.145550236804411e-05 mae 0.007711766753345728
training loss 7.912244159253496e-05 mae 0.006814570093125688
training loss 7.87769682520406e-05 mae 0.006837545631287417
training loss 8.109769175721131e-05 mae 0.006935614198199566
training loss 8.216270758403789e-05 mae 0.0069444814154794865
Epoch 87, training: loss: 0.0000828, mae: 0.0069708 test: loss0.0001060, mae:0.0076113
training loss 8.511503256158903e-05 mae 0.007237361278384924
training loss 7.857031082761856e-05 mae 0.00680851594855388
training loss 8.254596591226313e-05 mae 0.006971913200830765
training loss 8.206916174085022e-05 mae 0.006939831868258138
training loss 8.336988391976724e-05 mae 0.006990332272255896
Epoch 88, training: loss: 0.0000829, mae: 0.0069622 test: loss0.0001111, mae:0.0077876
training loss 4.862661080551334e-05 mae 0.00558893708512187
training loss 7.8767025900329e-05 mae 0.006834978730801274
training loss 8.113190847550436e-05 mae 0.006839146381151852
training loss 8.240400297264266e-05 mae 0.0068997755116283485
training loss 8.0779741017353e-05 mae 0.006863985748136815
Epoch 89, training: loss: 0.0000811, mae: 0.0068836 test: loss0.0001028, mae:0.0075828
training loss 5.458667874336243e-05 mae 0.0060940454714000225
training loss 8.200978767240932e-05 mae 0.006935716524501056
training loss 8.253820044839164e-05 mae 0.006938037139796975
training loss 8.246677106887023e-05 mae 0.006960709878422369
training loss 8.201246658865415e-05 mae 0.006935249492804062
Epoch 90, training: loss: 0.0000829, mae: 0.0069417 test: loss0.0001109, mae:0.0078143
training loss 7.072592416079715e-05 mae 0.006834801752120256
training loss 7.718585987401872e-05 mae 0.006818436268790094
training loss 7.478132588088073e-05 mae 0.006700772958079187
training loss 7.821109627442134e-05 mae 0.006783551635183643
training loss 7.852357859646242e-05 mae 0.006782524445237806
Epoch 91, training: loss: 0.0000788, mae: 0.0067850 test: loss0.0001048, mae:0.0075871
training loss 0.00011763576185330749 mae 0.007976643741130829
training loss 7.863780696175574e-05 mae 0.006792605690219823
training loss 8.066627207773482e-05 mae 0.006846112106917517
training loss 7.950583577795195e-05 mae 0.006830646255553165
training loss 7.785649883875917e-05 mae 0.006763722369938498
Epoch 92, training: loss: 0.0000776, mae: 0.0067677 test: loss0.0001116, mae:0.0077022
training loss 8.462278492515907e-05 mae 0.007057407405227423
training loss 7.908956874470117e-05 mae 0.006810629457308381
training loss 7.571895581025787e-05 mae 0.006693195049088484
training loss 7.691571891997075e-05 mae 0.006696563167805898
training loss 7.576736883927529e-05 mae 0.006661866703510874
Epoch 93, training: loss: 0.0000758, mae: 0.0066766 test: loss0.0001047, mae:0.0075637
training loss 5.70437841815874e-05 mae 0.006054908502846956
training loss 7.592765092233413e-05 mae 0.006694680582001512
training loss 7.470028269903431e-05 mae 0.0066710491157551816
training loss 7.541447393430416e-05 mae 0.006711588339843103
training loss 7.582649946084186e-05 mae 0.006723975885511188
Epoch 94, training: loss: 0.0000763, mae: 0.0067238 test: loss0.0000952, mae:0.0072718
training loss 7.246586756082252e-05 mae 0.0063398792408406734
training loss 6.647346505333723e-05 mae 0.006229580142626576
training loss 6.744247903133335e-05 mae 0.006340947749074736
training loss 6.935313839923329e-05 mae 0.0064423456664709075
training loss 7.002625502695195e-05 mae 0.006452793649291222
Epoch 95, training: loss: 0.0000706, mae: 0.0064764 test: loss0.0001023, mae:0.0075197
training loss 9.682113886810839e-05 mae 0.007086841389536858
training loss 8.098683817889633e-05 mae 0.006831950681539726
training loss 7.52810923910543e-05 mae 0.006656307325984287
training loss 7.321879798982028e-05 mae 0.006585664859053908
training loss 7.294559702077368e-05 mae 0.00654106300129597
Epoch 96, training: loss: 0.0000731, mae: 0.0065522 test: loss0.0000993, mae:0.0073612
training loss 5.721350680687465e-05 mae 0.005682369228452444
training loss 6.990293824107058e-05 mae 0.006423539272052983
training loss 7.074391338968474e-05 mae 0.006414363829253039
training loss 7.54655849586383e-05 mae 0.006641597268079092
training loss 7.508238371005815e-05 mae 0.006635252597153336
Epoch 97, training: loss: 0.0000750, mae: 0.0066295 test: loss0.0000966, mae:0.0073442
training loss 4.428324245964177e-05 mae 0.005371759179979563
training loss 7.57012734110878e-05 mae 0.006598580692547794
training loss 7.091372631608955e-05 mae 0.006424072336885011
training loss 7.111318758303384e-05 mae 0.006449471341927126
training loss 7.224393559579816e-05 mae 0.0064997883416267515
Epoch 98, training: loss: 0.0000724, mae: 0.0065011 test: loss0.0000993, mae:0.0073442
training loss 5.444341877591796e-05 mae 0.0052480013109743595
training loss 7.182004464637288e-05 mae 0.006479050644545579
training loss 7.388910351005282e-05 mae 0.006563837695313565
training loss 7.687386428699859e-05 mae 0.006665175771402407
training loss 7.769677917104081e-05 mae 0.006687219768984995
Epoch 99, training: loss: 0.0000773, mae: 0.0066763 test: loss0.0000979, mae:0.0073850
current learning rate: 0.00025
training loss 8.992839866550639e-05 mae 0.007441519293934107
training loss 6.512734990774688e-05 mae 0.006181899056422944
training loss 6.43522955467441e-05 mae 0.0061869390013784475
training loss 6.333220359456452e-05 mae 0.006135039117853375
training loss 6.315440853301258e-05 mae 0.0060938899544304
Epoch 100, training: loss: 0.0000633, mae: 0.0060979 test: loss0.0000864, mae:0.0068969
training loss 6.54330724501051e-05 mae 0.005683219525963068
training loss 5.63463772243053e-05 mae 0.005770287458218781
training loss 5.740691235001972e-05 mae 0.005819663126482674
training loss 5.7226168539140696e-05 mae 0.005835709293120052
training loss 5.702370152500603e-05 mae 0.005834913663149102
Epoch 101, training: loss: 0.0000569, mae: 0.0058370 test: loss0.0000884, mae:0.0068916
training loss 5.3192212362773716e-05 mae 0.00611511617898941
training loss 5.315791421190964e-05 mae 0.005682376980343285
training loss 5.6825779670316914e-05 mae 0.005810141448152832
training loss 5.562654442710117e-05 mae 0.005757922985103747
training loss 5.566788711269449e-05 mae 0.0057769406040138905
Epoch 102, training: loss: 0.0000557, mae: 0.0057847 test: loss0.0000864, mae:0.0068841
training loss 3.813321018242277e-05 mae 0.004736948758363724
training loss 5.334078299461443e-05 mae 0.005702916350142631
training loss 5.390275668339587e-05 mae 0.005728208671997092
training loss 5.5499466683288475e-05 mae 0.005764162161739064
training loss 5.5400136994621916e-05 mae 0.005760912265656376
Epoch 103, training: loss: 0.0000555, mae: 0.0057723 test: loss0.0000912, mae:0.0070771
training loss 4.60653864138294e-05 mae 0.005132813472300768
training loss 5.3528690163968307e-05 mae 0.005603832083151621
training loss 5.2219389022632266e-05 mae 0.005589706703356587
training loss 5.1883280079982194e-05 mae 0.00560929986882634
training loss 5.311353709825271e-05 mae 0.005670378771170381
Epoch 104, training: loss: 0.0000533, mae: 0.0056857 test: loss0.0000911, mae:0.0070411
training loss 7.003912469372153e-05 mae 0.006629920098930597
training loss 5.260884790914133e-05 mae 0.00570006075990843
training loss 5.083606863468724e-05 mae 0.005569952351068801
training loss 5.1980275581825975e-05 mae 0.0056005095205707645
training loss 5.2790185903932854e-05 mae 0.005635913482297265
Epoch 105, training: loss: 0.0000524, mae: 0.0056148 test: loss0.0000894, mae:0.0070472
training loss 3.772735726670362e-05 mae 0.004844126291573048
training loss 4.84036965074255e-05 mae 0.005458211042352168
training loss 5.063555426483556e-05 mae 0.005487060985795346
training loss 5.19457177826842e-05 mae 0.005584207631721603
training loss 5.206025898542294e-05 mae 0.005597661064001758
Epoch 106, training: loss: 0.0000520, mae: 0.0055938 test: loss0.0000853, mae:0.0068155
training loss 3.35060030920431e-05 mae 0.004558833781629801
training loss 4.9206475345200995e-05 mae 0.005510082687525189
training loss 5.04315296776701e-05 mae 0.00552775160326521
training loss 5.0936996004319075e-05 mae 0.005543443065950808
training loss 5.123771816713327e-05 mae 0.005543657861168111
Epoch 107, training: loss: 0.0000512, mae: 0.0055441 test: loss0.0000891, mae:0.0069534
training loss 4.9265672714682296e-05 mae 0.005544334650039673
training loss 4.752067266678091e-05 mae 0.005416724173461691
training loss 4.825476278008614e-05 mae 0.005387238189946898
training loss 4.8870970396747486e-05 mae 0.005420224343284668
training loss 4.988801954108503e-05 mae 0.005474565291211972
Epoch 108, training: loss: 0.0000498, mae: 0.0054760 test: loss0.0000876, mae:0.0069165
training loss 3.6450670449994504e-05 mae 0.004716856405138969
training loss 5.273910716698761e-05 mae 0.0056318143455713395
training loss 5.161009676608092e-05 mae 0.005583067650510237
training loss 5.129207850448149e-05 mae 0.005550787719398341
training loss 5.060736190505088e-05 mae 0.005503537775068287
Epoch 109, training: loss: 0.0000507, mae: 0.0055139 test: loss0.0000907, mae:0.0070771
training loss 4.768850703840144e-05 mae 0.005248295608907938
training loss 4.914971663418006e-05 mae 0.0055186231574956696
training loss 4.956044472012285e-05 mae 0.005496085222607643
training loss 4.931272418833404e-05 mae 0.0054780870401578
training loss 4.91818051584834e-05 mae 0.005484092594888889
Epoch 110, training: loss: 0.0000491, mae: 0.0054860 test: loss0.0000892, mae:0.0069719
training loss 5.433047408587299e-05 mae 0.0058089978992938995
training loss 4.443813843096782e-05 mae 0.005221742744027985
training loss 4.876172697730017e-05 mae 0.00543093594196852
training loss 4.837219839466934e-05 mae 0.005407519623119112
training loss 4.8171914712119325e-05 mae 0.00540543427060715
Epoch 111, training: loss: 0.0000482, mae: 0.0054123 test: loss0.0000887, mae:0.0069807
training loss 6.240395305212587e-05 mae 0.006609707605093718
training loss 4.6088475702323625e-05 mae 0.005321649414505444
training loss 4.798367769067813e-05 mae 0.0053923261665397935
training loss 4.813173706637501e-05 mae 0.005401282073214451
training loss 4.867871015521451e-05 mae 0.0054130053265826475
Epoch 112, training: loss: 0.0000488, mae: 0.0054178 test: loss0.0000893, mae:0.0069064
training loss 5.212965697865002e-05 mae 0.006200313102453947
training loss 4.837488075495724e-05 mae 0.005442366606610664
training loss 4.893561951873138e-05 mae 0.005453665012327754
training loss 4.905306349916302e-05 mae 0.005431749253527615
training loss 4.941412650320367e-05 mae 0.005442037296930877
Epoch 113, training: loss: 0.0000494, mae: 0.0054468 test: loss0.0000856, mae:0.0068343
training loss 4.438476389623247e-05 mae 0.0050742304883897305
training loss 5.175662059342608e-05 mae 0.005519438973244499
training loss 4.952503630841303e-05 mae 0.005434990920458393
training loss 4.7895093724561e-05 mae 0.005364993388227971
training loss 4.771261053033572e-05 mae 0.005361091684948526
Epoch 114, training: loss: 0.0000475, mae: 0.0053554 test: loss0.0000893, mae:0.0069486
training loss 3.733991979970597e-05 mae 0.004948799032717943
training loss 5.111974806983625e-05 mae 0.005580016256620486
training loss 4.9637657056256e-05 mae 0.005468775668080048
training loss 4.810253101112304e-05 mae 0.005380099952307267
training loss 4.7286185975674755e-05 mae 0.005337951183597101
Epoch 115, training: loss: 0.0000472, mae: 0.0053345 test: loss0.0000871, mae:0.0068087
training loss 3.026617196155712e-05 mae 0.004194220528006554
training loss 4.453767492212172e-05 mae 0.005258819316605143
training loss 4.522215202104965e-05 mae 0.005267175398582575
training loss 4.4926940370485985e-05 mae 0.005239532449412228
training loss 4.500374685014676e-05 mae 0.005238914660720237
Epoch 116, training: loss: 0.0000452, mae: 0.0052494 test: loss0.0000898, mae:0.0069783
training loss 4.026006718049757e-05 mae 0.005398701410740614
training loss 4.167244868072243e-05 mae 0.005046442781081969
training loss 4.2585742728015876e-05 mae 0.00510719503359039
training loss 4.345495098645163e-05 mae 0.005147226426182992
training loss 4.396805856394261e-05 mae 0.005186493874559948
Epoch 117, training: loss: 0.0000440, mae: 0.0051945 test: loss0.0000891, mae:0.0069368
training loss 4.028478360851295e-05 mae 0.004980906844139099
training loss 4.127086732278615e-05 mae 0.005026777530563814
training loss 4.1558712242963024e-05 mae 0.005060671920897349
training loss 4.340863158978022e-05 mae 0.00514973383335187
training loss 4.445263189312872e-05 mae 0.00519680981037777
Epoch 118, training: loss: 0.0000444, mae: 0.0051918 test: loss0.0000881, mae:0.0068668
training loss 3.791533163166605e-05 mae 0.004872333258390427
training loss 4.243614735149131e-05 mae 0.00508459280792843
training loss 4.327034939436061e-05 mae 0.005125058194769935
training loss 4.552627957454151e-05 mae 0.005227395541509553
training loss 4.4888043377605796e-05 mae 0.005197852439900386
Epoch 119, training: loss: 0.0000449, mae: 0.0052013 test: loss0.0000830, mae:0.0066846
training loss 3.5286193451611325e-05 mae 0.004489726386964321
training loss 4.042728395171591e-05 mae 0.004997509435805327
training loss 4.356944252619298e-05 mae 0.005127556841027471
training loss 4.432000386205252e-05 mae 0.005184979589705339
training loss 4.374658969009123e-05 mae 0.005153198836519574
Epoch 120, training: loss: 0.0000437, mae: 0.0051449 test: loss0.0000909, mae:0.0069615
training loss 3.13078380713705e-05 mae 0.0042737633921206
training loss 4.12851605135217e-05 mae 0.0050053234285145414
training loss 4.123159914622131e-05 mae 0.005002687470803018
training loss 4.1237012876370406e-05 mae 0.005007963302230695
training loss 4.196556684875569e-05 mae 0.005047698197098662
Epoch 121, training: loss: 0.0000422, mae: 0.0050588 test: loss0.0000837, mae:0.0067646
training loss 3.785710214287974e-05 mae 0.00475719291716814
training loss 3.8442550302373584e-05 mae 0.004886430044494132
training loss 3.9168420266607866e-05 mae 0.004915476041283496
training loss 4.007301791017918e-05 mae 0.00495623960776489
training loss 4.095697068122499e-05 mae 0.005014013436470016
Epoch 122, training: loss: 0.0000410, mae: 0.0050166 test: loss0.0000862, mae:0.0068416
training loss 4.324059045757167e-05 mae 0.005602686200290918
training loss 3.984496031668694e-05 mae 0.0049313580544263725
training loss 3.952965216560169e-05 mae 0.004918719545043636
training loss 3.984084535520819e-05 mae 0.004923375221647768
training loss 4.061787506575301e-05 mae 0.004972089746902091
Epoch 123, training: loss: 0.0000406, mae: 0.0049719 test: loss0.0000822, mae:0.0066377
training loss 3.202899461030029e-05 mae 0.004501115996390581
training loss 3.77769371042115e-05 mae 0.0048005702795789525
training loss 3.9481591097728155e-05 mae 0.004890941263241048
training loss 3.9513145018055174e-05 mae 0.004903088841858683
training loss 4.058188703008152e-05 mae 0.00497356411625645
Epoch 124, training: loss: 0.0000409, mae: 0.0049912 test: loss0.0000884, mae:0.0068563
training loss 3.7281479308148846e-05 mae 0.00492007564753294
training loss 3.963260678574443e-05 mae 0.004942723554467745
training loss 4.063017821512277e-05 mae 0.004978348687752197
training loss 4.0969891246795224e-05 mae 0.0049985220520109555
training loss 4.127913436505464e-05 mae 0.005007662435076132
Epoch 125, training: loss: 0.0000418, mae: 0.0050327 test: loss0.0000896, mae:0.0069378
training loss 3.559985998435877e-05 mae 0.004208819940686226
training loss 3.991808015973006e-05 mae 0.004944597553534834
training loss 3.889890912531801e-05 mae 0.00488347649150113
training loss 4.001992609070857e-05 mae 0.0049503159474902185
training loss 4.0644427695698484e-05 mae 0.004969913329447235
Epoch 126, training: loss: 0.0000407, mae: 0.0049777 test: loss0.0000869, mae:0.0068156
training loss 2.6313966372981668e-05 mae 0.004020619671791792
training loss 3.79056795032275e-05 mae 0.0048605881120060025
training loss 4.018177935337225e-05 mae 0.0049560625604031135
training loss 4.143013363216066e-05 mae 0.005011604334048881
training loss 4.195953331520232e-05 mae 0.005038195376548529
Epoch 127, training: loss: 0.0000419, mae: 0.0050358 test: loss0.0001014, mae:0.0072443
training loss 6.409322668332607e-05 mae 0.006352909374982119
training loss 4.3946450258612034e-05 mae 0.0050750692493702265
training loss 4.082433370418621e-05 mae 0.004970960408947107
training loss 4.039747766350144e-05 mae 0.0049477217870067475
training loss 3.939119848489024e-05 mae 0.004887272035054382
Epoch 128, training: loss: 0.0000394, mae: 0.0048926 test: loss0.0000869, mae:0.0068249
training loss 4.288741547497921e-05 mae 0.005328140687197447
training loss 3.5347561767112536e-05 mae 0.004650460537888256
training loss 3.7135183447389884e-05 mae 0.004747024193258571
training loss 3.7836657280205694e-05 mae 0.004805593600262279
training loss 3.843138303401514e-05 mae 0.004849792490551131
Epoch 129, training: loss: 0.0000384, mae: 0.0048501 test: loss0.0000845, mae:0.0067453
training loss 4.6528413804480806e-05 mae 0.005241501610726118
training loss 4.078196766118602e-05 mae 0.004917621493850856
training loss 4.0041083710741875e-05 mae 0.004901683213890042
training loss 4.007174682206727e-05 mae 0.004922863499983061
training loss 3.9428637802614775e-05 mae 0.004900531559506089
Epoch 130, training: loss: 0.0000394, mae: 0.0049021 test: loss0.0000869, mae:0.0067483
training loss 3.466273483354598e-05 mae 0.005077488254755735
training loss 3.6350383144214424e-05 mae 0.004728899346481935
training loss 3.772840480649231e-05 mae 0.0048074807376960425
training loss 3.792462429439954e-05 mae 0.004810180083311944
training loss 3.709267504058317e-05 mae 0.0047639808837620905
Epoch 131, training: loss: 0.0000373, mae: 0.0047729 test: loss0.0000844, mae:0.0067112
training loss 3.0239569241530262e-05 mae 0.004449627827852964
training loss 3.520841367871445e-05 mae 0.004633161837381183
training loss 3.5619413093888515e-05 mae 0.004657340833068928
training loss 3.5812082489958625e-05 mae 0.00468241217058917
training loss 3.7035773929894286e-05 mae 0.004756246762934017
Epoch 132, training: loss: 0.0000369, mae: 0.0047466 test: loss0.0000863, mae:0.0067077
training loss 2.8283442588872276e-05 mae 0.004110974259674549
training loss 3.47495310159181e-05 mae 0.004578497237982411
training loss 3.610099927322837e-05 mae 0.004681919334862051
training loss 3.7396719394173076e-05 mae 0.004755006199517591
training loss 3.7093219244203994e-05 mae 0.004739216570533924
Epoch 133, training: loss: 0.0000371, mae: 0.0047453 test: loss0.0000838, mae:0.0066865
training loss 3.883392855641432e-05 mae 0.004719503223896027
training loss 3.622623690342366e-05 mae 0.004623147745744563
training loss 3.612605710394306e-05 mae 0.004628303414671728
training loss 3.60119296336606e-05 mae 0.004633121762967467
training loss 3.5977400044946244e-05 mae 0.004641960573090768
Epoch 134, training: loss: 0.0000362, mae: 0.0046492 test: loss0.0000887, mae:0.0068709
training loss 4.038151018903591e-05 mae 0.005146844778209925
training loss 3.602336476558097e-05 mae 0.0046956582615772895
training loss 3.629844502596609e-05 mae 0.004707931052192604
training loss 3.6391269905467216e-05 mae 0.0047080614357347915
training loss 3.638984939904156e-05 mae 0.004707078573372755
Epoch 135, training: loss: 0.0000363, mae: 0.0047058 test: loss0.0000850, mae:0.0066919
training loss 4.365747736301273e-05 mae 0.004911752883344889
training loss 3.304349817934803e-05 mae 0.0045173588784082844
training loss 3.3199642863416226e-05 mae 0.0045125555260899924
training loss 3.5373442013175934e-05 mae 0.0046394576429580605
training loss 3.5767286447738635e-05 mae 0.004659703521484238
Epoch 136, training: loss: 0.0000358, mae: 0.0046591 test: loss0.0000869, mae:0.0068434
training loss 2.5715286028571427e-05 mae 0.004146387334913015
training loss 3.3964512576680064e-05 mae 0.00455818815138556
training loss 3.420017792180234e-05 mae 0.004574862394094615
training loss 3.511134061968342e-05 mae 0.004613379193839945
training loss 3.5584325321643956e-05 mae 0.00465564260523377
Epoch 137, training: loss: 0.0000354, mae: 0.0046504 test: loss0.0000842, mae:0.0066712
training loss 3.211019793525338e-05 mae 0.0045371269807219505
training loss 3.269407473756777e-05 mae 0.004487987447931778
training loss 3.399498364340635e-05 mae 0.004580013177329951
training loss 3.445647948052248e-05 mae 0.004600128149675417
training loss 3.522765009213107e-05 mae 0.004644275152944585
Epoch 138, training: loss: 0.0000351, mae: 0.0046379 test: loss0.0000840, mae:0.0066930
training loss 2.3872378733358346e-05 mae 0.003725918708369136
training loss 3.36887544964317e-05 mae 0.004528535456926213
training loss 3.750459995223924e-05 mae 0.004772556161895246
training loss 3.678478030349121e-05 mae 0.004734281812862351
training loss 3.669896284746705e-05 mae 0.004721214938028463
Epoch 139, training: loss: 0.0000365, mae: 0.0047069 test: loss0.0000851, mae:0.0067136
training loss 2.5368723072460853e-05 mae 0.0034781089052557945
training loss 3.902289066828932e-05 mae 0.004827124532312155
training loss 3.949579763909718e-05 mae 0.004850129460879041
training loss 3.756371419790079e-05 mae 0.004734625011398798
training loss 3.6649591809926905e-05 mae 0.004693283424921226
Epoch 140, training: loss: 0.0000365, mae: 0.0046840 test: loss0.0000827, mae:0.0066483
training loss 2.4517823476344347e-05 mae 0.0036119967699050903
training loss 3.24598336281305e-05 mae 0.0044741414067353685
training loss 3.355411519967871e-05 mae 0.00453127523893398
training loss 3.381392950859361e-05 mae 0.0045438598801338685
training loss 3.397378228934639e-05 mae 0.004554463366268951
Epoch 141, training: loss: 0.0000340, mae: 0.0045591 test: loss0.0000853, mae:0.0067459
training loss 3.153810030198656e-05 mae 0.00434725359082222
training loss 3.394607776107139e-05 mae 0.004518416486497894
training loss 3.483552440103621e-05 mae 0.004572768398274731
training loss 3.4622597103714524e-05 mae 0.004575827492043278
training loss 3.401692560942706e-05 mae 0.004540091445345189
Epoch 142, training: loss: 0.0000341, mae: 0.0045360 test: loss0.0000857, mae:0.0067683
training loss 2.8363318051560782e-05 mae 0.0043762908317148685
training loss 3.108389097963249e-05 mae 0.004343347455027932
training loss 3.160501074660822e-05 mae 0.004379678999378098
training loss 3.2265661032801004e-05 mae 0.004423018598879705
training loss 3.2354030586994555e-05 mae 0.004437826788145584
Epoch 143, training: loss: 0.0000323, mae: 0.0044362 test: loss0.0000861, mae:0.0067068
training loss 3.082397597609088e-05 mae 0.0043723382987082005
training loss 2.8628922675624402e-05 mae 0.004189653486451682
training loss 3.0246917778622633e-05 mae 0.004299761699789231
training loss 3.175115824171299e-05 mae 0.004409765462323236
training loss 3.175831381070823e-05 mae 0.004409037859396849
Epoch 144, training: loss: 0.0000318, mae: 0.0044106 test: loss0.0000859, mae:0.0068191
training loss 3.537357406457886e-05 mae 0.00441376306116581
training loss 3.202958959816754e-05 mae 0.004405174541341908
training loss 3.263814386907079e-05 mae 0.0044336224177686295
training loss 3.310819689154746e-05 mae 0.004490097531345705
training loss 3.3124264247355805e-05 mae 0.004502302758506874
Epoch 145, training: loss: 0.0000330, mae: 0.0044961 test: loss0.0000820, mae:0.0066369
training loss 3.435795588302426e-05 mae 0.004818261601030827
training loss 2.926765952068701e-05 mae 0.004228577246049455
training loss 3.125867799171108e-05 mae 0.004336080299505944
training loss 3.189715159610175e-05 mae 0.004395604475913262
training loss 3.14635552927233e-05 mae 0.0043764509902386685
Epoch 146, training: loss: 0.0000314, mae: 0.0043699 test: loss0.0000847, mae:0.0066729
training loss 4.606756192515604e-05 mae 0.005178655963391066
training loss 2.998566403631211e-05 mae 0.004297157521268316
training loss 3.0189482269706055e-05 mae 0.004300757239821672
training loss 3.0981762621791516e-05 mae 0.004356390260011944
training loss 3.155808263361007e-05 mae 0.004370406966202365
Epoch 147, training: loss: 0.0000316, mae: 0.0043688 test: loss0.0000836, mae:0.0067074
training loss 3.079684029216878e-05 mae 0.004042233806103468
training loss 3.14249280242674e-05 mae 0.004379336904807418
training loss 3.1285893603108166e-05 mae 0.0043604749158061675
training loss 3.1820593897807605e-05 mae 0.004395085459402365
training loss 3.242675678784825e-05 mae 0.004438192367006966
Epoch 148, training: loss: 0.0000323, mae: 0.0044304 test: loss0.0000908, mae:0.0070277
training loss 2.8803895474993624e-05 mae 0.004385759588330984
training loss 3.414664484700872e-05 mae 0.004589073113439716
training loss 3.293791433132372e-05 mae 0.0045018009172918495
training loss 3.259937629810631e-05 mae 0.004471189445748138
training loss 3.2772898887728896e-05 mae 0.004470769860387888
Epoch 149, training: loss: 0.0000326, mae: 0.0044632 test: loss0.0000845, mae:0.0067316
training loss 1.510811398475198e-05 mae 0.002929981565102935
training loss 3.271297038165857e-05 mae 0.004435678405285466
training loss 3.279987431416865e-05 mae 0.004461093752709503
training loss 3.255999794214253e-05 mae 0.004443793381804857
training loss 3.253969543721946e-05 mae 0.004434125782894108
Epoch 150, training: loss: 0.0000326, mae: 0.0044338 test: loss0.0000839, mae:0.0066976
training loss 2.326080903003458e-05 mae 0.003890104591846466
training loss 3.1324444303298193e-05 mae 0.004411852137897824
training loss 3.1246595768823346e-05 mae 0.004376389256835281
training loss 3.1012335238006457e-05 mae 0.004369084751048427
training loss 3.148598073978372e-05 mae 0.004385003189913074
Epoch 151, training: loss: 0.0000313, mae: 0.0043768 test: loss0.0000853, mae:0.0067378
training loss 3.321707117720507e-05 mae 0.00453916983678937
training loss 3.5776203624075545e-05 mae 0.004569163230959983
training loss 3.255097564562837e-05 mae 0.004398821700954498
training loss 3.213884694608151e-05 mae 0.004390345322005205
training loss 3.2147637850258034e-05 mae 0.004406135905747169
Epoch 152, training: loss: 0.0000323, mae: 0.0044165 test: loss0.0000883, mae:0.0068511
training loss 2.6553547286312096e-05 mae 0.003942475188523531
training loss 3.188707374005496e-05 mae 0.0043628455742317105
training loss 3.2476013435407e-05 mae 0.004423099473155666
training loss 3.179052225657159e-05 mae 0.00439925549786217
training loss 3.2112914591402734e-05 mae 0.004426600659423652
Epoch 153, training: loss: 0.0000321, mae: 0.0044246 test: loss0.0000832, mae:0.0066389
training loss 1.9026514564757235e-05 mae 0.0034510185942053795
training loss 3.006066460739912e-05 mae 0.004256872921342067
training loss 3.141118844777707e-05 mae 0.004349854306806581
training loss 3.186611671565256e-05 mae 0.004373770867110484
training loss 3.1765280811614784e-05 mae 0.004391227768900902
Epoch 154, training: loss: 0.0000315, mae: 0.0043774 test: loss0.0000834, mae:0.0065999
training loss 3.1923071219353005e-05 mae 0.004530112724751234
training loss 3.0832833241826535e-05 mae 0.004286972477155571
training loss 3.0716263185835e-05 mae 0.004310081802806493
training loss 3.1511169019469756e-05 mae 0.0043696141524228
training loss 3.0923177612961626e-05 mae 0.004334967241701275
Epoch 155, training: loss: 0.0000308, mae: 0.0043267 test: loss0.0000827, mae:0.0066540
training loss 2.9507034923881292e-05 mae 0.004052197095006704
training loss 3.03009579939124e-05 mae 0.004287463863946351
training loss 3.05685033627825e-05 mae 0.0043214776894391176
training loss 3.02489726578305e-05 mae 0.00429576043724175
training loss 3.0151926522144366e-05 mae 0.0042796166317157465
Epoch 156, training: loss: 0.0000301, mae: 0.0042724 test: loss0.0000817, mae:0.0066099
training loss 3.3028190955519676e-05 mae 0.004285907838493586
training loss 2.6205298555021946e-05 mae 0.00399805573910913
training loss 2.665635916081115e-05 mae 0.004021049650054016
training loss 2.8240047874189727e-05 mae 0.00413657506983357
training loss 2.928780682778231e-05 mae 0.0042148494967885
Epoch 157, training: loss: 0.0000292, mae: 0.0042111 test: loss0.0000832, mae:0.0066246
training loss 3.263189137214795e-05 mae 0.004593834746629
training loss 2.8129874033980098e-05 mae 0.004172978881636964
training loss 2.7989938500090063e-05 mae 0.004139041853146534
training loss 2.893333662210135e-05 mae 0.004211481643643301
training loss 2.969661011716492e-05 mae 0.004250199704278094
Epoch 158, training: loss: 0.0000298, mae: 0.0042511 test: loss0.0000823, mae:0.0066277
training loss 2.559611493779812e-05 mae 0.004003204870969057
training loss 2.716181172539055e-05 mae 0.0040813364344192485
training loss 2.686573368396915e-05 mae 0.004059574138004296
training loss 2.7753643006395115e-05 mae 0.004116399929124788
training loss 2.7983175229804746e-05 mae 0.004136036054013456
Epoch 159, training: loss: 0.0000281, mae: 0.0041455 test: loss0.0000856, mae:0.0067870
training loss 2.0039784430991858e-05 mae 0.003634645836427808
training loss 2.6827240296423793e-05 mae 0.004046799934596992
training loss 2.7827186530353944e-05 mae 0.004108005551400013
training loss 2.8196901198588728e-05 mae 0.00413757366658688
training loss 2.8766752648017006e-05 mae 0.004178772577367242
Epoch 160, training: loss: 0.0000288, mae: 0.0041843 test: loss0.0000858, mae:0.0067328
training loss 2.4901784854591824e-05 mae 0.004091487731784582
training loss 2.808526586900995e-05 mae 0.004169373741081241
training loss 2.8296691351276274e-05 mae 0.004161777834656951
training loss 2.967895719523208e-05 mae 0.0042322423487951
training loss 3.039560428377043e-05 mae 0.004291270561840624
Epoch 161, training: loss: 0.0000303, mae: 0.0042827 test: loss0.0000838, mae:0.0066387
training loss 2.7885747840628028e-05 mae 0.004343017470091581
training loss 3.0700222209013784e-05 mae 0.00438175845306878
training loss 3.0016733782560716e-05 mae 0.004292065413705636
training loss 3.0665445978084844e-05 mae 0.004310467298950581
training loss 3.1231087406234804e-05 mae 0.004332640964713932
Epoch 162, training: loss: 0.0000311, mae: 0.0043226 test: loss0.0000861, mae:0.0067319
training loss 2.7071399017586373e-05 mae 0.0036852334160357714
training loss 2.798278056420089e-05 mae 0.004111575501441371
training loss 2.787512764398073e-05 mae 0.004107243598134506
training loss 2.8270578695810383e-05 mae 0.004133713388482467
training loss 2.845678762748514e-05 mae 0.004153874395785863
Epoch 163, training: loss: 0.0000285, mae: 0.0041574 test: loss0.0000843, mae:0.0066620
training loss 1.9318960767122917e-05 mae 0.0035858657211065292
training loss 2.5935515605299e-05 mae 0.003934544730274115
training loss 2.6832296714586633e-05 mae 0.004024082644943997
training loss 2.690755979908516e-05 mae 0.00404480688847503
training loss 2.696755958216575e-05 mae 0.0040550629575899005
Epoch 164, training: loss: 0.0000271, mae: 0.0040582 test: loss0.0000845, mae:0.0067183
training loss 1.9869776224368252e-05 mae 0.00364418956451118
training loss 2.8280323271710416e-05 mae 0.004142734121677338
training loss 2.7439158261000966e-05 mae 0.004097876680719822
training loss 2.7407896505485283e-05 mae 0.0040724110860525554
training loss 2.7798543832527223e-05 mae 0.00410251796421646
Epoch 165, training: loss: 0.0000280, mae: 0.0041113 test: loss0.0000827, mae:0.0066299
training loss 2.149314241250977e-05 mae 0.0037130294367671013
training loss 2.8536783622043647e-05 mae 0.004185034286267326
training loss 2.6642414130686006e-05 mae 0.004065522536782934
training loss 2.6268138928619903e-05 mae 0.00402213130392973
training loss 2.6905660499450564e-05 mae 0.004066142773681986
Epoch 166, training: loss: 0.0000268, mae: 0.0040608 test: loss0.0000812, mae:0.0065388
training loss 1.781920036592055e-05 mae 0.0033333832398056984
training loss 2.4396578487985784e-05 mae 0.0038306292730803586
training loss 2.5415559276093563e-05 mae 0.0039387992813740644
training loss 2.6518789970534563e-05 mae 0.004011471880698539
training loss 2.6592805426020755e-05 mae 0.0040230969886819075
Epoch 167, training: loss: 0.0000264, mae: 0.0040100 test: loss0.0000831, mae:0.0066523
training loss 1.835849070630502e-05 mae 0.0035117778461426497
training loss 2.343164781931966e-05 mae 0.0037890990246452535
training loss 2.5460328342347516e-05 mae 0.003942342470446141
training loss 2.5700162408587414e-05 mae 0.003949931531293403
training loss 2.600624900344825e-05 mae 0.0039740300352745376
Epoch 168, training: loss: 0.0000261, mae: 0.0039802 test: loss0.0000844, mae:0.0066668
training loss 1.9719254851224832e-05 mae 0.0034656880889087915
training loss 3.0339944893129027e-05 mae 0.004198165761087747
training loss 2.7764032848149372e-05 mae 0.0040533683230491845
training loss 2.6701196113798694e-05 mae 0.003991981848013517
training loss 2.674159525699749e-05 mae 0.004010324779925727
Epoch 169, training: loss: 0.0000267, mae: 0.0040092 test: loss0.0000823, mae:0.0066185
training loss 2.205048076575622e-05 mae 0.003588990541175008
training loss 2.602456401755084e-05 mae 0.003954448705246927
training loss 2.4751566784424103e-05 mae 0.0038852964200558927
training loss 2.4538724375322046e-05 mae 0.0038614595719051875
training loss 2.5355565118495104e-05 mae 0.003919874017928443
Epoch 170, training: loss: 0.0000255, mae: 0.0039306 test: loss0.0000812, mae:0.0065716
training loss 1.8664419258129783e-05 mae 0.0034719055984169245
training loss 2.4152949431570996e-05 mae 0.0038589048975457745
training loss 2.4767277954206206e-05 mae 0.003910468967336386
training loss 2.6081447318598747e-05 mae 0.0039915323183433915
training loss 2.6549569925710916e-05 mae 0.0040223732491056294
Epoch 171, training: loss: 0.0000267, mae: 0.0040332 test: loss0.0000810, mae:0.0065104
training loss 3.057988578802906e-05 mae 0.00436349818482995
training loss 2.8472188954337445e-05 mae 0.004135410223777097
training loss 2.7364318877588825e-05 mae 0.004050657183097878
training loss 2.654068981994456e-05 mae 0.003989833104886757
training loss 2.6196290993756334e-05 mae 0.003976778768985279
Epoch 172, training: loss: 0.0000261, mae: 0.0039685 test: loss0.0000836, mae:0.0066528
training loss 2.4239356207544915e-05 mae 0.003859679214656353
training loss 2.3569334100066763e-05 mae 0.003791906491981126
training loss 2.4706477080765725e-05 mae 0.0038923264923736017
training loss 2.4853875805737144e-05 mae 0.0039041843959161174
training loss 2.558050033457134e-05 mae 0.003957089542907045
Epoch 173, training: loss: 0.0000256, mae: 0.0039596 test: loss0.0000814, mae:0.0065722
training loss 2.5708863176987506e-05 mae 0.004256362561136484
training loss 2.5053012619786847e-05 mae 0.0039018557416092537
training loss 2.5199354024495033e-05 mae 0.003916660761737293
training loss 2.654023768663805e-05 mae 0.004022434389292698
training loss 2.6493095802422678e-05 mae 0.00403063350116176
Epoch 174, training: loss: 0.0000266, mae: 0.0040389 test: loss0.0000849, mae:0.0066981
training loss 3.3003248972818255e-05 mae 0.004474745597690344
training loss 2.591272793021441e-05 mae 0.0039755784833402025
training loss 2.5372714529535495e-05 mae 0.003930468243629774
training loss 2.5533051562972748e-05 mae 0.003944398375807811
training loss 2.5885529943642822e-05 mae 0.003963984426259254
Epoch 175, training: loss: 0.0000259, mae: 0.0039710 test: loss0.0000863, mae:0.0067506
training loss 2.9292996259755455e-05 mae 0.004116336349397898
training loss 2.406091851869361e-05 mae 0.003850600490456119
training loss 2.3532331127144324e-05 mae 0.0037954898647927122
training loss 2.3999317246180838e-05 mae 0.003832804683645238
training loss 2.4032052849419122e-05 mae 0.0038270486567391847
Epoch 176, training: loss: 0.0000240, mae: 0.0038169 test: loss0.0000830, mae:0.0065585
training loss 2.8923715944983996e-05 mae 0.004061544314026833
training loss 2.4571694274188713e-05 mae 0.0038707055489295263
training loss 2.4409414004142017e-05 mae 0.003870964792584724
training loss 2.3924514499226185e-05 mae 0.0038280175329611604
training loss 2.385748675760849e-05 mae 0.003821637742082353
Epoch 177, training: loss: 0.0000238, mae: 0.0038151 test: loss0.0000842, mae:0.0066560
training loss 2.1267100237309933e-05 mae 0.0036153430119156837
training loss 2.328381965615371e-05 mae 0.003799793054806251
training loss 2.3770583845028948e-05 mae 0.003797746477833037
training loss 2.3717361330376583e-05 mae 0.0037980233473296206
training loss 2.368103049146574e-05 mae 0.0038008574982502734
Epoch 178, training: loss: 0.0000236, mae: 0.0037991 test: loss0.0000822, mae:0.0065884
training loss 1.4845999430690426e-05 mae 0.003017493523657322
training loss 2.4770207435757778e-05 mae 0.003838151232248136
training loss 2.4174448272148794e-05 mae 0.003826439903938387
training loss 2.469636935065925e-05 mae 0.0038703774419668697
training loss 2.5108034800660205e-05 mae 0.003910593712813596
Epoch 179, training: loss: 0.0000252, mae: 0.0039199 test: loss0.0000819, mae:0.0066115
training loss 1.757397512847092e-05 mae 0.0033993609249591827
training loss 2.2795115500247995e-05 mae 0.0037491100283303094
training loss 2.324428104396129e-05 mae 0.003810207373324304
training loss 2.3852593356736518e-05 mae 0.00384074919055284
training loss 2.3811630876367077e-05 mae 0.0038243634661020197
Epoch 180, training: loss: 0.0000238, mae: 0.0038250 test: loss0.0000827, mae:0.0065925
training loss 1.8690034266910516e-05 mae 0.0033356770873069763
training loss 2.1009714538533947e-05 mae 0.0035832876513036435
training loss 2.2391576749214447e-05 mae 0.003697312969511539
training loss 2.4521231039788424e-05 mae 0.0038705364313741377
training loss 2.4378341301024338e-05 mae 0.003860770720080355
Epoch 181, training: loss: 0.0000244, mae: 0.0038625 test: loss0.0000825, mae:0.0065509
training loss 1.9892924683517776e-05 mae 0.003565392689779401
training loss 2.325549139284561e-05 mae 0.003807527949010916
training loss 2.4693613908095704e-05 mae 0.003916914849127135
training loss 2.463682000436635e-05 mae 0.003898688083181517
training loss 2.5654294236956413e-05 mae 0.003955106586866565
Epoch 182, training: loss: 0.0000255, mae: 0.0039470 test: loss0.0000839, mae:0.0066116
training loss 2.9231179723865353e-05 mae 0.004019307438284159
training loss 2.5922172497695446e-05 mae 0.003969397573896192
training loss 2.5601511005883337e-05 mae 0.0039639285682599185
training loss 2.488076329397597e-05 mae 0.0038980054777016933
training loss 2.4324481703512734e-05 mae 0.003854816788761179
Epoch 183, training: loss: 0.0000242, mae: 0.0038414 test: loss0.0000844, mae:0.0066392
training loss 3.2858344638952985e-05 mae 0.004509229212999344
training loss 2.0463442542320857e-05 mae 0.00354648832067409
training loss 2.084047625457701e-05 mae 0.0035829082716920292
training loss 2.188280647913911e-05 mae 0.003671626622066987
training loss 2.2247126683066044e-05 mae 0.0036958657988157137
Epoch 184, training: loss: 0.0000223, mae: 0.0037067 test: loss0.0000853, mae:0.0067097
training loss 2.8296943128225394e-05 mae 0.004244525916874409
training loss 2.3712030341295183e-05 mae 0.003843813942854894
training loss 2.5065648686103315e-05 mae 0.0039029603888566544
training loss 2.5263142946497336e-05 mae 0.003929602097880269
training loss 2.5348665180379482e-05 mae 0.003931927587373636
Epoch 185, training: loss: 0.0000252, mae: 0.0039231 test: loss0.0000804, mae:0.0064991
training loss 2.2775040633860044e-05 mae 0.003755806013941765
training loss 2.0803702325650325e-05 mae 0.0035854604268702223
training loss 2.0772061568752884e-05 mae 0.003582688730837095
training loss 2.1229459678772334e-05 mae 0.0036143077296877145
training loss 2.2984093122241623e-05 mae 0.003718804272670132
Epoch 186, training: loss: 0.0000232, mae: 0.0037438 test: loss0.0000864, mae:0.0067547
training loss 2.117593976436183e-05 mae 0.003689846955239773
training loss 2.1983045246765955e-05 mae 0.0036852897838780692
training loss 2.2828195979665973e-05 mae 0.0037292237416610566
training loss 2.2589678271426532e-05 mae 0.003710835183730089
training loss 2.3512728872465155e-05 mae 0.0037769296679145367
Epoch 187, training: loss: 0.0000236, mae: 0.0037831 test: loss0.0000854, mae:0.0067448
training loss 2.6878109565586783e-05 mae 0.004226469900459051
training loss 2.3746407065368205e-05 mae 0.0038334388313267173
training loss 2.3121897779989498e-05 mae 0.0037540345922997677
training loss 2.313591268818304e-05 mae 0.0037513856131534107
training loss 2.3329441835052594e-05 mae 0.0037718514358133673
Epoch 188, training: loss: 0.0000234, mae: 0.0037743 test: loss0.0000809, mae:0.0065037
training loss 1.9018572857021354e-05 mae 0.0030605467036366463
training loss 2.1324793534386473e-05 mae 0.003587615338391533
training loss 2.170543948433069e-05 mae 0.0036205024306321183
training loss 2.1766154345370518e-05 mae 0.0036396058553842116
training loss 2.153966858682157e-05 mae 0.003631614075640022
Epoch 189, training: loss: 0.0000218, mae: 0.0036483 test: loss0.0000829, mae:0.0065817
training loss 2.496801062079612e-05 mae 0.003957369364798069
training loss 2.0864733396949445e-05 mae 0.0035469477777095395
training loss 2.1424510591579264e-05 mae 0.003603987690835896
training loss 2.127282386213935e-05 mae 0.003605155524063782
training loss 2.1629877931929814e-05 mae 0.0036329206447603553
Epoch 190, training: loss: 0.0000218, mae: 0.0036423 test: loss0.0000803, mae:0.0064842
training loss 2.3443848476745188e-05 mae 0.0037537289317697287
training loss 2.3714044461895337e-05 mae 0.003726172772254429
training loss 2.2702839865670812e-05 mae 0.0036729437386672386
training loss 2.2939462615608763e-05 mae 0.0037175341823835243
training loss 2.238322124853834e-05 mae 0.003672443011738545
Epoch 191, training: loss: 0.0000223, mae: 0.0036706 test: loss0.0000819, mae:0.0065597
training loss 1.5008527952886652e-05 mae 0.0029036926571279764
training loss 2.2784325094607105e-05 mae 0.003633696905465103
training loss 2.48463229942026e-05 mae 0.0038432463989731412
training loss 2.4658608401524967e-05 mae 0.00383040267937132
training loss 2.4082074931320864e-05 mae 0.0037983514841145537
Epoch 192, training: loss: 0.0000241, mae: 0.0038051 test: loss0.0000841, mae:0.0066227
training loss 1.7551108612678945e-05 mae 0.003411961020901799
training loss 2.1959105776891103e-05 mae 0.003663389226786938
training loss 2.317524042767462e-05 mae 0.0036862097095034193
training loss 2.2656288235899312e-05 mae 0.0036797289912348357
training loss 2.2695180419913556e-05 mae 0.0036962250530237885
Epoch 193, training: loss: 0.0000227, mae: 0.0036918 test: loss0.0000813, mae:0.0065574
training loss 1.2623461770999711e-05 mae 0.002842944348230958
training loss 2.1055269219990198e-05 mae 0.003551756698346021
training loss 2.1166281461917985e-05 mae 0.0035819752086507202
training loss 2.1236419330994046e-05 mae 0.003586808982804812
training loss 2.089370164455672e-05 mae 0.0035765765464183547
Epoch 194, training: loss: 0.0000210, mae: 0.0035867 test: loss0.0000836, mae:0.0066307
training loss 2.129344647983089e-05 mae 0.0036365569103509188
training loss 1.95816295957889e-05 mae 0.003461497155584248
training loss 1.895988050747102e-05 mae 0.0034001851707955097
training loss 1.9926944230444147e-05 mae 0.0034716982419127655
training loss 2.0652177914797825e-05 mae 0.0035458903386725573
Epoch 195, training: loss: 0.0000206, mae: 0.0035432 test: loss0.0000825, mae:0.0065720
training loss 4.135070412303321e-05 mae 0.004251005593687296
training loss 2.1122966405956137e-05 mae 0.0035495863423920145
training loss 2.0521879603188384e-05 mae 0.0035285812774949747
training loss 2.1027571773515023e-05 mae 0.0035843275416494406
training loss 2.080524266951292e-05 mae 0.003570415551046752
Epoch 196, training: loss: 0.0000208, mae: 0.0035738 test: loss0.0000818, mae:0.0065621
training loss 1.8680024368222803e-05 mae 0.0031462814658880234
training loss 1.9978800564251075e-05 mae 0.0034724923453348525
training loss 1.9496129371504196e-05 mae 0.0034574805067317323
training loss 2.003612337752118e-05 mae 0.003509446776576883
training loss 2.0585661798066046e-05 mae 0.003553180400495283
Epoch 197, training: loss: 0.0000206, mae: 0.0035571 test: loss0.0000810, mae:0.0065073
training loss 2.0811125068576075e-05 mae 0.0036290374118834734
training loss 2.236573465208993e-05 mae 0.0037284486872308396
training loss 2.107619709229514e-05 mae 0.003596221041487585
training loss 2.08215776623041e-05 mae 0.0035739149096489747
training loss 2.0690598161019445e-05 mae 0.0035734331821420462
Epoch 198, training: loss: 0.0000206, mae: 0.0035590 test: loss0.0000800, mae:0.0065045
training loss 2.0325656805653125e-05 mae 0.00346617610193789
training loss 1.827159385535268e-05 mae 0.0033398580956546698
training loss 1.8217772407351196e-05 mae 0.0033324781786834845
training loss 1.9278939609215e-05 mae 0.003429444923084107
training loss 1.9659773015383284e-05 mae 0.003472659151899209
Epoch 199, training: loss: 0.0000198, mae: 0.0034816 test: loss0.0000827, mae:0.0065758
current learning rate: 0.000125
training loss 1.7314028809778392e-05 mae 0.0033308544661849737
training loss 1.6912392507662182e-05 mae 0.0032173989605450756
training loss 1.6207622138608706e-05 mae 0.0031446464023193225
training loss 1.6289343327435732e-05 mae 0.003152102776278822
training loss 1.6139029229186663e-05 mae 0.0031324587924526407
Epoch 200, training: loss: 0.0000161, mae: 0.0031305 test: loss0.0000809, mae:0.0064815
training loss 1.1385978723410517e-05 mae 0.0025540294591337442
training loss 1.3501001180260318e-05 mae 0.002849607669072701
training loss 1.463227064311585e-05 mae 0.0029602020167478235
training loss 1.4718467457517163e-05 mae 0.002960801954128292
training loss 1.4630817160686816e-05 mae 0.0029536242138093975
Epoch 201, training: loss: 0.0000146, mae: 0.0029539 test: loss0.0000816, mae:0.0065123
training loss 1.4236396054911893e-05 mae 0.0026034147012978792
training loss 1.4143637974270593e-05 mae 0.0029058558866381645
training loss 1.4196938797912223e-05 mae 0.002921623484425173
training loss 1.4178378845802863e-05 mae 0.0029334497671969087
training loss 1.444764617660815e-05 mae 0.0029523206749625178
Epoch 202, training: loss: 0.0000145, mae: 0.0029521 test: loss0.0000804, mae:0.0064607
training loss 1.5104029444046319e-05 mae 0.0030575634445995092
training loss 1.4119942837472381e-05 mae 0.002893482124907714
training loss 1.409598977567506e-05 mae 0.002895618692077328
training loss 1.4080431244550207e-05 mae 0.002896687117658111
training loss 1.4326046040416745e-05 mae 0.0029258620149145522
Epoch 203, training: loss: 0.0000144, mae: 0.0029333 test: loss0.0000821, mae:0.0065417
training loss 1.3578355719801039e-05 mae 0.00286709051579237
training loss 1.428494982189709e-05 mae 0.002922735323070311
training loss 1.4031773038242485e-05 mae 0.002890693205915908
training loss 1.4168964750468822e-05 mae 0.0029150040339160446
training loss 1.4351386678175169e-05 mae 0.0029379622292570523
Epoch 204, training: loss: 0.0000143, mae: 0.0029351 test: loss0.0000836, mae:0.0066206
training loss 2.274191501783207e-05 mae 0.003826514817774296
training loss 1.3923384447515869e-05 mae 0.002864355212260111
training loss 1.3996068914686006e-05 mae 0.0028739959061477747
training loss 1.4135978762958572e-05 mae 0.002906420531787522
training loss 1.428396019118329e-05 mae 0.002927110548969822
Epoch 205, training: loss: 0.0000143, mae: 0.0029357 test: loss0.0000811, mae:0.0064945
training loss 1.0166317224502563e-05 mae 0.002612450858578086
training loss 1.4466082771547258e-05 mae 0.0028790512724834335
training loss 1.4855992310120844e-05 mae 0.0029474972576677503
training loss 1.4301812741254946e-05 mae 0.0029167805301851973
training loss 1.43849148768293e-05 mae 0.0029311643192887443
Epoch 206, training: loss: 0.0000144, mae: 0.0029301 test: loss0.0000808, mae:0.0064925
training loss 1.1130785424029455e-05 mae 0.0025145316030830145
training loss 1.3459683549287957e-05 mae 0.002829482212808786
training loss 1.2929661987090917e-05 mae 0.0027759535523998262
training loss 1.3614576517958743e-05 mae 0.0028462980917056666
training loss 1.3817787514461594e-05 mae 0.0028736862303012653
Epoch 207, training: loss: 0.0000139, mae: 0.0028832 test: loss0.0000828, mae:0.0065715
training loss 1.0304286661266815e-05 mae 0.0025112342555075884
training loss 1.3380767630259666e-05 mae 0.0028433237206556047
training loss 1.3075603842795804e-05 mae 0.0028148965280742778
training loss 1.3705567529758274e-05 mae 0.0028641267705844407
training loss 1.3731390189700026e-05 mae 0.002860844853468499
Epoch 208, training: loss: 0.0000138, mae: 0.0028659 test: loss0.0000823, mae:0.0065846
training loss 9.256253179046325e-06 mae 0.002520212670788169
training loss 1.3408038998308009e-05 mae 0.0028523993724044053
training loss 1.289350928904115e-05 mae 0.002804098828317653
training loss 1.3591612749476683e-05 mae 0.0028700659788577563
training loss 1.3885510324781977e-05 mae 0.0028913185065287854
Epoch 209, training: loss: 0.0000139, mae: 0.0028954 test: loss0.0000822, mae:0.0065439
training loss 1.1535133126017172e-05 mae 0.0028032620903104544
training loss 1.2563375736028642e-05 mae 0.0027450905044508335
training loss 1.324791984269822e-05 mae 0.002820059311308778
training loss 1.337653165384402e-05 mae 0.002836809993184954
training loss 1.342972446669157e-05 mae 0.002841000621258026
Epoch 210, training: loss: 0.0000135, mae: 0.0028507 test: loss0.0000828, mae:0.0065860
training loss 1.4437265235756058e-05 mae 0.002971736481413245
training loss 1.3436495758950888e-05 mae 0.0028367068084395094
training loss 1.3156097901457436e-05 mae 0.002805409407805613
training loss 1.3650108536504736e-05 mae 0.0028517488043274205
training loss 1.3624997162385365e-05 mae 0.002849915439601227
Epoch 211, training: loss: 0.0000136, mae: 0.0028490 test: loss0.0000805, mae:0.0064900
training loss 7.489185463782633e-06 mae 0.0020632799714803696
training loss 1.3201656273332783e-05 mae 0.0027962841330936143
training loss 1.3049181306276226e-05 mae 0.0028032039633334284
training loss 1.3404158751004407e-05 mae 0.0028346405851821244
training loss 1.353468555115892e-05 mae 0.0028508906090511607
Epoch 212, training: loss: 0.0000135, mae: 0.0028487 test: loss0.0000807, mae:0.0065253
training loss 1.1484888091217726e-05 mae 0.002542814938351512
training loss 1.3312989294183136e-05 mae 0.0028508458030866634
training loss 1.2935557989105361e-05 mae 0.002799278353387029
training loss 1.2941214226764304e-05 mae 0.002804117584107627
training loss 1.3389040056034134e-05 mae 0.00284739858378188
Epoch 213, training: loss: 0.0000135, mae: 0.0028527 test: loss0.0000835, mae:0.0066384
training loss 1.400687233399367e-05 mae 0.002948060864582658
training loss 1.401070587199487e-05 mae 0.0028971702079562582
training loss 1.409277570694552e-05 mae 0.00291083557162397
training loss 1.3898058421317127e-05 mae 0.0028949132164483825
training loss 1.3800212506717266e-05 mae 0.002886832651765599
Epoch 214, training: loss: 0.0000136, mae: 0.0028692 test: loss0.0000821, mae:0.0065916
training loss 1.0860312613658607e-05 mae 0.002813538536429405
training loss 1.302202897432817e-05 mae 0.0028205887533212996
training loss 1.3002115167074579e-05 mae 0.0028195885554103553
training loss 1.3011335653910334e-05 mae 0.0028072120806831398
training loss 1.3297795683331004e-05 mae 0.0028301402671605503
Epoch 215, training: loss: 0.0000133, mae: 0.0028337 test: loss0.0000826, mae:0.0065914
training loss 1.211177368531935e-05 mae 0.0025034919381141663
training loss 1.3272540230650968e-05 mae 0.002829311872083766
training loss 1.3229092158749472e-05 mae 0.0028199529238414046
training loss 1.3154292714445537e-05 mae 0.002805150374585133
training loss 1.3076186459967625e-05 mae 0.002805363092645632
Epoch 216, training: loss: 0.0000131, mae: 0.0028104 test: loss0.0000845, mae:0.0066357
training loss 1.211941253131954e-05 mae 0.0028442658949643373
training loss 1.387795985055181e-05 mae 0.002882572256174741
training loss 1.3524399469358763e-05 mae 0.0028372111045617127
training loss 1.3477470965640437e-05 mae 0.0028341037602108345
training loss 1.3687737808240347e-05 mae 0.0028616941959101385
Epoch 217, training: loss: 0.0000137, mae: 0.0028641 test: loss0.0000841, mae:0.0066094
training loss 1.5794132195878774e-05 mae 0.003011630615219474
training loss 1.3336318528779566e-05 mae 0.0028265247934077882
training loss 1.3270486936625558e-05 mae 0.0028341303229110653
training loss 1.3140809717498505e-05 mae 0.002827467831678144
training loss 1.3410040673310262e-05 mae 0.0028517005909512298
Epoch 218, training: loss: 0.0000134, mae: 0.0028481 test: loss0.0000818, mae:0.0065572
training loss 1.9634893760667183e-05 mae 0.00294743780978024
training loss 1.3132726151382606e-05 mae 0.0028183453341982527
training loss 1.284531967624812e-05 mae 0.0027817441104441
training loss 1.2991657124846538e-05 mae 0.0027935725824895074
training loss 1.3213921681484352e-05 mae 0.0028177018854211055
Epoch 219, training: loss: 0.0000132, mae: 0.0028182 test: loss0.0000838, mae:0.0066490
training loss 1.341872393822996e-05 mae 0.002890108386054635
training loss 1.2533368037254025e-05 mae 0.0027701461107890106
training loss 1.2476740127144382e-05 mae 0.002745318206921589
training loss 1.2890829319509814e-05 mae 0.0027838328850165704
training loss 1.3030270316783718e-05 mae 0.00280170460157467
Epoch 220, training: loss: 0.0000131, mae: 0.0028108 test: loss0.0000850, mae:0.0066597
training loss 1.5740355593152344e-05 mae 0.002968081971630454
training loss 1.2293262728895105e-05 mae 0.0027218775038479587
training loss 1.264323542849218e-05 mae 0.002754341206965177
training loss 1.26296906592864e-05 mae 0.0027561239953714117
training loss 1.284073627247928e-05 mae 0.0027741664135596963
Epoch 221, training: loss: 0.0000128, mae: 0.0027741 test: loss0.0000832, mae:0.0066085
training loss 9.574968316883314e-06 mae 0.0026303036138415337
training loss 1.1500186960178076e-05 mae 0.0026672047005929784
training loss 1.2044829670437795e-05 mae 0.002701408010769156
training loss 1.2457525920262021e-05 mae 0.0027373931494029543
training loss 1.247362083111385e-05 mae 0.00274652132384516
Epoch 222, training: loss: 0.0000124, mae: 0.0027443 test: loss0.0000834, mae:0.0065857
training loss 1.5641169738955796e-05 mae 0.0033416245132684708
training loss 1.1883341259960492e-05 mae 0.0026853764064463915
training loss 1.2309767061926301e-05 mae 0.0027305990040283824
training loss 1.2109558680239334e-05 mae 0.002708811717669152
training loss 1.2309508919365476e-05 mae 0.0027226550992235383
Epoch 223, training: loss: 0.0000124, mae: 0.0027295 test: loss0.0000839, mae:0.0066478
training loss 8.874681043380406e-06 mae 0.0023175885435193777
training loss 1.2468109812913349e-05 mae 0.002715523012311142
training loss 1.2647204751527456e-05 mae 0.002743660940299971
training loss 1.2693797703374628e-05 mae 0.002761040569054862
training loss 1.2770381099426464e-05 mae 0.002765773958874059
Epoch 224, training: loss: 0.0000128, mae: 0.0027705 test: loss0.0000843, mae:0.0066042
training loss 1.0379085324530024e-05 mae 0.002429099753499031
training loss 1.1807146540163292e-05 mae 0.002651560161372318
training loss 1.207478901123257e-05 mae 0.0026776848642137098
training loss 1.2084711004910712e-05 mae 0.002682858512934224
training loss 1.225745958282319e-05 mae 0.00270829619017351
Epoch 225, training: loss: 0.0000123, mae: 0.0027101 test: loss0.0000849, mae:0.0066263
training loss 6.691024736937834e-06 mae 0.002198815578594804
training loss 1.1361384693765443e-05 mae 0.002636061081954954
training loss 1.2133279191817062e-05 mae 0.0026933747956916546
training loss 1.22855476065705e-05 mae 0.0027099404567297526
training loss 1.2422781939260218e-05 mae 0.0027238245571578924
Epoch 226, training: loss: 0.0000124, mae: 0.0027216 test: loss0.0000826, mae:0.0065969
training loss 1.0300188478140626e-05 mae 0.0023799871560186148
training loss 1.1996073270911084e-05 mae 0.002667078821390283
training loss 1.2165565258688775e-05 mae 0.0027044701555946683
training loss 1.2335266501696548e-05 mae 0.0027275845463948932
training loss 1.2383931298426448e-05 mae 0.002729781894059621
Epoch 227, training: loss: 0.0000124, mae: 0.0027304 test: loss0.0000829, mae:0.0065882
training loss 1.1490731594676618e-05 mae 0.0027140143793076277
training loss 1.1389528400982e-05 mae 0.002611437575051598
training loss 1.1520204711499408e-05 mae 0.0026264470059074252
training loss 1.1826219091720769e-05 mae 0.0026533409069594465
training loss 1.2125117434120676e-05 mae 0.0026851487224365577
Epoch 228, training: loss: 0.0000122, mae: 0.0026952 test: loss0.0000829, mae:0.0066307
training loss 1.4242948964238167e-05 mae 0.0030119288712739944
training loss 1.1956410866796252e-05 mae 0.0026674810066527016
training loss 1.1955735272290486e-05 mae 0.002649705661091917
training loss 1.2123012022238262e-05 mae 0.0026781768877281267
training loss 1.200362107082226e-05 mae 0.0026710427804057728
Epoch 229, training: loss: 0.0000121, mae: 0.0026817 test: loss0.0000834, mae:0.0065898
training loss 1.6870741092134267e-05 mae 0.0036075084935873747
training loss 1.2192326489728691e-05 mae 0.0027561815291204874
training loss 1.1711099400914949e-05 mae 0.0026705952103037636
training loss 1.1642544390065112e-05 mae 0.002663516248298796
training loss 1.1758174950593446e-05 mae 0.002660043110181368
Epoch 230, training: loss: 0.0000117, mae: 0.0026580 test: loss0.0000839, mae:0.0066285
training loss 7.473082860087743e-06 mae 0.0019693048670887947
training loss 1.1958987545535911e-05 mae 0.0026706709462564955
training loss 1.1830579323671348e-05 mae 0.002671466792223625
training loss 1.1930304213181453e-05 mae 0.002681019221006936
training loss 1.2001864614835517e-05 mae 0.002685168700695816
Epoch 231, training: loss: 0.0000120, mae: 0.0026814 test: loss0.0000844, mae:0.0066660
training loss 1.5006361536507029e-05 mae 0.0030101619195193052
training loss 1.1693759051031164e-05 mae 0.0026701609010570766
training loss 1.193575235897742e-05 mae 0.0026954391298096357
training loss 1.2174845110076298e-05 mae 0.0027195047448383045
training loss 1.2220120504084365e-05 mae 0.0027206093669447356
Epoch 232, training: loss: 0.0000122, mae: 0.0027233 test: loss0.0000841, mae:0.0066156
training loss 9.086251338885631e-06 mae 0.0022999730426818132
training loss 1.0911871841552272e-05 mae 0.002540782566511017
training loss 1.149097461886485e-05 mae 0.0026083218457066634
training loss 1.1502219979395503e-05 mae 0.002613319920542471
training loss 1.1562404424196513e-05 mae 0.0026284742536282725
Epoch 233, training: loss: 0.0000116, mae: 0.0026310 test: loss0.0000835, mae:0.0065789
training loss 1.0436087904963642e-05 mae 0.0024987731594592333
training loss 1.1137851306200755e-05 mae 0.002564022860800226
training loss 1.0961460004794629e-05 mae 0.0025591029840899577
training loss 1.1512837700620186e-05 mae 0.002615940138783972
training loss 1.1743887368009302e-05 mae 0.0026469594342587743
Epoch 234, training: loss: 0.0000117, mae: 0.0026479 test: loss0.0000871, mae:0.0067169
training loss 7.856123374949675e-06 mae 0.0023063828703016043
training loss 1.1187922235181119e-05 mae 0.0026226951923294392
training loss 1.16613714813902e-05 mae 0.002662528495276623
training loss 1.1873049823178323e-05 mae 0.00267519046003554
training loss 1.1949618076312672e-05 mae 0.0026871798483571454
Epoch 235, training: loss: 0.0000120, mae: 0.0026889 test: loss0.0000847, mae:0.0066917
training loss 1.2711193448922131e-05 mae 0.002931212307885289
training loss 1.1918165223594805e-05 mae 0.002657944578494804
training loss 1.1344139225137514e-05 mae 0.0025815312476312326
training loss 1.1597866432566666e-05 mae 0.002617369562430641
training loss 1.1747897886705885e-05 mae 0.0026411895277627293
Epoch 236, training: loss: 0.0000118, mae: 0.0026530 test: loss0.0000841, mae:0.0066197
training loss 9.297635187976994e-06 mae 0.002149081090465188
training loss 1.1411240081418099e-05 mae 0.002622402446600152
training loss 1.1262407335181867e-05 mae 0.002590678857021624
training loss 1.1342571339045152e-05 mae 0.0026057368839801035
training loss 1.152530844298432e-05 mae 0.0026338357942883132
Epoch 237, training: loss: 0.0000115, mae: 0.0026316 test: loss0.0000839, mae:0.0066386
training loss 1.188314945466118e-05 mae 0.002624076558277011
training loss 1.0664895479053754e-05 mae 0.0025436812623714404
training loss 1.0799609285776128e-05 mae 0.0025561234169169376
training loss 1.0953021308595879e-05 mae 0.0025666447784998356
training loss 1.1136658601449808e-05 mae 0.002579078578102907
Epoch 238, training: loss: 0.0000111, mae: 0.0025813 test: loss0.0000842, mae:0.0066292
training loss 8.789712410361972e-06 mae 0.002361745573580265
training loss 1.0617280988263505e-05 mae 0.0024943739248841417
training loss 1.066981451796243e-05 mae 0.0025151711404508
training loss 1.1059774119560746e-05 mae 0.0025731006757485748
training loss 1.1289200604854458e-05 mae 0.002599866131774674
Epoch 239, training: loss: 0.0000114, mae: 0.0026096 test: loss0.0000852, mae:0.0066846
training loss 1.280934793612687e-05 mae 0.002691063331440091
training loss 1.1735670653376709e-05 mae 0.002635984690677302
training loss 1.1470124268226298e-05 mae 0.002617081787464864
training loss 1.1265860457148329e-05 mae 0.0025907767364533236
training loss 1.1423112916974735e-05 mae 0.0026144074532087557
Epoch 240, training: loss: 0.0000115, mae: 0.0026211 test: loss0.0000852, mae:0.0066626
training loss 1.8151220501749776e-05 mae 0.003035071073099971
training loss 1.0851659520566877e-05 mae 0.002575256569566679
training loss 1.0992649707320493e-05 mae 0.002566083374728291
training loss 1.0928954576012615e-05 mae 0.0025575577631432385
training loss 1.1103124520491632e-05 mae 0.002588168228513084
Epoch 241, training: loss: 0.0000112, mae: 0.0025956 test: loss0.0000842, mae:0.0066209
training loss 6.801767540309811e-06 mae 0.002012629294767976
training loss 1.091857599449748e-05 mae 0.0025673469793343656
training loss 1.1520166553243981e-05 mae 0.002635213229482495
training loss 1.1574800418208382e-05 mae 0.0026362068227967174
training loss 1.1684763986201476e-05 mae 0.0026479389186049632
Epoch 242, training: loss: 0.0000117, mae: 0.0026474 test: loss0.0000844, mae:0.0066416
training loss 6.6387451624905225e-06 mae 0.0018702183151617646
training loss 1.017234967102744e-05 mae 0.0024900865083669907
training loss 1.0454840686170518e-05 mae 0.0025281341432934947
training loss 1.0887971826221308e-05 mae 0.002561419858406022
training loss 1.1039444260275754e-05 mae 0.0025713466813287425
Epoch 243, training: loss: 0.0000112, mae: 0.0025825 test: loss0.0000853, mae:0.0066819
training loss 1.2699479157163296e-05 mae 0.002779618604108691
training loss 1.0626855238989069e-05 mae 0.002513308838174185
training loss 1.1032105604580567e-05 mae 0.0025626152447231192
training loss 1.1008672114884341e-05 mae 0.0025756101051518164
training loss 1.1342153918044847e-05 mae 0.002623296964251024
Epoch 244, training: loss: 0.0000113, mae: 0.0026210 test: loss0.0000849, mae:0.0066762
training loss 7.993582585186232e-06 mae 0.0022377402056008577
training loss 1.1577218548850127e-05 mae 0.002631625366013716
training loss 1.1144620189178125e-05 mae 0.002607081753967126
training loss 1.1101448049186536e-05 mae 0.0025943427783011487
training loss 1.1271458252347433e-05 mae 0.0026089429145271133
Epoch 245, training: loss: 0.0000113, mae: 0.0026122 test: loss0.0000851, mae:0.0066507
training loss 1.0039985681942198e-05 mae 0.0023959942627698183
training loss 1.0810954666108136e-05 mae 0.0025335850477145585
training loss 1.0521928567805566e-05 mae 0.0025092271124335505
training loss 1.0756967893162889e-05 mae 0.0025366025881237344
training loss 1.0819047540407433e-05 mae 0.0025482950940616993
Epoch 246, training: loss: 0.0000108, mae: 0.0025453 test: loss0.0000846, mae:0.0066394
training loss 1.1290678230579942e-05 mae 0.002833791309967637
training loss 1.0241928182450449e-05 mae 0.0024574735252113613
training loss 1.0422866449912943e-05 mae 0.0024887371905111155
training loss 1.0577492509239631e-05 mae 0.0025159541551418472
training loss 1.063602265692417e-05 mae 0.0025237525305919248
Epoch 247, training: loss: 0.0000107, mae: 0.0025250 test: loss0.0000862, mae:0.0066965
training loss 1.4979811567172874e-05 mae 0.0027906561736017466
training loss 1.0617935512367265e-05 mae 0.0025461308484641364
training loss 1.0439575204811876e-05 mae 0.002511539026209623
training loss 1.0483248707708643e-05 mae 0.002508514306341477
training loss 1.0857414938652085e-05 mae 0.002548787200740023
Epoch 248, training: loss: 0.0000109, mae: 0.0025482 test: loss0.0000840, mae:0.0066176
training loss 1.1557523066585418e-05 mae 0.0023430243600159883
training loss 1.0039599111027486e-05 mae 0.0024413357936211074
training loss 1.0142958379417742e-05 mae 0.0024570258735393227
training loss 1.0415676509887665e-05 mae 0.0024902804242755397
training loss 1.0596706456612197e-05 mae 0.0025174238537183356
Epoch 249, training: loss: 0.0000106, mae: 0.0025236 test: loss0.0000850, mae:0.0066710
training loss 1.0964159628201742e-05 mae 0.002645232016220689
training loss 9.874843361139226e-06 mae 0.002412894838854817
training loss 1.0184347024958267e-05 mae 0.0024623183284581893
training loss 1.036624041039401e-05 mae 0.002478036736539066
training loss 1.068517626640754e-05 mae 0.0025246358877372243
Epoch 250, training: loss: 0.0000107, mae: 0.0025263 test: loss0.0000846, mae:0.0066441
training loss 1.1017796168744098e-05 mae 0.002543289912864566
training loss 9.870560871586707e-06 mae 0.002442081682566626
training loss 1.0257035849616216e-05 mae 0.002477478622090033
training loss 1.0175296780132894e-05 mae 0.002473254441376158
training loss 1.0223679951263532e-05 mae 0.0024722482139170536
Epoch 251, training: loss: 0.0000102, mae: 0.0024738 test: loss0.0000849, mae:0.0066514
training loss 7.247791018016869e-06 mae 0.0020380544010549784
training loss 1.0278421479026787e-05 mae 0.0024806096665926417
training loss 1.0354308577205135e-05 mae 0.0024947413141922174
training loss 1.0385599378556156e-05 mae 0.002506893968505673
training loss 1.046322180835755e-05 mae 0.002518832051677082
Epoch 252, training: loss: 0.0000105, mae: 0.0025226 test: loss0.0000847, mae:0.0066578
training loss 7.732925041636918e-06 mae 0.0020734171848744154
training loss 1.0729683619466218e-05 mae 0.002513453309588572
training loss 1.0489032486628563e-05 mae 0.002504632361048814
training loss 1.0448484025626334e-05 mae 0.0024913262270563692
training loss 1.0575216673719271e-05 mae 0.002509152781987443
Epoch 253, training: loss: 0.0000106, mae: 0.0025142 test: loss0.0000848, mae:0.0066636
training loss 9.556446457281709e-06 mae 0.0025309354532510042
training loss 1.0026159124404956e-05 mae 0.002448498586411862
training loss 1.0116677044201843e-05 mae 0.002458657183002053
training loss 1.0312385133364318e-05 mae 0.0024916740876024233
training loss 1.0443862010020423e-05 mae 0.002513812821872756
Epoch 254, training: loss: 0.0000104, mae: 0.0025151 test: loss0.0000847, mae:0.0066425
training loss 1.0422991181258112e-05 mae 0.0024305449333041906
training loss 9.458209553807083e-06 mae 0.0023672028704929877
training loss 9.723483907520749e-06 mae 0.00240251096541818
training loss 9.887338801611302e-06 mae 0.002437310786732774
training loss 1.0054797070126672e-05 mae 0.002453781034222882
Epoch 255, training: loss: 0.0000100, mae: 0.0024514 test: loss0.0000858, mae:0.0066717
training loss 9.636391951062251e-06 mae 0.002503218362107873
training loss 9.386562102436606e-06 mae 0.0024060503392498572
training loss 9.364178669496296e-06 mae 0.0023813844752949807
training loss 9.739122214528197e-06 mae 0.002414125266842644
training loss 1.0163609095431653e-05 mae 0.0024612399474926196
Epoch 256, training: loss: 0.0000102, mae: 0.0024643 test: loss0.0000856, mae:0.0066638
training loss 6.478521299868589e-06 mae 0.002133016474545002
training loss 1.0255898573899791e-05 mae 0.002456153430245524
training loss 1.0839095070292781e-05 mae 0.002524703153233865
training loss 1.0791553085562323e-05 mae 0.0025326623735542337
training loss 1.0640290806963208e-05 mae 0.002515202068459633
Epoch 257, training: loss: 0.0000106, mae: 0.0025122 test: loss0.0000861, mae:0.0066922
training loss 7.239058959385147e-06 mae 0.0021445888560265303
training loss 9.515936377354982e-06 mae 0.0023637125474017333
training loss 9.884847182403593e-06 mae 0.002422905487865947
training loss 1.0242000582173271e-05 mae 0.0024679308874372215
training loss 1.0277542107577392e-05 mae 0.002479581043022263
Epoch 258, training: loss: 0.0000103, mae: 0.0024815 test: loss0.0000854, mae:0.0066879
training loss 8.594389328209218e-06 mae 0.0023473280016332865
training loss 9.417592857918113e-06 mae 0.002361137378851281
training loss 9.45815255174311e-06 mae 0.002372112209300077
training loss 9.919720737072555e-06 mae 0.002424469486723919
training loss 1.013642825125149e-05 mae 0.0024555020413097378
Epoch 259, training: loss: 0.0000101, mae: 0.0024573 test: loss0.0000847, mae:0.0066656
training loss 8.804254321148619e-06 mae 0.0023493634071201086
training loss 9.546630352298392e-06 mae 0.002389927209793206
training loss 9.436922649154727e-06 mae 0.0023723264008133427
training loss 9.937978810083351e-06 mae 0.0024397809702952377
training loss 1.0120984739362605e-05 mae 0.0024591413377529374
Epoch 260, training: loss: 0.0000102, mae: 0.0024698 test: loss0.0000841, mae:0.0066424
training loss 1.2401030289765913e-05 mae 0.0027085125911980867
training loss 9.791233807453528e-06 mae 0.0024119004602635324
training loss 1.010109285248696e-05 mae 0.002443917794153095
training loss 1.020080513409621e-05 mae 0.0024660543834679584
training loss 1.016715435555251e-05 mae 0.002461324431423784
Epoch 261, training: loss: 0.0000102, mae: 0.0024631 test: loss0.0000842, mae:0.0066168
training loss 6.542486062244279e-06 mae 0.0019845347851514816
training loss 9.687656342173476e-06 mae 0.002381429340982554
training loss 1.0233309820809028e-05 mae 0.0024695380009932084
training loss 1.0258305328594563e-05 mae 0.0024746898122267507
training loss 1.0262665649798098e-05 mae 0.0024786550454350548
Epoch 262, training: loss: 0.0000102, mae: 0.0024752 test: loss0.0000860, mae:0.0066734
training loss 7.80519076215569e-06 mae 0.002036685822531581
training loss 9.23711504761812e-06 mae 0.0023572259518664838
training loss 9.300731080927136e-06 mae 0.00235848332695331
training loss 9.538897112534625e-06 mae 0.002392547637600426
training loss 9.847112079426815e-06 mae 0.0024290213173727942
Epoch 263, training: loss: 0.0000099, mae: 0.0024309 test: loss0.0000863, mae:0.0067146
training loss 8.020285349630285e-06 mae 0.0022867328952997923
training loss 1.0797622659689903e-05 mae 0.0025074987138128464
training loss 1.0402436637017636e-05 mae 0.002493012154769928
training loss 1.0455828394106896e-05 mae 0.00248447830159686
training loss 1.0571603441312799e-05 mae 0.002511854735272588
Epoch 264, training: loss: 0.0000106, mae: 0.0025106 test: loss0.0000865, mae:0.0067021
training loss 1.1249004273850005e-05 mae 0.0025754186790436506
training loss 9.375548398013197e-06 mae 0.0023550088540194376
training loss 9.305962431417448e-06 mae 0.0023566341606236175
training loss 9.492093887268658e-06 mae 0.0023749972429952973
training loss 9.559899825632526e-06 mae 0.0023866853582909073
Epoch 265, training: loss: 0.0000096, mae: 0.0023894 test: loss0.0000856, mae:0.0066850
training loss 5.996094841975719e-06 mae 0.0018695397302508354
training loss 9.7697461837122e-06 mae 0.002391622465688223
training loss 9.713288962680817e-06 mae 0.002400838200574611
training loss 9.673829481471327e-06 mae 0.00239698741344476
training loss 9.942367339601729e-06 mae 0.0024272028173769446
Epoch 266, training: loss: 0.0000100, mae: 0.0024317 test: loss0.0000853, mae:0.0066764
training loss 8.588910532125738e-06 mae 0.0023264531046152115
training loss 9.675710771261947e-06 mae 0.0024370340009530382
training loss 9.906131335000204e-06 mae 0.0024384978033676007
training loss 9.684087383101722e-06 mae 0.00241262172826794
training loss 9.87217581677218e-06 mae 0.002429837232979191
Epoch 267, training: loss: 0.0000099, mae: 0.0024384 test: loss0.0000854, mae:0.0066742
training loss 8.756606803217437e-06 mae 0.0023611325304955244
training loss 9.386228326797671e-06 mae 0.0023689485502009296
training loss 9.14684629732268e-06 mae 0.0023338944203967218
training loss 9.728111793367663e-06 mae 0.002405906476956231
training loss 1.0211102295641163e-05 mae 0.002457164656556217
Epoch 268, training: loss: 0.0000102, mae: 0.0024629 test: loss0.0000868, mae:0.0067830
training loss 1.2949426491104532e-05 mae 0.002917716046795249
training loss 9.899298264844771e-06 mae 0.0024272131062496242
training loss 9.710333915340882e-06 mae 0.0024180212784281905
training loss 9.988867360569457e-06 mae 0.002453452495037769
training loss 9.929090972593205e-06 mae 0.0024491027675207643
Epoch 269, training: loss: 0.0000100, mae: 0.0024477 test: loss0.0000866, mae:0.0067201
training loss 9.752920959726907e-06 mae 0.002482212148606777
training loss 8.794245626203892e-06 mae 0.0023100670591435016
training loss 9.177133830631319e-06 mae 0.002357059025832701
training loss 9.317681327571476e-06 mae 0.0023666145858225354
training loss 9.437992132043109e-06 mae 0.0023778846860395297
Epoch 270, training: loss: 0.0000094, mae: 0.0023728 test: loss0.0000868, mae:0.0067122
training loss 7.95809319242835e-06 mae 0.0021995215211063623
training loss 8.719607346089711e-06 mae 0.002255332710988382
training loss 9.139958491335948e-06 mae 0.002326842081631618
training loss 9.095097427674264e-06 mae 0.00232945023712067
training loss 9.116807295279824e-06 mae 0.002335667596847302
Epoch 271, training: loss: 0.0000091, mae: 0.0023355 test: loss0.0000872, mae:0.0067485
training loss 6.899172149132937e-06 mae 0.0020590315107256174
training loss 9.026481397906499e-06 mae 0.0023147933751198587
training loss 1.0130237943386494e-05 mae 0.0024512518764390518
training loss 1.0152750861223292e-05 mae 0.0024591086483221284
training loss 1.0092574957434859e-05 mae 0.0024530924850870947
Epoch 272, training: loss: 0.0000101, mae: 0.0024516 test: loss0.0000854, mae:0.0066950
training loss 9.9858143585152e-06 mae 0.0025260031688958406
training loss 9.524359198871489e-06 mae 0.002367792475292933
training loss 9.43029768263436e-06 mae 0.002368059026602986
training loss 9.655501982694439e-06 mae 0.0023940236867399287
training loss 9.685239212413585e-06 mae 0.0024096193985170245
Epoch 273, training: loss: 0.0000097, mae: 0.0024109 test: loss0.0000885, mae:0.0068071
training loss 9.137492270383518e-06 mae 0.0024495553225278854
training loss 1.0344576598892635e-05 mae 0.0024377650960220715
training loss 1.0255440429318698e-05 mae 0.0024636747894144742
training loss 1.035343711316953e-05 mae 0.002484374520035818
training loss 1.0272059193881085e-05 mae 0.002479057548224779
Epoch 274, training: loss: 0.0000103, mae: 0.0024777 test: loss0.0000858, mae:0.0066910
training loss 1.012453230941901e-05 mae 0.002595345489680767
training loss 9.951361611385047e-06 mae 0.002439518363269813
training loss 1.000366811771672e-05 mae 0.0024346752693913502
training loss 9.711121721607588e-06 mae 0.0024085632781343943
training loss 9.643077737111967e-06 mae 0.0024054295473745963
Epoch 275, training: loss: 0.0000096, mae: 0.0024044 test: loss0.0000857, mae:0.0067164
training loss 8.098229045572225e-06 mae 0.002132639056071639
training loss 9.09067116213812e-06 mae 0.0022945442127392576
training loss 8.919515016939413e-06 mae 0.002292669673293534
training loss 8.761238417032508e-06 mae 0.0022828865400052914
training loss 9.071036266274199e-06 mae 0.0023266196729318784
Epoch 276, training: loss: 0.0000091, mae: 0.0023317 test: loss0.0000858, mae:0.0067020
training loss 6.191854026837973e-06 mae 0.002040064660832286
training loss 8.369762638846855e-06 mae 0.002238906915390901
training loss 8.717412133546982e-06 mae 0.0022752747633540543
training loss 8.897989709118177e-06 mae 0.0022855435046274823
training loss 8.892389303410659e-06 mae 0.002293021144201765
Epoch 277, training: loss: 0.0000089, mae: 0.0022950 test: loss0.0000866, mae:0.0067403
training loss 1.0100476174557116e-05 mae 0.0024180614855140448
training loss 8.209076610250582e-06 mae 0.0022096182196857587
training loss 8.318796368833924e-06 mae 0.002247705426306861
training loss 8.76770677045566e-06 mae 0.0022940713394161896
training loss 8.988629050876605e-06 mae 0.0023240744685337185
Epoch 278, training: loss: 0.0000090, mae: 0.0023264 test: loss0.0000858, mae:0.0067090
training loss 9.242249689123128e-06 mae 0.002206169767305255
training loss 8.115561054271926e-06 mae 0.0022260854477245437
training loss 8.557369251365803e-06 mae 0.0022712439831143417
training loss 8.910525334824154e-06 mae 0.0023119346206292328
training loss 9.23350476408253e-06 mae 0.0023565566803175797
Epoch 279, training: loss: 0.0000092, mae: 0.0023582 test: loss0.0000860, mae:0.0067004
training loss 1.002250883175293e-05 mae 0.002613839926198125
training loss 9.22726091327301e-06 mae 0.002325338841068979
training loss 9.124412818072421e-06 mae 0.0023274345745304876
training loss 9.065526496398874e-06 mae 0.002323222946412616
training loss 9.162341072864072e-06 mae 0.0023368244842436195
Epoch 280, training: loss: 0.0000092, mae: 0.0023441 test: loss0.0000876, mae:0.0067695
training loss 5.019801847083727e-06 mae 0.0016488004475831985
training loss 9.27678902549886e-06 mae 0.0023641014065337823
training loss 9.55765382950811e-06 mae 0.0023891511603747265
training loss 9.525939869779795e-06 mae 0.002383146415748295
training loss 9.392045923498281e-06 mae 0.002364637393305144
Epoch 281, training: loss: 0.0000094, mae: 0.0023645 test: loss0.0000864, mae:0.0067074
training loss 8.686840374139138e-06 mae 0.002206164412200451
training loss 8.468655341501151e-06 mae 0.0022490183165406476
training loss 8.356229005080921e-06 mae 0.002233575403662675
training loss 8.521361134915312e-06 mae 0.0022512484281724753
training loss 8.770602453839288e-06 mae 0.002286477371776913
Epoch 282, training: loss: 0.0000087, mae: 0.0022830 test: loss0.0000853, mae:0.0066611
training loss 7.277580152731389e-06 mae 0.0021496813278645277
training loss 8.100842273515628e-06 mae 0.0022082735884788574
training loss 8.5767828525127e-06 mae 0.002252656250666496
training loss 8.679882525848974e-06 mae 0.002269359219924159
training loss 8.85772413909505e-06 mae 0.0022977161167921575
Epoch 283, training: loss: 0.0000088, mae: 0.0022901 test: loss0.0000867, mae:0.0067293
training loss 6.266040145419538e-06 mae 0.0019797896966338158
training loss 9.413484518767755e-06 mae 0.0023348392786312036
training loss 9.356482588364176e-06 mae 0.0023600518113315694
training loss 9.225806795877465e-06 mae 0.002339612361080699
training loss 9.221221719133212e-06 mae 0.0023437698623072823
Epoch 284, training: loss: 0.0000093, mae: 0.0023484 test: loss0.0000877, mae:0.0067685
training loss 6.5394815464969724e-06 mae 0.0021246743854135275
training loss 8.162675296226357e-06 mae 0.002256163925516839
training loss 8.412236755896513e-06 mae 0.002259229566892702
training loss 8.630037777134896e-06 mae 0.0022865260275857927
training loss 8.829031658159927e-06 mae 0.0023085045754511037
Epoch 285, training: loss: 0.0000088, mae: 0.0023120 test: loss0.0000868, mae:0.0067439
training loss 9.791245247470215e-06 mae 0.0022171831224113703
training loss 9.118441958693726e-06 mae 0.002334340995944598
training loss 9.176096043147212e-06 mae 0.002335087316673212
training loss 9.44662019267809e-06 mae 0.002375119971728186
training loss 9.510796954415149e-06 mae 0.0023863778880505414
Epoch 286, training: loss: 0.0000095, mae: 0.0023868 test: loss0.0000873, mae:0.0067744
training loss 8.052372322708834e-06 mae 0.0022049937397241592
training loss 9.81639740369728e-06 mae 0.00240733152182371
training loss 9.48933415211196e-06 mae 0.002378810328514416
training loss 9.334020976604843e-06 mae 0.0023755005611698853
training loss 9.474353778862125e-06 mae 0.002391021034972215
Epoch 287, training: loss: 0.0000095, mae: 0.0023904 test: loss0.0000877, mae:0.0067563
training loss 1.0137650860997383e-05 mae 0.002617606660351157
training loss 9.255931521086799e-06 mae 0.002362548086025259
training loss 8.979424531542747e-06 mae 0.002330288964111616
training loss 8.935247876330405e-06 mae 0.0023254774193223935
training loss 8.914692222077886e-06 mae 0.0023200659889876093
Epoch 288, training: loss: 0.0000089, mae: 0.0023193 test: loss0.0000861, mae:0.0067030
training loss 1.2027344382659066e-05 mae 0.0026732292026281357
training loss 8.125362843630309e-06 mae 0.0022091353266481674
training loss 7.97415052624958e-06 mae 0.0021944673844880532
training loss 8.4161940806655e-06 mae 0.0022502981445486855
training loss 8.524704668813155e-06 mae 0.0022636838946537815
Epoch 289, training: loss: 0.0000086, mae: 0.0022747 test: loss0.0000882, mae:0.0067997
training loss 4.681675363826798e-06 mae 0.0016781765734776855
training loss 8.307565862838491e-06 mae 0.002201066779283185
training loss 7.990991641893533e-06 mae 0.0021795935313318757
training loss 8.358928681861426e-06 mae 0.002237959293068836
training loss 8.518084754387787e-06 mae 0.002261665690833915
Epoch 290, training: loss: 0.0000085, mae: 0.0022589 test: loss0.0000867, mae:0.0067420
training loss 6.3931188378774095e-06 mae 0.0019528921693563461
training loss 7.934566784730552e-06 mae 0.002194124728203842
training loss 7.81780803357257e-06 mae 0.0021645836589747284
training loss 8.038936414699407e-06 mae 0.002203054442409648
training loss 8.408841877460885e-06 mae 0.002247958626500818
Epoch 291, training: loss: 0.0000085, mae: 0.0022550 test: loss0.0000863, mae:0.0067307
training loss 1.0859072062885389e-05 mae 0.0026827005203813314
training loss 7.87585046661358e-06 mae 0.00217873836164454
training loss 7.934887561181186e-06 mae 0.002189455829367116
training loss 8.262775814812554e-06 mae 0.00222499184665079
training loss 8.487243463261518e-06 mae 0.0022558966351543283
Epoch 292, training: loss: 0.0000086, mae: 0.0022646 test: loss0.0000870, mae:0.0067371
training loss 9.005961146613117e-06 mae 0.002366960747167468
training loss 1.0062445377187426e-05 mae 0.0024430367264750546
training loss 9.726983066819697e-06 mae 0.002412171711647274
training loss 9.40410657357467e-06 mae 0.0023760002319213776
training loss 9.395964320788714e-06 mae 0.0023721362323968196
Epoch 293, training: loss: 0.0000094, mae: 0.0023696 test: loss0.0000876, mae:0.0067542
training loss 7.432096026604995e-06 mae 0.0022821249440312386
training loss 8.18931978167658e-06 mae 0.0022131873810590772
training loss 8.255529751700617e-06 mae 0.002221988743438505
training loss 8.25297589496122e-06 mae 0.0022128106431036397
training loss 8.349892700087238e-06 mae 0.002225296835957178
Epoch 294, training: loss: 0.0000083, mae: 0.0022228 test: loss0.0000877, mae:0.0067872
training loss 6.129338999016909e-06 mae 0.0019970035646110773
training loss 7.622006375872757e-06 mae 0.0021366031187604745
training loss 8.042902371214787e-06 mae 0.0022132837266596677
training loss 8.199606721768606e-06 mae 0.002230111213511979
training loss 8.182256593653754e-06 mae 0.0022196363685513614
Epoch 295, training: loss: 0.0000082, mae: 0.0022256 test: loss0.0000858, mae:0.0067214
training loss 6.773786026315065e-06 mae 0.0019515156745910645
training loss 8.412176389267154e-06 mae 0.002236118316467779
training loss 8.28986032213987e-06 mae 0.0022257830807017073
training loss 8.30067915106439e-06 mae 0.002232127829103281
training loss 8.318796614698998e-06 mae 0.0022246215981315477
Epoch 296, training: loss: 0.0000084, mae: 0.0022308 test: loss0.0000870, mae:0.0067467
training loss 7.1793510869611055e-06 mae 0.001973549136891961
training loss 7.796335351075886e-06 mae 0.0021527124381642425
training loss 7.6500372739643e-06 mae 0.002130788678331025
training loss 7.726222150008862e-06 mae 0.0021462435147953265
training loss 8.051748454591908e-06 mae 0.002191447462325926
Epoch 297, training: loss: 0.0000080, mae: 0.0021925 test: loss0.0000885, mae:0.0067937
training loss 7.275299594766693e-06 mae 0.002119912998750806
training loss 9.396992383842062e-06 mae 0.002357798938949903
training loss 9.057185396537384e-06 mae 0.0023259905047050802
training loss 9.025600127715247e-06 mae 0.002315861267031522
training loss 9.064259012659055e-06 mae 0.0023229996498850226
Epoch 298, training: loss: 0.0000091, mae: 0.0023239 test: loss0.0000873, mae:0.0067660
training loss 6.765419584553456e-06 mae 0.0021514815744012594
training loss 8.740436183585439e-06 mae 0.002298653459030331
training loss 8.682732470225514e-06 mae 0.002281048890701172
training loss 8.623842988876372e-06 mae 0.0022700313472540563
training loss 8.606459829676552e-06 mae 0.002270111605057039
Epoch 299, training: loss: 0.0000086, mae: 0.0022660 test: loss0.0000866, mae:0.0067573
current learning rate: 6.25e-05
training loss 6.785694040445378e-06 mae 0.001917022280395031
training loss 6.692537877539757e-06 mae 0.001962536932243144
training loss 6.48296622334738e-06 mae 0.0019308334380148512
training loss 6.4079346511247405e-06 mae 0.001920204800831166
training loss 6.4356048919287444e-06 mae 0.0019262207953481755
Epoch 300, training: loss: 0.0000065, mae: 0.0019313 test: loss0.0000865, mae:0.0067124
training loss 5.71391001358279e-06 mae 0.0016438917955383658
training loss 5.49162897793091e-06 mae 0.0017654555575812567
training loss 5.745354908165356e-06 mae 0.0018034640810277208
training loss 6.036871565618708e-06 mae 0.0018442307116266416
training loss 6.006369602715011e-06 mae 0.001841333614249913
Epoch 301, training: loss: 0.0000060, mae: 0.0018461 test: loss0.0000863, mae:0.0067207
training loss 4.364338110462995e-06 mae 0.001621981500647962
training loss 5.745581106834031e-06 mae 0.0017946910583322823
training loss 5.924139164598819e-06 mae 0.001813242374458304
training loss 5.99752267939135e-06 mae 0.0018265953939716445
training loss 6.012993940180537e-06 mae 0.001831272244569266
Epoch 302, training: loss: 0.0000060, mae: 0.0018284 test: loss0.0000876, mae:0.0067746
training loss 5.719721229979768e-06 mae 0.0019039219478145242
training loss 5.795473716894848e-06 mae 0.0017977097304537895
training loss 5.76629505564714e-06 mae 0.0017902582035501406
training loss 5.871451422469499e-06 mae 0.0018083155332503232
training loss 5.994244813237441e-06 mae 0.0018311436409686704
Epoch 303, training: loss: 0.0000060, mae: 0.0018307 test: loss0.0000867, mae:0.0067237
training loss 4.5653237066289876e-06 mae 0.0015935605624690652
training loss 5.777549992725493e-06 mae 0.001787160265295967
training loss 5.877951210068475e-06 mae 0.001808590196893726
training loss 6.0567272308219725e-06 mae 0.0018487318144457428
training loss 6.082721117322695e-06 mae 0.0018495115317144796
Epoch 304, training: loss: 0.0000061, mae: 0.0018503 test: loss0.0000870, mae:0.0067507
training loss 4.7923454076226335e-06 mae 0.0016820415621623397
training loss 5.616085971661389e-06 mae 0.0017766019959441007
training loss 5.777119852582742e-06 mae 0.0017951140364650456
training loss 5.882460742900516e-06 mae 0.001815454692838849
training loss 5.92404523391379e-06 mae 0.0018217031182535923
Epoch 305, training: loss: 0.0000060, mae: 0.0018301 test: loss0.0000877, mae:0.0067729
training loss 6.039740128471749e-06 mae 0.0018299786606803536
training loss 5.893135146513541e-06 mae 0.0017906039629095036
training loss 5.741826300598602e-06 mae 0.00179670011108317
training loss 5.887855717904412e-06 mae 0.0018338473006429172
training loss 6.006884298662567e-06 mae 0.0018509429762492075
Epoch 306, training: loss: 0.0000060, mae: 0.0018469 test: loss0.0000865, mae:0.0067306
training loss 6.015213784849038e-06 mae 0.0017489887541159987
training loss 5.839900928085803e-06 mae 0.0017947822069639674
training loss 5.8787166759285876e-06 mae 0.001819655693385123
training loss 6.097006043760392e-06 mae 0.001855112343181591
training loss 6.107411070445036e-06 mae 0.0018668484066691787
Epoch 307, training: loss: 0.0000061, mae: 0.0018658 test: loss0.0000869, mae:0.0067457
training loss 9.91443721432006e-06 mae 0.0022555142641067505
training loss 5.812159623233396e-06 mae 0.0017969802515033415
training loss 5.7265930387651375e-06 mae 0.0017996270642938593
training loss 5.8470828867180504e-06 mae 0.001817654058898473
training loss 5.900412811592903e-06 mae 0.0018297127617143134
Epoch 308, training: loss: 0.0000059, mae: 0.0018368 test: loss0.0000885, mae:0.0068125
training loss 9.147530363406986e-06 mae 0.0020888689905405045
training loss 5.759199118127819e-06 mae 0.0018124963617974924
training loss 5.928307306777862e-06 mae 0.001830799316083736
training loss 5.913363590016221e-06 mae 0.0018283808272368958
training loss 5.99496878006975e-06 mae 0.0018444217988220388
Epoch 309, training: loss: 0.0000060, mae: 0.0018466 test: loss0.0000876, mae:0.0067605
training loss 5.808236892335117e-06 mae 0.0016828831285238266
training loss 5.716700267536605e-06 mae 0.0018022058887735884
training loss 5.908004170285693e-06 mae 0.001837756552751923
training loss 5.874261896618715e-06 mae 0.0018354962088031979
training loss 5.989408930361432e-06 mae 0.0018550737005709414
Epoch 310, training: loss: 0.0000060, mae: 0.0018594 test: loss0.0000872, mae:0.0067573
training loss 4.721092864201637e-06 mae 0.0018085645278915763
training loss 5.6609755405339e-06 mae 0.0018018857260946846
training loss 5.709995615923098e-06 mae 0.001808160834134289
training loss 5.715654529825808e-06 mae 0.0018041192897786662
training loss 5.9093562451642855e-06 mae 0.0018325841560524268
Epoch 311, training: loss: 0.0000059, mae: 0.0018399 test: loss0.0000887, mae:0.0068159
training loss 9.549127753416542e-06 mae 0.0024703543167561293
training loss 5.7866056397638285e-06 mae 0.0018256938714972318
training loss 5.499644027559675e-06 mae 0.0017809374661341606
training loss 5.6697492544008866e-06 mae 0.001797327340043933
training loss 5.845491915211615e-06 mae 0.0018265877047609256
Epoch 312, training: loss: 0.0000059, mae: 0.0018319 test: loss0.0000880, mae:0.0067754
training loss 6.397679044312099e-06 mae 0.0018837526440620422
training loss 5.683738418512947e-06 mae 0.001775803229333285
training loss 5.7593717298401005e-06 mae 0.0018019972087833845
training loss 5.800595247078883e-06 mae 0.0018170110770309995
training loss 5.899326326281014e-06 mae 0.0018335583258364623
Epoch 313, training: loss: 0.0000059, mae: 0.0018312 test: loss0.0000879, mae:0.0067658
training loss 5.317187969922088e-06 mae 0.0018965769559144974
training loss 5.459422852129815e-06 mae 0.001761740308674966
training loss 5.655750672952194e-06 mae 0.0017832197712291736
training loss 5.660081587727791e-06 mae 0.0017876893479303023
training loss 5.7500259226256285e-06 mae 0.0018105225780033574
Epoch 314, training: loss: 0.0000058, mae: 0.0018109 test: loss0.0000893, mae:0.0068351
training loss 5.437337222247152e-06 mae 0.0018184924265369773
training loss 5.722254333223273e-06 mae 0.0017918926188904868
training loss 5.6562608589578465e-06 mae 0.0017884197082407407
training loss 5.668077387811641e-06 mae 0.001794179725701249
training loss 5.830310373995639e-06 mae 0.001814810830210128
Epoch 315, training: loss: 0.0000058, mae: 0.0018172 test: loss0.0000887, mae:0.0067955
training loss 6.228475740499562e-06 mae 0.0018429309129714966
training loss 5.526979449831404e-06 mae 0.001769747961696018
training loss 5.55369118019641e-06 mae 0.0017736373022014253
training loss 5.579395712557498e-06 mae 0.0017799762962694388
training loss 5.736146516512543e-06 mae 0.0018046762538258337
Epoch 316, training: loss: 0.0000057, mae: 0.0018067 test: loss0.0000897, mae:0.0068341
training loss 6.4077025854203384e-06 mae 0.0018408832838758826
training loss 5.406628454639918e-06 mae 0.001760374270744768
training loss 5.5185445386213005e-06 mae 0.0017686525836937354
training loss 5.608722450528607e-06 mae 0.0017830957180592205
training loss 5.625127916790742e-06 mae 0.001786133406489198
Epoch 317, training: loss: 0.0000057, mae: 0.0017905 test: loss0.0000897, mae:0.0068314
training loss 5.5749965213180985e-06 mae 0.0018189729889854789
training loss 5.388395114399222e-06 mae 0.001751356687415026
training loss 5.47579488603774e-06 mae 0.00176275411210401
training loss 5.539429148424258e-06 mae 0.0017832583201962788
training loss 5.71723295089119e-06 mae 0.0018105931210549387
Epoch 318, training: loss: 0.0000057, mae: 0.0018162 test: loss0.0000903, mae:0.0068599
training loss 5.524915195564972e-06 mae 0.0017885485431179404
training loss 5.822524523561395e-06 mae 0.0018102845870981029
training loss 5.6898369877834036e-06 mae 0.0018076131196397515
training loss 5.670218889457264e-06 mae 0.0018071984041124485
training loss 5.7233479984728135e-06 mae 0.0018155714381134036
Epoch 319, training: loss: 0.0000057, mae: 0.0018166 test: loss0.0000902, mae:0.0068485
training loss 4.549073764792411e-06 mae 0.0015376880764961243
training loss 5.332407431970019e-06 mae 0.0017588666649352685
training loss 5.605153105399482e-06 mae 0.0017918141312104202
training loss 5.528142526973713e-06 mae 0.001782125712446375
training loss 5.701386313394234e-06 mae 0.0018058055386280826
Epoch 320, training: loss: 0.0000057, mae: 0.0018051 test: loss0.0000897, mae:0.0068449
training loss 4.145039838476805e-06 mae 0.0015012212097644806
training loss 5.376677456618326e-06 mae 0.0017553443436090852
training loss 5.437743390424352e-06 mae 0.0017725610333301205
training loss 5.4554865820311585e-06 mae 0.0017701312598914208
training loss 5.572569254165762e-06 mae 0.0017858477927911193
Epoch 321, training: loss: 0.0000057, mae: 0.0017985 test: loss0.0000895, mae:0.0068200
training loss 6.866335752420127e-06 mae 0.002215692074969411
training loss 5.897777363512146e-06 mae 0.001811010429781734
training loss 5.8429450944296845e-06 mae 0.001826377726150769
training loss 5.849077072808776e-06 mae 0.0018244660737005293
training loss 5.813751327262601e-06 mae 0.001813168043215105
Epoch 322, training: loss: 0.0000058, mae: 0.0018129 test: loss0.0000884, mae:0.0068001
training loss 4.409099346958101e-06 mae 0.0015417924150824547
training loss 5.494113886871834e-06 mae 0.0017709098117170384
training loss 5.511326246122924e-06 mae 0.001763819391829986
training loss 5.435110973463596e-06 mae 0.001759569675643121
training loss 5.61434706297355e-06 mae 0.0017928768057070346
Epoch 323, training: loss: 0.0000057, mae: 0.0018005 test: loss0.0000900, mae:0.0068563
training loss 9.198917723551858e-06 mae 0.0022276872768998146
training loss 5.227320923272511e-06 mae 0.0017296249553670775
training loss 5.446895921513802e-06 mae 0.0017590917586948313
training loss 5.573522525540565e-06 mae 0.0017736790877526365
training loss 5.603097981148304e-06 mae 0.001785457049107262
Epoch 324, training: loss: 0.0000056, mae: 0.0017872 test: loss0.0000897, mae:0.0068508
training loss 4.243013790983241e-06 mae 0.0014529628679156303
training loss 5.398034911159616e-06 mae 0.0017604315219739194
training loss 5.393905597218077e-06 mae 0.0017589231858509446
training loss 5.47018797060566e-06 mae 0.0017711587893215325
training loss 5.470257087723093e-06 mae 0.0017677563531046255
Epoch 325, training: loss: 0.0000055, mae: 0.0017727 test: loss0.0000901, mae:0.0068659
training loss 4.1027255974768195e-06 mae 0.00158028956502676
training loss 5.259691975237835e-06 mae 0.0017255448905166752
training loss 5.4206189334124265e-06 mae 0.0017534962585564742
training loss 5.433009018273854e-06 mae 0.0017547983805716832
training loss 5.5711397321127905e-06 mae 0.001780017427358758
Epoch 326, training: loss: 0.0000056, mae: 0.0017762 test: loss0.0000897, mae:0.0068513
training loss 5.453512130770832e-06 mae 0.0017173351952806115
training loss 5.109700561090494e-06 mae 0.0017037559643059094
training loss 5.332251442151836e-06 mae 0.0017434471957662177
training loss 5.521597459780562e-06 mae 0.0017723162613796782
training loss 5.563523191357045e-06 mae 0.00178414305083602
Epoch 327, training: loss: 0.0000055, mae: 0.0017820 test: loss0.0000901, mae:0.0068643
training loss 6.507183115900261e-06 mae 0.0019513791194185615
training loss 5.510478411665913e-06 mae 0.001770398068223514
training loss 5.484071956437553e-06 mae 0.0017661125742655136
training loss 5.471749825980643e-06 mae 0.00176203118742055
training loss 5.523034043782261e-06 mae 0.0017756115539759323
Epoch 328, training: loss: 0.0000055, mae: 0.0017756 test: loss0.0000909, mae:0.0068950
training loss 4.0568370422988664e-06 mae 0.0016225833678618073
training loss 5.121551232471522e-06 mae 0.0017137941790233344
training loss 5.394778885476068e-06 mae 0.0017450771070924576
training loss 5.443697535529825e-06 mae 0.001758631736669221
training loss 5.3745491220355195e-06 mae 0.0017525690685336792
Epoch 329, training: loss: 0.0000054, mae: 0.0017597 test: loss0.0000911, mae:0.0068709
training loss 6.109572950663278e-06 mae 0.0019922268111258745
training loss 5.095216492383555e-06 mae 0.0017186724717783578
training loss 5.2721047025145964e-06 mae 0.0017308051511184263
training loss 5.3261805490949325e-06 mae 0.0017394753395030824
training loss 5.340506296437779e-06 mae 0.0017371553190942129
Epoch 330, training: loss: 0.0000053, mae: 0.0017374 test: loss0.0000909, mae:0.0068965
training loss 4.9238801693718415e-06 mae 0.0017650481313467026
training loss 5.333735173860308e-06 mae 0.0017465933102785664
training loss 5.464812575425665e-06 mae 0.0017716231500029122
training loss 5.387695911235369e-06 mae 0.001758981126033707
training loss 5.430562327209689e-06 mae 0.0017655228041066086
Epoch 331, training: loss: 0.0000054, mae: 0.0017698 test: loss0.0000903, mae:0.0068557
training loss 5.296770268614637e-06 mae 0.0019226083531975746
training loss 5.037636547771686e-06 mae 0.0017091145204818428
training loss 5.3075202978818365e-06 mae 0.0017509258439819704
training loss 5.422743068217658e-06 mae 0.0017679959376780494
training loss 5.390957033147123e-06 mae 0.0017609515207685274
Epoch 332, training: loss: 0.0000054, mae: 0.0017571 test: loss0.0000909, mae:0.0068884
training loss 4.243704097461887e-06 mae 0.0015919655561447144
training loss 5.104691639394779e-06 mae 0.0017111240886151793
training loss 5.328481780218294e-06 mae 0.0017289714876211962
training loss 5.281898733758963e-06 mae 0.001713529567616616
training loss 5.323292968944713e-06 mae 0.0017256147745968913
Epoch 333, training: loss: 0.0000053, mae: 0.0017283 test: loss0.0000908, mae:0.0068714
training loss 3.6575456761056557e-06 mae 0.0014772819122299552
training loss 5.150418358015503e-06 mae 0.001711694813589109
training loss 5.276673080894251e-06 mae 0.0017285881953609014
training loss 5.315246358347111e-06 mae 0.0017259902920610949
training loss 5.26195552522574e-06 mae 0.0017275599414130563
Epoch 334, training: loss: 0.0000053, mae: 0.0017258 test: loss0.0000908, mae:0.0068733
training loss 4.843087026529247e-06 mae 0.0017313113203272223
training loss 5.1087400455192755e-06 mae 0.0017265644878623823
training loss 5.021023800491275e-06 mae 0.001717779278238811
training loss 5.175243621196814e-06 mae 0.0017390525044059218
training loss 5.319413140338171e-06 mae 0.0017487185906424228
Epoch 335, training: loss: 0.0000053, mae: 0.0017486 test: loss0.0000915, mae:0.0069217
training loss 3.278999201938859e-06 mae 0.001397203071974218
training loss 5.141858171590881e-06 mae 0.0017055101240711177
training loss 5.191612348029916e-06 mae 0.0017291407209203238
training loss 5.316084109878243e-06 mae 0.0017426477603318278
training loss 5.3264831912930246e-06 mae 0.0017432349969155676
Epoch 336, training: loss: 0.0000053, mae: 0.0017375 test: loss0.0000899, mae:0.0068515
training loss 5.570900157181313e-06 mae 0.0018010139465332031
training loss 4.8850559092077705e-06 mae 0.0016700280967223292
training loss 5.186780778992872e-06 mae 0.001715621598627381
training loss 5.2116325644461875e-06 mae 0.0017280914475339534
training loss 5.1830965133634524e-06 mae 0.0017270721577395181
Epoch 337, training: loss: 0.0000052, mae: 0.0017265 test: loss0.0000912, mae:0.0069002
training loss 4.889104275207501e-06 mae 0.001788440509699285
training loss 4.91651677989199e-06 mae 0.0016920728010910693
training loss 5.158659659812629e-06 mae 0.0017217202238867632
training loss 5.21617273790486e-06 mae 0.0017325055888786124
training loss 5.352797595843362e-06 mae 0.0017423646139972542
Epoch 338, training: loss: 0.0000053, mae: 0.0017416 test: loss0.0000900, mae:0.0068731
training loss 4.792403615283547e-06 mae 0.001665892661549151
training loss 5.0363608209533825e-06 mae 0.0016761170038222973
training loss 4.991643100937037e-06 mae 0.0016824247679756126
training loss 5.284381770240324e-06 mae 0.0017384347360926552
training loss 5.289186555640288e-06 mae 0.0017374415027061063
Epoch 339, training: loss: 0.0000053, mae: 0.0017365 test: loss0.0000903, mae:0.0068889
training loss 4.492436346481554e-06 mae 0.0015030255308374763
training loss 5.4422344722600745e-06 mae 0.0017417595018724011
training loss 5.2424772552467885e-06 mae 0.0017206752292424589
training loss 5.198579490463199e-06 mae 0.0017122525485873908
training loss 5.168279161427644e-06 mae 0.0017092293209456542
Epoch 340, training: loss: 0.0000052, mae: 0.0017100 test: loss0.0000916, mae:0.0069153
training loss 4.571314548229566e-06 mae 0.0017380369827151299
training loss 5.170092159531515e-06 mae 0.0016955726725213672
training loss 5.2033726435616e-06 mae 0.001703860223210316
training loss 5.214280442324882e-06 mae 0.001714198025936471
training loss 5.247021969504811e-06 mae 0.0017275713449019705
Epoch 341, training: loss: 0.0000052, mae: 0.0017254 test: loss0.0000906, mae:0.0068980
training loss 2.1700741399399703e-06 mae 0.0011277062585577369
training loss 5.358013350262041e-06 mae 0.0017342566393827117
training loss 5.181689785597041e-06 mae 0.0017169092788757523
training loss 5.195836567376592e-06 mae 0.0017249226471446207
training loss 5.185046118197021e-06 mae 0.0017158499089491301
Epoch 342, training: loss: 0.0000052, mae: 0.0017209 test: loss0.0000922, mae:0.0069199
training loss 5.372326995711774e-06 mae 0.0016451490810140967
training loss 5.366129783521733e-06 mae 0.0017666882497495882
training loss 5.443467493437718e-06 mae 0.0017910736802006421
training loss 5.384056596796164e-06 mae 0.0017728390648578671
training loss 5.333604844760085e-06 mae 0.0017601582256437688
Epoch 343, training: loss: 0.0000053, mae: 0.0017570 test: loss0.0000912, mae:0.0069093
training loss 3.3397870993212564e-06 mae 0.0013834661804139614
training loss 4.747826968317289e-06 mae 0.0016411313645578187
training loss 4.915521443682238e-06 mae 0.0016705377504579945
training loss 5.096038492995273e-06 mae 0.0017062435034563802
training loss 5.201479296632518e-06 mae 0.0017264424301961904
Epoch 344, training: loss: 0.0000052, mae: 0.0017281 test: loss0.0000905, mae:0.0068811
training loss 5.923896878812229e-06 mae 0.0018976855790242553
training loss 5.050658461079686e-06 mae 0.0016804320244666409
training loss 5.08611007339058e-06 mae 0.001699192958911604
training loss 5.04024959032627e-06 mae 0.0017009116416266611
training loss 5.1430592681405795e-06 mae 0.0017176168085775555
Epoch 345, training: loss: 0.0000052, mae: 0.0017189 test: loss0.0000921, mae:0.0069419
training loss 4.615243597072549e-06 mae 0.0017090989276766777
training loss 4.894039969204803e-06 mae 0.0016719170174944926
training loss 4.7805705331610305e-06 mae 0.001652542281536256
training loss 4.905361554738919e-06 mae 0.0016750273694105392
training loss 5.002388733921173e-06 mae 0.0016911194754755162
Epoch 346, training: loss: 0.0000051, mae: 0.0016997 test: loss0.0000911, mae:0.0069087
training loss 5.492289346875623e-06 mae 0.0017012212192639709
training loss 4.960861681812437e-06 mae 0.0016659821872142897
training loss 4.95984864746878e-06 mae 0.0016701947217622754
training loss 5.122990268505692e-06 mae 0.0017005006354055459
training loss 5.0835548261521095e-06 mae 0.0017002420882180106
Epoch 347, training: loss: 0.0000051, mae: 0.0017007 test: loss0.0000921, mae:0.0069519
training loss 3.1859444789006375e-06 mae 0.0014210818335413933
training loss 4.930853795788911e-06 mae 0.0017082110828007845
training loss 4.882748104893547e-06 mae 0.001685956015832501
training loss 5.055971186513055e-06 mae 0.0016991399191566652
training loss 5.085140230393384e-06 mae 0.0017063405946249244
Epoch 348, training: loss: 0.0000051, mae: 0.0017039 test: loss0.0000912, mae:0.0069181
training loss 7.111883405741537e-06 mae 0.0019432561239227653
training loss 4.789463753909597e-06 mae 0.001643730793148279
training loss 5.088915558379013e-06 mae 0.0016925623029525768
training loss 5.15320684766061e-06 mae 0.0017069732509738455
training loss 5.072184081311106e-06 mae 0.0016977686773225725
Epoch 349, training: loss: 0.0000051, mae: 0.0017002 test: loss0.0000908, mae:0.0068839
training loss 4.670243924920214e-06 mae 0.0016382703324779868
training loss 4.7157782051619835e-06 mae 0.0016304708757054274
training loss 4.757337470743219e-06 mae 0.001632087369504763
training loss 4.987173676472473e-06 mae 0.0016662912814884101
training loss 4.921078227732382e-06 mae 0.0016629130669883394
Epoch 350, training: loss: 0.0000049, mae: 0.0016664 test: loss0.0000922, mae:0.0069378
training loss 3.2748903322499245e-06 mae 0.0014601461589336395
training loss 5.22845336009221e-06 mae 0.001710253717450827
training loss 5.053061802683071e-06 mae 0.0016909610862742269
training loss 4.966113638508751e-06 mae 0.0016780846446925245
training loss 4.878922504079793e-06 mae 0.0016576079182577003
Epoch 351, training: loss: 0.0000049, mae: 0.0016562 test: loss0.0000918, mae:0.0069341
training loss 5.4847728279128205e-06 mae 0.0017446879064664245
training loss 4.8813468917311784e-06 mae 0.0016707329093204706
training loss 4.737705532336057e-06 mae 0.0016423696502213282
training loss 4.917031626082343e-06 mae 0.0016711644908499643
training loss 5.000948474400902e-06 mae 0.00168512344582757
Epoch 352, training: loss: 0.0000050, mae: 0.0016857 test: loss0.0000915, mae:0.0069277
training loss 3.120074325124733e-06 mae 0.0013624917482957244
training loss 4.753289373433753e-06 mae 0.0016419776719903537
training loss 4.887483416073023e-06 mae 0.0016668028472507795
training loss 5.002073515586275e-06 mae 0.0016850671478059907
training loss 5.046733753288074e-06 mae 0.0017035230832053948
Epoch 353, training: loss: 0.0000050, mae: 0.0017023 test: loss0.0000916, mae:0.0069146
training loss 2.3933660031616455e-06 mae 0.00125947839114815
training loss 4.6996937289448265e-06 mae 0.0016627453048439586
training loss 4.753523529474803e-06 mae 0.0016552235488549317
training loss 4.839560466846267e-06 mae 0.0016706059532647106
training loss 4.871486694287681e-06 mae 0.001674408140587644
Epoch 354, training: loss: 0.0000049, mae: 0.0016835 test: loss0.0000922, mae:0.0069460
training loss 3.3152991818496957e-06 mae 0.0014498885720968246
training loss 4.7249788363557195e-06 mae 0.0016421347459339918
training loss 4.951744331535362e-06 mae 0.001683629650419744
training loss 4.993102396874502e-06 mae 0.0016870514053345619
training loss 4.927297553858893e-06 mae 0.0016760582536394096
Epoch 355, training: loss: 0.0000049, mae: 0.0016720 test: loss0.0000914, mae:0.0069136
training loss 3.7624397464242065e-06 mae 0.0014845667174085975
training loss 4.760458367337717e-06 mae 0.001633340538497649
training loss 4.904841124707583e-06 mae 0.0016587243293236828
training loss 4.9177452300808945e-06 mae 0.001667996870033098
training loss 4.941364873568215e-06 mae 0.0016844916517674855
Epoch 356, training: loss: 0.0000049, mae: 0.0016821 test: loss0.0000921, mae:0.0069357
training loss 2.9044476832496002e-06 mae 0.0013696657260879874
training loss 4.5296476623751036e-06 mae 0.001604580935592964
training loss 4.6029502340893005e-06 mae 0.0016239539513797825
training loss 4.803438019096588e-06 mae 0.0016525145161385023
training loss 4.8856646544799555e-06 mae 0.0016675361600676695
Epoch 357, training: loss: 0.0000049, mae: 0.0016694 test: loss0.0000912, mae:0.0069005
training loss 5.686939857696416e-06 mae 0.0018140139291062951
training loss 4.626165170574333e-06 mae 0.001631603651570485
training loss 4.749946583615382e-06 mae 0.001650523250267869
training loss 4.796583550539455e-06 mae 0.00165753288731127
training loss 4.837455649134996e-06 mae 0.0016671984296625674
Epoch 358, training: loss: 0.0000049, mae: 0.0016708 test: loss0.0000907, mae:0.0068792
training loss 4.0432310015603434e-06 mae 0.001602131873369217
training loss 4.967881469543929e-06 mae 0.0016990694966094169
training loss 4.844334459295792e-06 mae 0.0016656761325084346
training loss 4.786906668456466e-06 mae 0.0016589504373660745
training loss 4.8393221856545946e-06 mae 0.0016712544622613271
Epoch 359, training: loss: 0.0000048, mae: 0.0016709 test: loss0.0000905, mae:0.0068822
training loss 4.804635409527691e-06 mae 0.0017399555072188377
training loss 4.5842616823481915e-06 mae 0.0016196533220400118
training loss 4.787421378543765e-06 mae 0.001652092675189718
training loss 4.813091618717523e-06 mae 0.0016561607114773315
training loss 4.846254549971238e-06 mae 0.0016635196440996816
Epoch 360, training: loss: 0.0000049, mae: 0.0016675 test: loss0.0000938, mae:0.0069895
training loss 3.2252548862743424e-06 mae 0.0013265054440125823
training loss 4.396965773683441e-06 mae 0.001565347831951929
training loss 4.60373849810002e-06 mae 0.001615093393933655
training loss 4.666945576129655e-06 mae 0.0016300128795983676
training loss 4.751889205475332e-06 mae 0.0016431520728913333
Epoch 361, training: loss: 0.0000048, mae: 0.0016463 test: loss0.0000935, mae:0.0070010
training loss 3.33551065523352e-06 mae 0.001458485028706491
training loss 4.585612879393511e-06 mae 0.0016134592120115668
training loss 4.7105471560709056e-06 mae 0.001629734981233383
training loss 4.782413061427679e-06 mae 0.001649939913814579
training loss 4.770279145924716e-06 mae 0.0016471810172084922
Epoch 362, training: loss: 0.0000048, mae: 0.0016442 test: loss0.0000935, mae:0.0069847
training loss 4.70943405161961e-06 mae 0.0016827946528792381
training loss 4.664749266179141e-06 mae 0.0016341006814721308
training loss 4.708235850881863e-06 mae 0.001648974853295759
training loss 4.772986343090188e-06 mae 0.0016571483370502618
training loss 4.7506365032194195e-06 mae 0.001651167045104137
Epoch 363, training: loss: 0.0000047, mae: 0.0016499 test: loss0.0000926, mae:0.0069412
training loss 4.25282678406802e-06 mae 0.0015565591165795922
training loss 4.76259511368185e-06 mae 0.0016355029097301702
training loss 4.588749084402959e-06 mae 0.0016063683721614945
training loss 4.568045962803702e-06 mae 0.001615928234655829
training loss 4.606843173692979e-06 mae 0.0016183326002303044
Epoch 364, training: loss: 0.0000046, mae: 0.0016246 test: loss0.0000969, mae:0.0070183
training loss 3.3801552490331233e-06 mae 0.0015719156945124269
training loss 4.5920723292225625e-06 mae 0.001611524304904628
training loss 4.593893582651698e-06 mae 0.0016278500373632008
training loss 4.745687856604899e-06 mae 0.0016439296490210573
training loss 4.825076640051308e-06 mae 0.0016622166780625195
Epoch 365, training: loss: 0.0000048, mae: 0.0016662 test: loss0.0000927, mae:0.0069570
training loss 6.413775281544076e-06 mae 0.0018796048825606704
training loss 4.591115005799414e-06 mae 0.0016362831982619625
training loss 4.62805381418845e-06 mae 0.0016311346126794074
training loss 4.719173547293143e-06 mae 0.0016467185268511633
training loss 4.783725288105608e-06 mae 0.0016618459520222081
Epoch 366, training: loss: 0.0000048, mae: 0.0016611 test: loss0.0000940, mae:0.0069953
training loss 2.5868223474390106e-06 mae 0.0012712031602859497
training loss 4.323238704719368e-06 mae 0.0015746278108517624
training loss 4.7121898836002704e-06 mae 0.0016406991076425162
training loss 4.706857775367679e-06 mae 0.0016475714510306718
training loss 4.679842339472055e-06 mae 0.0016445298997722379
Epoch 367, training: loss: 0.0000047, mae: 0.0016441 test: loss0.0000934, mae:0.0069852
training loss 6.571744961547665e-06 mae 0.0018273986643180251
training loss 4.4414060343400465e-06 mae 0.0015932482795095908
training loss 4.604199659189533e-06 mae 0.0016050458065058925
training loss 4.567211417090809e-06 mae 0.001601453388683399
training loss 4.588710834608926e-06 mae 0.0016168037780435445
Epoch 368, training: loss: 0.0000046, mae: 0.0016221 test: loss0.0000917, mae:0.0069169
training loss 3.511718205118086e-06 mae 0.0013439679751172662
training loss 4.579500138753051e-06 mae 0.001611336946998741
training loss 4.642562093932152e-06 mae 0.0016197002571223696
training loss 4.719772610673353e-06 mae 0.0016354059882227657
training loss 4.735697294830543e-06 mae 0.001643965826995337
Epoch 369, training: loss: 0.0000047, mae: 0.0016467 test: loss0.0000924, mae:0.0069696
training loss 3.7236659409245476e-06 mae 0.001342062489129603
training loss 4.481447488205067e-06 mae 0.0015921489467990463
training loss 4.613722593252407e-06 mae 0.0016252680711614175
training loss 4.7835572524418714e-06 mae 0.0016529220040727229
training loss 4.750890473958826e-06 mae 0.0016467505828500614
Epoch 370, training: loss: 0.0000047, mae: 0.0016401 test: loss0.0000934, mae:0.0069861
training loss 3.139521140838042e-06 mae 0.0013738904381170869
training loss 4.466712612959192e-06 mae 0.0015896097848228379
training loss 4.76367561775375e-06 mae 0.0016492258833426213
training loss 4.6558971549553916e-06 mae 0.0016329305166292273
training loss 4.696003909871318e-06 mae 0.0016427264828003253
Epoch 371, training: loss: 0.0000047, mae: 0.0016379 test: loss0.0000925, mae:0.0069595
training loss 4.356955741968704e-06 mae 0.001600281335413456
training loss 4.541435984327682e-06 mae 0.0016059556966830123
training loss 4.46689480866391e-06 mae 0.0016026191590907113
training loss 4.5321279887370394e-06 mae 0.0016143220493767332
training loss 4.540087847411299e-06 mae 0.001618534924267833
Epoch 372, training: loss: 0.0000046, mae: 0.0016243 test: loss0.0000921, mae:0.0069687
training loss 4.456805982044898e-06 mae 0.0015258664498105645
training loss 4.864587039906731e-06 mae 0.0016521525240558036
training loss 4.731278671321287e-06 mae 0.0016462230503946397
training loss 4.6613822783513495e-06 mae 0.0016356357697101425
training loss 4.6254982335447505e-06 mae 0.0016241307174601584
Epoch 373, training: loss: 0.0000046, mae: 0.0016213 test: loss0.0000928, mae:0.0069863
training loss 2.2790086404711474e-06 mae 0.0011092961067333817
training loss 4.432114770215954e-06 mae 0.0015863898123486659
training loss 4.466594162133136e-06 mae 0.001593553085378048
training loss 4.481152923320325e-06 mae 0.0016012355285875547
training loss 4.593772383029781e-06 mae 0.0016220515091157515
Epoch 374, training: loss: 0.0000046, mae: 0.0016226 test: loss0.0000949, mae:0.0070253
training loss 3.3947426345548593e-06 mae 0.0014378674095496535
training loss 4.5649694941091055e-06 mae 0.001620312483853423
training loss 4.422350752299666e-06 mae 0.0015852281023831208
training loss 4.451385806207675e-06 mae 0.0015895884416207969
training loss 4.53414380519543e-06 mae 0.0016041918071589218
Epoch 375, training: loss: 0.0000045, mae: 0.0016052 test: loss0.0000935, mae:0.0070111
training loss 3.4525448882050114e-06 mae 0.0014342563226819038
training loss 4.4514611045979485e-06 mae 0.0015850931465370107
training loss 4.4449810673649e-06 mae 0.0015945724375306233
training loss 4.4847568548017525e-06 mae 0.0015984144939159021
training loss 4.529716671183914e-06 mae 0.0016068586893148004
Epoch 376, training: loss: 0.0000045, mae: 0.0016049 test: loss0.0000929, mae:0.0069811
training loss 4.1556359064998105e-06 mae 0.0015516147250309587
training loss 4.530318914048087e-06 mae 0.0016078209668836168
training loss 4.351486038581018e-06 mae 0.0015678673217275941
training loss 4.453016998507381e-06 mae 0.0015892640283540972
training loss 4.483307330742515e-06 mae 0.0015975171369986972
Epoch 377, training: loss: 0.0000045, mae: 0.0016012 test: loss0.0000925, mae:0.0069666
training loss 5.024424353905488e-06 mae 0.0018409794429317117
training loss 4.299111170065075e-06 mae 0.0015661239112709083
training loss 4.359849465953009e-06 mae 0.0015751128499903301
training loss 4.446807435477313e-06 mae 0.0015933144635625726
training loss 4.526064844075655e-06 mae 0.0016063934369290724
Epoch 378, training: loss: 0.0000045, mae: 0.0016103 test: loss0.0000932, mae:0.0069777
training loss 3.296579279776779e-06 mae 0.001435439451597631
training loss 4.549351147301529e-06 mae 0.0016051316359902133
training loss 4.437781295690561e-06 mae 0.0015820404180408552
training loss 4.416040819826276e-06 mae 0.0015729251903611298
training loss 4.407676815797238e-06 mae 0.0015775614976419592
Epoch 379, training: loss: 0.0000044, mae: 0.0015813 test: loss0.0000932, mae:0.0069782
training loss 2.228737457699026e-06 mae 0.0012031454825773835
training loss 4.3768445141490775e-06 mae 0.0015653473525947216
training loss 4.284134973042256e-06 mae 0.0015597661719703583
training loss 4.449115835423853e-06 mae 0.0015925596751864778
training loss 4.551127783916349e-06 mae 0.001609778741782355
Epoch 380, training: loss: 0.0000046, mae: 0.0016095 test: loss0.0000933, mae:0.0069725
training loss 4.316743343224516e-06 mae 0.0015461444854736328
training loss 4.132884472995761e-06 mae 0.001542438412833886
training loss 4.382200257727037e-06 mae 0.0015846059362560805
training loss 4.396271256215391e-06 mae 0.0015880247460067617
training loss 4.558407820116539e-06 mae 0.001617422353172795
Epoch 381, training: loss: 0.0000046, mae: 0.0016180 test: loss0.0000934, mae:0.0069855
training loss 3.1881754694040865e-06 mae 0.0013324273750185966
training loss 4.2420305179820964e-06 mae 0.0015726213973453818
training loss 4.311927711731386e-06 mae 0.001576451958147901
training loss 4.322812421697556e-06 mae 0.0015761809938568737
training loss 4.309945387858399e-06 mae 0.0015696224261918538
Epoch 382, training: loss: 0.0000044, mae: 0.0015768 test: loss0.0000923, mae:0.0069726
training loss 4.306297796574654e-06 mae 0.0015348264714702964
training loss 4.308882787828385e-06 mae 0.00156215989423514
training loss 4.159932768068235e-06 mae 0.0015422691000939008
training loss 4.245699570246691e-06 mae 0.001552619323726522
training loss 4.337303964723959e-06 mae 0.0015678635401076724
Epoch 383, training: loss: 0.0000044, mae: 0.0015729 test: loss0.0000939, mae:0.0070254
training loss 4.003652520623291e-06 mae 0.00167680939193815
training loss 4.505682474754232e-06 mae 0.0015876837389762783
training loss 4.37003648873389e-06 mae 0.0015744463595939735
training loss 4.409449047670931e-06 mae 0.0015865486818334913
training loss 4.431174912311707e-06 mae 0.0015905090336646164
Epoch 384, training: loss: 0.0000044, mae: 0.0015908 test: loss0.0000932, mae:0.0070065
training loss 4.551521215034882e-06 mae 0.0016785688931122422
training loss 4.32784978044105e-06 mae 0.0015689751872902408
training loss 4.285088897910945e-06 mae 0.0015620204578568736
training loss 4.28084351086674e-06 mae 0.0015593187811025822
training loss 4.3896189387638655e-06 mae 0.001583958307596211
Epoch 385, training: loss: 0.0000044, mae: 0.0015909 test: loss0.0000948, mae:0.0070402
training loss 3.097150738540222e-06 mae 0.0013846667716279626
training loss 4.707794821168884e-06 mae 0.0016468947698526526
training loss 4.575320683288016e-06 mae 0.001612836995281943
training loss 4.537536812454132e-06 mae 0.0016155279062639008
training loss 4.49118653221068e-06 mae 0.0016074377478132794
Epoch 386, training: loss: 0.0000045, mae: 0.0016083 test: loss0.0000934, mae:0.0069997
training loss 6.251473678275943e-06 mae 0.001793839386664331
training loss 3.7993488871613994e-06 mae 0.001481576457473577
training loss 3.98144763521216e-06 mae 0.0015116265311025754
training loss 4.234997034876604e-06 mae 0.0015596261298876036
training loss 4.332348547208155e-06 mae 0.0015731337723502916
Epoch 387, training: loss: 0.0000043, mae: 0.0015756 test: loss0.0000951, mae:0.0070512
training loss 5.339610197552247e-06 mae 0.001697488478384912
training loss 4.28920335769933e-06 mae 0.001566648814261106
training loss 4.27707621581996e-06 mae 0.0015676491490408488
training loss 4.336008847253986e-06 mae 0.0015746779379046327
training loss 4.319472563749054e-06 mae 0.0015675552468515236
Epoch 388, training: loss: 0.0000044, mae: 0.0015724 test: loss0.0000941, mae:0.0070267
training loss 4.22710036218632e-06 mae 0.0016917064785957336
training loss 4.059433357653296e-06 mae 0.001516971778234138
training loss 4.154746786505458e-06 mae 0.0015373042897588688
training loss 4.231200369572163e-06 mae 0.0015483113955582998
training loss 4.318179276423285e-06 mae 0.0015683135606200241
Epoch 389, training: loss: 0.0000043, mae: 0.0015697 test: loss0.0000937, mae:0.0070157
training loss 3.6035928587807575e-06 mae 0.0013148827711120248
training loss 4.03528419797915e-06 mae 0.0015299272970022522
training loss 4.308411989048108e-06 mae 0.0015721312265308332
training loss 4.248172795779819e-06 mae 0.0015653827603881716
training loss 4.292815183543102e-06 mae 0.0015651594797631175
Epoch 390, training: loss: 0.0000043, mae: 0.0015649 test: loss0.0000954, mae:0.0070643
training loss 4.1596254050091375e-06 mae 0.0015066914493218064
training loss 3.916861344930344e-06 mae 0.0015017957581828039
training loss 4.119582622524382e-06 mae 0.0015325827713096787
training loss 4.107885980067521e-06 mae 0.0015326228072953148
training loss 4.193298619426075e-06 mae 0.0015485881021552123
Epoch 391, training: loss: 0.0000042, mae: 0.0015519 test: loss0.0000930, mae:0.0069903
training loss 3.7595864341710694e-06 mae 0.0015663746744394302
training loss 4.118871313003896e-06 mae 0.0015271970175900586
training loss 4.171100207002221e-06 mae 0.0015323877191668988
training loss 4.275016345185193e-06 mae 0.0015546134511156887
training loss 4.25599599527879e-06 mae 0.0015567440854672766
Epoch 392, training: loss: 0.0000043, mae: 0.0015581 test: loss0.0000983, mae:0.0070500
training loss 4.859285127167823e-06 mae 0.0017122129211202264
training loss 4.6593302604556865e-06 mae 0.001623625796306513
training loss 4.40714507697134e-06 mae 0.0015934585930207873
training loss 4.503397513384315e-06 mae 0.0016159515053847963
training loss 4.4748056774352446e-06 mae 0.0016138682099040686
Epoch 393, training: loss: 0.0000045, mae: 0.0016147 test: loss0.0000936, mae:0.0070090
training loss 4.357358193374239e-06 mae 0.0017082582926377654
training loss 3.943864706289948e-06 mae 0.0015010799637393043
training loss 3.9340247365832354e-06 mae 0.0014883225500159475
training loss 3.968135963487228e-06 mae 0.0014938530635508081
training loss 4.117714266992979e-06 mae 0.001521547366656474
Epoch 394, training: loss: 0.0000041, mae: 0.0015255 test: loss0.0000942, mae:0.0070349
training loss 3.212679757780279e-06 mae 0.001342619420029223
training loss 3.906208656403846e-06 mae 0.0014927357951106104
training loss 4.161915355082489e-06 mae 0.0015317736525494274
training loss 4.123890098586551e-06 mae 0.001534756988484339
training loss 4.213434784046811e-06 mae 0.0015553366612939543
Epoch 395, training: loss: 0.0000042, mae: 0.0015552 test: loss0.0000941, mae:0.0070276
training loss 5.085149041406112e-06 mae 0.0016886992380023003
training loss 4.444413956234176e-06 mae 0.0015948421833123644
training loss 4.145293162017165e-06 mae 0.001548960508714144
training loss 4.183098922684746e-06 mae 0.0015495828328197768
training loss 4.218380148478755e-06 mae 0.0015532833985076168
Epoch 396, training: loss: 0.0000043, mae: 0.0015611 test: loss0.0000942, mae:0.0070286
training loss 4.8468978093296755e-06 mae 0.001649336889386177
training loss 4.12788071401997e-06 mae 0.001533785661426829
training loss 4.14040462349269e-06 mae 0.0015228252851007746
training loss 4.109764881681657e-06 mae 0.0015317147377175312
training loss 4.202180750339656e-06 mae 0.001553400866200786
Epoch 397, training: loss: 0.0000042, mae: 0.0015508 test: loss0.0000940, mae:0.0070270
training loss 4.5030169530946296e-06 mae 0.0018069181824102998
training loss 4.287602829814812e-06 mae 0.0015710080065307955
training loss 4.171762876323178e-06 mae 0.0015370625838877104
training loss 4.238622685061394e-06 mae 0.001551284343744351
training loss 4.231770608995833e-06 mae 0.0015496588989730876
Epoch 398, training: loss: 0.0000042, mae: 0.0015504 test: loss0.0000943, mae:0.0070336
training loss 2.3160189357440686e-06 mae 0.0011038553202524781
training loss 3.943572450867471e-06 mae 0.0015141440455016548
training loss 4.004082817424782e-06 mae 0.0015176676829244094
training loss 4.0610854064905775e-06 mae 0.0015279495470312161
training loss 4.139836920190436e-06 mae 0.0015374827134974334
Epoch 399, training: loss: 0.0000041, mae: 0.0015413 test: loss0.0000932, mae:0.0069872
current learning rate: 3.125e-05
training loss 3.877608378388686e-06 mae 0.001500168233178556
training loss 3.4815405244586943e-06 mae 0.001407709470310924
training loss 3.5618254503446015e-06 mae 0.0013982317984417673
training loss 3.47973033609342e-06 mae 0.0013835816312618304
training loss 3.468381848746215e-06 mae 0.0013776478060493387
Epoch 400, training: loss: 0.0000035, mae: 0.0013774 test: loss0.0000942, mae:0.0070245
training loss 2.0798531750187976e-06 mae 0.0011045867577195168
training loss 3.1670467728426773e-06 mae 0.001285133949553996
training loss 3.225340163784369e-06 mae 0.0012958284863962396
training loss 3.2062017515164174e-06 mae 0.0013069527247606071
training loss 3.254107917910998e-06 mae 0.0013212131070487417
Epoch 401, training: loss: 0.0000033, mae: 0.0013292 test: loss0.0000945, mae:0.0070488
training loss 2.6443024125910597e-06 mae 0.0012281564995646477
training loss 3.11180166608763e-06 mae 0.0012848647939059518
training loss 3.1601386833451887e-06 mae 0.001291288930281886
training loss 3.2263071317409675e-06 mae 0.00130739601040629
training loss 3.2463804221952706e-06 mae 0.001313739845735387
Epoch 402, training: loss: 0.0000033, mae: 0.0013155 test: loss0.0000946, mae:0.0070327
training loss 3.2648204069118947e-06 mae 0.0013043828075751662
training loss 3.189652063031413e-06 mae 0.001288194835925584
training loss 3.309753676355943e-06 mae 0.00130069847933321
training loss 3.2803537979268505e-06 mae 0.0013069989983091096
training loss 3.2642583256116515e-06 mae 0.0013071635806937677
Epoch 403, training: loss: 0.0000032, mae: 0.0013054 test: loss0.0000941, mae:0.0070120
training loss 2.615767471070285e-06 mae 0.0011883191764354706
training loss 3.195531763487773e-06 mae 0.0012964716206268208
training loss 3.16324169129039e-06 mae 0.0012925118156063304
training loss 3.2231647974306222e-06 mae 0.0013039972107327042
training loss 3.238209750865035e-06 mae 0.0013126449431269548
Epoch 404, training: loss: 0.0000032, mae: 0.0013148 test: loss0.0000948, mae:0.0070530
training loss 4.074880052939989e-06 mae 0.0014462064718827605
training loss 3.0740339400604256e-06 mae 0.0012749934354431777
training loss 3.153394341142761e-06 mae 0.0012937256591486752
training loss 3.1879965105107584e-06 mae 0.0013066406542163913
training loss 3.2387357430987735e-06 mae 0.0013151535711051964
Epoch 405, training: loss: 0.0000033, mae: 0.0013176 test: loss0.0000946, mae:0.0070468
training loss 3.538431656124885e-06 mae 0.0014863557880744338
training loss 3.2668700513051016e-06 mae 0.00131448590671461
training loss 3.2678406418343383e-06 mae 0.0013174310143308553
training loss 3.228330630213501e-06 mae 0.0013121329683545258
training loss 3.2758317814371897e-06 mae 0.001325793970035695
Epoch 406, training: loss: 0.0000033, mae: 0.0013261 test: loss0.0000952, mae:0.0070580
training loss 2.6145787614950677e-06 mae 0.0012287205317988992
training loss 3.3303998589854377e-06 mae 0.00134859333161776
training loss 3.284931195405151e-06 mae 0.0013316394265575134
training loss 3.2650415338344074e-06 mae 0.0013279328708666444
training loss 3.303131913087249e-06 mae 0.0013317521609393737
Epoch 407, training: loss: 0.0000033, mae: 0.0013274 test: loss0.0000947, mae:0.0070527
training loss 2.274265398227726e-06 mae 0.0011851725867018104
training loss 3.2942149881123707e-06 mae 0.0013222078126216047
training loss 3.246512002790203e-06 mae 0.0013148672463796508
training loss 3.232969614097802e-06 mae 0.0013117040858283287
training loss 3.2380813243308417e-06 mae 0.0013110775692244774
Epoch 408, training: loss: 0.0000032, mae: 0.0013108 test: loss0.0000953, mae:0.0070665
training loss 1.8863582909034449e-06 mae 0.0011232178658246994
training loss 2.9338922955752888e-06 mae 0.001256423610874324
training loss 3.016690196109086e-06 mae 0.0012782871346266038
training loss 3.127347756030124e-06 mae 0.0013003682348389945
training loss 3.232764500317169e-06 mae 0.0013179722825871476
Epoch 409, training: loss: 0.0000032, mae: 0.0013169 test: loss0.0000947, mae:0.0070575
training loss 2.4804619442875264e-06 mae 0.0012486992636695504
training loss 3.233767426768193e-06 mae 0.0013117824838606313
training loss 3.1569326830042943e-06 mae 0.0013048650407564302
training loss 3.2714578894578242e-06 mae 0.0013241650118794761
training loss 3.285273237452201e-06 mae 0.001324770970395141
Epoch 410, training: loss: 0.0000033, mae: 0.0013253 test: loss0.0000950, mae:0.0070600
training loss 3.2768421078799292e-06 mae 0.0013476628810167313
training loss 3.24097500732237e-06 mae 0.0013225350446779938
training loss 3.2535869935924375e-06 mae 0.001333732793043082
training loss 3.2547168528566867e-06 mae 0.0013280614970763841
training loss 3.2511724881278385e-06 mae 0.001327843190719888
Epoch 411, training: loss: 0.0000033, mae: 0.0013320 test: loss0.0000960, mae:0.0070917
training loss 2.5049332634807797e-06 mae 0.0012575943255797029
training loss 3.2310819809329904e-06 mae 0.001299429641506982
training loss 3.2037269260633763e-06 mae 0.0012984655573871906
training loss 3.2364330173544103e-06 mae 0.0013153742742878908
training loss 3.2387024752757835e-06 mae 0.001319704028263465
Epoch 412, training: loss: 0.0000032, mae: 0.0013194 test: loss0.0000955, mae:0.0070786
training loss 2.884150489990134e-06 mae 0.0013728082412853837
training loss 3.5561058666644385e-06 mae 0.0013799432792938225
training loss 3.259958518383157e-06 mae 0.0013357237843556861
training loss 3.330864315584951e-06 mae 0.0013425187793689845
training loss 3.251423253011548e-06 mae 0.0013275791717744173
Epoch 413, training: loss: 0.0000032, mae: 0.0013271 test: loss0.0000953, mae:0.0070753
training loss 1.700337634247262e-06 mae 0.0010801628232002258
training loss 2.84289165963356e-06 mae 0.0012240351905918447
training loss 3.0745974108507606e-06 mae 0.001281163169965517
training loss 3.0763953301048704e-06 mae 0.0012812018857047659
training loss 3.158290614897053e-06 mae 0.0012986276609202227
Epoch 414, training: loss: 0.0000032, mae: 0.0013021 test: loss0.0000949, mae:0.0070464
training loss 4.220757091388805e-06 mae 0.0014060130342841148
training loss 3.2388659069421884e-06 mae 0.0012919885042470462
training loss 3.264703147380876e-06 mae 0.0013097184952256262
training loss 3.1832185154809637e-06 mae 0.0013016568776682664
training loss 3.2370365428577855e-06 mae 0.0013182962655590204
Epoch 415, training: loss: 0.0000032, mae: 0.0013217 test: loss0.0000951, mae:0.0070728
training loss 4.134029495617142e-06 mae 0.0014167753979563713
training loss 3.3934983766990692e-06 mae 0.001344721766608749
training loss 3.255338165983251e-06 mae 0.0013165317987562115
training loss 3.170121909979629e-06 mae 0.0013017413971193191
training loss 3.2102610216656205e-06 mae 0.0013135045081080492
Epoch 416, training: loss: 0.0000032, mae: 0.0013114 test: loss0.0000955, mae:0.0070866
training loss 2.7438727556727827e-06 mae 0.0013325907057151198
training loss 3.05647758041767e-06 mae 0.0012822994692981533
training loss 3.075928273997621e-06 mae 0.0012866923571702574
training loss 3.144802472672009e-06 mae 0.0012978824543217755
training loss 3.226858236067796e-06 mae 0.00131415155807751
Epoch 417, training: loss: 0.0000032, mae: 0.0013155 test: loss0.0000951, mae:0.0070640
training loss 3.2069817734736716e-06 mae 0.0013893265277147293
training loss 2.8811280072641594e-06 mae 0.0012499486876870781
training loss 3.0563477434277435e-06 mae 0.0012831148175098523
training loss 3.2098109442138835e-06 mae 0.0013104472625445555
training loss 3.2018059635592354e-06 mae 0.0013136072921223793
Epoch 418, training: loss: 0.0000032, mae: 0.0013120 test: loss0.0000970, mae:0.0071144
training loss 2.328005166418734e-06 mae 0.001224763342179358
training loss 2.967939087070323e-06 mae 0.0012618370694290923
training loss 3.0788905590243573e-06 mae 0.0012811882862830445
training loss 3.116578612972317e-06 mae 0.0012938572507851223
training loss 3.1555334144014974e-06 mae 0.0013061378415284756
Epoch 419, training: loss: 0.0000032, mae: 0.0013082 test: loss0.0000955, mae:0.0070803
training loss 3.923013082385296e-06 mae 0.0013289647176861763
training loss 3.1858028184133744e-06 mae 0.0012893707687765654
training loss 3.13048204670039e-06 mae 0.0012899044904590466
training loss 3.17423052142628e-06 mae 0.0013022950502723115
training loss 3.1276633157701445e-06 mae 0.0012956119738449694
Epoch 420, training: loss: 0.0000031, mae: 0.0012991 test: loss0.0000961, mae:0.0070878
training loss 2.135512204404222e-06 mae 0.0011080438271164894
training loss 3.308720383132572e-06 mae 0.0012981356121599672
training loss 3.166209584117379e-06 mae 0.0012943015317432582
training loss 3.162854708510632e-06 mae 0.0012942462541249314
training loss 3.1543015411579534e-06 mae 0.0012996530534458621
Epoch 421, training: loss: 0.0000032, mae: 0.0013044 test: loss0.0000956, mae:0.0070857
training loss 4.205410732538439e-06 mae 0.0014921026304364204
training loss 3.106519924960379e-06 mae 0.0012883229372913343
training loss 3.074726448788412e-06 mae 0.00128292192827154
training loss 3.078042109547631e-06 mae 0.0012862602120037158
training loss 3.1489195610117184e-06 mae 0.0013044156080264766
Epoch 422, training: loss: 0.0000031, mae: 0.0013020 test: loss0.0000965, mae:0.0071075
training loss 2.9392911073955474e-06 mae 0.0012488458305597305
training loss 2.7965613575289873e-06 mae 0.0012316995074369886
training loss 2.9097643011194275e-06 mae 0.0012475559055703778
training loss 3.034099020756635e-06 mae 0.0012719083607462022
training loss 3.0971091087957963e-06 mae 0.001287493674346335
Epoch 423, training: loss: 0.0000031, mae: 0.0012874 test: loss0.0000963, mae:0.0071075
training loss 2.4244275209639454e-06 mae 0.0011816279729828238
training loss 2.9656771001849414e-06 mae 0.0012511869312684034
training loss 3.0568804945822675e-06 mae 0.0012811028276394942
training loss 3.1143849621954686e-06 mae 0.001294538195611946
training loss 3.1281584179891763e-06 mae 0.001300916453042712
Epoch 424, training: loss: 0.0000031, mae: 0.0013009 test: loss0.0000962, mae:0.0070970
training loss 1.9522728962328983e-06 mae 0.0010702651925384998
training loss 3.0168912706928913e-06 mae 0.0012628241058658147
training loss 2.9566560582193558e-06 mae 0.0012611893909328638
training loss 2.975257111516021e-06 mae 0.0012638919831821011
training loss 3.0619567805435736e-06 mae 0.0012825512992153262
Epoch 425, training: loss: 0.0000031, mae: 0.0012794 test: loss0.0000967, mae:0.0071323
training loss 3.3537107810843736e-06 mae 0.0014178374549373984
training loss 2.8744113599125685e-06 mae 0.0012380872640813537
training loss 3.0427929968396658e-06 mae 0.001271163754275825
training loss 3.043025991330719e-06 mae 0.0012713347986457715
training loss 3.085303864738751e-06 mae 0.0012857013654811733
Epoch 426, training: loss: 0.0000031, mae: 0.0012811 test: loss0.0000963, mae:0.0071043
training loss 3.338065880598151e-06 mae 0.0013324106112122536
training loss 2.8447097417715578e-06 mae 0.0012281114719442877
training loss 2.9458965231566653e-06 mae 0.001252906977332723
training loss 3.000341434655627e-06 mae 0.0012683882763522948
training loss 3.1020956521840643e-06 mae 0.0012912528273135203
Epoch 427, training: loss: 0.0000031, mae: 0.0012916 test: loss0.0000965, mae:0.0071227
training loss 2.0959585071977926e-06 mae 0.0011769585544243455
training loss 2.925467610486307e-06 mae 0.0012520857465763886
training loss 2.9191444051176924e-06 mae 0.001255515372117815
training loss 2.9770645628348766e-06 mae 0.0012628244970838798
training loss 3.053964249135169e-06 mae 0.0012819172738615734
Epoch 428, training: loss: 0.0000031, mae: 0.0012903 test: loss0.0000962, mae:0.0071138
training loss 2.7377266178518767e-06 mae 0.0012516878778114915
training loss 3.042479331234776e-06 mae 0.0012819454388912105
training loss 3.1621174702924676e-06 mae 0.001303688551548241
training loss 3.154365277918717e-06 mae 0.0013026155886820066
training loss 3.1405677413005017e-06 mae 0.0013001275527411472
Epoch 429, training: loss: 0.0000031, mae: 0.0013001 test: loss0.0000962, mae:0.0071058
training loss 1.985670451176702e-06 mae 0.0010194549104198813
training loss 2.7952190102237802e-06 mae 0.0012226355714066068
training loss 2.9500741722568084e-06 mae 0.0012555723239916687
training loss 2.995209247153571e-06 mae 0.0012658324126548917
training loss 3.0302916173069156e-06 mae 0.0012752352669297615
Epoch 430, training: loss: 0.0000031, mae: 0.0012819 test: loss0.0000963, mae:0.0071137
training loss 1.6012854757718742e-06 mae 0.0009674591128714383
training loss 2.9287625024795383e-06 mae 0.0012268182118514584
training loss 2.9424180931329993e-06 mae 0.0012479547461918966
training loss 2.927570323440675e-06 mae 0.0012541754273019282
training loss 3.002884798195339e-06 mae 0.0012690599275688031
Epoch 431, training: loss: 0.0000030, mae: 0.0012777 test: loss0.0000969, mae:0.0071333
training loss 1.5443619076904724e-06 mae 0.0009085231577046216
training loss 3.1824379599735644e-06 mae 0.0012942001887816283
training loss 3.0008053297231276e-06 mae 0.0012652125914977628
training loss 3.0403284699485886e-06 mae 0.0012766127775281368
training loss 3.0433530782965375e-06 mae 0.0012785618260632552
Epoch 432, training: loss: 0.0000030, mae: 0.0012789 test: loss0.0000960, mae:0.0070817
training loss 3.219332256776397e-06 mae 0.0013725083554163575
training loss 2.86997601283385e-06 mae 0.0012350771233768144
training loss 2.8950579698995163e-06 mae 0.0012367813453760621
training loss 2.951888651771859e-06 mae 0.0012515588734446966
training loss 3.0190571959932845e-06 mae 0.0012698575375898187
Epoch 433, training: loss: 0.0000030, mae: 0.0012709 test: loss0.0000963, mae:0.0071030
training loss 3.906745860149385e-06 mae 0.0013788240030407906
training loss 3.116087214038998e-06 mae 0.0012978696305414334
training loss 3.008204021846335e-06 mae 0.0012757070714119122
training loss 3.006387789313196e-06 mae 0.0012768898733632507
training loss 3.026138257037606e-06 mae 0.001281110230724406
Epoch 434, training: loss: 0.0000030, mae: 0.0012804 test: loss0.0000969, mae:0.0071307
training loss 2.9891455142205814e-06 mae 0.001269145286642015
training loss 2.9826252887455756e-06 mae 0.0012613584346813608
training loss 2.914667013353416e-06 mae 0.0012498842004806463
training loss 2.9210306733124076e-06 mae 0.0012501988017203794
training loss 2.9903185118223406e-06 mae 0.0012667834722849343
Epoch 435, training: loss: 0.0000030, mae: 0.0012682 test: loss0.0000962, mae:0.0071065
training loss 3.337310090500978e-06 mae 0.0013519792119041085
training loss 2.8416003844683353e-06 mae 0.0012425155477032216
training loss 2.9377316290229297e-06 mae 0.0012517483711316445
training loss 2.951818492688019e-06 mae 0.0012628463638903243
training loss 2.987793191752092e-06 mae 0.0012678139566777472
Epoch 436, training: loss: 0.0000030, mae: 0.0012680 test: loss0.0000964, mae:0.0071058
training loss 3.296687964393641e-06 mae 0.0012728748843073845
training loss 2.8692645873532307e-06 mae 0.001253630798173082
training loss 2.8917569870197375e-06 mae 0.0012543736369173863
training loss 2.9166325594628415e-06 mae 0.0012601934911510519
training loss 2.964526485282395e-06 mae 0.0012626694972900246
Epoch 437, training: loss: 0.0000030, mae: 0.0012623 test: loss0.0000974, mae:0.0071527
training loss 1.8551048697190708e-06 mae 0.0009761464898474514
training loss 2.8313463711610354e-06 mae 0.001231090687260981
training loss 2.9130219546810743e-06 mae 0.0012473214433832772
training loss 3.038537041697813e-06 mae 0.0012731058029642952
training loss 3.0359029490533275e-06 mae 0.0012749371319240072
Epoch 438, training: loss: 0.0000030, mae: 0.0012757 test: loss0.0000968, mae:0.0071295
training loss 2.964620762213599e-06 mae 0.0012442764127627015
training loss 2.79032143921273e-06 mae 0.0012398781951096863
training loss 2.926713136730542e-06 mae 0.0012572794861049567
training loss 2.966794922850261e-06 mae 0.001263387387522823
training loss 2.954543145763035e-06 mae 0.0012612695378872256
Epoch 439, training: loss: 0.0000030, mae: 0.0012609 test: loss0.0000978, mae:0.0071591
training loss 2.0121071884204866e-06 mae 0.0010234551737084985
training loss 2.9725894271049427e-06 mae 0.0012655783588450182
training loss 2.8392393816001558e-06 mae 0.0012308352241882736
training loss 2.9017175081466808e-06 mae 0.0012466332224499903
training loss 2.9630109956182854e-06 mae 0.001260557035574996
Epoch 440, training: loss: 0.0000030, mae: 0.0012640 test: loss0.0000971, mae:0.0071462
training loss 3.0086011975072324e-06 mae 0.001183419139124453
training loss 2.8730685400267818e-06 mae 0.0012532808582809772
training loss 2.9163187401527566e-06 mae 0.0012548396399585837
training loss 2.9153422891439984e-06 mae 0.0012499683595182283
training loss 2.9673983726073614e-06 mae 0.0012604221380176715
Epoch 441, training: loss: 0.0000030, mae: 0.0012607 test: loss0.0000977, mae:0.0071539
training loss 2.2545734736922896e-06 mae 0.0011780051281675696
training loss 2.718132265602204e-06 mae 0.0012104208147445438
training loss 2.7841611613844687e-06 mae 0.0012311908228571828
training loss 2.84782224647739e-06 mae 0.0012453476179835217
training loss 2.9392421643634998e-06 mae 0.0012598028423876236
Epoch 442, training: loss: 0.0000029, mae: 0.0012574 test: loss0.0000972, mae:0.0071517
training loss 2.4764633508311817e-06 mae 0.0012133276322856545
training loss 2.9399141892866403e-06 mae 0.0012391113285797047
training loss 2.9033106471561577e-06 mae 0.0012467849018780685
training loss 2.9282204502804548e-06 mae 0.0012495722768162164
training loss 2.963511751636474e-06 mae 0.001262257809852672
Epoch 443, training: loss: 0.0000030, mae: 0.0012619 test: loss0.0000970, mae:0.0071356
training loss 1.4618422028433997e-06 mae 0.0009234373574145138
training loss 2.8731152987543916e-06 mae 0.0012526862088170854
training loss 2.9049950786195575e-06 mae 0.0012522499586483187
training loss 2.889153568394973e-06 mae 0.0012537537586708731
training loss 2.9468519811932462e-06 mae 0.0012548135634655342
Epoch 444, training: loss: 0.0000029, mae: 0.0012515 test: loss0.0000969, mae:0.0071379
training loss 1.5172772691585124e-06 mae 0.0009701928938739002
training loss 2.7447529169412703e-06 mae 0.0012240738187934838
training loss 2.791580138166118e-06 mae 0.0012344759278054725
training loss 2.8572952594504098e-06 mae 0.0012378320733967259
training loss 2.9145055685293495e-06 mae 0.001252398143038247
Epoch 445, training: loss: 0.0000029, mae: 0.0012518 test: loss0.0000968, mae:0.0071379
training loss 2.5793767690629466e-06 mae 0.0012260464718565345
training loss 2.7335599729578423e-06 mae 0.001214963759469124
training loss 2.8234410635404578e-06 mae 0.001225187123919108
training loss 2.820042744958559e-06 mae 0.0012298614104333892
training loss 2.8972394547103906e-06 mae 0.0012477824026579727
Epoch 446, training: loss: 0.0000029, mae: 0.0012504 test: loss0.0000971, mae:0.0071459
training loss 2.0488575955823762e-06 mae 0.001056166016496718
training loss 2.8137924123167934e-06 mae 0.0012252709643422244
training loss 2.7417755239247812e-06 mae 0.0012289925736587235
training loss 2.8735178328748127e-06 mae 0.0012565460302774475
training loss 2.939991703947022e-06 mae 0.0012624618818575688
Epoch 447, training: loss: 0.0000029, mae: 0.0012588 test: loss0.0000980, mae:0.0071666
training loss 1.9891056126652984e-06 mae 0.0010880889603868127
training loss 2.938960358947309e-06 mae 0.0012563309134623292
training loss 2.985268307915392e-06 mae 0.0012590281306858189
training loss 2.924230006137564e-06 mae 0.0012449783794211827
training loss 2.884976265365245e-06 mae 0.0012392041441956674
Epoch 448, training: loss: 0.0000029, mae: 0.0012374 test: loss0.0000969, mae:0.0071328
training loss 2.1178589122428093e-06 mae 0.0011189710348844528
training loss 2.8843211094128632e-06 mae 0.0012336461964592925
training loss 2.9248561286474803e-06 mae 0.00123783135164234
training loss 2.842116307103713e-06 mae 0.0012279250385356909
training loss 2.832712148368619e-06 mae 0.0012311030275406157
Epoch 449, training: loss: 0.0000029, mae: 0.0012384 test: loss0.0000978, mae:0.0071720
training loss 3.0922874429961666e-06 mae 0.0013936981558799744
training loss 2.8060850666531454e-06 mae 0.0012355535610706783
training loss 2.665888737485851e-06 mae 0.0012068310297223379
training loss 2.8214592765569814e-06 mae 0.0012322219377013523
training loss 2.8707160804605803e-06 mae 0.0012385062415810738
Epoch 450, training: loss: 0.0000029, mae: 0.0012397 test: loss0.0000972, mae:0.0071428
training loss 1.8210726011602674e-06 mae 0.001071058213710785
training loss 2.7797063074674452e-06 mae 0.0012140420183320254
training loss 2.8124933610027682e-06 mae 0.0012208301266240376
training loss 2.92031931594646e-06 mae 0.0012475254691810343
training loss 2.876526406876269e-06 mae 0.0012422438820393106
Epoch 451, training: loss: 0.0000029, mae: 0.0012402 test: loss0.0000982, mae:0.0071638
training loss 1.3238592373454594e-06 mae 0.0009046106715686619
training loss 2.7928543908737817e-06 mae 0.0012040882674958923
training loss 2.8139641805261487e-06 mae 0.0012116100994113944
training loss 2.832317801185369e-06 mae 0.0012211769569467827
training loss 2.814396665445062e-06 mae 0.0012231080532792166
Epoch 452, training: loss: 0.0000028, mae: 0.0012260 test: loss0.0000977, mae:0.0071627
training loss 2.113589061991661e-06 mae 0.0011678828159347177
training loss 2.6868008195806927e-06 mae 0.001185898532104843
training loss 2.706245081213583e-06 mae 0.0011915834436174663
training loss 2.697840362552536e-06 mae 0.0012011613696042615
training loss 2.8241894426345467e-06 mae 0.0012286697133254273
Epoch 453, training: loss: 0.0000028, mae: 0.0012311 test: loss0.0000975, mae:0.0071572
training loss 3.34229230247729e-06 mae 0.001393058686517179
training loss 2.5969709997975855e-06 mae 0.0011965790181420746
training loss 2.787937536469495e-06 mae 0.0012268200971988388
training loss 2.74589427930707e-06 mae 0.001220758034869263
training loss 2.8248439903606846e-06 mae 0.0012297520630608367
Epoch 454, training: loss: 0.0000029, mae: 0.0012391 test: loss0.0000993, mae:0.0072160
training loss 2.4920675514295e-06 mae 0.001240281737409532
training loss 2.7658150654632688e-06 mae 0.0012198320028426889
training loss 2.72731468029552e-06 mae 0.0012062341556884342
training loss 2.8397682586842914e-06 mae 0.0012284311745778358
training loss 2.8277688246436885e-06 mae 0.0012314099649347323
Epoch 455, training: loss: 0.0000028, mae: 0.0012323 test: loss0.0000980, mae:0.0071626
training loss 3.692439349833876e-06 mae 0.0012513110414147377
training loss 2.639750304671693e-06 mae 0.0011890094473903231
training loss 2.930178222620929e-06 mae 0.0012486654536632623
training loss 2.8690290917932584e-06 mae 0.0012425362515658001
training loss 2.864076112751692e-06 mae 0.0012414607689113586
Epoch 456, training: loss: 0.0000029, mae: 0.0012406 test: loss0.0000979, mae:0.0071619
training loss 3.137409294140525e-06 mae 0.001199078164063394
training loss 2.6347379634542304e-06 mae 0.0011815654191498957
training loss 2.63529219040774e-06 mae 0.0011908419362250898
training loss 2.7522446860241063e-06 mae 0.0012184021279789408
training loss 2.800806164170119e-06 mae 0.0012329666060395539
Epoch 457, training: loss: 0.0000028, mae: 0.0012400 test: loss0.0000984, mae:0.0071948
training loss 2.9621903649967862e-06 mae 0.0013086196267977357
training loss 2.6567633816668214e-06 mae 0.0011965408053833475
training loss 2.7251189321489597e-06 mae 0.0012117022329176043
training loss 2.803430277630386e-06 mae 0.0012224915002577552
training loss 2.8112511961183763e-06 mae 0.0012276965032438224
Epoch 458, training: loss: 0.0000028, mae: 0.0012307 test: loss0.0000977, mae:0.0071616
training loss 3.5763212054007454e-06 mae 0.0012692464515566826
training loss 2.624133331022784e-06 mae 0.0011693358172516466
training loss 2.8209685042654223e-06 mae 0.0012190102554608766
training loss 2.8446425242552944e-06 mae 0.0012297630838167834
training loss 2.7719237736146277e-06 mae 0.0012183429062857633
Epoch 459, training: loss: 0.0000028, mae: 0.0012175 test: loss0.0000982, mae:0.0071811
training loss 2.3717116164334584e-06 mae 0.001071115373633802
training loss 2.6586282782612696e-06 mae 0.0011705313466818012
training loss 2.699941313778146e-06 mae 0.001191485586165828
training loss 2.7041935210933633e-06 mae 0.0011992249121297368
training loss 2.7316990028013427e-06 mae 0.0012050794002562942
Epoch 460, training: loss: 0.0000028, mae: 0.0012108 test: loss0.0000978, mae:0.0071600
training loss 2.7039804990636185e-06 mae 0.00125877873506397
training loss 2.8539607607550956e-06 mae 0.001250860084980434
training loss 2.8179215146652974e-06 mae 0.0012360046546223882
training loss 2.7723211190199334e-06 mae 0.0012286087048597732
training loss 2.811024029454786e-06 mae 0.0012350544021858727
Epoch 461, training: loss: 0.0000028, mae: 0.0012331 test: loss0.0001014, mae:0.0072294
training loss 2.3494967535953037e-06 mae 0.0011192085221409798
training loss 2.533108524367694e-06 mae 0.0011722535709850492
training loss 2.6747668662570543e-06 mae 0.0011901546190123968
training loss 2.7697309511399032e-06 mae 0.0012124408555328104
training loss 2.7529379659467655e-06 mae 0.001213190378910811
Epoch 462, training: loss: 0.0000028, mae: 0.0012169 test: loss0.0000999, mae:0.0072307
training loss 3.4924021292681573e-06 mae 0.0013409139355644584
training loss 2.637582443800485e-06 mae 0.0011660492495067566
training loss 2.6535597100411176e-06 mae 0.0011813164669052135
training loss 2.6733110028406092e-06 mae 0.001193325463158159
training loss 2.748536606042257e-06 mae 0.00121149261318731
Epoch 463, training: loss: 0.0000027, mae: 0.0012123 test: loss0.0000982, mae:0.0071858
training loss 3.081615432165563e-06 mae 0.0013603608822450042
training loss 2.634667490989766e-06 mae 0.001189457382732893
training loss 2.5954424841930254e-06 mae 0.0011868763145631046
training loss 2.700546943495717e-06 mae 0.001203341035647256
training loss 2.737743689542538e-06 mae 0.0012134614566211297
Epoch 464, training: loss: 0.0000027, mae: 0.0012134 test: loss0.0000980, mae:0.0071715
training loss 2.150861973859719e-06 mae 0.0010736981639638543
training loss 2.4580834548817394e-06 mae 0.0011675247977323395
training loss 2.7045885457986934e-06 mae 0.001203820412508258
training loss 2.7168269001632374e-06 mae 0.0012097590181217928
training loss 2.751499885141415e-06 mae 0.0012151112155038956
Epoch 465, training: loss: 0.0000028, mae: 0.0012152 test: loss0.0000982, mae:0.0071782
training loss 1.8600854900796548e-06 mae 0.001005052006803453
training loss 2.5884304009463567e-06 mae 0.001165284201110184
training loss 2.6693145175591104e-06 mae 0.0011966973122458282
training loss 2.702034090807561e-06 mae 0.0012044429506132056
training loss 2.7322714188573027e-06 mae 0.0012154730175278942
Epoch 466, training: loss: 0.0000027, mae: 0.0012182 test: loss0.0000986, mae:0.0071922
training loss 3.4800921184796607e-06 mae 0.0013795640552416444
training loss 2.739267201564764e-06 mae 0.0012236485581881569
training loss 2.7419288143040692e-06 mae 0.001216167106297752
training loss 2.6951026984232955e-06 mae 0.0012081099542271068
training loss 2.712008069207893e-06 mae 0.001208218044499445
Epoch 467, training: loss: 0.0000027, mae: 0.0012085 test: loss0.0000985, mae:0.0071919
training loss 1.7614584066905081e-06 mae 0.001042135409079492
training loss 2.6571990207123547e-06 mae 0.001183169599458137
training loss 2.6999845226557665e-06 mae 0.0011930919620024023
training loss 2.7417185241129063e-06 mae 0.001210324298544164
training loss 2.716780466751932e-06 mae 0.0012076910317583539
Epoch 468, training: loss: 0.0000027, mae: 0.0012088 test: loss0.0000977, mae:0.0071509
training loss 2.9703676318604266e-06 mae 0.0012400398263707757
training loss 2.4856227315360545e-06 mae 0.001151357799101913
training loss 2.656163283828923e-06 mae 0.0011869992111062648
training loss 2.712962822328492e-06 mae 0.0012027232758145753
training loss 2.712519521404132e-06 mae 0.0012045323678445236
Epoch 469, training: loss: 0.0000027, mae: 0.0012072 test: loss0.0000983, mae:0.0071777
training loss 4.2564020077406894e-06 mae 0.0014090826734900475
training loss 2.625824734815408e-06 mae 0.001191230598088427
training loss 2.6407278670997737e-06 mae 0.0011815694195338252
training loss 2.6436533251200184e-06 mae 0.0011920677891173797
training loss 2.6795773209894256e-06 mae 0.001197383540684811
Epoch 470, training: loss: 0.0000027, mae: 0.0012020 test: loss0.0000986, mae:0.0071942
training loss 2.3274153591046343e-06 mae 0.001204810687340796
training loss 2.470197400458239e-06 mae 0.0011675533069315936
training loss 2.5669249018644432e-06 mae 0.0011822182430406235
training loss 2.659141229730045e-06 mae 0.0011999161886236326
training loss 2.7172016216218944e-06 mae 0.0012065501161387642
Epoch 471, training: loss: 0.0000027, mae: 0.0012093 test: loss0.0000978, mae:0.0071618
training loss 3.5533473692339612e-06 mae 0.0013359260046854615
training loss 2.5567835229915494e-06 mae 0.0011688668244317464
training loss 2.5653226085784935e-06 mae 0.001163896954143512
training loss 2.6512080710937812e-06 mae 0.0011825952416791618
training loss 2.6607639963334073e-06 mae 0.0011886534427607714
Epoch 472, training: loss: 0.0000027, mae: 0.0011903 test: loss0.0000991, mae:0.0072090
training loss 1.823862817218469e-06 mae 0.0010141025995835662
training loss 2.5763999884457362e-06 mae 0.0011660337434424202
training loss 2.600665203488569e-06 mae 0.0011819461141574653
training loss 2.619177744929029e-06 mae 0.0011912048551476013
training loss 2.6808582223887457e-06 mae 0.0011987433935612883
Epoch 473, training: loss: 0.0000027, mae: 0.0011996 test: loss0.0000991, mae:0.0072114
training loss 2.2220594928512583e-06 mae 0.001207605004310608
training loss 2.609616201737606e-06 mae 0.0011835744976540846
training loss 2.5460800599722632e-06 mae 0.001175403151209328
training loss 2.539052016101585e-06 mae 0.0011709560489818597
training loss 2.6449827706229945e-06 mae 0.001191112391224399
Epoch 474, training: loss: 0.0000027, mae: 0.0011946 test: loss0.0000990, mae:0.0072194
training loss 3.4288032111362554e-06 mae 0.0012444345047697425
training loss 2.6097571801038412e-06 mae 0.0011767421417650493
training loss 2.5616549518786948e-06 mae 0.0011763076060497665
training loss 2.6218937084243397e-06 mae 0.0011940519622255681
training loss 2.71711914945681e-06 mae 0.00121640739470155
Epoch 475, training: loss: 0.0000027, mae: 0.0012191 test: loss0.0000999, mae:0.0072335
training loss 2.8848196507169632e-06 mae 0.0012423951411619782
training loss 2.6143012207160383e-06 mae 0.0011935054755988807
training loss 2.619885301758379e-06 mae 0.0011950867667812669
training loss 2.664856236193699e-06 mae 0.00120464832728593
training loss 2.67326797836009e-06 mae 0.0012034961741779061
Epoch 476, training: loss: 0.0000027, mae: 0.0012056 test: loss0.0000981, mae:0.0071808
training loss 2.385943389526801e-06 mae 0.0011766761308535933
training loss 2.720993823494806e-06 mae 0.0012037617681712354
training loss 2.656825316053344e-06 mae 0.001193592954187258
training loss 2.6281605711264047e-06 mae 0.0011830330974366891
training loss 2.630097157376821e-06 mae 0.0011855574450409278
Epoch 477, training: loss: 0.0000026, mae: 0.0011856 test: loss0.0000993, mae:0.0072168
training loss 2.7376383968658047e-06 mae 0.0012646960094571114
training loss 2.5278910307932564e-06 mae 0.0011628357860186667
training loss 2.561339200194271e-06 mae 0.001172701172337009
training loss 2.6188085740276757e-06 mae 0.0011855191481762268
training loss 2.6612793039840204e-06 mae 0.0011969602157695413
Epoch 478, training: loss: 0.0000027, mae: 0.0012014 test: loss0.0000997, mae:0.0072337
training loss 3.5306491099618142e-06 mae 0.001215465017594397
training loss 2.7487937105059456e-06 mae 0.0012126643388696457
training loss 2.6621026939543607e-06 mae 0.0011969346554239865
training loss 2.6610637742906817e-06 mae 0.0011897865217950026
training loss 2.6407906719879376e-06 mae 0.0011877687801421619
Epoch 479, training: loss: 0.0000026, mae: 0.0011869 test: loss0.0000986, mae:0.0071816
training loss 2.86761746792763e-06 mae 0.0012266431003808975
training loss 2.4851966889971172e-06 mae 0.0011477393586663345
training loss 2.5130701996573354e-06 mae 0.0011597900244518008
training loss 2.606908533410308e-06 mae 0.0011800050034668815
training loss 2.636254640691645e-06 mae 0.001186694488258319
Epoch 480, training: loss: 0.0000026, mae: 0.0011859 test: loss0.0000990, mae:0.0072057
training loss 2.910948069256847e-06 mae 0.0012998826568946242
training loss 2.7300067109072363e-06 mae 0.0011912038670761473
training loss 2.700059082085903e-06 mae 0.001196534036875398
training loss 2.646352910558213e-06 mae 0.0011937139931270568
training loss 2.6165867182191227e-06 mae 0.0011888065694516236
Epoch 481, training: loss: 0.0000026, mae: 0.0011879 test: loss0.0000991, mae:0.0072174
training loss 1.990546479646582e-06 mae 0.0010232441127300262
training loss 2.5422118730656457e-06 mae 0.0011593240082683953
training loss 2.5211024583966555e-06 mae 0.001157432364641071
training loss 2.5946403894938117e-06 mae 0.0011787048590351018
training loss 2.5917939650463796e-06 mae 0.0011808916011857636
Epoch 482, training: loss: 0.0000026, mae: 0.0011841 test: loss0.0000992, mae:0.0072234
training loss 1.5941313904477283e-06 mae 0.0009594922885298729
training loss 2.410595675874171e-06 mae 0.0011432916519469487
training loss 2.484177691645293e-06 mae 0.0011525579479218708
training loss 2.5278891619726716e-06 mae 0.0011661001596760174
training loss 2.592773571156265e-06 mae 0.0011760965841057806
Epoch 483, training: loss: 0.0000026, mae: 0.0011775 test: loss0.0001046, mae:0.0072978
training loss 2.2995930066826986e-06 mae 0.0011263213818892837
training loss 2.5704679251694136e-06 mae 0.001173780227819567
training loss 2.6176186901295185e-06 mae 0.0011847857616399185
training loss 2.6142878338657646e-06 mae 0.0011909344381597264
training loss 2.641532511974833e-06 mae 0.0011969495374148725
Epoch 484, training: loss: 0.0000026, mae: 0.0011940 test: loss0.0000993, mae:0.0072232
training loss 2.329269591427874e-06 mae 0.001096302061341703
training loss 2.377783160084743e-06 mae 0.0011358763508991724
training loss 2.594262459841966e-06 mae 0.0011738531330674153
training loss 2.5954413667209607e-06 mae 0.001180003922192121
training loss 2.592246405855356e-06 mae 0.0011794810074460301
Epoch 485, training: loss: 0.0000026, mae: 0.0011810 test: loss0.0000995, mae:0.0072238
training loss 2.124295178873581e-06 mae 0.0011347526451572776
training loss 2.383610243597621e-06 mae 0.0011242318124619914
training loss 2.4629886375256345e-06 mae 0.001151702933285459
training loss 2.5395420703680662e-06 mae 0.0011670284685586264
training loss 2.5989539994160945e-06 mae 0.0011788562683861198
Epoch 486, training: loss: 0.0000026, mae: 0.0011785 test: loss0.0000990, mae:0.0072017
training loss 1.8530423631091253e-06 mae 0.0010915929451584816
training loss 2.520777303181924e-06 mae 0.0011598217887712604
training loss 2.6398162162877266e-06 mae 0.0011812212403245863
training loss 2.5826069900748864e-06 mae 0.0011763813105929919
training loss 2.6001552982531734e-06 mae 0.0011796681045332178
Epoch 487, training: loss: 0.0000026, mae: 0.0011767 test: loss0.0000987, mae:0.0071970
training loss 2.9766772513539763e-06 mae 0.0012959850719198585
training loss 2.632454193279081e-06 mae 0.0011821766886129684
training loss 2.568481634868773e-06 mae 0.0011770337252960644
training loss 2.5453303875255126e-06 mae 0.0011724219796253948
training loss 2.5884425081786855e-06 mae 0.0011831632145189337
Epoch 488, training: loss: 0.0000026, mae: 0.0011827 test: loss0.0000997, mae:0.0072413
training loss 1.502947270637378e-06 mae 0.0009269232978112996
training loss 2.583183768989578e-06 mae 0.0011830919298945982
training loss 2.5649827580985005e-06 mae 0.001172879483725986
training loss 2.5439162075882673e-06 mae 0.0011696632680119256
training loss 2.6000180103673027e-06 mae 0.001182534813009832
Epoch 489, training: loss: 0.0000026, mae: 0.0011823 test: loss0.0000996, mae:0.0072364
training loss 2.8155634481663583e-06 mae 0.0012456569820642471
training loss 2.58712289531318e-06 mae 0.0011726930217050454
training loss 2.4791580237955945e-06 mae 0.0011510761301402042
training loss 2.538106865894773e-06 mae 0.0011664467986321559
training loss 2.592539673213791e-06 mae 0.0011798176330639358
Epoch 490, training: loss: 0.0000026, mae: 0.0011769 test: loss0.0000987, mae:0.0071886
training loss 3.215586957594496e-06 mae 0.0013309806818142533
training loss 2.2096389918314184e-06 mae 0.0011023832256814431
training loss 2.471490287831275e-06 mae 0.0011515062893648629
training loss 2.502308577716978e-06 mae 0.0011550438083825477
training loss 2.5006162450697055e-06 mae 0.001155295652228486
Epoch 491, training: loss: 0.0000025, mae: 0.0011610 test: loss0.0000996, mae:0.0072321
training loss 2.186741994592012e-06 mae 0.0011357096955180168
training loss 2.5658166288278644e-06 mae 0.0011625902250171725
training loss 2.5726949264809614e-06 mae 0.0011599323202828216
training loss 2.5183513979459796e-06 mae 0.0011561443579740918
training loss 2.516387276860335e-06 mae 0.0011615380905429596
Epoch 492, training: loss: 0.0000025, mae: 0.0011620 test: loss0.0000996, mae:0.0072221
training loss 1.8249843378725927e-06 mae 0.0010634480277076364
training loss 2.5804592839859636e-06 mae 0.0011715085117840296
training loss 2.568067739363445e-06 mae 0.0011700788353763442
training loss 2.5674797652393556e-06 mae 0.001170636572572865
training loss 2.5351153361052598e-06 mae 0.0011637858986803585
Epoch 493, training: loss: 0.0000025, mae: 0.0011646 test: loss0.0001002, mae:0.0072464
training loss 3.5933417166233994e-06 mae 0.001284705474972725
training loss 2.47283563727679e-06 mae 0.0011599684275650215
training loss 2.527591710973091e-06 mae 0.0011690380028679525
training loss 2.509394857500694e-06 mae 0.0011615019483608967
training loss 2.524564610465501e-06 mae 0.0011632099425633302
Epoch 494, training: loss: 0.0000025, mae: 0.0011608 test: loss0.0000994, mae:0.0072134
training loss 4.176281890977407e-06 mae 0.0013771686935797334
training loss 2.3837008297157493e-06 mae 0.001115664950453256
training loss 2.407357660648822e-06 mae 0.0011273874413673898
training loss 2.5183275252158446e-06 mae 0.0011549572057910997
training loss 2.5517179905074586e-06 mae 0.0011709292569262583
Epoch 495, training: loss: 0.0000026, mae: 0.0011758 test: loss0.0000991, mae:0.0072235
training loss 2.9247546535771107e-06 mae 0.0012461753794923425
training loss 2.36307377521121e-06 mae 0.0011240162318298485
training loss 2.4827464351414396e-06 mae 0.0011616479599673188
training loss 2.538699147215155e-06 mae 0.0011762679687127566
training loss 2.563540962358305e-06 mae 0.0011804306173618345
Epoch 496, training: loss: 0.0000026, mae: 0.0011805 test: loss0.0001000, mae:0.0072395
training loss 2.7668156690197065e-06 mae 0.0012596948072314262
training loss 2.3968221476738397e-06 mae 0.0011405561030294528
training loss 2.4451333598731848e-06 mae 0.001156761503483326
training loss 2.4954260819663277e-06 mae 0.00116178979680855
training loss 2.515543769732176e-06 mae 0.001165663976099491
Epoch 497, training: loss: 0.0000025, mae: 0.0011689 test: loss0.0001002, mae:0.0072573
training loss 2.3137672542361543e-06 mae 0.0010529948631301522
training loss 2.1959714442845336e-06 mae 0.0010796454221503264
training loss 2.309661287501224e-06 mae 0.0011086221876563417
training loss 2.3932445630321684e-06 mae 0.00112277829737421
training loss 2.426519536943963e-06 mae 0.0011357055769640196
Epoch 498, training: loss: 0.0000025, mae: 0.0011416 test: loss0.0000991, mae:0.0072123
training loss 2.5363749500684207e-06 mae 0.0012127436930313706
training loss 2.4230947233531074e-06 mae 0.001150802289154015
training loss 2.476959762719675e-06 mae 0.001150396271000182
training loss 2.4342726625783315e-06 mae 0.001137402273301306
training loss 2.479148659390442e-06 mae 0.0011491930006252626
Epoch 499, training: loss: 0.0000025, mae: 0.0011517 test: loss0.0001008, mae:0.0072780
current learning rate: 1.5625e-05
training loss 2.6213735964120133e-06 mae 0.0011400965740904212
training loss 2.2505422353948667e-06 mae 0.0010802762482461394
training loss 2.265941775319091e-06 mae 0.0010772073528461977
training loss 2.2906401392257438e-06 mae 0.0010780601408363433
training loss 2.2322233622612684e-06 mae 0.0010651267534446222
Epoch 500, training: loss: 0.0000022, mae: 0.0010659 test: loss0.0000998, mae:0.0072320
training loss 2.5554611511324765e-06 mae 0.0012060875305905938
training loss 2.004968276945446e-06 mae 0.0010201051641785192
training loss 2.074564336133803e-06 mae 0.0010396208012439677
training loss 2.100629297421848e-06 mae 0.0010397347420398517
training loss 2.1629579923660567e-06 mae 0.0010471464895217021
Epoch 501, training: loss: 0.0000022, mae: 0.0010479 test: loss0.0001007, mae:0.0072723
training loss 1.3265279221741366e-06 mae 0.0008576515247114003
training loss 2.1383627904024068e-06 mae 0.001019307551448982
training loss 2.039992638725776e-06 mae 0.0010005953407034942
training loss 2.09130917028233e-06 mae 0.0010192877218275277
training loss 2.161385734791509e-06 mae 0.0010411730864367905
Epoch 502, training: loss: 0.0000022, mae: 0.0010437 test: loss0.0000998, mae:0.0072348
training loss 2.028275730481255e-06 mae 0.0008814192260615528
training loss 2.0812457830520453e-06 mae 0.0010259713002882314
training loss 2.1770850023608325e-06 mae 0.0010444996434743384
training loss 2.153157229047986e-06 mae 0.0010369463587821225
training loss 2.1644710510549025e-06 mae 0.0010451596494954066
Epoch 503, training: loss: 0.0000022, mae: 0.0010479 test: loss0.0001001, mae:0.0072452
training loss 2.2995484414423117e-06 mae 0.0011302161728963256
training loss 2.021901659467332e-06 mae 0.0010066676540208947
training loss 2.087335602563243e-06 mae 0.0010251214659754368
training loss 2.1838567423606187e-06 mae 0.0010440250063031317
training loss 2.1691073691306252e-06 mae 0.001041153183471022
Epoch 504, training: loss: 0.0000022, mae: 0.0010379 test: loss0.0000998, mae:0.0072353
training loss 2.9114316930645145e-06 mae 0.0011071044718846679
training loss 2.07981016572768e-06 mae 0.0010081136601465734
training loss 2.107821528917866e-06 mae 0.001009725949999019
training loss 2.120867249674805e-06 mae 0.0010188288850993515
training loss 2.146888975213225e-06 mae 0.0010285890633379346
Epoch 505, training: loss: 0.0000022, mae: 0.0010324 test: loss0.0001002, mae:0.0072522
training loss 1.9779090507654473e-06 mae 0.001027644262649119
training loss 1.9062013520584407e-06 mae 0.0009817789050786958
training loss 2.0535826254195173e-06 mae 0.001014267546263072
training loss 2.133672458494523e-06 mae 0.0010365592959546636
training loss 2.168413667601019e-06 mae 0.0010451900532969568
Epoch 506, training: loss: 0.0000022, mae: 0.0010424 test: loss0.0001000, mae:0.0072386
training loss 1.51830079175852e-06 mae 0.0009587251697666943
training loss 2.1096847862567043e-06 mae 0.0010372999959680088
training loss 2.1637130484492375e-06 mae 0.0010437005975209915
training loss 2.1474115583832034e-06 mae 0.001038801189456505
training loss 2.1709067786630024e-06 mae 0.0010438589480548021
Epoch 507, training: loss: 0.0000022, mae: 0.0010461 test: loss0.0001008, mae:0.0072699
training loss 2.251574869660544e-06 mae 0.0010277495021000504
training loss 2.24926006855158e-06 mae 0.0010445009718429957
training loss 2.237841873929848e-06 mae 0.0010470157260960421
training loss 2.1997779135323466e-06 mae 0.0010489568330921126
training loss 2.17635024189541e-06 mae 0.0010473987169271853
Epoch 508, training: loss: 0.0000022, mae: 0.0010435 test: loss0.0001001, mae:0.0072364
training loss 1.2429060234353528e-06 mae 0.0008813804015517235
training loss 2.122472383190125e-06 mae 0.0010096795557925078
training loss 2.0901374546381278e-06 mae 0.0010215528701238415
training loss 2.1532311778185658e-06 mae 0.0010367556338642557
training loss 2.1660637702806156e-06 mae 0.001043460054719236
Epoch 509, training: loss: 0.0000022, mae: 0.0010396 test: loss0.0001002, mae:0.0072463
training loss 2.605670715638553e-06 mae 0.0011418400099501014
training loss 2.2257184011822593e-06 mae 0.0010689918018019229
training loss 2.2295386201005732e-06 mae 0.0010616325324926856
training loss 2.1936919156721484e-06 mae 0.0010548869509478126
training loss 2.177117844959994e-06 mae 0.0010501058492462026
Epoch 510, training: loss: 0.0000022, mae: 0.0010472 test: loss0.0001008, mae:0.0072767
training loss 3.310815372969955e-06 mae 0.0010282009607180953
training loss 2.1132516940715684e-06 mae 0.0010268192900362992
training loss 2.094425761553415e-06 mae 0.0010243522615337951
training loss 2.0827729116302208e-06 mae 0.0010205343036468306
training loss 2.1370772900926877e-06 mae 0.0010351700264038472
Epoch 511, training: loss: 0.0000022, mae: 0.0010371 test: loss0.0001009, mae:0.0072719
training loss 2.787561243167147e-06 mae 0.0010829459642991424
training loss 2.1445380788659966e-06 mae 0.0010283377977982888
training loss 2.169189888218621e-06 mae 0.0010393366037803411
training loss 2.121818690302341e-06 mae 0.0010307374663206907
training loss 2.1448936329205086e-06 mae 0.0010365734999855769
Epoch 512, training: loss: 0.0000022, mae: 0.0010406 test: loss0.0001005, mae:0.0072619
training loss 2.2408505628845887e-06 mae 0.0011135820532217622
training loss 2.0680990438253783e-06 mae 0.001020791457609876
training loss 2.015608500452872e-06 mae 0.001000318503915563
training loss 2.071885570023599e-06 mae 0.0010188548804094262
training loss 2.115968987342982e-06 mae 0.0010314660948418555
Epoch 513, training: loss: 0.0000021, mae: 0.0010338 test: loss0.0001003, mae:0.0072504
training loss 2.6974487354891608e-06 mae 0.0010628694435581565
training loss 2.0287575809971698e-06 mae 0.0010170602986096023
training loss 2.1073055741494092e-06 mae 0.001030004233476881
training loss 2.1480352010836e-06 mae 0.0010344496417778325
training loss 2.114628348600894e-06 mae 0.0010328511617711718
Epoch 514, training: loss: 0.0000021, mae: 0.0010359 test: loss0.0001000, mae:0.0072363
training loss 4.236240329191787e-06 mae 0.0013804681366309524
training loss 1.988053612627018e-06 mae 0.001007536423903908
training loss 2.052837562969937e-06 mae 0.001021834919667148
training loss 2.126159158525593e-06 mae 0.0010347489286090764
training loss 2.1328322648614133e-06 mae 0.0010331479246978332
Epoch 515, training: loss: 0.0000021, mae: 0.0010346 test: loss0.0001012, mae:0.0072724
training loss 1.846988652687287e-06 mae 0.0008833495085127652
training loss 1.9782850499188284e-06 mae 0.0010065979269497536
training loss 2.041486850723033e-06 mae 0.0010135317426451511
training loss 2.1031643798880917e-06 mae 0.0010252967126056901
training loss 2.14095544195977e-06 mae 0.0010380390513136948
Epoch 516, training: loss: 0.0000021, mae: 0.0010376 test: loss0.0001007, mae:0.0072785
training loss 1.5929030041661463e-06 mae 0.0010365754133090377
training loss 2.0348098848230346e-06 mae 0.0010178972094995423
training loss 2.0714815954000518e-06 mae 0.0010229556540045698
training loss 2.0793284812763327e-06 mae 0.0010236110380196957
training loss 2.1309718069111957e-06 mae 0.001033467255690279
Epoch 517, training: loss: 0.0000021, mae: 0.0010315 test: loss0.0001002, mae:0.0072493
training loss 1.6095510773084243e-06 mae 0.0009269888396374881
training loss 2.1230374547004056e-06 mae 0.0010428374242859289
training loss 2.0771303279796508e-06 mae 0.0010207483497688007
training loss 2.0943576700366555e-06 mae 0.0010264962946299136
training loss 2.1032647456688483e-06 mae 0.00102803368029875
Epoch 518, training: loss: 0.0000021, mae: 0.0010265 test: loss0.0001000, mae:0.0072408
training loss 2.0807246983167715e-06 mae 0.000952162139583379
training loss 2.186827079158855e-06 mae 0.001018881458816502
training loss 2.1138183711573756e-06 mae 0.0010213608390162929
training loss 2.084487014385468e-06 mae 0.0010209971705809333
training loss 2.1056120553410756e-06 mae 0.001026462087064833
Epoch 519, training: loss: 0.0000021, mae: 0.0010310 test: loss0.0001012, mae:0.0072776
training loss 2.227915047114948e-06 mae 0.001092297025024891
training loss 2.0472706343271596e-06 mae 0.001010577184120741
training loss 2.1177341330591247e-06 mae 0.0010190720008317348
training loss 2.0773619331334616e-06 mae 0.0010169713772580477
training loss 2.0998098824209553e-06 mae 0.0010240238619764431
Epoch 520, training: loss: 0.0000021, mae: 0.0010252 test: loss0.0001007, mae:0.0072803
training loss 2.0966335796401836e-06 mae 0.0010205708676949143
training loss 2.0106419924559313e-06 mae 0.001003847774757328
training loss 2.0135346786257364e-06 mae 0.001004302397596253
training loss 2.0443019716307186e-06 mae 0.0010088366554242042
training loss 2.0913737735348726e-06 mae 0.0010245753963598369
Epoch 521, training: loss: 0.0000021, mae: 0.0010266 test: loss0.0001009, mae:0.0072809
training loss 2.604685278129182e-06 mae 0.001161998719908297
training loss 1.9687864912210734e-06 mae 0.00099404836210477
training loss 2.094708343695919e-06 mae 0.0010296968330564618
training loss 2.159578349730979e-06 mae 0.0010445077821846728
training loss 2.108841231268284e-06 mae 0.001032412678248884
Epoch 522, training: loss: 0.0000021, mae: 0.0010325 test: loss0.0001010, mae:0.0072856
training loss 1.5530991959167295e-06 mae 0.0009421746362932026
training loss 2.0341180469158105e-06 mae 0.0010091457093664102
training loss 2.1156881200455463e-06 mae 0.0010286210021833977
training loss 2.1347803371045662e-06 mae 0.0010379392149494284
training loss 2.103233031283168e-06 mae 0.0010334157375665154
Epoch 523, training: loss: 0.0000021, mae: 0.0010329 test: loss0.0001009, mae:0.0072601
training loss 1.279670073017769e-06 mae 0.000829821452498436
training loss 2.014833041009885e-06 mae 0.0010030344265567906
training loss 2.0311643751402595e-06 mae 0.0010167164376494245
training loss 2.027881596660752e-06 mae 0.0010121429635100803
training loss 2.091093302450151e-06 mae 0.0010258758446984725
Epoch 524, training: loss: 0.0000021, mae: 0.0010273 test: loss0.0001006, mae:0.0072677
training loss 1.7688539628579747e-06 mae 0.0009321626275777817
training loss 1.9824100865448317e-06 mae 0.0009866928939671052
training loss 2.0331263955302e-06 mae 0.0010054103809980545
training loss 2.067971934515214e-06 mae 0.0010157139757524755
training loss 2.085400537274365e-06 mae 0.00102143503079163
Epoch 525, training: loss: 0.0000021, mae: 0.0010211 test: loss0.0001012, mae:0.0072870
training loss 1.625347408662492e-06 mae 0.0009473891113884747
training loss 2.0679687520218887e-06 mae 0.0010022814442659268
training loss 2.056006196923616e-06 mae 0.0010079505114521056
training loss 2.0774431269930442e-06 mae 0.001018165353614689
training loss 2.088999061658314e-06 mae 0.001024394932050091
Epoch 526, training: loss: 0.0000021, mae: 0.0010258 test: loss0.0001007, mae:0.0072688
training loss 1.729833002173109e-06 mae 0.0009997561573982239
training loss 1.9444789903026707e-06 mae 0.0009883269553492761
training loss 2.0414176863524335e-06 mae 0.0010095284162504174
training loss 2.0892583877216348e-06 mae 0.0010220066494684645
training loss 2.0828358507327284e-06 mae 0.0010269663507461696
Epoch 527, training: loss: 0.0000021, mae: 0.0010302 test: loss0.0001010, mae:0.0072834
training loss 1.1339631100781844e-06 mae 0.0007308743079192936
training loss 2.012018734487855e-06 mae 0.0009922359376123137
training loss 2.0267599748282497e-06 mae 0.001011446233617492
training loss 2.0578145004278585e-06 mae 0.001015895658052396
training loss 2.070512547916721e-06 mae 0.0010180370048133284
Epoch 528, training: loss: 0.0000021, mae: 0.0010186 test: loss0.0001011, mae:0.0072764
training loss 1.4866840274407878e-06 mae 0.0009151706472039223
training loss 1.8583983406268304e-06 mae 0.0009743280471393877
training loss 1.951846138919143e-06 mae 0.00098829465228972
training loss 2.0424529922416963e-06 mae 0.001012791357148093
training loss 2.0619431171751483e-06 mae 0.0010187555126502954
Epoch 529, training: loss: 0.0000021, mae: 0.0010181 test: loss0.0001018, mae:0.0073061
training loss 2.4819410100462846e-06 mae 0.00114434864372015
training loss 2.101689118512249e-06 mae 0.0010183544923076588
training loss 2.0804968693311698e-06 mae 0.0010225699821459408
training loss 2.0700755784438758e-06 mae 0.001022397335048223
training loss 2.0952043873356023e-06 mae 0.0010302109109233165
Epoch 530, training: loss: 0.0000021, mae: 0.0010314 test: loss0.0001018, mae:0.0073174
training loss 1.0907015166594647e-06 mae 0.0007877557654865086
training loss 2.02864613334432e-06 mae 0.0010038635969691566
training loss 2.0409295645981957e-06 mae 0.0010111650756311292
training loss 2.0344380329141216e-06 mae 0.0010093730158906493
training loss 2.0688876547503243e-06 mae 0.0010205611192148675
Epoch 531, training: loss: 0.0000021, mae: 0.0010202 test: loss0.0001008, mae:0.0072654
training loss 1.4589280681320815e-06 mae 0.0008958106045611203
training loss 1.9317380796493645e-06 mae 0.0009997638818972253
training loss 2.024510258076527e-06 mae 0.0010062883548106593
training loss 1.9911336638815924e-06 mae 0.0010048117251937212
training loss 2.0586636837471813e-06 mae 0.0010199032953259217
Epoch 532, training: loss: 0.0000021, mae: 0.0010192 test: loss0.0001015, mae:0.0072905
training loss 1.0759214319477906e-06 mae 0.000773451931308955
training loss 1.9811357730099645e-06 mae 0.0009963629794690538
training loss 2.0110485163266016e-06 mae 0.0009982038668522812
training loss 2.0390595180601266e-06 mae 0.0010087220607300764
training loss 2.0514141891483033e-06 mae 0.001015845492461335
Epoch 533, training: loss: 0.0000021, mae: 0.0010194 test: loss0.0001015, mae:0.0072938
training loss 2.048474243565579e-06 mae 0.0010587977012619376
training loss 1.9635083011356378e-06 mae 0.0009976006238538702
training loss 2.0155487681501585e-06 mae 0.0010029833290108145
training loss 2.0469355422843987e-06 mae 0.0010131345896068839
training loss 2.0467792449299343e-06 mae 0.0010156170349215996
Epoch 534, training: loss: 0.0000020, mae: 0.0010148 test: loss0.0001015, mae:0.0072939
training loss 1.646758391871117e-06 mae 0.0010266794124618173
training loss 1.8918354362986243e-06 mae 0.000986257061252699
training loss 1.9198442761971998e-06 mae 0.000989349787169606
training loss 2.072252033145403e-06 mae 0.001017219652591384
training loss 2.0489564561858837e-06 mae 0.0010154565520566047
Epoch 535, training: loss: 0.0000020, mae: 0.0010146 test: loss0.0001015, mae:0.0073005
training loss 1.9705296381289372e-06 mae 0.0009931683307513595
training loss 2.1470278585077137e-06 mae 0.001022006221818646
training loss 2.037901683391706e-06 mae 0.0010057150652530687
training loss 2.0198366537472897e-06 mae 0.0010131213676017896
training loss 2.027878972184501e-06 mae 0.001012144774193899
Epoch 536, training: loss: 0.0000020, mae: 0.0010113 test: loss0.0001010, mae:0.0072836
training loss 2.1343489606806543e-06 mae 0.0010801010066643357
training loss 2.026961506178885e-06 mae 0.0009980993870390103
training loss 2.0465056439720097e-06 mae 0.0010058657170531544
training loss 1.999575909542047e-06 mae 0.0009986844980805513
training loss 2.0298610054815244e-06 mae 0.0010078060510220804
Epoch 537, training: loss: 0.0000020, mae: 0.0010078 test: loss0.0001027, mae:0.0073323
training loss 1.8609727021612343e-06 mae 0.0008700117468833923
training loss 1.8792099790873325e-06 mae 0.0009656110356160093
training loss 1.9603407215054363e-06 mae 0.0009907027620056847
training loss 2.0005783699613576e-06 mae 0.0009995291386545935
training loss 2.0177462467463606e-06 mae 0.0010068161023284923
Epoch 538, training: loss: 0.0000020, mae: 0.0010076 test: loss0.0001015, mae:0.0072943
training loss 1.5369827224276378e-06 mae 0.000988269574008882
training loss 1.915976312437695e-06 mae 0.000984314490137074
training loss 1.8957964128969603e-06 mae 0.0009840470462078512
training loss 1.9862810820044293e-06 mae 0.000999013353247097
training loss 2.019208855911153e-06 mae 0.0010084863146882863
Epoch 539, training: loss: 0.0000020, mae: 0.0010084 test: loss0.0001015, mae:0.0072937
training loss 2.610235924294102e-06 mae 0.001033180975355208
training loss 1.9200337276867916e-06 mae 0.0009938992306535294
training loss 1.9399225941537465e-06 mae 0.0009993755075352097
training loss 2.0230783183558517e-06 mae 0.0010121745324927988
training loss 2.038354328541306e-06 mae 0.0010164755877844086
Epoch 540, training: loss: 0.0000020, mae: 0.0010145 test: loss0.0001053, mae:0.0073260
training loss 2.4480548290739534e-06 mae 0.0012255897745490074
training loss 1.921027867812133e-06 mae 0.000985801327224894
training loss 1.9809295333225877e-06 mae 0.0010031641732975106
training loss 1.9658162548676173e-06 mae 0.0009996968052436337
training loss 2.015132909025453e-06 mae 0.0010065431764525417
Epoch 541, training: loss: 0.0000020, mae: 0.0010064 test: loss0.0001012, mae:0.0072818
training loss 1.727309154375689e-06 mae 0.0009901070734485984
training loss 2.0191056320054845e-06 mae 0.0010021738628126387
training loss 1.944521268387239e-06 mae 0.0009910407583321452
training loss 1.9949688163215137e-06 mae 0.0010022565017805886
training loss 2.006707103221142e-06 mae 0.0010043836122874481
Epoch 542, training: loss: 0.0000020, mae: 0.0010085 test: loss0.0001016, mae:0.0073018
training loss 1.7680249584373087e-06 mae 0.0009908523643389344
training loss 2.0430812442635434e-06 mae 0.0010086182943161793
training loss 1.965315509506122e-06 mae 0.0009903011268401279
training loss 1.9700827954012915e-06 mae 0.0009918295560922743
training loss 2.0063887597136993e-06 mae 0.0010034150924111954
Epoch 543, training: loss: 0.0000020, mae: 0.0010040 test: loss0.0001022, mae:0.0073228
training loss 1.6843215462358785e-06 mae 0.0009485489572398365
training loss 1.950852380310143e-06 mae 0.000985923947939905
training loss 1.9281878201915183e-06 mae 0.0009832147492722855
training loss 1.985348520920887e-06 mae 0.0009976553338246776
training loss 1.974958189266681e-06 mae 0.000996679071086779
Epoch 544, training: loss: 0.0000020, mae: 0.0010014 test: loss0.0001012, mae:0.0072769
training loss 2.5189895040966803e-06 mae 0.0009489388321526349
training loss 2.02571881653271e-06 mae 0.0010033763646075103
training loss 2.0421097347160434e-06 mae 0.0010124461686164884
training loss 2.0396214965680093e-06 mae 0.0010106266007468797
training loss 2.014998330958689e-06 mae 0.0010077065726816288
Epoch 545, training: loss: 0.0000020, mae: 0.0010087 test: loss0.0001018, mae:0.0073135
training loss 1.3155381566321012e-06 mae 0.000874178484082222
training loss 2.045942083254084e-06 mae 0.0010079938493778593
training loss 2.034179543681945e-06 mae 0.0010047265349214178
training loss 2.005281699090022e-06 mae 0.0009993555533872385
training loss 2.0216808890940614e-06 mae 0.0010073720845301167
Epoch 546, training: loss: 0.0000020, mae: 0.0010070 test: loss0.0001018, mae:0.0073001
training loss 8.606916708231438e-07 mae 0.0007130891899578273
training loss 1.857084821851601e-06 mae 0.0009643169218108204
training loss 1.993332147105664e-06 mae 0.0009937189671552114
training loss 2.013864761333302e-06 mae 0.000999817560698269
training loss 2.005390583622441e-06 mae 0.0010046693177856684
Epoch 547, training: loss: 0.0000020, mae: 0.0010053 test: loss0.0001018, mae:0.0073036
training loss 2.2676290427625645e-06 mae 0.0009579754550941288
training loss 2.0155565507326064e-06 mae 0.001002622882415559
training loss 2.0334048151585867e-06 mae 0.001012421014262271
training loss 2.0181654327637603e-06 mae 0.0010075157697910805
training loss 1.990526308999782e-06 mae 0.0010042244855740768
Epoch 548, training: loss: 0.0000020, mae: 0.0010059 test: loss0.0001025, mae:0.0073366
training loss 2.4753499019425362e-06 mae 0.0010601856047287583
training loss 1.8738733673422293e-06 mae 0.000979169971519607
training loss 1.931183713748955e-06 mae 0.0009906844265924974
training loss 1.9839578383966343e-06 mae 0.0010018559575191899
training loss 1.994121462088227e-06 mae 0.001007233548270011
Epoch 549, training: loss: 0.0000020, mae: 0.0010058 test: loss0.0001019, mae:0.0073155
training loss 2.487175834176014e-06 mae 0.001110988319851458
training loss 2.074855672162813e-06 mae 0.0009983739582821731
training loss 1.960914921080012e-06 mae 0.0009908142250658265
training loss 1.966400740734239e-06 mae 0.0009914296343830093
training loss 1.974371485433568e-06 mae 0.0009989042445526118
Epoch 550, training: loss: 0.0000020, mae: 0.0010031 test: loss0.0001024, mae:0.0073317
training loss 1.3137365613147267e-06 mae 0.0008483802084811032
training loss 1.9717450886317093e-06 mae 0.0009880804025348931
training loss 1.9780389565501464e-06 mae 0.0009976720661017116
training loss 2.0348931759102603e-06 mae 0.0010092923177147145
training loss 2.003880259356569e-06 mae 0.0010025464583290227
Epoch 551, training: loss: 0.0000020, mae: 0.0010005 test: loss0.0001016, mae:0.0072993
training loss 2.838436557794921e-06 mae 0.0011391373118385673
training loss 2.0139935317024983e-06 mae 0.0009826927722029971
training loss 1.9253374170832533e-06 mae 0.0009769443687660123
training loss 1.974769834937763e-06 mae 0.0009907153542986172
training loss 1.9820740331100857e-06 mae 0.0009960543937096134
Epoch 552, training: loss: 0.0000020, mae: 0.0009923 test: loss0.0001024, mae:0.0073294
training loss 1.4790684872423299e-06 mae 0.0009167299722321332
training loss 1.8412239888212023e-06 mae 0.0009428574304169447
training loss 1.948196245506335e-06 mae 0.0009806504632152695
training loss 1.94608671655254e-06 mae 0.000984100025110194
training loss 1.9519789450566185e-06 mae 0.0009857796660198174
Epoch 553, training: loss: 0.0000020, mae: 0.0009866 test: loss0.0001021, mae:0.0073196
training loss 1.8365681171417236e-06 mae 0.0009888556087389588
training loss 2.0204293725649168e-06 mae 0.0009751187639730965
training loss 2.0013033672692504e-06 mae 0.0009973256751931833
training loss 1.997825121324188e-06 mae 0.0010006692139462666
training loss 1.9696719608697083e-06 mae 0.0009943511272173617
Epoch 554, training: loss: 0.0000020, mae: 0.0009952 test: loss0.0001018, mae:0.0073033
training loss 1.2528336128525552e-06 mae 0.0008099426631815732
training loss 1.906735481801055e-06 mae 0.000955928176385807
training loss 1.9589401784575632e-06 mae 0.0009829975534368788
training loss 1.929480838167147e-06 mae 0.0009868338230738284
training loss 1.940677136859739e-06 mae 0.0009887755309250107
Epoch 555, training: loss: 0.0000020, mae: 0.0009942 test: loss0.0001021, mae:0.0073093
training loss 1.363960450362356e-06 mae 0.0008439825032837689
training loss 1.8970115322600558e-06 mae 0.0009801624144208342
training loss 1.9017162913985545e-06 mae 0.000980523765308432
training loss 1.9677159182581525e-06 mae 0.0009911708997883246
training loss 1.968711071798797e-06 mae 0.0009941671808732131
Epoch 556, training: loss: 0.0000020, mae: 0.0009911 test: loss0.0001024, mae:0.0073318
training loss 2.3558659449918196e-06 mae 0.0010874783620238304
training loss 1.9329537363488516e-06 mae 0.0009897318234959361
training loss 1.8607221397477325e-06 mae 0.0009685347962082537
training loss 1.9048353759604194e-06 mae 0.0009829028959028325
training loss 1.9732320523152307e-06 mae 0.000999317341136621
Epoch 557, training: loss: 0.0000020, mae: 0.0009969 test: loss0.0001017, mae:0.0073112
training loss 8.498147963109659e-07 mae 0.0007289024069905281
training loss 1.9238173504787733e-06 mae 0.0009865018952290945
training loss 1.8982200654451418e-06 mae 0.0009753446389764251
training loss 1.8863678424800465e-06 mae 0.0009741226769366997
training loss 1.9218740932268504e-06 mae 0.0009825939259871805
Epoch 558, training: loss: 0.0000019, mae: 0.0009852 test: loss0.0001024, mae:0.0073290
training loss 1.6612956414974178e-06 mae 0.0009884638711810112
training loss 1.931471563149863e-06 mae 0.0009780284405375521
training loss 1.9332122037937574e-06 mae 0.0009818061111992833
training loss 1.934198961266131e-06 mae 0.0009847987872634747
training loss 1.9526441174116275e-06 mae 0.0009907693567393877
Epoch 559, training: loss: 0.0000019, mae: 0.0009899 test: loss0.0001026, mae:0.0073389
training loss 2.070409664156614e-06 mae 0.001027346937917173
training loss 1.8094704135400408e-06 mae 0.0009572569123359725
training loss 1.892312628898605e-06 mae 0.0009796025835457117
training loss 1.9468124059889713e-06 mae 0.0009912586854215687
training loss 1.9756631825576234e-06 mae 0.0009973473251514617
Epoch 560, training: loss: 0.0000020, mae: 0.0009958 test: loss0.0001019, mae:0.0073149
training loss 1.1373543884474202e-06 mae 0.0007526672561652958
training loss 2.0226833567885838e-06 mae 0.0009926445085975324
training loss 1.8934014426374438e-06 mae 0.0009667210161022033
training loss 1.8984071682959453e-06 mae 0.0009751429562364392
training loss 1.940401465508874e-06 mae 0.0009850346273626786
Epoch 561, training: loss: 0.0000019, mae: 0.0009859 test: loss0.0001029, mae:0.0073539
training loss 2.327945594515768e-06 mae 0.0009968267986550927
training loss 1.7623702286287238e-06 mae 0.0009448317041638873
training loss 1.858729194286707e-06 mae 0.0009719714393267539
training loss 1.918379802950255e-06 mae 0.0009830024379416556
training loss 1.9585986270178533e-06 mae 0.0009950865091945958
Epoch 562, training: loss: 0.0000019, mae: 0.0009919 test: loss0.0001024, mae:0.0073385
training loss 2.2893157165526645e-06 mae 0.0010153647745028138
training loss 1.942972978534218e-06 mae 0.0009845346212387087
training loss 1.9567051183030082e-06 mae 0.0009815907849247878
training loss 1.9661303764940457e-06 mae 0.0009857063861687158
training loss 1.9476938070366296e-06 mae 0.0009867672840902463
Epoch 563, training: loss: 0.0000019, mae: 0.0009874 test: loss0.0001023, mae:0.0073375
training loss 4.1028338273463305e-06 mae 0.001187338144518435
training loss 1.9243093871124324e-06 mae 0.0009732270218413688
training loss 1.9040420274995082e-06 mae 0.000972051195924788
training loss 1.9198765101205224e-06 mae 0.0009792413630380931
training loss 1.9318370477346754e-06 mae 0.000983124153470778
Epoch 564, training: loss: 0.0000019, mae: 0.0009858 test: loss0.0001019, mae:0.0073148
training loss 1.1147208169859368e-06 mae 0.0007490661810152233
training loss 1.8391397914313654e-06 mae 0.0009591039909305527
training loss 1.868145372469317e-06 mae 0.0009606556187725009
training loss 1.9079891297748103e-06 mae 0.0009778143050895376
training loss 1.9519770909996334e-06 mae 0.000988101198648413
Epoch 565, training: loss: 0.0000019, mae: 0.0009879 test: loss0.0001025, mae:0.0073398
training loss 3.0150520160532324e-06 mae 0.0010701467981562018
training loss 1.851376521021759e-06 mae 0.0009661513099939946
training loss 1.9072043985466043e-06 mae 0.0009784575668163598
training loss 1.904849748310675e-06 mae 0.0009774642588650897
training loss 1.9179785599997343e-06 mae 0.000984890841408656
Epoch 566, training: loss: 0.0000019, mae: 0.0009889 test: loss0.0001026, mae:0.0073290
training loss 1.3831382830176153e-06 mae 0.0008834923501126468
training loss 1.9280562188374894e-06 mae 0.0009746777279503353
training loss 1.906924606796881e-06 mae 0.000969963764315379
training loss 1.9059756078869574e-06 mae 0.000975024322470317
training loss 1.9310971525845534e-06 mae 0.0009797803770324484
Epoch 567, training: loss: 0.0000019, mae: 0.0009784 test: loss0.0001029, mae:0.0073513
training loss 1.2805204505639267e-06 mae 0.0009221440996043384
training loss 1.8934739823991696e-06 mae 0.0009722626487286216
training loss 1.77747396453852e-06 mae 0.0009527833294353939
training loss 1.8510054932575324e-06 mae 0.0009680986188549476
training loss 1.8986571026900167e-06 mae 0.0009768152790751068
Epoch 568, training: loss: 0.0000019, mae: 0.0009782 test: loss0.0001029, mae:0.0073526
training loss 2.171660753447213e-06 mae 0.0010296801337972283
training loss 1.974901396369758e-06 mae 0.0010054237563071737
training loss 1.909873303432479e-06 mae 0.0009836543133565326
training loss 1.9052978702034708e-06 mae 0.0009822463990645546
training loss 1.9323343385667395e-06 mae 0.0009873077529366705
Epoch 569, training: loss: 0.0000019, mae: 0.0009862 test: loss0.0001022, mae:0.0073243
training loss 1.7309204167759162e-06 mae 0.001027355552650988
training loss 1.8182683146840325e-06 mae 0.0009446813092640074
training loss 1.9071122913842238e-06 mae 0.0009720886741663412
training loss 1.9363716255280235e-06 mae 0.0009806949756299421
training loss 1.9120834930554654e-06 mae 0.0009784420349273776
Epoch 570, training: loss: 0.0000019, mae: 0.0009796 test: loss0.0001033, mae:0.0073693
training loss 9.341778763882758e-07 mae 0.0007490779389627278
training loss 1.854333240370419e-06 mae 0.0009530956204067549
training loss 1.8820200619590494e-06 mae 0.0009671085580980569
training loss 1.890559722382841e-06 mae 0.000975875045166702
training loss 1.9035067674084712e-06 mae 0.0009799552780219271
Epoch 571, training: loss: 0.0000019, mae: 0.0009811 test: loss0.0001026, mae:0.0073311
training loss 2.5336955786769977e-06 mae 0.0011007171124219894
training loss 1.7351062346207588e-06 mae 0.000944586171695561
training loss 1.8129734636060299e-06 mae 0.0009463303128838318
training loss 1.8580856629383883e-06 mae 0.0009636574760942861
training loss 1.87841333036655e-06 mae 0.0009722420994993946
Epoch 572, training: loss: 0.0000019, mae: 0.0009716 test: loss0.0001025, mae:0.0073259
training loss 1.828638801271154e-06 mae 0.0009736406500451267
training loss 1.8362677620893472e-06 mae 0.000947033424916513
training loss 1.8646497898033085e-06 mae 0.0009606677582395254
training loss 1.8620651539944241e-06 mae 0.0009633447286450854
training loss 1.8996549965438013e-06 mae 0.000972585059906268
Epoch 573, training: loss: 0.0000019, mae: 0.0009712 test: loss0.0001028, mae:0.0073453
training loss 1.3747002185482415e-06 mae 0.0008083907887339592
training loss 1.9629324338340802e-06 mae 0.000972671581514399
training loss 1.8726442623020455e-06 mae 0.0009642339353034699
training loss 1.857350985842187e-06 mae 0.0009671278334136387
training loss 1.8955912475205003e-06 mae 0.0009760304321697103
Epoch 574, training: loss: 0.0000019, mae: 0.0009770 test: loss0.0001028, mae:0.0073399
training loss 1.676975330155983e-06 mae 0.000979220145381987
training loss 1.730509632793923e-06 mae 0.0009421167162466138
training loss 1.8196909785287838e-06 mae 0.0009569854695709552
training loss 1.8534461795096062e-06 mae 0.00096381281807846
training loss 1.8888661080083366e-06 mae 0.0009724668334863747
Epoch 575, training: loss: 0.0000019, mae: 0.0009721 test: loss0.0001024, mae:0.0073325
training loss 1.4963112562327296e-06 mae 0.0008866917341947556
training loss 1.7647929397236452e-06 mae 0.0009441305906055312
training loss 1.762203723528458e-06 mae 0.0009475820184925036
training loss 1.8230208848727003e-06 mae 0.0009591496978972773
training loss 1.8805873937414272e-06 mae 0.0009724181278641175
Epoch 576, training: loss: 0.0000019, mae: 0.0009747 test: loss0.0001028, mae:0.0073397
training loss 9.572160024617915e-07 mae 0.0007232840289361775
training loss 1.8664765093490128e-06 mae 0.0009403811589650372
training loss 1.8563658751181807e-06 mae 0.000950675515923649
training loss 1.889236879279204e-06 mae 0.0009667880271117369
training loss 1.8827444600856544e-06 mae 0.0009681117629391415
Epoch 577, training: loss: 0.0000019, mae: 0.0009673 test: loss0.0001023, mae:0.0073208
training loss 1.4756957398276427e-06 mae 0.0009010641952045262
training loss 1.7872887193940796e-06 mae 0.0009338317151345753
training loss 1.8296127457165985e-06 mae 0.0009522058627577405
training loss 1.8121845235309716e-06 mae 0.0009492439497724815
training loss 1.8578721778794716e-06 mae 0.0009583152456450001
Epoch 578, training: loss: 0.0000019, mae: 0.0009609 test: loss0.0001031, mae:0.0073507
training loss 1.9645858628791757e-06 mae 0.0010431064292788506
training loss 1.749495857429971e-06 mae 0.0009418404330134243
training loss 1.851480136554186e-06 mae 0.0009734103667070293
training loss 1.8761376123012386e-06 mae 0.0009796691885748446
training loss 1.8950949159564825e-06 mae 0.0009832273956170464
Epoch 579, training: loss: 0.0000019, mae: 0.0009850 test: loss0.0001026, mae:0.0073396
training loss 1.5302971405617427e-06 mae 0.0009182728826999664
training loss 1.7470403097867538e-06 mae 0.0009327757380007969
training loss 1.8111149106201511e-06 mae 0.0009537281481189523
training loss 1.8667404777634696e-06 mae 0.0009670963411416786
training loss 1.859133110836194e-06 mae 0.0009684470005504859
Epoch 580, training: loss: 0.0000019, mae: 0.0009711 test: loss0.0001033, mae:0.0073598
training loss 1.0509926369195455e-06 mae 0.0007704763556830585
training loss 1.740128758612235e-06 mae 0.0009348609612560741
training loss 1.7562696803222504e-06 mae 0.0009466394658735265
training loss 1.8548457198195918e-06 mae 0.0009680480491184933
training loss 1.8645040288571928e-06 mae 0.0009735517828743813
Epoch 581, training: loss: 0.0000019, mae: 0.0009730 test: loss0.0001021, mae:0.0073249
training loss 1.3330624142326997e-06 mae 0.0008379255305044353
training loss 1.7883335582761597e-06 mae 0.0009347382926053421
training loss 1.8341618575540997e-06 mae 0.0009446521047594967
training loss 1.8641778510845448e-06 mae 0.0009611722294685265
training loss 1.8778340710416872e-06 mae 0.0009696372525201209
Epoch 582, training: loss: 0.0000019, mae: 0.0009697 test: loss0.0001029, mae:0.0073514
training loss 2.2082510895415908e-06 mae 0.0011140936985611916
training loss 1.8081433415104315e-06 mae 0.0009532980643687585
training loss 1.8546165200283523e-06 mae 0.000956234307198942
training loss 1.8853725435786186e-06 mae 0.0009696128641699276
training loss 1.8875068065254648e-06 mae 0.0009774693678276831
Epoch 583, training: loss: 0.0000019, mae: 0.0009774 test: loss0.0001029, mae:0.0073467
training loss 2.1649279915436637e-06 mae 0.0009598471224308014
training loss 1.7980187550949945e-06 mae 0.0009326697749487471
training loss 1.8489016166439442e-06 mae 0.0009559335355361058
training loss 1.862785719617781e-06 mae 0.0009608998719683002
training loss 1.850676656248203e-06 mae 0.0009585571459689132
Epoch 584, training: loss: 0.0000018, mae: 0.0009580 test: loss0.0001031, mae:0.0073515
training loss 2.3522504761785967e-06 mae 0.0010187519947066903
training loss 1.9046359355735482e-06 mae 0.0009625322110129189
training loss 1.8381676628081607e-06 mae 0.0009579086200205034
training loss 1.8842344197523826e-06 mae 0.0009721389529141071
training loss 1.8761231483302753e-06 mae 0.0009693726905927393
Epoch 585, training: loss: 0.0000019, mae: 0.0009672 test: loss0.0001029, mae:0.0073430
training loss 1.5728677453807904e-06 mae 0.0008954483200795949
training loss 1.7514167670835344e-06 mae 0.0009379070400552568
training loss 1.7750705369913288e-06 mae 0.000942569534354244
training loss 1.8476964857612666e-06 mae 0.000957497166599215
training loss 1.8485823973569684e-06 mae 0.0009621336441649244
Epoch 586, training: loss: 0.0000018, mae: 0.0009615 test: loss0.0001030, mae:0.0073495
training loss 1.553631932438293e-06 mae 0.0008423784747719765
training loss 1.8643820734563203e-06 mae 0.0009645932495557502
training loss 1.807452058033455e-06 mae 0.000954242173633031
training loss 1.8275132037016247e-06 mae 0.0009612917525397724
training loss 1.862128562462018e-06 mae 0.000968935051515921
Epoch 587, training: loss: 0.0000019, mae: 0.0009704 test: loss0.0001033, mae:0.0073653
training loss 3.389609446458053e-06 mae 0.0013334158575162292
training loss 1.8382104828971006e-06 mae 0.0009504347439289236
training loss 1.8393014948463191e-06 mae 0.0009613848236011392
training loss 1.8224374127764544e-06 mae 0.0009580996821924348
training loss 1.8371024138878728e-06 mae 0.0009618539337661878
Epoch 588, training: loss: 0.0000018, mae: 0.0009601 test: loss0.0001028, mae:0.0073432
training loss 2.068619323836174e-06 mae 0.0009691352024674416
training loss 1.8516676596382415e-06 mae 0.0009499048967115289
training loss 1.8313601506831965e-06 mae 0.0009448105350625474
training loss 1.8454084643686083e-06 mae 0.0009537294525529774
training loss 1.8582813308695677e-06 mae 0.0009623738196886967
Epoch 589, training: loss: 0.0000018, mae: 0.0009592 test: loss0.0001033, mae:0.0073624
training loss 6.655808988398348e-07 mae 0.0006127084488980472
training loss 1.763320959329862e-06 mae 0.0009387556792182081
training loss 1.7598294943631532e-06 mae 0.0009427296417295049
training loss 1.8330135776974832e-06 mae 0.0009572694728780069
training loss 1.835661178979784e-06 mae 0.0009612683690412533
Epoch 590, training: loss: 0.0000018, mae: 0.0009625 test: loss0.0001031, mae:0.0073505
training loss 2.281817614857573e-06 mae 0.0011479416862130165
training loss 1.8774254100457723e-06 mae 0.0009823441813590335
training loss 1.8330981776112947e-06 mae 0.0009692724405714115
training loss 1.8251604071627194e-06 mae 0.0009620097833562223
training loss 1.8513871002856902e-06 mae 0.000966758191567007
Epoch 591, training: loss: 0.0000018, mae: 0.0009663 test: loss0.0001037, mae:0.0073584
training loss 2.4784865217952756e-06 mae 0.0011587539920583367
training loss 1.726424237207824e-06 mae 0.0009218285513092198
training loss 1.820527338262727e-06 mae 0.0009512578518738471
training loss 1.7797047198896412e-06 mae 0.0009434125852610744
training loss 1.8361386173185105e-06 mae 0.0009580845058436938
Epoch 592, training: loss: 0.0000018, mae: 0.0009570 test: loss0.0001036, mae:0.0073659
training loss 1.235585727954458e-06 mae 0.0008232587133534253
training loss 1.8276346355271334e-06 mae 0.0009535992867313325
training loss 1.8458389866615407e-06 mae 0.0009617679723593783
training loss 1.8206163555523806e-06 mae 0.0009591892636874503
training loss 1.827140729165872e-06 mae 0.0009598613320502323
Epoch 593, training: loss: 0.0000018, mae: 0.0009577 test: loss0.0001034, mae:0.0073641
training loss 1.9561809949664166e-06 mae 0.0009572987328283489
training loss 1.7330124262348275e-06 mae 0.0009268381365794031
training loss 1.7644200602487697e-06 mae 0.0009358210949052013
training loss 1.7702140188257487e-06 mae 0.0009428954155905952
training loss 1.810403218984283e-06 mae 0.0009525210025548863
Epoch 594, training: loss: 0.0000018, mae: 0.0009519 test: loss0.0001038, mae:0.0073737
training loss 7.512642810070247e-07 mae 0.0006449536303989589
training loss 1.9137184171638947e-06 mae 0.0009514684845949066
training loss 1.8484953100164378e-06 mae 0.0009506456225437322
training loss 1.846926824364105e-06 mae 0.0009535875611642487
training loss 1.822944413654105e-06 mae 0.0009528351820245457
Epoch 595, training: loss: 0.0000018, mae: 0.0009524 test: loss0.0001035, mae:0.0073652
training loss 1.8757767747956677e-06 mae 0.0010248370235785842
training loss 1.8400398612713758e-06 mae 0.0009512543865461268
training loss 1.7430649856380274e-06 mae 0.0009296370121548965
training loss 1.7696657614157104e-06 mae 0.0009443767639401287
training loss 1.8351345416722234e-06 mae 0.0009569247500549311
Epoch 596, training: loss: 0.0000018, mae: 0.0009553 test: loss0.0001033, mae:0.0073698
training loss 1.58025716245902e-06 mae 0.000925418280530721
training loss 1.723789499045975e-06 mae 0.0009444686127643959
training loss 1.755143734323547e-06 mae 0.0009459377241108826
training loss 1.799980952985408e-06 mae 0.0009567347063763152
training loss 1.8076840529576104e-06 mae 0.0009542762661533453
Epoch 597, training: loss: 0.0000018, mae: 0.0009566 test: loss0.0001036, mae:0.0073698
training loss 2.426816763545503e-06 mae 0.0009169895201921463
training loss 1.6454712439839316e-06 mae 0.0009200804451407461
training loss 1.7611850410611448e-06 mae 0.0009484563194593358
training loss 1.8064709584808984e-06 mae 0.000955442338256745
training loss 1.8236377953334296e-06 mae 0.0009592844457689916
Epoch 598, training: loss: 0.0000018, mae: 0.0009609 test: loss0.0001034, mae:0.0073769
training loss 2.0686518382717622e-06 mae 0.0010527903214097023
training loss 1.703957257088369e-06 mae 0.0009255472630006718
training loss 1.789500327229707e-06 mae 0.0009481236592193349
training loss 1.813837468790667e-06 mae 0.0009513946827531899
training loss 1.8254673200000747e-06 mae 0.0009561345190856961
Epoch 599, training: loss: 0.0000018, mae: 0.0009570 test: loss0.0001038, mae:0.0073740
current learning rate: 7.8125e-06
training loss 1.0192891295446316e-06 mae 0.0007308358326554298
training loss 1.6470006897304532e-06 mae 0.0008970645379603785
training loss 1.6548222482348444e-06 mae 0.0008949232506704067
training loss 1.7129539346931183e-06 mae 0.0009053389808557778
training loss 1.7085567746315833e-06 mae 0.0009055861983730904
Epoch 600, training: loss: 0.0000017, mae: 0.0009022 test: loss0.0001034, mae:0.0073654
training loss 1.110295556827623e-06 mae 0.0008214724366553128
training loss 1.5589292408602723e-06 mae 0.0008680079550044062
training loss 1.6412872538906116e-06 mae 0.0008784842004280278
training loss 1.6671671420341074e-06 mae 0.0008903364369503855
training loss 1.6787050018631176e-06 mae 0.0008968159099415274
Epoch 601, training: loss: 0.0000017, mae: 0.0008974 test: loss0.0001033, mae:0.0073625
training loss 1.1111961839560536e-06 mae 0.0007255639065988362
training loss 1.6909266400498204e-06 mae 0.0008909317479907152
training loss 1.6822621018540718e-06 mae 0.0008882598266798524
training loss 1.6869781060998003e-06 mae 0.0008934571608956534
training loss 1.663799850600629e-06 mae 0.0008917746683744833
Epoch 602, training: loss: 0.0000017, mae: 0.0008967 test: loss0.0001032, mae:0.0073552
training loss 1.826200787036214e-06 mae 0.0010325489565730095
training loss 1.6763413276410077e-06 mae 0.000892067733266409
training loss 1.6651032714579211e-06 mae 0.0008917500335252891
training loss 1.6613362054881417e-06 mae 0.0008918426773628059
training loss 1.6774730853507016e-06 mae 0.0008936096469301786
Epoch 603, training: loss: 0.0000017, mae: 0.0008922 test: loss0.0001034, mae:0.0073609
training loss 1.2752174143315642e-06 mae 0.0007972407038323581
training loss 1.738145196365319e-06 mae 0.0008941062271832398
training loss 1.713042085458877e-06 mae 0.0008908300435788338
training loss 1.6907294212729393e-06 mae 0.0008939425102496726
training loss 1.6728858686767485e-06 mae 0.0008920463443784716
Epoch 604, training: loss: 0.0000017, mae: 0.0008904 test: loss0.0001030, mae:0.0073573
training loss 1.0308245919077308e-06 mae 0.0007171733304858208
training loss 1.5910473496428584e-06 mae 0.0008662975285950973
training loss 1.6219886070208336e-06 mae 0.0008814597276435926
training loss 1.6206625039540204e-06 mae 0.0008773742060929489
training loss 1.6648625168231978e-06 mae 0.0008897331832277949
Epoch 605, training: loss: 0.0000017, mae: 0.0008910 test: loss0.0001079, mae:0.0074256
training loss 1.9791993963735877e-06 mae 0.0009355954825878143
training loss 1.6607762520335182e-06 mae 0.0008874736787915667
training loss 1.6626315772159028e-06 mae 0.000885419378697983
training loss 1.6847954840755503e-06 mae 0.000895843516174852
training loss 1.6517775424543835e-06 mae 0.0008887352951250933
Epoch 606, training: loss: 0.0000017, mae: 0.0008909 test: loss0.0001044, mae:0.0073966
training loss 1.9648823581519537e-06 mae 0.0009656722540967166
training loss 1.6562971282775264e-06 mae 0.000895168746401574
training loss 1.6454521731603645e-06 mae 0.0008905586062718442
training loss 1.6778748273802959e-06 mae 0.0008985887590752581
training loss 1.6616310605133633e-06 mae 0.0008916686238623711
Epoch 607, training: loss: 0.0000017, mae: 0.0008957 test: loss0.0001033, mae:0.0073618
training loss 2.2948743207962252e-06 mae 0.0009544740314595401
training loss 1.47334298514811e-06 mae 0.0008352332173244041
training loss 1.6471204973938015e-06 mae 0.000877929545966632
training loss 1.6779680281886625e-06 mae 0.0008855269815532229
training loss 1.6700641978460944e-06 mae 0.0008919878349644104
Epoch 608, training: loss: 0.0000017, mae: 0.0008910 test: loss0.0001037, mae:0.0073769
training loss 1.7187427374665276e-06 mae 0.0008807322010397911
training loss 1.551588094990424e-06 mae 0.0008661592774111413
training loss 1.6036321544516328e-06 mae 0.0008740523463520823
training loss 1.6277215252479345e-06 mae 0.0008810559678545211
training loss 1.664570525472254e-06 mae 0.0008918893780091322
Epoch 609, training: loss: 0.0000017, mae: 0.0008932 test: loss0.0001038, mae:0.0073845
training loss 1.4368437177836313e-06 mae 0.0007883263751864433
training loss 1.5892440724290317e-06 mae 0.0008718159538237196
training loss 1.5913506002665478e-06 mae 0.0008700725681259123
training loss 1.6476807994401336e-06 mae 0.000885150877570746
training loss 1.6735487144108252e-06 mae 0.0008935088622476445
Epoch 610, training: loss: 0.0000017, mae: 0.0008937 test: loss0.0001034, mae:0.0073611
training loss 9.681851906861993e-07 mae 0.0007394201238639653
training loss 1.6379016232367223e-06 mae 0.0008734378121409786
training loss 1.6603431759850172e-06 mae 0.0008862188378680903
training loss 1.6565274416050986e-06 mae 0.0008867283286958536
training loss 1.6634839264735267e-06 mae 0.0008886056571077572
Epoch 611, training: loss: 0.0000017, mae: 0.0008896 test: loss0.0001051, mae:0.0074171
training loss 1.9200836050004e-06 mae 0.0009009037166833878
training loss 1.6895489294506213e-06 mae 0.0009002105547480431
training loss 1.7059570688282898e-06 mae 0.0008974640790400747
training loss 1.6982772413843158e-06 mae 0.0008951025750543551
training loss 1.6773983150446676e-06 mae 0.0008945878903817068
Epoch 612, training: loss: 0.0000017, mae: 0.0008936 test: loss0.0001045, mae:0.0073973
training loss 1.546212047287554e-06 mae 0.0008740806952118874
training loss 1.553888034331798e-06 mae 0.000868354976524179
training loss 1.6087140939852907e-06 mae 0.0008787581423933779
training loss 1.6191553183959115e-06 mae 0.0008812634507965127
training loss 1.6468680843981522e-06 mae 0.0008859299534505506
Epoch 613, training: loss: 0.0000017, mae: 0.0008875 test: loss0.0001041, mae:0.0073850
training loss 2.2760339106753236e-06 mae 0.0009864106541499496
training loss 1.5553439759648477e-06 mae 0.0008718320899003861
training loss 1.6409213218490677e-06 mae 0.000885133122642626
training loss 1.6788844973144175e-06 mae 0.0008947935247553251
training loss 1.6564809496217164e-06 mae 0.0008884136778168117
Epoch 614, training: loss: 0.0000017, mae: 0.0008888 test: loss0.0001040, mae:0.0073779
training loss 8.386633112422714e-07 mae 0.0007064773817546666
training loss 1.6700416067043776e-06 mae 0.0008776286244848926
training loss 1.6268741660496748e-06 mae 0.0008755904532471062
training loss 1.6441143797213509e-06 mae 0.0008832062625431063
training loss 1.654979087106767e-06 mae 0.0008884675329347227
Epoch 615, training: loss: 0.0000017, mae: 0.0008887 test: loss0.0001043, mae:0.0073866
training loss 1.073635985449073e-06 mae 0.0007370843668468297
training loss 1.6214673147866658e-06 mae 0.0008803299427324649
training loss 1.6384661076960705e-06 mae 0.0008875778824376812
training loss 1.6048064388249999e-06 mae 0.0008775328705395689
training loss 1.6384399548009407e-06 mae 0.0008824265316425274
Epoch 616, training: loss: 0.0000016, mae: 0.0008848 test: loss0.0001037, mae:0.0073811
training loss 1.276026068808278e-06 mae 0.0007780312444083393
training loss 1.716746210461804e-06 mae 0.0008970810483921978
training loss 1.6583090021611337e-06 mae 0.0008851177022229919
training loss 1.676965761262844e-06 mae 0.0008888898297083499
training loss 1.6600325960016969e-06 mae 0.0008877726921589297
Epoch 617, training: loss: 0.0000017, mae: 0.0008872 test: loss0.0001043, mae:0.0073966
training loss 2.1778096197522245e-06 mae 0.0010177589720115066
training loss 1.6126256219186143e-06 mae 0.0008692556373574133
training loss 1.6491935504277674e-06 mae 0.0008800910234838577
training loss 1.6348883382276556e-06 mae 0.0008782341591366227
training loss 1.6579044236480413e-06 mae 0.0008853559775523544
Epoch 618, training: loss: 0.0000017, mae: 0.0008867 test: loss0.0001039, mae:0.0073857
training loss 1.096205551220919e-06 mae 0.0007492983713746071
training loss 1.6038802674997635e-06 mae 0.0008685185663018594
training loss 1.6427937570108838e-06 mae 0.0008805865585564238
training loss 1.6236846878405386e-06 mae 0.0008774862180630498
training loss 1.6324628633688622e-06 mae 0.0008830886846176576
Epoch 619, training: loss: 0.0000016, mae: 0.0008864 test: loss0.0001055, mae:0.0074314
training loss 1.903546603898576e-06 mae 0.0008965746383182704
training loss 1.6270895702844287e-06 mae 0.0008710673165635444
training loss 1.6350788061149299e-06 mae 0.0008870864968221303
training loss 1.6365713815129674e-06 mae 0.0008897129484816201
training loss 1.655720401079937e-06 mae 0.000885731992091687
Epoch 620, training: loss: 0.0000016, mae: 0.0008834 test: loss0.0001037, mae:0.0073735
training loss 1.6340527508873492e-06 mae 0.0008474346250295639
training loss 1.7095586176141044e-06 mae 0.000896302340597864
training loss 1.6356805769988781e-06 mae 0.0008823252628824792
training loss 1.620310965053014e-06 mae 0.0008769675654464012
training loss 1.6494289700664068e-06 mae 0.0008865730218422501
Epoch 621, training: loss: 0.0000016, mae: 0.0008845 test: loss0.0001044, mae:0.0074011
training loss 1.3784920156467706e-06 mae 0.0008780807256698608
training loss 1.572999218635731e-06 mae 0.0008508320783684946
training loss 1.6061235975606597e-06 mae 0.0008686374795696891
training loss 1.6298632241515158e-06 mae 0.0008689708327751187
training loss 1.6397687375895505e-06 mae 0.0008802267002284673
Epoch 622, training: loss: 0.0000016, mae: 0.0008792 test: loss0.0001049, mae:0.0074150
training loss 2.1831447156728245e-06 mae 0.0010330419754609466
training loss 1.7087120619519328e-06 mae 0.0008641914475946593
training loss 1.7072765503468363e-06 mae 0.0008797254476760798
training loss 1.6572785340266164e-06 mae 0.0008812001971867152
training loss 1.6436945601576303e-06 mae 0.0008832281710237112
Epoch 623, training: loss: 0.0000016, mae: 0.0008842 test: loss0.0001036, mae:0.0073721
training loss 1.4729915847055963e-06 mae 0.0008928391034714878
training loss 1.6488265014588393e-06 mae 0.0008822857041605838
training loss 1.6761866558595954e-06 mae 0.0008863634879396545
training loss 1.645437274774602e-06 mae 0.0008842636050890811
training loss 1.6457609420181656e-06 mae 0.0008873830379139803
Epoch 624, training: loss: 0.0000016, mae: 0.0008861 test: loss0.0001048, mae:0.0074057
training loss 8.077067832346074e-07 mae 0.0007163845002651215
training loss 1.5918505125994797e-06 mae 0.0008703141185619371
training loss 1.607018606758327e-06 mae 0.0008782133256029888
training loss 1.615572915512373e-06 mae 0.0008821368131891432
training loss 1.6265175143199657e-06 mae 0.000882214510382445
Epoch 625, training: loss: 0.0000016, mae: 0.0008835 test: loss0.0001042, mae:0.0073901
training loss 1.8725244217421277e-06 mae 0.0009174716542474926
training loss 1.5537688225366788e-06 mae 0.0008586242865752795
training loss 1.5960937794756823e-06 mae 0.0008709845916912107
training loss 1.6324893008850692e-06 mae 0.0008792318889427164
training loss 1.627907415661581e-06 mae 0.0008813293618312225
Epoch 626, training: loss: 0.0000016, mae: 0.0008815 test: loss0.0001041, mae:0.0073939
training loss 1.5355184359577834e-06 mae 0.0008673711563460529
training loss 1.6043464225446105e-06 mae 0.0008838672348407701
training loss 1.635829173575066e-06 mae 0.0008872700826320247
training loss 1.652461578291666e-06 mae 0.0008829805764079789
training loss 1.650718226882694e-06 mae 0.000886432582973526
Epoch 627, training: loss: 0.0000016, mae: 0.0008841 test: loss0.0001040, mae:0.0073791
training loss 1.3868296946384362e-06 mae 0.000807236589025706
training loss 1.537635075335714e-06 mae 0.0008504446481774541
training loss 1.5983916544221102e-06 mae 0.0008608420756368867
training loss 1.6134979240548354e-06 mae 0.0008719919194677067
training loss 1.6238382854612944e-06 mae 0.0008791342689841057
Epoch 628, training: loss: 0.0000016, mae: 0.0008803 test: loss0.0001044, mae:0.0074127
training loss 9.7803751941683e-07 mae 0.0007381392642855644
training loss 1.6391572497762834e-06 mae 0.0008701795207646984
training loss 1.628281774002863e-06 mae 0.0008747234265089475
training loss 1.6296143192430584e-06 mae 0.0008810614370617593
training loss 1.6323235124345327e-06 mae 0.0008824519628881293
Epoch 629, training: loss: 0.0000016, mae: 0.0008822 test: loss0.0001040, mae:0.0073860
training loss 1.7766124074114487e-06 mae 0.0009740479290485382
training loss 1.5867453438498638e-06 mae 0.0008644952082677798
training loss 1.5098236071980415e-06 mae 0.0008523948842170879
training loss 1.5658867132575521e-06 mae 0.000861385461353136
training loss 1.6147242328331609e-06 mae 0.0008779188623276561
Epoch 630, training: loss: 0.0000016, mae: 0.0008807 test: loss0.0001061, mae:0.0074180
training loss 1.0348820751460153e-06 mae 0.0007089304854162037
training loss 1.6164394185348187e-06 mae 0.000864022980928056
training loss 1.6099736442491596e-06 mae 0.0008697962629871611
training loss 1.6275616230166706e-06 mae 0.0008774829566993729
training loss 1.6227441411350197e-06 mae 0.000877536227318472
Epoch 631, training: loss: 0.0000016, mae: 0.0008775 test: loss0.0001050, mae:0.0074227
training loss 1.3593686389867798e-06 mae 0.0008486760780215263
training loss 1.5415110229856939e-06 mae 0.0008657013337748745
training loss 1.5020633048215411e-06 mae 0.0008457732037641106
training loss 1.5535848706048345e-06 mae 0.0008635072053767838
training loss 1.597286906070988e-06 mae 0.0008730897503274846
Epoch 632, training: loss: 0.0000016, mae: 0.0008786 test: loss0.0001045, mae:0.0074032
training loss 2.060789711322286e-06 mae 0.0009712359751574695
training loss 1.6532345659256592e-06 mae 0.0008740255063600545
training loss 1.64888322700844e-06 mae 0.0008774238637427221
training loss 1.6392376888938843e-06 mae 0.0008806450508723025
training loss 1.6260358176738793e-06 mae 0.0008809924056459292
Epoch 633, training: loss: 0.0000016, mae: 0.0008811 test: loss0.0001039, mae:0.0073715
training loss 1.1602024869716843e-06 mae 0.000752322084736079
training loss 1.6620727456475104e-06 mae 0.000875464042502583
training loss 1.6476935133870921e-06 mae 0.0008781983408763412
training loss 1.6325337667435845e-06 mae 0.0008751141541137893
training loss 1.636138558774178e-06 mae 0.0008809970922314035
Epoch 634, training: loss: 0.0000016, mae: 0.0008792 test: loss0.0001047, mae:0.0074091
training loss 1.3614547924589715e-06 mae 0.0009112786501646042
training loss 1.6542256331626514e-06 mae 0.0008821366126557773
training loss 1.6320443502072609e-06 mae 0.0008790948500829092
training loss 1.603886763128106e-06 mae 0.0008731482589820905
training loss 1.622690185842603e-06 mae 0.0008792852894843219
Epoch 635, training: loss: 0.0000016, mae: 0.0008788 test: loss0.0001041, mae:0.0073857
training loss 1.2411342140694614e-06 mae 0.0007774823461659253
training loss 1.6086090547961408e-06 mae 0.0008694908385365909
training loss 1.5757475309085454e-06 mae 0.0008717079244618588
training loss 1.5997865776830773e-06 mae 0.0008764070572812626
training loss 1.6172120249348384e-06 mae 0.0008818248929154702
Epoch 636, training: loss: 0.0000016, mae: 0.0008807 test: loss0.0001045, mae:0.0074049
training loss 2.88527189695742e-06 mae 0.0009881822625175118
training loss 1.5535965780609151e-06 mae 0.0008501042862988861
training loss 1.58365545226699e-06 mae 0.0008622575954690871
training loss 1.6166572778871006e-06 mae 0.0008692405485726944
training loss 1.6208186318307806e-06 mae 0.0008744100758583465
Epoch 637, training: loss: 0.0000016, mae: 0.0008737 test: loss0.0001038, mae:0.0073725
training loss 1.862712110778375e-06 mae 0.0009681719238869846
training loss 1.6014884714226642e-06 mae 0.0008726540208775915
training loss 1.5777805487286433e-06 mae 0.0008619048011415442
training loss 1.5482303722560791e-06 mae 0.0008656941620256773
training loss 1.5954625719720547e-06 mae 0.000874544081187793
Epoch 638, training: loss: 0.0000016, mae: 0.0008791 test: loss0.0001049, mae:0.0074003
training loss 1.8577297851152252e-06 mae 0.000996523885987699
training loss 1.5991598094361134e-06 mae 0.000870724404961163
training loss 1.5891664605591693e-06 mae 0.0008716505818474706
training loss 1.5978670695397693e-06 mae 0.0008732876293237833
training loss 1.6179732892512103e-06 mae 0.0008760690173616087
Epoch 639, training: loss: 0.0000016, mae: 0.0008769 test: loss0.0001045, mae:0.0074044
training loss 1.3684360737897805e-06 mae 0.0007944392855279148
training loss 1.6813780201332785e-06 mae 0.0008876927530246915
training loss 1.6757736788538674e-06 mae 0.000889703682692961
training loss 1.6529707138774538e-06 mae 0.0008902858818622167
training loss 1.6180401696341443e-06 mae 0.0008792382620381589
Epoch 640, training: loss: 0.0000016, mae: 0.0008767 test: loss0.0001046, mae:0.0074100
training loss 1.3609115967483376e-06 mae 0.0008377904887311161
training loss 1.5260468455612594e-06 mae 0.000862337452560371
training loss 1.562404233959344e-06 mae 0.0008597434014266376
training loss 1.58621814808907e-06 mae 0.0008661551636162625
training loss 1.5928949233509701e-06 mae 0.0008695593166897137
Epoch 641, training: loss: 0.0000016, mae: 0.0008750 test: loss0.0001044, mae:0.0074015
training loss 1.2124344266339904e-06 mae 0.0007324491743929684
training loss 1.5578084892641683e-06 mae 0.0008478066427450552
training loss 1.536578586543364e-06 mae 0.0008570384216900584
training loss 1.5414616890762736e-06 mae 0.0008576392972477969
training loss 1.594064757517488e-06 mae 0.0008722621207913177
Epoch 642, training: loss: 0.0000016, mae: 0.0008752 test: loss0.0001045, mae:0.0074080
training loss 1.332701685896609e-06 mae 0.0008187986095435917
training loss 1.5519085402903487e-06 mae 0.0008498524069128667
training loss 1.6056738423001227e-06 mae 0.0008662250129238584
training loss 1.5859988548373734e-06 mae 0.0008645719395247316
training loss 1.6029160332011398e-06 mae 0.0008715495371041976
Epoch 643, training: loss: 0.0000016, mae: 0.0008732 test: loss0.0001047, mae:0.0074138
training loss 1.0016810847446322e-06 mae 0.0007052247528918087
training loss 1.6129181938809141e-06 mae 0.0008705541555422777
training loss 1.6459818398559074e-06 mae 0.000881435477525217
training loss 1.5985037952612177e-06 mae 0.0008729142144581441
training loss 1.6058856830052573e-06 mae 0.000875474811260547
Epoch 644, training: loss: 0.0000016, mae: 0.0008744 test: loss0.0001044, mae:0.0073958
training loss 1.1466128171377932e-06 mae 0.0008208658546209335
training loss 1.4660270866728506e-06 mae 0.0008488337876841279
training loss 1.5878279291788235e-06 mae 0.000871862715921632
training loss 1.5595093420554189e-06 mae 0.0008664948222788654
training loss 1.6076762357108097e-06 mae 0.0008776048125944733
Epoch 645, training: loss: 0.0000016, mae: 0.0008771 test: loss0.0001044, mae:0.0074039
training loss 1.3799864291286212e-06 mae 0.0007760925218462944
training loss 1.5873854687354241e-06 mae 0.0008614259388516929
training loss 1.6001663442297611e-06 mae 0.0008768270144902977
training loss 1.6294287247275852e-06 mae 0.0008820238479240394
training loss 1.5965161904757195e-06 mae 0.000874271577824166
Epoch 646, training: loss: 0.0000016, mae: 0.0008757 test: loss0.0001045, mae:0.0074014
training loss 1.3149356163921766e-06 mae 0.0007629130850546062
training loss 1.5312517923442046e-06 mae 0.0008521106587175061
training loss 1.597532472336896e-06 mae 0.0008689739302234115
training loss 1.6086944251522958e-06 mae 0.0008709776945356651
training loss 1.583351788316966e-06 mae 0.0008699845560868062
Epoch 647, training: loss: 0.0000016, mae: 0.0008727 test: loss0.0001041, mae:0.0073929
training loss 1.2304933534323936e-06 mae 0.0008441709796898067
training loss 1.5273436523713436e-06 mae 0.0008602237098338995
training loss 1.5795406140190758e-06 mae 0.000866722212966052
training loss 1.616538039593102e-06 mae 0.0008773247176312551
training loss 1.5966402539240123e-06 mae 0.000872458362273075
Epoch 648, training: loss: 0.0000016, mae: 0.0008761 test: loss0.0001045, mae:0.0074068
training loss 1.0902396070378018e-06 mae 0.0008101898129098117
training loss 1.641945875333334e-06 mae 0.0008786267226598427
training loss 1.6061910965879295e-06 mae 0.0008741959901803201
training loss 1.5862880756583231e-06 mae 0.0008692762726572468
training loss 1.5930351314614306e-06 mae 0.0008733018245826014
Epoch 649, training: loss: 0.0000016, mae: 0.0008737 test: loss0.0001050, mae:0.0074189
training loss 1.576212184772885e-06 mae 0.0009733019396662712
training loss 1.5324672595656296e-06 mae 0.0008525952660277778
training loss 1.6001266928496717e-06 mae 0.0008694851658039608
training loss 1.5792019667813125e-06 mae 0.0008686207969586632
training loss 1.5881245139125761e-06 mae 0.0008700099589525546
Epoch 650, training: loss: 0.0000016, mae: 0.0008705 test: loss0.0001048, mae:0.0074194
training loss 1.1463729379102006e-06 mae 0.0006837447290308774
training loss 1.657322014562298e-06 mae 0.0008790697035032744
training loss 1.5918407655698322e-06 mae 0.0008643827329638716
training loss 1.6146988402836934e-06 mae 0.0008710870166179656
training loss 1.5865801460948651e-06 mae 0.0008692915880918243
Epoch 651, training: loss: 0.0000016, mae: 0.0008685 test: loss0.0001051, mae:0.0074174
training loss 2.0700354070868343e-06 mae 0.0008742365171201527
training loss 1.566509926011777e-06 mae 0.0008653971633654745
training loss 1.558425835675267e-06 mae 0.0008665595617431153
training loss 1.5846038992690984e-06 mae 0.0008688985084072496
training loss 1.5816549811556329e-06 mae 0.0008671175722097653
Epoch 652, training: loss: 0.0000016, mae: 0.0008688 test: loss0.0001044, mae:0.0073954
training loss 1.2460568541428074e-06 mae 0.0007886377279646695
training loss 1.5394012995570582e-06 mae 0.0008528690874193084
training loss 1.5710300038398878e-06 mae 0.0008622917380082505
training loss 1.5569799269909685e-06 mae 0.0008633145114209579
training loss 1.5864794218190702e-06 mae 0.0008668715866347451
Epoch 653, training: loss: 0.0000016, mae: 0.0008674 test: loss0.0001049, mae:0.0074189
training loss 5.175501769372204e-07 mae 0.0005210954695940018
training loss 1.541660092065214e-06 mae 0.000864967287235035
training loss 1.5342337836964274e-06 mae 0.0008567308927517349
training loss 1.5783894032843238e-06 mae 0.000864388848676513
training loss 1.595567236260346e-06 mae 0.0008691823395520488
Epoch 654, training: loss: 0.0000016, mae: 0.0008678 test: loss0.0001049, mae:0.0074215
training loss 1.4152150242807693e-06 mae 0.0008100534905679524
training loss 1.4476183332144321e-06 mae 0.0008280717712991378
training loss 1.5585795414054785e-06 mae 0.0008551823107830002
training loss 1.5788693281288546e-06 mae 0.0008647145398133836
training loss 1.5849139215412895e-06 mae 0.0008663144747411548
Epoch 655, training: loss: 0.0000016, mae: 0.0008668 test: loss0.0001048, mae:0.0074124
training loss 1.840759978222195e-06 mae 0.0009638375486247241
training loss 1.383086195498133e-06 mae 0.000813234102108753
training loss 1.4983692930364127e-06 mae 0.0008410170465698557
training loss 1.5186284082384553e-06 mae 0.0008553105155129404
training loss 1.5749638764284098e-06 mae 0.0008668146065473371
Epoch 656, training: loss: 0.0000016, mae: 0.0008661 test: loss0.0001045, mae:0.0074023
training loss 1.3164271877030842e-06 mae 0.0008429484441876411
training loss 1.7065833015508626e-06 mae 0.0009004185877868211
training loss 1.6093703107339982e-06 mae 0.0008779611809658681
training loss 1.5886744647755445e-06 mae 0.0008670691757721963
training loss 1.583331646233278e-06 mae 0.0008706784435311581
Epoch 657, training: loss: 0.0000016, mae: 0.0008706 test: loss0.0001051, mae:0.0074211
training loss 1.3196580539442948e-06 mae 0.0007695695385336876
training loss 1.5398961917365823e-06 mae 0.0008581368978518774
training loss 1.5305319916793938e-06 mae 0.0008520309549369064
training loss 1.5617659888924376e-06 mae 0.0008585080084242914
training loss 1.5782223640463376e-06 mae 0.0008655862349307916
Epoch 658, training: loss: 0.0000016, mae: 0.0008660 test: loss0.0001044, mae:0.0073974
training loss 1.2383919738567783e-06 mae 0.0007865180377848446
training loss 1.5058544704515955e-06 mae 0.0008429101298508397
training loss 1.4978759022912103e-06 mae 0.0008423311150595911
training loss 1.5213596682601354e-06 mae 0.0008495422249770912
training loss 1.5578394667482009e-06 mae 0.0008604316766586948
Epoch 659, training: loss: 0.0000016, mae: 0.0008610 test: loss0.0001047, mae:0.0074109
training loss 8.929543469093915e-07 mae 0.000669111788738519
training loss 1.5323823511019273e-06 mae 0.0008460343324615822
training loss 1.575332887996681e-06 mae 0.0008550380554861655
training loss 1.5667527069331456e-06 mae 0.0008606354020603787
training loss 1.5798709196291506e-06 mae 0.0008655414836648014
Epoch 660, training: loss: 0.0000016, mae: 0.0008629 test: loss0.0001049, mae:0.0074194
training loss 1.4120404330242309e-06 mae 0.0007818884332664311
training loss 1.4692760837355238e-06 mae 0.0008365793790503898
training loss 1.5182590281670355e-06 mae 0.0008466505165451603
training loss 1.5585429312806144e-06 mae 0.0008582088711597521
training loss 1.5534035242691876e-06 mae 0.0008589733506098799
Epoch 661, training: loss: 0.0000016, mae: 0.0008601 test: loss0.0001049, mae:0.0074162
training loss 9.458235581405461e-07 mae 0.0006735877250321209
training loss 1.5566299123070077e-06 mae 0.0008518527451373054
training loss 1.586465533130262e-06 mae 0.000865608782249291
training loss 1.5693080354441123e-06 mae 0.0008653535797501244
training loss 1.575234719765112e-06 mae 0.0008659287725357495
Epoch 662, training: loss: 0.0000016, mae: 0.0008650 test: loss0.0001055, mae:0.0074263
training loss 7.065636964398436e-07 mae 0.0006398043478839099
training loss 1.5508508247020312e-06 mae 0.0008469398621944529
training loss 1.5575653214279496e-06 mae 0.0008560080285239415
training loss 1.5695655560832157e-06 mae 0.0008591244263969214
training loss 1.5600040593307675e-06 mae 0.0008607806075498723
Epoch 663, training: loss: 0.0000016, mae: 0.0008603 test: loss0.0001048, mae:0.0074086
training loss 1.2624857390619582e-06 mae 0.0008271231199614704
training loss 1.4323240262170905e-06 mae 0.0008287423840431754
training loss 1.5546943381880258e-06 mae 0.0008573457598686218
training loss 1.544459679500252e-06 mae 0.000859654338148404
training loss 1.5624428569302365e-06 mae 0.0008676532057660691
Epoch 664, training: loss: 0.0000016, mae: 0.0008643 test: loss0.0001045, mae:0.0074060
training loss 1.5275454643415287e-06 mae 0.0008797381888143718
training loss 1.4263457202622368e-06 mae 0.0008369179056756489
training loss 1.5266325612858484e-06 mae 0.000852432160748925
training loss 1.5306584380849147e-06 mae 0.0008542193431370206
training loss 1.5590089788020119e-06 mae 0.000863019239437762
Epoch 665, training: loss: 0.0000016, mae: 0.0008648 test: loss0.0001043, mae:0.0073911
training loss 1.4827252243776456e-06 mae 0.0008185894112102687
training loss 1.4831973312628214e-06 mae 0.0008502333532270117
training loss 1.4890398269566467e-06 mae 0.0008430274855806538
training loss 1.506699771616936e-06 mae 0.000851319657965557
training loss 1.545415725853111e-06 mae 0.0008629325772079627
Epoch 666, training: loss: 0.0000016, mae: 0.0008684 test: loss0.0001053, mae:0.0074190
training loss 1.9070588450631476e-06 mae 0.000879218801856041
training loss 1.4411933360700946e-06 mae 0.0008449313372337062
training loss 1.4757438884546427e-06 mae 0.0008500247656616686
training loss 1.5302668166682365e-06 mae 0.0008570614509093297
training loss 1.5479545534468182e-06 mae 0.0008608969878312665
Epoch 667, training: loss: 0.0000016, mae: 0.0008641 test: loss0.0001052, mae:0.0074325
training loss 2.1616112917399732e-06 mae 0.0010234126821160316
training loss 1.4327078875955037e-06 mae 0.0008322512719542814
training loss 1.4996007331040407e-06 mae 0.0008424038475490001
training loss 1.5072228018423943e-06 mae 0.0008490083736077642
training loss 1.5600295560079525e-06 mae 0.0008611226079647256
Epoch 668, training: loss: 0.0000016, mae: 0.0008615 test: loss0.0001055, mae:0.0074445
training loss 9.925062158799847e-07 mae 0.0006509839440695941
training loss 1.411156960347951e-06 mae 0.0008062967418820834
training loss 1.4718344153485609e-06 mae 0.0008333939344317902
training loss 1.5190230909179301e-06 mae 0.000849934398128793
training loss 1.5515927392657643e-06 mae 0.0008595765571791066
Epoch 669, training: loss: 0.0000016, mae: 0.0008615 test: loss0.0001055, mae:0.0074266
training loss 1.1103862789241248e-06 mae 0.0007155320490710437
training loss 1.6206140705488744e-06 mae 0.0008663581067915346
training loss 1.550763806002272e-06 mae 0.0008509285911677939
training loss 1.5834860967768827e-06 mae 0.0008638462915998073
training loss 1.561368678925538e-06 mae 0.0008607822191201854
Epoch 670, training: loss: 0.0000016, mae: 0.0008593 test: loss0.0001050, mae:0.0074246
training loss 1.1126618346679606e-06 mae 0.0007495020399801433
training loss 1.6395427350931533e-06 mae 0.0008790802436552064
training loss 1.6195584450569823e-06 mae 0.0008690322450786314
training loss 1.5551788522990792e-06 mae 0.0008592255246233413
training loss 1.5459130472279062e-06 mae 0.0008561734691614383
Epoch 671, training: loss: 0.0000015, mae: 0.0008560 test: loss0.0001056, mae:0.0074440
training loss 1.0520234354771674e-06 mae 0.000752015970647335
training loss 1.3604907871476646e-06 mae 0.0008216855965335582
training loss 1.5066995987050035e-06 mae 0.000856005282390236
training loss 1.534482984145192e-06 mae 0.0008559208694808431
training loss 1.5594329201307108e-06 mae 0.0008626794926150335
Epoch 672, training: loss: 0.0000016, mae: 0.0008613 test: loss0.0001077, mae:0.0074509
training loss 1.2116644256821019e-06 mae 0.0007931996951811016
training loss 1.6228217385319371e-06 mae 0.0008646967779735431
training loss 1.6278052803860223e-06 mae 0.0008700999118950184
training loss 1.5886735620568776e-06 mae 0.0008629675135818244
training loss 1.5580527525882674e-06 mae 0.0008582848159536089
Epoch 673, training: loss: 0.0000016, mae: 0.0008576 test: loss0.0001052, mae:0.0074324
training loss 1.250205400538107e-06 mae 0.0008456424693576992
training loss 1.5473604325872626e-06 mae 0.0008475892713713441
training loss 1.5233827464879424e-06 mae 0.0008502719908869724
training loss 1.5345520978371496e-06 mae 0.0008561053307147217
training loss 1.5418082083312557e-06 mae 0.0008570594305927817
Epoch 674, training: loss: 0.0000015, mae: 0.0008577 test: loss0.0001049, mae:0.0074182
training loss 1.8124977714251145e-06 mae 0.0009045724873431027
training loss 1.4709080248537123e-06 mae 0.0008245528077560605
training loss 1.5053288016493789e-06 mae 0.0008432361283477877
training loss 1.5645361456233702e-06 mae 0.000854199191721909
training loss 1.5463354585919456e-06 mae 0.0008542723727843436
Epoch 675, training: loss: 0.0000015, mae: 0.0008543 test: loss0.0001053, mae:0.0074372
training loss 1.2541721616798895e-06 mae 0.0008309222757816315
training loss 1.503524680513282e-06 mae 0.0008398171048611403
training loss 1.5006347261492411e-06 mae 0.000849609135876013
training loss 1.5629582167364798e-06 mae 0.0008569325930253905
training loss 1.5568871194614867e-06 mae 0.0008580827044753996
Epoch 676, training: loss: 0.0000015, mae: 0.0008553 test: loss0.0001061, mae:0.0074538
training loss 1.3566472034653998e-06 mae 0.0007614524220116436
training loss 1.4457433630745004e-06 mae 0.0008312097177658156
training loss 1.50238312221743e-06 mae 0.0008382795356665876
training loss 1.5276545513796804e-06 mae 0.0008404984104983657
training loss 1.5325114643485393e-06 mae 0.0008495280878799063
Epoch 677, training: loss: 0.0000015, mae: 0.0008513 test: loss0.0001056, mae:0.0074414
training loss 3.9100218600651715e-06 mae 0.0013404259225353599
training loss 1.5791134294030439e-06 mae 0.0008609118316706051
training loss 1.5584031489602732e-06 mae 0.000852372738795521
training loss 1.5588200655343489e-06 mae 0.0008518507658651973
training loss 1.5476408363149433e-06 mae 0.0008570235352893707
Epoch 678, training: loss: 0.0000015, mae: 0.0008564 test: loss0.0001059, mae:0.0074552
training loss 1.208970729749126e-06 mae 0.0008433647453784943
training loss 1.5132589398900544e-06 mae 0.0008531508946736508
training loss 1.5294207444883183e-06 mae 0.0008531021476391299
training loss 1.5270955257180094e-06 mae 0.0008508070666552677
training loss 1.5345245023369137e-06 mae 0.0008550271874326121
Epoch 679, training: loss: 0.0000015, mae: 0.0008558 test: loss0.0001054, mae:0.0074291
training loss 1.1701661151164444e-06 mae 0.0008064921130426228
training loss 1.4044045005279007e-06 mae 0.0008281614487607251
training loss 1.5225163289671557e-06 mae 0.0008465325809083877
training loss 1.5089473517404233e-06 mae 0.0008451819897086561
training loss 1.5301337652230565e-06 mae 0.0008517217110559824
Epoch 680, training: loss: 0.0000015, mae: 0.0008544 test: loss0.0001057, mae:0.0074450
training loss 5.466694688038842e-07 mae 0.000544331967830658
training loss 1.4958393666900375e-06 mae 0.0008362558905455265
training loss 1.584873262295827e-06 mae 0.0008533140494589612
training loss 1.5961961305146543e-06 mae 0.000865411750790324
training loss 1.5417113245212694e-06 mae 0.0008526909024922633
Epoch 681, training: loss: 0.0000015, mae: 0.0008521 test: loss0.0001058, mae:0.0074282
training loss 1.8036280380329117e-06 mae 0.0009450297802686691
training loss 1.4963552196016228e-06 mae 0.0008418275301308172
training loss 1.4305452615496177e-06 mae 0.0008274194551631808
training loss 1.5162953717518635e-06 mae 0.0008453963187967261
training loss 1.53366978701573e-06 mae 0.0008517520966133412
Epoch 682, training: loss: 0.0000015, mae: 0.0008536 test: loss0.0001051, mae:0.0074244
training loss 2.109871957145515e-06 mae 0.0009580341284163296
training loss 1.534438104490797e-06 mae 0.0008498265433088674
training loss 1.486033146656974e-06 mae 0.0008449604709420611
training loss 1.5466289610140545e-06 mae 0.000851583790453459
training loss 1.5347823586492258e-06 mae 0.0008531550489324352
Epoch 683, training: loss: 0.0000015, mae: 0.0008529 test: loss0.0001054, mae:0.0074385
training loss 1.0089256647916045e-06 mae 0.0006981100887060165
training loss 1.5250246136412326e-06 mae 0.0008508914798571197
training loss 1.4616772218552326e-06 mae 0.0008287363279942829
training loss 1.4828215886540274e-06 mae 0.0008404466257251752
training loss 1.5182691231869402e-06 mae 0.0008498768299581735
Epoch 684, training: loss: 0.0000015, mae: 0.0008521 test: loss0.0001047, mae:0.0073999
training loss 1.3690428204427008e-06 mae 0.0008006980642676353
training loss 1.5476035273690395e-06 mae 0.0008559634842836828
training loss 1.5424101340302892e-06 mae 0.0008531021188234562
training loss 1.5242989042346847e-06 mae 0.0008483675589155875
training loss 1.5352881243825549e-06 mae 0.0008527264665043102
Epoch 685, training: loss: 0.0000015, mae: 0.0008525 test: loss0.0001054, mae:0.0074338
training loss 2.2416536467062542e-06 mae 0.0009972304105758667
training loss 1.5120319490586768e-06 mae 0.000839802526695398
training loss 1.5182333760265577e-06 mae 0.0008489171931713743
training loss 1.521517005949738e-06 mae 0.0008540564707776419
training loss 1.5189124767188044e-06 mae 0.0008515844828754993
Epoch 686, training: loss: 0.0000015, mae: 0.0008546 test: loss0.0001054, mae:0.0074291
training loss 1.5691481394242146e-06 mae 0.0008756046299822628
training loss 1.4863476810303696e-06 mae 0.000835568999058987
training loss 1.493560044608974e-06 mae 0.000835173013550921
training loss 1.4830258001420712e-06 mae 0.0008404245565411025
training loss 1.5246309971951918e-06 mae 0.0008524055158330214
Epoch 687, training: loss: 0.0000015, mae: 0.0008542 test: loss0.0001053, mae:0.0074325
training loss 1.3947928891866468e-06 mae 0.0009316199575550854
training loss 1.3741839543578021e-06 mae 0.000823346254251459
training loss 1.4655705289181128e-06 mae 0.0008408858920309211
training loss 1.467583077579993e-06 mae 0.0008399502234687623
training loss 1.5148236193069454e-06 mae 0.0008526569416524095
Epoch 688, training: loss: 0.0000015, mae: 0.0008526 test: loss0.0001056, mae:0.0074414
training loss 6.960651148801844e-07 mae 0.0006714134360663593
training loss 1.466279737836457e-06 mae 0.0008307629351631977
training loss 1.4470716773937985e-06 mae 0.0008344729160106196
training loss 1.4900843559599103e-06 mae 0.0008415481690872484
training loss 1.5223065231645765e-06 mae 0.0008514624028767815
Epoch 689, training: loss: 0.0000015, mae: 0.0008525 test: loss0.0001055, mae:0.0074339
training loss 7.254039360304887e-07 mae 0.0005952166393399239
training loss 1.5486546078814262e-06 mae 0.0008469117775687253
training loss 1.5071956194432382e-06 mae 0.0008442574261346519
training loss 1.5133513170741146e-06 mae 0.0008468632054336321
training loss 1.523129386516384e-06 mae 0.000849809649919359
Epoch 690, training: loss: 0.0000015, mae: 0.0008485 test: loss0.0001061, mae:0.0074553
training loss 1.683794948803552e-06 mae 0.0008420779486186802
training loss 1.400472437277737e-06 mae 0.0008085231221390557
training loss 1.4742045996262289e-06 mae 0.000832275840186394
training loss 1.5195779297418214e-06 mae 0.0008448646870281371
training loss 1.5171446441298089e-06 mae 0.0008458899411104095
Epoch 691, training: loss: 0.0000015, mae: 0.0008448 test: loss0.0001051, mae:0.0074254
training loss 2.1383184503065422e-06 mae 0.0010536942863836884
training loss 1.4379314316503204e-06 mae 0.0008301844286239323
training loss 1.4533843751554618e-06 mae 0.0008363053761653163
training loss 1.4906973446601799e-06 mae 0.0008459929160971124
training loss 1.5195752219163874e-06 mae 0.0008498382840342637
Epoch 692, training: loss: 0.0000015, mae: 0.0008504 test: loss0.0001072, mae:0.0074703
training loss 1.1078916486439994e-06 mae 0.0007773088291287422
training loss 1.4025619277929884e-06 mae 0.0008273987857369231
training loss 1.4750681997395417e-06 mae 0.0008333896230306879
training loss 1.5031751208940687e-06 mae 0.000843533441973295
training loss 1.5138797388616112e-06 mae 0.0008456558371379751
Epoch 693, training: loss: 0.0000015, mae: 0.0008458 test: loss0.0001056, mae:0.0074320
training loss 1.7734895436660736e-06 mae 0.0009085086057893932
training loss 1.5484296127129603e-06 mae 0.0008551423727790367
training loss 1.5354950631821213e-06 mae 0.0008465705063691968
training loss 1.5072170760913951e-06 mae 0.0008419402235107408
training loss 1.5096540200309266e-06 mae 0.0008448917402501276
Epoch 694, training: loss: 0.0000015, mae: 0.0008451 test: loss0.0001051, mae:0.0074149
training loss 1.983838956221007e-06 mae 0.0008768551051616669
training loss 1.5113512245971279e-06 mae 0.0008479149044290476
training loss 1.4671104120697965e-06 mae 0.0008400510621976367
training loss 1.5386572971728796e-06 mae 0.0008535615021258051
training loss 1.5125197281086181e-06 mae 0.0008469024570010119
Epoch 695, training: loss: 0.0000015, mae: 0.0008477 test: loss0.0001055, mae:0.0074376
training loss 1.8731305999608594e-06 mae 0.000913349271286279
training loss 1.5367733770325885e-06 mae 0.0008474528508753901
training loss 1.5216538927369142e-06 mae 0.0008500067270499201
training loss 1.4893767033920189e-06 mae 0.000839423219787438
training loss 1.51494249148052e-06 mae 0.0008458424952061874
Epoch 696, training: loss: 0.0000015, mae: 0.0008461 test: loss0.0001056, mae:0.0074392
training loss 1.4775691852264572e-06 mae 0.0007750426302663982
training loss 1.4717725965359327e-06 mae 0.0008272498334739722
training loss 1.4686492754167782e-06 mae 0.0008345733830096697
training loss 1.5061209983256865e-06 mae 0.0008492981571710315
training loss 1.4958000241986883e-06 mae 0.000847917696488882
Epoch 697, training: loss: 0.0000015, mae: 0.0008482 test: loss0.0001060, mae:0.0074590
training loss 1.8226137399324216e-06 mae 0.0009053892572410405
training loss 1.5140455391202505e-06 mae 0.0008312879819605571
training loss 1.4952076615165673e-06 mae 0.0008393713569333132
training loss 1.5342005209150485e-06 mae 0.0008492749774156273
training loss 1.5176826950868305e-06 mae 0.0008503416447728796
Epoch 698, training: loss: 0.0000015, mae: 0.0008484 test: loss0.0001053, mae:0.0074269
training loss 2.3076402158039855e-06 mae 0.0011123331496492028
training loss 1.4735674364030774e-06 mae 0.0008371328060770883
training loss 1.4904281768097702e-06 mae 0.0008384050907438052
training loss 1.4991549401141828e-06 mae 0.0008464034581988655
training loss 1.4993092536518702e-06 mae 0.0008444090513396655
Epoch 699, training: loss: 0.0000015, mae: 0.0008457 test: loss0.0001053, mae:0.0074424
current learning rate: 3.90625e-06
training loss 1.7118827599915676e-06 mae 0.0008927344460971653
training loss 1.4771395019222694e-06 mae 0.000814930950725159
training loss 1.478443099606264e-06 mae 0.0008174510292897903
training loss 1.4565028880870708e-06 mae 0.0008133798831879523
training loss 1.4490152039883951e-06 mae 0.0008154748920154341
Epoch 700, training: loss: 0.0000015, mae: 0.0008175 test: loss0.0001063, mae:0.0074607
training loss 1.2337828820818686e-06 mae 0.0007468129624612629
training loss 1.3708095963795808e-06 mae 0.0007907434905349625
training loss 1.4413622675599996e-06 mae 0.0008092626380632713
training loss 1.4421819028240662e-06 mae 0.0008096864326727509
training loss 1.4439815642721825e-06 mae 0.0008127505596222083
Epoch 701, training: loss: 0.0000014, mae: 0.0008126 test: loss0.0001054, mae:0.0074321
training loss 1.0270767916154e-06 mae 0.000674096227157861
training loss 1.3342510296261736e-06 mae 0.0007799801755669145
training loss 1.3914389966214343e-06 mae 0.000804140846565421
training loss 1.3733386061624856e-06 mae 0.0007990961287260083
training loss 1.4353668221142988e-06 mae 0.0008132570266517449
Epoch 702, training: loss: 0.0000014, mae: 0.0008139 test: loss0.0001060, mae:0.0074476
training loss 1.3311764632817358e-06 mae 0.0008059681276790798
training loss 1.4545996223472788e-06 mae 0.0008167726998947854
training loss 1.4071123371082864e-06 mae 0.0008080195904119253
training loss 1.428806019959172e-06 mae 0.0008124682546550076
training loss 1.4488704645242003e-06 mae 0.0008158611567501001
Epoch 703, training: loss: 0.0000014, mae: 0.0008143 test: loss0.0001061, mae:0.0074486
training loss 9.343846727460914e-07 mae 0.0007358572329394519
training loss 1.4149742556208583e-06 mae 0.0008021372578143342
training loss 1.4665401949113778e-06 mae 0.0008259858823113954
training loss 1.4737704261266804e-06 mae 0.0008211038522545668
training loss 1.4504870806100842e-06 mae 0.0008155534652627039
Epoch 704, training: loss: 0.0000014, mae: 0.0008131 test: loss0.0001056, mae:0.0074400
training loss 1.2455066098482348e-06 mae 0.0008146579493768513
training loss 1.2950164106911741e-06 mae 0.0007711712843921108
training loss 1.378927644473511e-06 mae 0.0007962576479024003
training loss 1.4315633180311563e-06 mae 0.0008113024057049477
training loss 1.4405936509767834e-06 mae 0.0008146929553482889
Epoch 705, training: loss: 0.0000014, mae: 0.0008129 test: loss0.0001066, mae:0.0074703
training loss 1.4905172065482475e-06 mae 0.0008131098002195358
training loss 1.4024245706869076e-06 mae 0.0007982192579291616
training loss 1.4510736251746615e-06 mae 0.0008164251454884539
training loss 1.4287092999408636e-06 mae 0.0008098717107412884
training loss 1.4385339231898908e-06 mae 0.0008134353353957003
Epoch 706, training: loss: 0.0000014, mae: 0.0008139 test: loss0.0001056, mae:0.0074373
training loss 8.635960853098368e-07 mae 0.0006256603519432247
training loss 1.4829870551961103e-06 mae 0.0008271073765011832
training loss 1.4284154517689346e-06 mae 0.0008061393901779509
training loss 1.4124795081551572e-06 mae 0.0008052902886742687
training loss 1.4320342665317853e-06 mae 0.0008103699777831339
Epoch 707, training: loss: 0.0000014, mae: 0.0008131 test: loss0.0001057, mae:0.0074434
training loss 1.3868076393919182e-06 mae 0.0008671097457408905
training loss 1.3875279580811454e-06 mae 0.0007987659671069944
training loss 1.4865543918030306e-06 mae 0.0008201535749664103
training loss 1.4392196853564294e-06 mae 0.0008108143379375188
training loss 1.4416915420196494e-06 mae 0.0008103004787047741
Epoch 708, training: loss: 0.0000014, mae: 0.0008103 test: loss0.0001055, mae:0.0074295
training loss 7.671442290302366e-07 mae 0.0006451383233070374
training loss 1.3899376085646509e-06 mae 0.0008036845327154096
training loss 1.4539910973263152e-06 mae 0.0008188161421521097
training loss 1.4762106058700114e-06 mae 0.0008263032280513091
training loss 1.4375230886914204e-06 mae 0.000812763632020325
Epoch 709, training: loss: 0.0000014, mae: 0.0008119 test: loss0.0001056, mae:0.0074345
training loss 1.5821648275959888e-06 mae 0.0009803393622860312
training loss 1.3139215063935856e-06 mae 0.000791598435527846
training loss 1.4644461666558524e-06 mae 0.0008166531034016815
training loss 1.4475345639257651e-06 mae 0.0008133681231203011
training loss 1.4370551781298245e-06 mae 0.000813405817740629
Epoch 710, training: loss: 0.0000014, mae: 0.0008145 test: loss0.0001067, mae:0.0074688
training loss 9.968528047465952e-07 mae 0.0006324434652924538
training loss 1.5397096773335316e-06 mae 0.0008316519076698554
training loss 1.471488823401862e-06 mae 0.0008168952345921853
training loss 1.4281309996546184e-06 mae 0.0008120628487047379
training loss 1.4355693439035774e-06 mae 0.000812101748943403
Epoch 711, training: loss: 0.0000014, mae: 0.0008120 test: loss0.0001060, mae:0.0074576
training loss 1.6331445067407913e-06 mae 0.0008779112249612808
training loss 1.456705280914178e-06 mae 0.000807941184771777
training loss 1.4245430034267465e-06 mae 0.0007955338540809728
training loss 1.4230255588633685e-06 mae 0.0008052178436507848
training loss 1.4347098384638423e-06 mae 0.0008099357086692508
Epoch 712, training: loss: 0.0000014, mae: 0.0008103 test: loss0.0001059, mae:0.0074518
training loss 1.2998625606996939e-06 mae 0.0007765774498693645
training loss 1.4926328706714425e-06 mae 0.0008258520746512302
training loss 1.4159325686061565e-06 mae 0.0008053939719223369
training loss 1.3905537033248485e-06 mae 0.0008002607289097262
training loss 1.4287811064996586e-06 mae 0.0008120820469533402
Epoch 713, training: loss: 0.0000014, mae: 0.0008141 test: loss0.0001058, mae:0.0074507
training loss 1.9999918094981695e-06 mae 0.0008098579128272831
training loss 1.565629755826675e-06 mae 0.0008271272983584625
training loss 1.4616790470355034e-06 mae 0.00081632673477588
training loss 1.4410017654999263e-06 mae 0.0008152744565883784
training loss 1.4280019958949929e-06 mae 0.0008103312266844585
Epoch 714, training: loss: 0.0000014, mae: 0.0008119 test: loss0.0001060, mae:0.0074480
training loss 1.132146394411393e-06 mae 0.0007365855271928012
training loss 1.4309858162211156e-06 mae 0.0008150345540842882
training loss 1.4438355370963685e-06 mae 0.000815470588836118
training loss 1.4149269708203626e-06 mae 0.0008115497684115482
training loss 1.4422280591941734e-06 mae 0.0008134248415100856
Epoch 715, training: loss: 0.0000014, mae: 0.0008111 test: loss0.0001057, mae:0.0074343
training loss 1.6129421283039846e-06 mae 0.0008378814090974629
training loss 1.3454837478414397e-06 mae 0.0007831631182674682
training loss 1.3555088717922524e-06 mae 0.0007914685566983382
training loss 1.4105213995659765e-06 mae 0.0008076103007118681
training loss 1.4372105601808868e-06 mae 0.000813950949860505
Epoch 716, training: loss: 0.0000014, mae: 0.0008133 test: loss0.0001058, mae:0.0074442
training loss 1.089542820409406e-06 mae 0.000717582181096077
training loss 1.4736248337666276e-06 mae 0.0008198321253682176
training loss 1.4599279112270875e-06 mae 0.0008205659618361455
training loss 1.462062439846427e-06 mae 0.0008191136868411542
training loss 1.4297487880763194e-06 mae 0.0008083120059680693
Epoch 717, training: loss: 0.0000014, mae: 0.0008101 test: loss0.0001063, mae:0.0074683
training loss 1.760954546625726e-06 mae 0.0008410261943936348
training loss 1.4149561548929703e-06 mae 0.0008111449116019202
training loss 1.4184761441653556e-06 mae 0.0008051998989361496
training loss 1.4391750485912112e-06 mae 0.0008120469595552332
training loss 1.4357881090532985e-06 mae 0.0008114583690972787
Epoch 718, training: loss: 0.0000014, mae: 0.0008107 test: loss0.0001062, mae:0.0074649
training loss 8.209015049942536e-07 mae 0.0005603041499853134
training loss 1.3629022086937694e-06 mae 0.0007869881342676486
training loss 1.3961666027036227e-06 mae 0.000806652842836436
training loss 1.40667664472918e-06 mae 0.0008087010062700572
training loss 1.4208091290776293e-06 mae 0.0008088298135006163
Epoch 719, training: loss: 0.0000014, mae: 0.0008106 test: loss0.0001059, mae:0.0074489
training loss 1.8433624973113183e-06 mae 0.0008394538308493793
training loss 1.2768803128214953e-06 mae 0.0007514427712771529
training loss 1.3635868303891124e-06 mae 0.0007831299179556347
training loss 1.3958359595231527e-06 mae 0.0007957317292017999
training loss 1.414101190497133e-06 mae 0.0008056386152690102
Epoch 720, training: loss: 0.0000014, mae: 0.0008094 test: loss0.0001063, mae:0.0074638
training loss 9.353719292448659e-07 mae 0.0006961217150092125
training loss 1.3245415032667788e-06 mae 0.000781296314123799
training loss 1.3911959578002437e-06 mae 0.0007896724227345596
training loss 1.3866788640145075e-06 mae 0.0007950294699630892
training loss 1.4213694486560918e-06 mae 0.0008050586069141752
Epoch 721, training: loss: 0.0000014, mae: 0.0008066 test: loss0.0001059, mae:0.0074534
training loss 1.657861503190361e-06 mae 0.000797195069026202
training loss 1.4086053952536636e-06 mae 0.0008011633911462247
training loss 1.4035954209946184e-06 mae 0.0007953841013299062
training loss 1.4037344005707889e-06 mae 0.0008026979719473679
training loss 1.4270215187516272e-06 mae 0.0008099275323752294
Epoch 722, training: loss: 0.0000014, mae: 0.0008134 test: loss0.0001057, mae:0.0074423
training loss 2.2632830223301426e-06 mae 0.0009879491990432143
training loss 1.4268030796239485e-06 mae 0.0008016261654476838
training loss 1.4713895061301837e-06 mae 0.0008137976421656476
training loss 1.4578337056447698e-06 mae 0.0008173513826405297
training loss 1.424109996292348e-06 mae 0.000807795208092288
Epoch 723, training: loss: 0.0000014, mae: 0.0008085 test: loss0.0001062, mae:0.0074649
training loss 8.870119927451015e-07 mae 0.0006491380627267063
training loss 1.3405601637215096e-06 mae 0.000798924231454365
training loss 1.3631570024051268e-06 mae 0.0007897003353250649
training loss 1.3792400889674997e-06 mae 0.0007968387656709462
training loss 1.4099747873001103e-06 mae 0.0008038288887024887
Epoch 724, training: loss: 0.0000014, mae: 0.0008081 test: loss0.0001063, mae:0.0074727
training loss 1.739317212923197e-06 mae 0.0009298892691731453
training loss 1.451253400746817e-06 mae 0.0008166242475329222
training loss 1.4144639735739945e-06 mae 0.0008070602422893638
training loss 1.4277585211062466e-06 mae 0.0008075288658813629
training loss 1.4289104285298044e-06 mae 0.0008079197089215604
Epoch 725, training: loss: 0.0000014, mae: 0.0008072 test: loss0.0001060, mae:0.0074553
training loss 5.713082487091015e-07 mae 0.0005559045821428299
training loss 1.4245486066583373e-06 mae 0.0008043851886017653
training loss 1.398862365260699e-06 mae 0.000805062872339225
training loss 1.3941682368567852e-06 mae 0.0008049318223840109
training loss 1.4327949923910943e-06 mae 0.0008122983260603802
Epoch 726, training: loss: 0.0000014, mae: 0.0008106 test: loss0.0001057, mae:0.0074507
training loss 1.814413280953886e-06 mae 0.0007904556696303189
training loss 1.5368294413622377e-06 mae 0.000827881481324998
training loss 1.462724639514989e-06 mae 0.0008149659543501047
training loss 1.4164731012728751e-06 mae 0.0008041678942487094
training loss 1.4269161502836683e-06 mae 0.0008091281865496399
Epoch 727, training: loss: 0.0000014, mae: 0.0008087 test: loss0.0001071, mae:0.0074881
training loss 1.7295800489591784e-06 mae 0.0008875069324858487
training loss 1.5313147525607659e-06 mae 0.0008332115260190241
training loss 1.4675008005702015e-06 mae 0.0008197760031936635
training loss 1.4390134192234841e-06 mae 0.0008126355581207583
training loss 1.4353670219146742e-06 mae 0.0008114738466880698
Epoch 728, training: loss: 0.0000014, mae: 0.0008091 test: loss0.0001058, mae:0.0074480
training loss 2.1593143628706457e-06 mae 0.000848056108225137
training loss 1.3488957829751743e-06 mae 0.0007954783698854349
training loss 1.406701123374155e-06 mae 0.0008092174643078946
training loss 1.445345186757609e-06 mae 0.0008140342284629228
training loss 1.4343552347940311e-06 mae 0.0008118049695100912
Epoch 729, training: loss: 0.0000014, mae: 0.0008088 test: loss0.0001062, mae:0.0074512
training loss 2.3476063688576687e-06 mae 0.0010169610613957047
training loss 1.4350553614099675e-06 mae 0.0008009207428104297
training loss 1.3980416082010223e-06 mae 0.0007986572201105551
training loss 1.4193373809178328e-06 mae 0.0008037025116643071
training loss 1.4197954375610528e-06 mae 0.0008060180041151333
Epoch 730, training: loss: 0.0000014, mae: 0.0008071 test: loss0.0001060, mae:0.0074554
training loss 1.1131674000353087e-06 mae 0.000685842998791486
training loss 1.2973269139918419e-06 mae 0.0007816399059568839
training loss 1.4143296340026244e-06 mae 0.0008009989944166771
training loss 1.4180211384268212e-06 mae 0.0008003079810022496
training loss 1.4183599803198654e-06 mae 0.000806673390956466
Epoch 731, training: loss: 0.0000014, mae: 0.0008058 test: loss0.0001059, mae:0.0074508
training loss 2.6252046154695563e-06 mae 0.0010015893494710326
training loss 1.2877475941708078e-06 mae 0.0007854058503118506
training loss 1.341648279470397e-06 mae 0.0007931792896499138
training loss 1.3965344318788968e-06 mae 0.0008007467150786857
training loss 1.4220221167315642e-06 mae 0.00080870782530204
Epoch 732, training: loss: 0.0000014, mae: 0.0008068 test: loss0.0001062, mae:0.0074584
training loss 1.566127593832789e-06 mae 0.0007600054959766567
training loss 1.3611000265516887e-06 mae 0.0007748960268537642
training loss 1.3610214832376843e-06 mae 0.0007807382516157215
training loss 1.4068176787098473e-06 mae 0.0007966082710433531
training loss 1.4124199981664e-06 mae 0.0008062525757759886
Epoch 733, training: loss: 0.0000014, mae: 0.0008078 test: loss0.0001070, mae:0.0074873
training loss 7.128741685846762e-07 mae 0.0006592981517314911
training loss 1.3462157450668004e-06 mae 0.0007956879203180401
training loss 1.4252605702643779e-06 mae 0.0008088536557653864
training loss 1.412906075851701e-06 mae 0.0008056328652368006
training loss 1.4152203511595633e-06 mae 0.0008048211441280332
Epoch 734, training: loss: 0.0000014, mae: 0.0008065 test: loss0.0001068, mae:0.0074650
training loss 1.447346221539192e-06 mae 0.0007218795944936574
training loss 1.510372986139569e-06 mae 0.0008184255486555107
training loss 1.4595923830981548e-06 mae 0.0008057581985802711
training loss 1.4163540745417976e-06 mae 0.0008023444282520092
training loss 1.4219324356331943e-06 mae 0.0008059422411974427
Epoch 735, training: loss: 0.0000014, mae: 0.0008045 test: loss0.0001069, mae:0.0074790
training loss 1.1284837455605157e-06 mae 0.0006312187761068344
training loss 1.4882185417254907e-06 mae 0.0008172187965600658
training loss 1.4524029301927575e-06 mae 0.0008140504688697654
training loss 1.4111314764527048e-06 mae 0.0008023611396329093
training loss 1.409871772177972e-06 mae 0.0008041791392569033
Epoch 736, training: loss: 0.0000014, mae: 0.0008050 test: loss0.0001062, mae:0.0074619
training loss 3.235093345210771e-06 mae 0.0011264277854934335
training loss 1.3103929790322226e-06 mae 0.0007736548386495926
training loss 1.4051550124912693e-06 mae 0.0007978595371385763
training loss 1.4004007121137874e-06 mae 0.0007984401110408595
training loss 1.4119049685112121e-06 mae 0.0008051687837358265
Epoch 737, training: loss: 0.0000014, mae: 0.0008060 test: loss0.0001061, mae:0.0074610
training loss 1.4166071196086705e-06 mae 0.000781231268774718
training loss 1.4652941977860165e-06 mae 0.0008181608749973566
training loss 1.4251614097342054e-06 mae 0.0008082592941700744
training loss 1.4218066435988408e-06 mae 0.0008065585807946491
training loss 1.415526041774452e-06 mae 0.0008061061142966966
Epoch 738, training: loss: 0.0000014, mae: 0.0008041 test: loss0.0001061, mae:0.0074545
training loss 1.61852494784398e-06 mae 0.000824261165689677
training loss 1.4319736511852705e-06 mae 0.0008057136654707731
training loss 1.4283474017544665e-06 mae 0.0008124975126817604
training loss 1.4095189801793322e-06 mae 0.0008063413999166761
training loss 1.4159282601502498e-06 mae 0.0008063659234672083
Epoch 739, training: loss: 0.0000014, mae: 0.0008047 test: loss0.0001061, mae:0.0074603
training loss 1.1627463436525431e-06 mae 0.0007363874465227127
training loss 1.30786220748347e-06 mae 0.0007925404329309424
training loss 1.336453456786168e-06 mae 0.0007893368567220203
training loss 1.3917906358827004e-06 mae 0.0007992361811766812
training loss 1.4010628558706925e-06 mae 0.0008013394986457698
Epoch 740, training: loss: 0.0000014, mae: 0.0008049 test: loss0.0001064, mae:0.0074633
training loss 1.2746580750899739e-06 mae 0.0007398960296995938
training loss 1.3427490572149848e-06 mae 0.0007838492735982963
training loss 1.335618866796091e-06 mae 0.0007805331984170629
training loss 1.3978045192078036e-06 mae 0.0007936424637037566
training loss 1.3887160512704166e-06 mae 0.0007980481551471281
Epoch 741, training: loss: 0.0000014, mae: 0.0008021 test: loss0.0001059, mae:0.0074510
training loss 1.3929925444244873e-06 mae 0.0008861981332302094
training loss 1.3515701131355482e-06 mae 0.0007952981646738801
training loss 1.4121745795530996e-06 mae 0.0008090585896671557
training loss 1.4234713746450117e-06 mae 0.0008069509285693335
training loss 1.411909733036085e-06 mae 0.0008048219742387793
Epoch 742, training: loss: 0.0000014, mae: 0.0008035 test: loss0.0001057, mae:0.0074344
training loss 8.869707812664274e-07 mae 0.0006877416744828224
training loss 1.3982726437800508e-06 mae 0.0008116418258378318
training loss 1.4339989466762827e-06 mae 0.0008092223070700201
training loss 1.4065651521980041e-06 mae 0.000802576932007607
training loss 1.402294713135862e-06 mae 0.0008011662354345308
Epoch 743, training: loss: 0.0000014, mae: 0.0008036 test: loss0.0001059, mae:0.0074455
training loss 1.0488519137652474e-06 mae 0.0006436898256652057
training loss 1.3357538420018057e-06 mae 0.0007787422464210905
training loss 1.3541434326509602e-06 mae 0.0007888543325024519
training loss 1.3578096363232442e-06 mae 0.0007923175145205826
training loss 1.4105824431649286e-06 mae 0.0008033694084316714
Epoch 744, training: loss: 0.0000014, mae: 0.0008037 test: loss0.0001065, mae:0.0074785
training loss 1.8078807215715642e-06 mae 0.0008244458585977554
training loss 1.284746393175024e-06 mae 0.000766821496654302
training loss 1.3717460964615763e-06 mae 0.0007976720571785355
training loss 1.369088919702504e-06 mae 0.0007933504387600631
training loss 1.4172603964177694e-06 mae 0.0008060977541128013
Epoch 745, training: loss: 0.0000014, mae: 0.0008059 test: loss0.0001058, mae:0.0074401
training loss 1.2998315241929959e-06 mae 0.0007756051491014659
training loss 1.374951065211944e-06 mae 0.0008026975019853196
training loss 1.4006819534899642e-06 mae 0.0008055365109576448
training loss 1.3858218243067247e-06 mae 0.0008041377022048297
training loss 1.4074723805009197e-06 mae 0.0008054323528026836
Epoch 746, training: loss: 0.0000014, mae: 0.0008046 test: loss0.0001063, mae:0.0074696
training loss 1.0675382782210363e-06 mae 0.0007713387603871524
training loss 1.3878976097194206e-06 mae 0.0007990243589030758
training loss 1.399378495624669e-06 mae 0.0008005733990184224
training loss 1.3792033014878872e-06 mae 0.0007956620677380468
training loss 1.4094534890318753e-06 mae 0.0008027120962488785
Epoch 747, training: loss: 0.0000014, mae: 0.0008019 test: loss0.0001059, mae:0.0074435
training loss 2.1323760392988333e-06 mae 0.0009695089538581669
training loss 1.4397331133473999e-06 mae 0.0008074158526427458
training loss 1.4133151388128992e-06 mae 0.0008000949285878327
training loss 1.4029954946630264e-06 mae 0.0008000684104125873
training loss 1.4011922245633447e-06 mae 0.0008015027628813069
Epoch 748, training: loss: 0.0000014, mae: 0.0008027 test: loss0.0001061, mae:0.0074594
training loss 1.251387629963574e-06 mae 0.0007867102394811809
training loss 1.3858491813295461e-06 mae 0.000790062651503831
training loss 1.4400404926064848e-06 mae 0.0008009416005103895
training loss 1.409232545815247e-06 mae 0.0008006491104683634
training loss 1.4100821079577222e-06 mae 0.0008041707718332494
Epoch 749, training: loss: 0.0000014, mae: 0.0008033 test: loss0.0001107, mae:0.0074951
training loss 5.631845851894468e-07 mae 0.0005445473943836987
training loss 1.3685479199038502e-06 mae 0.0007802465532258081
training loss 1.3541721422357253e-06 mae 0.0007850866915201674
training loss 1.3651003662620713e-06 mae 0.0007939488125505505
training loss 1.3975361998089837e-06 mae 0.0007995422402470936
Epoch 750, training: loss: 0.0000014, mae: 0.0008023 test: loss0.0001062, mae:0.0074532
training loss 1.6935485973590403e-06 mae 0.0009488224168308079
training loss 1.3784145436397474e-06 mae 0.0008160193227048891
training loss 1.3295197539597522e-06 mae 0.0007969857623459462
training loss 1.3643491817993884e-06 mae 0.0007978096197279087
training loss 1.4161866915426224e-06 mae 0.0008072324736572029
Epoch 751, training: loss: 0.0000014, mae: 0.0008040 test: loss0.0001063, mae:0.0074673
training loss 1.4332689488583128e-06 mae 0.0008039232343435287
training loss 1.3817607819588625e-06 mae 0.0007895090087663895
training loss 1.381796345534588e-06 mae 0.0007848537343757063
training loss 1.3922661570562772e-06 mae 0.0007926302970485725
training loss 1.4072295069543276e-06 mae 0.0008020995518387252
Epoch 752, training: loss: 0.0000014, mae: 0.0008016 test: loss0.0001063, mae:0.0074634
training loss 1.928932761074975e-06 mae 0.0009116111323237419
training loss 1.391879655053416e-06 mae 0.0007841069246733598
training loss 1.3799435742560553e-06 mae 0.0007876552676546876
training loss 1.3989545278259386e-06 mae 0.000794200223387953
training loss 1.398686806618371e-06 mae 0.000799385971645709
Epoch 753, training: loss: 0.0000014, mae: 0.0007989 test: loss0.0001059, mae:0.0074420
training loss 6.157528105177335e-07 mae 0.0005594594404101372
training loss 1.392747677465534e-06 mae 0.0007926044955977475
training loss 1.3708338220603402e-06 mae 0.0007959616486914459
training loss 1.399819018915396e-06 mae 0.0008039723188209267
training loss 1.400999863040282e-06 mae 0.0008018101482716996
Epoch 754, training: loss: 0.0000014, mae: 0.0008017 test: loss0.0001065, mae:0.0074656
training loss 1.4023584071765072e-06 mae 0.0007910703425295651
training loss 1.4691219600500324e-06 mae 0.0008100756368709399
training loss 1.4102429917788822e-06 mae 0.0007974553915515247
training loss 1.4118345423966523e-06 mae 0.0008044581777810903
training loss 1.4131848183072843e-06 mae 0.0008055717572546682
Epoch 755, training: loss: 0.0000014, mae: 0.0008031 test: loss0.0001063, mae:0.0074652
training loss 1.2806264066966833e-06 mae 0.0007956360350362957
training loss 1.4507686946850374e-06 mae 0.000806551714263418
training loss 1.4319104232274887e-06 mae 0.0008030726521340485
training loss 1.3785238724303537e-06 mae 0.0007928924739903567
training loss 1.3931521483013126e-06 mae 0.0007981801641854775
Epoch 756, training: loss: 0.0000014, mae: 0.0007997 test: loss0.0001061, mae:0.0074610
training loss 2.080278591165552e-06 mae 0.0009820949053391814
training loss 1.3566516015856118e-06 mae 0.0007786882907727404
training loss 1.358000867576896e-06 mae 0.0007885469179623526
training loss 1.3693986719146106e-06 mae 0.0007951558798486797
training loss 1.4021384709392362e-06 mae 0.0008023563566541211
Epoch 757, training: loss: 0.0000014, mae: 0.0008027 test: loss0.0001057, mae:0.0074317
training loss 1.752681896505237e-06 mae 0.0009609063272364438
training loss 1.346981418116818e-06 mae 0.0007868556319462023
training loss 1.335940698193707e-06 mae 0.0007897481872091568
training loss 1.4046406102123317e-06 mae 0.0008008990086775444
training loss 1.3905476167854801e-06 mae 0.0007973132381072052
Epoch 758, training: loss: 0.0000014, mae: 0.0007986 test: loss0.0001061, mae:0.0074567
training loss 8.249899678958172e-07 mae 0.000685492530465126
training loss 1.2945788835524722e-06 mae 0.0007704932073715565
training loss 1.3551465597197194e-06 mae 0.000782959378730174
training loss 1.4156561687380097e-06 mae 0.0008028205503425905
training loss 1.4004469660528289e-06 mae 0.000800561985181673
Epoch 759, training: loss: 0.0000014, mae: 0.0007994 test: loss0.0001064, mae:0.0074623
training loss 1.154558390226157e-06 mae 0.0007161485846154392
training loss 1.4087755588144656e-06 mae 0.0008003420730614485
training loss 1.4007716214505892e-06 mae 0.00079935207492718
training loss 1.3902585145095883e-06 mae 0.0007985895002316312
training loss 1.3982371231394166e-06 mae 0.0007998505332136518
Epoch 760, training: loss: 0.0000014, mae: 0.0007993 test: loss0.0001081, mae:0.0075028
training loss 1.0404348813608522e-06 mae 0.0007731386576779187
training loss 1.3015383793643588e-06 mae 0.0007749229541742335
training loss 1.3210804862108776e-06 mae 0.0007776004402889029
training loss 1.368732915016876e-06 mae 0.0007915812716449281
training loss 1.3787035888313926e-06 mae 0.0007950054366947772
Epoch 761, training: loss: 0.0000014, mae: 0.0007994 test: loss0.0001068, mae:0.0074774
training loss 1.1101310519734398e-06 mae 0.0007130277226679027
training loss 1.3127278001703936e-06 mae 0.0007689010206947796
training loss 1.3774600875372504e-06 mae 0.000794339891336397
training loss 1.4096051740010954e-06 mae 0.0008029344116587387
training loss 1.3964787085999553e-06 mae 0.0008004027158450516
Epoch 762, training: loss: 0.0000014, mae: 0.0008001 test: loss0.0001063, mae:0.0074664
training loss 1.7374359231325798e-06 mae 0.0009425028110854328
training loss 1.240514881614859e-06 mae 0.0007724567389517438
training loss 1.3449837618029753e-06 mae 0.0007851980753372061
training loss 1.388523848741681e-06 mae 0.0007905972853999901
training loss 1.3907349588366945e-06 mae 0.0007972855625363684
Epoch 763, training: loss: 0.0000014, mae: 0.0007971 test: loss0.0001061, mae:0.0074557
training loss 1.102340661418566e-06 mae 0.0006531731341965497
training loss 1.4491665829868283e-06 mae 0.0008008040786793856
training loss 1.4171918388739756e-06 mae 0.0007965874334907916
training loss 1.405953558231952e-06 mae 0.0007993817539108508
training loss 1.3959129504354823e-06 mae 0.0007987701208377021
Epoch 764, training: loss: 0.0000014, mae: 0.0007988 test: loss0.0001074, mae:0.0074790
training loss 1.2779547660102253e-06 mae 0.0008911738987080753
training loss 1.331095002204201e-06 mae 0.0007815132038576493
training loss 1.3617300586846265e-06 mae 0.0007836673389152062
training loss 1.400479399404467e-06 mae 0.0007961991367189353
training loss 1.3903114830018646e-06 mae 0.0007972170593592328
Epoch 765, training: loss: 0.0000014, mae: 0.0007985 test: loss0.0001088, mae:0.0074953
training loss 7.052147452668578e-07 mae 0.0006411336362361908
training loss 1.3358889905738774e-06 mae 0.0007858983213629792
training loss 1.3862505083201685e-06 mae 0.000789188416813048
training loss 1.3826438023051682e-06 mae 0.0007933812483448214
training loss 1.3774508485384643e-06 mae 0.0007932863944784662
Epoch 766, training: loss: 0.0000014, mae: 0.0007965 test: loss0.0001079, mae:0.0074936
training loss 1.500639541518467e-06 mae 0.0008132128859870136
training loss 1.3732389431077212e-06 mae 0.0008063294020864893
training loss 1.4002038021276752e-06 mae 0.0007993303098725062
training loss 1.3917529138746957e-06 mae 0.0007985086282036755
training loss 1.399075612492546e-06 mae 0.0008031910850634028
Epoch 767, training: loss: 0.0000014, mae: 0.0007982 test: loss0.0001061, mae:0.0074580
training loss 1.1539763136170222e-06 mae 0.0007480687345378101
training loss 1.3641882576081907e-06 mae 0.0007851511666861674
training loss 1.3795025526210029e-06 mae 0.0007908627699765536
training loss 1.390120493418344e-06 mae 0.0007951182582329295
training loss 1.3930893909044668e-06 mae 0.0007981535346149954
Epoch 768, training: loss: 0.0000014, mae: 0.0007986 test: loss0.0001065, mae:0.0074709
training loss 1.8297254200660973e-06 mae 0.0008592642843723297
training loss 1.2047081933512047e-06 mae 0.0007437558995340675
training loss 1.350983644321183e-06 mae 0.0007832969981476224
training loss 1.3596921450475716e-06 mae 0.000788929007194986
training loss 1.3797846528082589e-06 mae 0.0007958052192226205
Epoch 769, training: loss: 0.0000014, mae: 0.0007967 test: loss0.0001070, mae:0.0074884
training loss 8.504964625899447e-07 mae 0.0006645734538324177
training loss 1.3439784020326493e-06 mae 0.0007744426217030587
training loss 1.3564581324051617e-06 mae 0.0007869043513685281
training loss 1.3744644073895626e-06 mae 0.0007958535722916366
training loss 1.3624879958325756e-06 mae 0.0007945305161903249
Epoch 770, training: loss: 0.0000014, mae: 0.0007989 test: loss0.0001065, mae:0.0074690
training loss 1.5940262301228358e-06 mae 0.0008968822658061981
training loss 1.3908436302449886e-06 mae 0.000789159843835103
training loss 1.387992909096085e-06 mae 0.0008006017242493753
training loss 1.3873743042820582e-06 mae 0.0007997937307736238
training loss 1.3884487900589127e-06 mae 0.0007999795454262355
Epoch 771, training: loss: 0.0000014, mae: 0.0007984 test: loss0.0001068, mae:0.0074824
training loss 2.0291154214646667e-06 mae 0.0010176055366173387
training loss 1.3623903604567717e-06 mae 0.0007892782604732713
training loss 1.3413849255992333e-06 mae 0.0007902333748614468
training loss 1.3580144164336986e-06 mae 0.0007917177738137664
training loss 1.3817661581119336e-06 mae 0.0007972091406558299
Epoch 772, training: loss: 0.0000014, mae: 0.0007970 test: loss0.0001075, mae:0.0074879
training loss 1.3265553207020275e-06 mae 0.0008970964699983597
training loss 1.2096167234390388e-06 mae 0.0007551432611174226
training loss 1.2694461574510295e-06 mae 0.0007732167369155859
training loss 1.3226409294773304e-06 mae 0.0007847168685988651
training loss 1.3716680586585689e-06 mae 0.0007946316842904861
Epoch 773, training: loss: 0.0000014, mae: 0.0007967 test: loss0.0001114, mae:0.0075188
training loss 7.374940764748317e-07 mae 0.0006445233593694866
training loss 1.365823786549228e-06 mae 0.0007853797846930285
training loss 1.4350131323226376e-06 mae 0.000804455250059024
training loss 1.3988506412077196e-06 mae 0.0007983578086849178
training loss 1.3837533223052187e-06 mae 0.0007957117073122056
Epoch 774, training: loss: 0.0000014, mae: 0.0007961 test: loss0.0001067, mae:0.0074788
training loss 5.967998504274874e-07 mae 0.0005524987354874611
training loss 1.3245871663653533e-06 mae 0.0007666125220210091
training loss 1.316808743524126e-06 mae 0.0007708334994483923
training loss 1.3563603570806385e-06 mae 0.000784408139212933
training loss 1.3784860073539577e-06 mae 0.0007951710930914244
Epoch 775, training: loss: 0.0000014, mae: 0.0007965 test: loss0.0001063, mae:0.0074617
training loss 1.2496623185143108e-06 mae 0.0007907937397249043
training loss 1.436604758081932e-06 mae 0.0008105762501466363
training loss 1.3431721400944423e-06 mae 0.0007888305134665553
training loss 1.3967761259549818e-06 mae 0.0007945435500968964
training loss 1.3730347912165435e-06 mae 0.0007912761509182185
Epoch 776, training: loss: 0.0000014, mae: 0.0007943 test: loss0.0001071, mae:0.0074832
training loss 1.6924426518016844e-06 mae 0.0009512295946478844
training loss 1.3665816257959227e-06 mae 0.0007860374776189963
training loss 1.3129011985918372e-06 mae 0.0007742470208702992
training loss 1.3380972174766285e-06 mae 0.0007868291982079523
training loss 1.3575619957758477e-06 mae 0.0007882435005088677
Epoch 777, training: loss: 0.0000014, mae: 0.0007924 test: loss0.0001068, mae:0.0074832
training loss 1.7356902617393644e-06 mae 0.0008908637682907283
training loss 1.4439399420802607e-06 mae 0.0008080434350881214
training loss 1.380031939883302e-06 mae 0.0007949490630353738
training loss 1.3588681436006586e-06 mae 0.0007885611023975919
training loss 1.3660615932461396e-06 mae 0.0007917924896484957
Epoch 778, training: loss: 0.0000014, mae: 0.0007966 test: loss0.0001072, mae:0.0074913
training loss 1.5415395182571956e-06 mae 0.0008976866374723613
training loss 1.3670904095038966e-06 mae 0.0007944969464933464
training loss 1.4473918088355064e-06 mae 0.0008099608297349129
training loss 1.402956598706666e-06 mae 0.0007971992314521791
training loss 1.383221785717119e-06 mae 0.000794877276167422
Epoch 779, training: loss: 0.0000014, mae: 0.0007919 test: loss0.0001077, mae:0.0074978
training loss 2.1443195237225154e-06 mae 0.0008999087731353939
training loss 1.3491138532777308e-06 mae 0.0007801800178747404
training loss 1.3428001111741216e-06 mae 0.0007865876322473851
training loss 1.372069672642049e-06 mae 0.0007913509350646264
training loss 1.3791781182576487e-06 mae 0.0007954496210927507
Epoch 780, training: loss: 0.0000014, mae: 0.0007957 test: loss0.0001063, mae:0.0074619
training loss 1.1794519423347083e-06 mae 0.000696923874784261
training loss 1.421932551033487e-06 mae 0.0008048688908399758
training loss 1.3656340832645891e-06 mae 0.0007932083439174119
training loss 1.397359706091368e-06 mae 0.000797503265098897
training loss 1.3757775202543488e-06 mae 0.000794449168377885
Epoch 781, training: loss: 0.0000014, mae: 0.0007939 test: loss0.0001064, mae:0.0074644
training loss 1.2480851410145988e-06 mae 0.000806223601102829
training loss 1.2781552220976846e-06 mae 0.0007742878937564206
training loss 1.3437098014510228e-06 mae 0.0007883935992716632
training loss 1.3747350507648227e-06 mae 0.0007975335319261279
training loss 1.3626428923107506e-06 mae 0.0007929268311326902
Epoch 782, training: loss: 0.0000014, mae: 0.0007961 test: loss0.0001078, mae:0.0074941
training loss 9.364837865177833e-07 mae 0.0006540259346365929
training loss 1.324482591193302e-06 mae 0.0007822872150926762
training loss 1.3758027596913332e-06 mae 0.0008022775302991495
training loss 1.3910683623216516e-06 mae 0.0008012231012238872
training loss 1.3641916984399806e-06 mae 0.00079205468156843
Epoch 783, training: loss: 0.0000014, mae: 0.0007930 test: loss0.0001081, mae:0.0075145
training loss 1.667738615651615e-06 mae 0.0008769395644776523
training loss 1.3229369662493651e-06 mae 0.0007817468265820222
training loss 1.320624683598502e-06 mae 0.0007867133492686887
training loss 1.366255928783386e-06 mae 0.0007938482684533523
training loss 1.3607025744474877e-06 mae 0.0007902181991700321
Epoch 784, training: loss: 0.0000014, mae: 0.0007907 test: loss0.0001079, mae:0.0075100
training loss 1.7219568917425931e-06 mae 0.0008526314049959183
training loss 1.4452118826810355e-06 mae 0.0008006358311475053
training loss 1.4542568960272958e-06 mae 0.0008094932349158984
training loss 1.3994697673032177e-06 mae 0.0007989617524859768
training loss 1.3779997779401222e-06 mae 0.0007939343714266458
Epoch 785, training: loss: 0.0000014, mae: 0.0007930 test: loss0.0001067, mae:0.0074808
training loss 2.232922497569234e-06 mae 0.0009721058304421604
training loss 1.2748128870266645e-06 mae 0.0007633412354990988
training loss 1.3770594492116558e-06 mae 0.0007843824897634584
training loss 1.3778315978894974e-06 mae 0.0007883968534848547
training loss 1.3786373540878862e-06 mae 0.0007956347669201405
Epoch 786, training: loss: 0.0000014, mae: 0.0007927 test: loss0.0001076, mae:0.0074855
training loss 1.8590076251712162e-06 mae 0.0009659736533649266
training loss 1.379668671073459e-06 mae 0.0007701402155718967
training loss 1.3386963928496698e-06 mae 0.0007773877581406274
training loss 1.3380278875161672e-06 mae 0.0007829419352632286
training loss 1.3635384561328568e-06 mae 0.0007912562091473433
Epoch 787, training: loss: 0.0000014, mae: 0.0007924 test: loss0.0001064, mae:0.0074671
training loss 1.213859832205344e-06 mae 0.0008050442556850612
training loss 1.4025373914984237e-06 mae 0.0007816488973285055
training loss 1.4126427942918065e-06 mae 0.0007984724897421011
training loss 1.3904618046448648e-06 mae 0.0007954163805254744
training loss 1.3718284859165587e-06 mae 0.0007925019668999007
Epoch 788, training: loss: 0.0000014, mae: 0.0007913 test: loss0.0001062, mae:0.0074624
training loss 1.447754584660288e-06 mae 0.0008100231061689556
training loss 1.3890727130978581e-06 mae 0.0007938664639368653
training loss 1.3144774210095505e-06 mae 0.000780252091434443
training loss 1.3232534760412064e-06 mae 0.0007827099530636917
training loss 1.3587264575322763e-06 mae 0.0007906714445605539
Epoch 789, training: loss: 0.0000014, mae: 0.0007934 test: loss0.0001070, mae:0.0074877
training loss 1.2076498023816384e-06 mae 0.0008590255747549236
training loss 1.2525097301976545e-06 mae 0.0007628845433275417
training loss 1.31195935520088e-06 mae 0.0007728000075238215
training loss 1.3382756043595248e-06 mae 0.0007833537064717825
training loss 1.3703129569428214e-06 mae 0.0007915131048941104
Epoch 790, training: loss: 0.0000014, mae: 0.0007894 test: loss0.0001068, mae:0.0074850
training loss 1.8349500123804319e-06 mae 0.000927753106225282
training loss 1.3489616522372886e-06 mae 0.0007671899705504377
training loss 1.2939798735526055e-06 mae 0.0007580927611181126
training loss 1.3310799036296296e-06 mae 0.0007780519150666249
training loss 1.3530182407032666e-06 mae 0.0007855118940403654
Epoch 791, training: loss: 0.0000014, mae: 0.0007899 test: loss0.0001088, mae:0.0075079
training loss 1.2869137435700395e-06 mae 0.0008198721334338188
training loss 1.3574057788900973e-06 mae 0.0007796716686867762
training loss 1.3069147769891799e-06 mae 0.0007694711698919977
training loss 1.3490307232800751e-06 mae 0.0007862017924792519
training loss 1.3790943135244568e-06 mae 0.0007956297057495444
Epoch 792, training: loss: 0.0000014, mae: 0.0007930 test: loss0.0001071, mae:0.0074904
training loss 8.179474093594763e-07 mae 0.0006498138536699116
training loss 1.4557290598407003e-06 mae 0.0008084628144370428
training loss 1.3839058875410136e-06 mae 0.0007912276438345192
training loss 1.3815368804338596e-06 mae 0.0007937154825160048
training loss 1.3653686490631761e-06 mae 0.0007911039885847058
Epoch 793, training: loss: 0.0000014, mae: 0.0007905 test: loss0.0001075, mae:0.0074948
training loss 2.9865984743082663e-06 mae 0.0010458827018737793
training loss 1.3414750663501117e-06 mae 0.0007897633830925412
training loss 1.3606808816929167e-06 mae 0.0007885758785789112
training loss 1.356894265668325e-06 mae 0.0007860355525561271
training loss 1.356005683911503e-06 mae 0.0007893624118950907
Epoch 794, training: loss: 0.0000014, mae: 0.0007905 test: loss0.0001074, mae:0.0075016
training loss 1.4159386410028674e-06 mae 0.0008159615099430084
training loss 1.4039159904153648e-06 mae 0.0007996015962870679
training loss 1.382519491750764e-06 mae 0.0007922256718122279
training loss 1.3775760366543368e-06 mae 0.0007882597509290504
training loss 1.3671335181587084e-06 mae 0.0007930394549928234
Epoch 795, training: loss: 0.0000014, mae: 0.0007920 test: loss0.0001103, mae:0.0075097
training loss 1.0688545444281772e-06 mae 0.0007230059127323329
training loss 1.270133507249757e-06 mae 0.0007633663869152467
training loss 1.382743757610773e-06 mae 0.0007954924920629157
training loss 1.3692034339695944e-06 mae 0.0007969556327767373
training loss 1.3725049295296787e-06 mae 0.0007930866087003797
Epoch 796, training: loss: 0.0000014, mae: 0.0007910 test: loss0.0001067, mae:0.0074749
training loss 1.4439783626585267e-06 mae 0.0008147268672473729
training loss 1.3930089632559239e-06 mae 0.0008004195177836308
training loss 1.3641990261309575e-06 mae 0.000790596215773632
training loss 1.3604152893293349e-06 mae 0.0007886843989442858
training loss 1.3602767360757377e-06 mae 0.0007901218136545473
Epoch 797, training: loss: 0.0000014, mae: 0.0007904 test: loss0.0001066, mae:0.0074780
training loss 2.1176967948122183e-06 mae 0.0008874290506355464
training loss 1.2711542188869728e-06 mae 0.0007618992194515086
training loss 1.2895496899662893e-06 mae 0.0007744844493428522
training loss 1.3340940191386865e-06 mae 0.0007844990225730848
training loss 1.351708225151177e-06 mae 0.0007871017332040515
Epoch 798, training: loss: 0.0000014, mae: 0.0007896 test: loss0.0001066, mae:0.0074714
training loss 2.506226564946701e-06 mae 0.0008593539823777974
training loss 1.2849387067213754e-06 mae 0.0007721647938631256
training loss 1.3380145461913278e-06 mae 0.000783164121007713
training loss 1.3297920100056533e-06 mae 0.0007831420620675168
training loss 1.3598903714991415e-06 mae 0.0007901187903312643
Epoch 799, training: loss: 0.0000014, mae: 0.0007898 test: loss0.0001068, mae:0.0074848
current learning rate: 1.953125e-06

Process finished with exit code 0


















ssh://jeffzhu@172.16.46.217:22/home/jeffzhu/anaconda3/bin/python3.6 -u /home/jeffzhu/AL/pre_training/non_ptrain_cls.py
Namespace(al_method='msg_mask', bald_ft_epochs=5, batch_data_num=100, batchsize=48, data_mix=False, data_mixing_rate=0.5, dataset='qm9', device=1, epochs=800, ft_epochs=5, ft_method='by_valid', init_data_num=5000, k_center_ft_epochs=10, lr=0.0005, mc_sampling_num=80, model_num=4, multi_gpu=False, prop_name='homo', qbc_ft_epochs=5, re_init=False, save_model=False, shuffle=True, test_freq=5, test_use_all=False, use_default=False, use_tb=True, workers=0)
1119_23_02  model SchNetModel(
  (activation): ShiftSoftplus(
    beta=1, threshold=20
    (softplus): Softplus(beta=1, threshold=20)
  )
  (embedding_layer): AtomEmbedding(
    (embedding): Embedding(100, 48, padding_idx=0)
  )
  (rbf_layer): RBFLayer()
  (conv_layers): ModuleList(
    (0): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (1): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (2): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (3): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
  )
  (atom_dense_layer1): Linear(in_features=48, out_features=64, bias=True)
  (atom_dense_layer2): Linear(in_features=64, out_features=1, bias=True)
)  optimizer Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
inference 24.493228673934937
Iteration 1 Mean MSE 0.0009376993984915316
Iteration 2 Mean MSE 0.0007360547315329313
Iteration 3 Mean MSE 0.0006919215084053576
Iteration 4 Mean MSE 0.0006767620798200369
Iteration 5 Mean MSE 0.0006752996705472469
Iteration 6 Mean MSE 0.0006750315660610795
Iteration 7 Mean MSE 0.0006750000175088644
Iteration 8 Mean MSE 0.0006749999010935426
Iteration 9 Mean MSE 0.0006749999010935426
Iteration 10 Mean MSE 0.0006749999010935426
start
SchNetModel(
  (activation): ShiftSoftplus(
    beta=1, threshold=20
    (softplus): Softplus(beta=1, threshold=20)
  )
  (embedding_layer): AtomEmbedding(
    (embedding): Embedding(100, 48, padding_idx=0)
  )
  (rbf_layer): RBFLayer()
  (conv_layers): ModuleList(
    (0): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (1): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (2): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
    (3): Interaction(
      (activation): Softplus(beta=0.5, threshold=14)
      (node_layer1): Linear(in_features=48, out_features=48, bias=False)
      (cfconv): CFConv(
        (linear_layer1): Linear(in_features=10, out_features=48, bias=True)
        (linear_layer2): Linear(in_features=48, out_features=48, bias=True)
        (activation): Softplus(beta=0.5, threshold=14)
      )
      (node_layer2): Linear(in_features=48, out_features=48, bias=True)
      (node_layer3): Linear(in_features=48, out_features=48, bias=True)
    )
  )
  (atom_dense_layer1): Linear(in_features=48, out_features=64, bias=True)
  (atom_dense_layer2): Linear(in_features=64, out_features=1, bias=True)
)
-0.23988008499145508 0.022041337564587593
training loss 0.005313605070114136 mae 0.05100754275918007
training loss 0.0008447620453879093 mae 0.020325492610972305
training loss 0.000654290275507616 mae 0.01819838704264696
training loss 0.0005743061160531503 mae 0.01712213385702166
training loss 0.0005329288468425126 mae 0.01649787988680512
Epoch  0, training: loss: 0.0005306, mae: 0.0164727 test: loss0.0004192, mae:0.0148751
training loss 0.0003694701299536973 mae 0.013919033110141754
training loss 0.00044311601645769736 mae 0.01565320028320831
training loss 0.0004375617516614349 mae 0.015389324949666997
training loss 0.00042484253656507664 mae 0.01516436317947921
training loss 0.000422085903379927 mae 0.01512846466969346
Epoch  1, training: loss: 0.0004225, mae: 0.0151089 test: loss0.0004280, mae:0.0149806
training loss 0.0007619151729159057 mae 0.018621476367115974
training loss 0.0004265894374339933 mae 0.014839159014324345
training loss 0.00040888637511778884 mae 0.014687408038442677
training loss 0.00041373033829754976 mae 0.01481664642099513
training loss 0.0004089409660661489 mae 0.014767384809900577
Epoch  2, training: loss: 0.0004101, mae: 0.0147966 test: loss0.0004160, mae:0.0148674
training loss 0.0003938909212592989 mae 0.014105523936450481
training loss 0.00041877265246904584 mae 0.014889756747174496
training loss 0.0004021689259172833 mae 0.014696476354014755
training loss 0.000403863100129926 mae 0.01472184226681657
training loss 0.000409792801373944 mae 0.01481921268877253
Epoch  3, training: loss: 0.0004095, mae: 0.0148120 test: loss0.0004178, mae:0.0150515
training loss 0.00041031185537576675 mae 0.015172384679317474
training loss 0.00038902747374046235 mae 0.014480089604416312
training loss 0.000388332015441703 mae 0.014422772323141002
training loss 0.000401980630453639 mae 0.014705543973736024
training loss 0.00041082892626285365 mae 0.014868651164593688
Epoch  4, training: loss: 0.0004111, mae: 0.0148942 test: loss0.0004084, mae:0.0148401
training loss 0.0002597580023575574 mae 0.012527953833341599
training loss 0.00039328483712868585 mae 0.014546014167660595
training loss 0.0003892440322353181 mae 0.01454178905944423
training loss 0.00039417794506629877 mae 0.014641556975166533
training loss 0.0003860587667122444 mae 0.014520278770420985
Epoch  5, training: loss: 0.0003899, mae: 0.0145575 test: loss0.0003832, mae:0.0144359
training loss 0.00024359785311389714 mae 0.01107541099190712
training loss 0.0003621630435101871 mae 0.013961849606358542
training loss 0.00035973485097597985 mae 0.014039158682790726
training loss 0.0003489470186439769 mae 0.013831187644048242
training loss 0.0003448812061091376 mae 0.01372378511789871
Epoch  6, training: loss: 0.0003422, mae: 0.0136921 test: loss0.0003660, mae:0.0140568
training loss 0.000348163623129949 mae 0.015342486090958118
training loss 0.0002928078482749269 mae 0.01273152964445306
training loss 0.00030723291231402294 mae 0.012953372838178486
training loss 0.00030467232410944005 mae 0.013011133742352196
training loss 0.0003081071394697221 mae 0.013067120296964007
Epoch  7, training: loss: 0.0003089, mae: 0.0130548 test: loss0.0003077, mae:0.0129849
training loss 0.0002638638543430716 mae 0.012974828481674194
training loss 0.0002910997373321294 mae 0.012721682825655327
training loss 0.0002901590547967 mae 0.012715608886636722
training loss 0.0002865658499399164 mae 0.012672253390959166
training loss 0.0002849433597597753 mae 0.012610003194628077
Epoch  8, training: loss: 0.0002850, mae: 0.0126047 test: loss0.0003517, mae:0.0145864
training loss 0.00025210363673977554 mae 0.01282400544732809
training loss 0.0002755259201341473 mae 0.012405958793619101
training loss 0.0002798873734934773 mae 0.012610920566585979
training loss 0.000277670366216963 mae 0.01249992886299131
training loss 0.0002751937599398605 mae 0.01240961604394871
Epoch  9, training: loss: 0.0002740, mae: 0.0123665 test: loss0.0002743, mae:0.0124200
training loss 0.00015185715164989233 mae 0.009630476124584675
training loss 0.000253941594992819 mae 0.012026711294025771
training loss 0.00026646644320214483 mae 0.012322577048498807
training loss 0.0002644753603215347 mae 0.012243504512635684
training loss 0.0002679865575188177 mae 0.01229198121776184
Epoch 10, training: loss: 0.0002670, mae: 0.0122804 test: loss0.0002739, mae:0.0121663
training loss 0.0004189589526504278 mae 0.012225967831909657
training loss 0.0002585339264923653 mae 0.01205457456629066
training loss 0.00025093771567992365 mae 0.01188114713324179
training loss 0.0002535854507505969 mae 0.01191906239960762
training loss 0.00025609500178562223 mae 0.012006733561889157
Epoch 11, training: loss: 0.0002578, mae: 0.0120268 test: loss0.0002822, mae:0.0125637
training loss 0.00020725947979371995 mae 0.012052487581968307
training loss 0.00024371218094921797 mae 0.01175888837771673
training loss 0.0002454510127196531 mae 0.01185822741377472
training loss 0.00024790621750227893 mae 0.011864245866135457
training loss 0.0002494130292426639 mae 0.011855630261999612
Epoch 12, training: loss: 0.0002497, mae: 0.0118507 test: loss0.0002674, mae:0.0121285
training loss 0.00016552921442780644 mae 0.01040706504136324
training loss 0.0002606506587814211 mae 0.012108252694209419
training loss 0.00025410614538900914 mae 0.011968441040135258
training loss 0.0002597232359046708 mae 0.012112381800219719
training loss 0.00025893996495884424 mae 0.012068098040288366
Epoch 13, training: loss: 0.0002564, mae: 0.0120175 test: loss0.0002641, mae:0.0121088
training loss 0.00019539885397534817 mae 0.011211377568542957
training loss 0.0002447384758852422 mae 0.01177399399160754
training loss 0.0002500252607439894 mae 0.011860964948883149
training loss 0.0002507084006014193 mae 0.011856722053362436
training loss 0.0002501557717016963 mae 0.011848844054956632
Epoch 14, training: loss: 0.0002480, mae: 0.0118094 test: loss0.0002719, mae:0.0119517
training loss 0.0003068981459364295 mae 0.012688525021076202
training loss 0.00027488327702717373 mae 0.012433614015725312
training loss 0.0002635344756319934 mae 0.012143218760738282
training loss 0.00024826290118292154 mae 0.011787685497805771
training loss 0.00024929751070976175 mae 0.011818561582497103
Epoch 15, training: loss: 0.0002485, mae: 0.0118022 test: loss0.0002566, mae:0.0118304
training loss 0.0002799218927975744 mae 0.01318140234798193
training loss 0.00023285797871329695 mae 0.011437490107674224
training loss 0.00024651070494241664 mae 0.011803592859518406
training loss 0.00024557220897967287 mae 0.011782743511659808
training loss 0.0002483039698650394 mae 0.01182746646267858
Epoch 16, training: loss: 0.0002481, mae: 0.0118404 test: loss0.0002566, mae:0.0118232
training loss 0.0002026107395067811 mae 0.010828986763954163
training loss 0.00023931838536644167 mae 0.011393958100062954
training loss 0.00024658820239163124 mae 0.011728057481053438
training loss 0.0002517068139138624 mae 0.011849635737078468
training loss 0.00024600083092968917 mae 0.01176976825838065
Epoch 17, training: loss: 0.0002446, mae: 0.0117406 test: loss0.0002782, mae:0.0127541
training loss 0.00028022416518069804 mae 0.013603921979665756
training loss 0.00023113625074449163 mae 0.011544385924935344
training loss 0.0002321122452071499 mae 0.011538186221208314
training loss 0.00024285004103197002 mae 0.011683105324573867
training loss 0.00024374279789991492 mae 0.011748150194907074
Epoch 18, training: loss: 0.0002441, mae: 0.0117674 test: loss0.0002711, mae:0.0122654
training loss 0.00027567122015170753 mae 0.013074113987386227
training loss 0.00023448964074144465 mae 0.011654532733647265
training loss 0.000231496405633314 mae 0.011427758067920065
training loss 0.00024272704771442332 mae 0.01172862701512725
training loss 0.00023895876139833313 mae 0.011613625421452878
Epoch 19, training: loss: 0.0002376, mae: 0.0115927 test: loss0.0002349, mae:0.0113095
training loss 0.0002057275123661384 mae 0.011377345770597458
training loss 0.00023002726626217217 mae 0.011344458013042518
training loss 0.00022753293758274356 mae 0.011283129398332965
training loss 0.00022816893727316625 mae 0.011325883142502104
training loss 0.00023284221387442794 mae 0.011446678454973809
Epoch 20, training: loss: 0.0002310, mae: 0.0114106 test: loss0.0002728, mae:0.0123747
training loss 0.00024184254289139062 mae 0.011627639643847942
training loss 0.00021580794076298307 mae 0.011160054901505219
training loss 0.00022768756839513572 mae 0.011408170320941961
training loss 0.00022660309255326667 mae 0.011357623722946993
training loss 0.00022553879433469053 mae 0.01136607064897963
Epoch 21, training: loss: 0.0002267, mae: 0.0113757 test: loss0.0003495, mae:0.0145634
training loss 0.00040467982762493193 mae 0.015996010974049568
training loss 0.0002721066052532371 mae 0.01236324992907398
training loss 0.0002484987838548783 mae 0.011890666218160996
training loss 0.0002351518956399276 mae 0.01157548596823452
training loss 0.00023246569971933922 mae 0.011510179526362553
Epoch 22, training: loss: 0.0002302, mae: 0.0114620 test: loss0.0002483, mae:0.0117898
training loss 0.00027961155865341425 mae 0.013329346664249897
training loss 0.0002086534494118693 mae 0.010911712185571004
training loss 0.00021191396160364747 mae 0.010932997625210494
training loss 0.00021297196508385244 mae 0.010982173232867421
training loss 0.00021697537288931546 mae 0.01111128281069854
Epoch 23, training: loss: 0.0002162, mae: 0.0110856 test: loss0.0002670, mae:0.0126680
training loss 0.00014124882000032812 mae 0.009790451265871525
training loss 0.0002356605360617715 mae 0.011607807275711323
training loss 0.00023422208359628676 mae 0.011539175313445605
training loss 0.0002238670273337803 mae 0.01128233328387635
training loss 0.000223822459353792 mae 0.011271163494562482
Epoch 24, training: loss: 0.0002224, mae: 0.0112418 test: loss0.0002578, mae:0.0119282
training loss 0.0002816666092257947 mae 0.013200768269598484
training loss 0.0002133581372475544 mae 0.01105453042934338
training loss 0.000208823296104876 mae 0.010940318115719478
training loss 0.00020570550837657346 mae 0.010837354031175575
training loss 0.00021365423640757515 mae 0.011042121663430144
Epoch 25, training: loss: 0.0002138, mae: 0.0110494 test: loss0.0002242, mae:0.0112522
training loss 0.0001868921099230647 mae 0.011312364600598812
training loss 0.00022827545534728058 mae 0.011389050988809149
training loss 0.00021347539233721534 mae 0.01104746564835458
training loss 0.00021185861713104935 mae 0.010990691719737076
training loss 0.00020863881786568184 mae 0.01089817195185529
Epoch 26, training: loss: 0.0002097, mae: 0.0109164 test: loss0.0002537, mae:0.0123091
training loss 0.00021548393124248832 mae 0.011509825475513935
training loss 0.00021375613983300534 mae 0.011086465498688176
training loss 0.0002150476750535061 mae 0.011108026526278196
training loss 0.00020998016455432787 mae 0.010927597893438988
training loss 0.00020849698349724617 mae 0.010879584666879022
Epoch 27, training: loss: 0.0002082, mae: 0.0108802 test: loss0.0002243, mae:0.0111732
training loss 0.00017306786321569234 mae 0.010184158571064472
training loss 0.00019984574150750593 mae 0.01069565131968143
training loss 0.00021953111372387848 mae 0.011159692595869602
training loss 0.00021305481429305856 mae 0.010987405529529452
training loss 0.00020780697916993477 mae 0.01084555999667787
Epoch 28, training: loss: 0.0002068, mae: 0.0108224 test: loss0.0002126, mae:0.0109644
training loss 0.000296151265501976 mae 0.012962203472852707
training loss 0.00019537669925507635 mae 0.010494720720339051
training loss 0.00020577840106204468 mae 0.010787373694526675
training loss 0.00020383499767623577 mae 0.01076743339145223
training loss 0.0002048121567538115 mae 0.010772314365137725
Epoch 29, training: loss: 0.0002041, mae: 0.0107494 test: loss0.0002107, mae:0.0108672
training loss 0.00020407837291713804 mae 0.010998692363500595
training loss 0.00018983291988215395 mae 0.010393547131588644
training loss 0.00019922309751348055 mae 0.010677076608754024
training loss 0.00020221937099889546 mae 0.010697180318911339
training loss 0.00019849762702053774 mae 0.01058447593591403
Epoch 30, training: loss: 0.0001970, mae: 0.0105454 test: loss0.0002034, mae:0.0105182
training loss 0.00021878251573070884 mae 0.010682240128517151
training loss 0.00018490087734558165 mae 0.010161971846851064
training loss 0.00019109571836128058 mae 0.010345938822973778
training loss 0.00019368925970279 mae 0.010411339503595768
training loss 0.00019017026442727435 mae 0.010336049287973798
Epoch 31, training: loss: 0.0001898, mae: 0.0103258 test: loss0.0001924, mae:0.0103755
training loss 0.00019820169836748391 mae 0.01012380514293909
training loss 0.0001768789993993956 mae 0.01000222650484419
training loss 0.00019599200617683362 mae 0.010500877325672028
training loss 0.00019283365127314436 mae 0.01042581143301745
training loss 0.00019346007240926888 mae 0.010434860245449781
Epoch 32, training: loss: 0.0001939, mae: 0.0104481 test: loss0.0001977, mae:0.0105230
training loss 0.00013112768647260964 mae 0.009517352096736431
training loss 0.00018061169790670127 mae 0.010070279404959255
training loss 0.00018236093153361016 mae 0.01011879413877383
training loss 0.00018649721411766106 mae 0.01021932802526959
training loss 0.00018590847835343202 mae 0.010226298540608207
Epoch 33, training: loss: 0.0001861, mae: 0.0102288 test: loss0.0001882, mae:0.0102857
training loss 0.00021246085816528648 mae 0.011677507311105728
training loss 0.000184599445692227 mae 0.010265572261357422
training loss 0.00018303887263034922 mae 0.010201056003754976
training loss 0.0001774003120356653 mae 0.010051414179338134
training loss 0.0001828635566614786 mae 0.010160564757716747
Epoch 34, training: loss: 0.0001831, mae: 0.0101515 test: loss0.0001823, mae:0.0100973
training loss 0.00016548442363273352 mae 0.010231781750917435
training loss 0.00016886058994500404 mae 0.00964130224733084
training loss 0.0001740134947894067 mae 0.009842828358754075
training loss 0.00017275776590154333 mae 0.00986663614467576
training loss 0.00017510674936136245 mae 0.009950758393538832
Epoch 35, training: loss: 0.0001755, mae: 0.0099454 test: loss0.0001891, mae:0.0104259
training loss 0.00016102670633699745 mae 0.010466433130204678
training loss 0.00016326137576434837 mae 0.00974388371276505
training loss 0.0001717452862499384 mae 0.00992485393060021
training loss 0.00017403053437415141 mae 0.009928687093224354
training loss 0.00017547004433222517 mae 0.00997051866771422
Epoch 36, training: loss: 0.0001755, mae: 0.0099717 test: loss0.0001987, mae:0.0106393
training loss 0.0002650195674505085 mae 0.012632761150598526
training loss 0.00017322125552477786 mae 0.009853734105241065
training loss 0.0001767496970288529 mae 0.009963380444868667
training loss 0.0001728551074209475 mae 0.00989149610777169
training loss 0.00016779884298109634 mae 0.009732421983348491
Epoch 37, training: loss: 0.0001674, mae: 0.0097306 test: loss0.0001884, mae:0.0099635
training loss 0.00015668946434743702 mae 0.009311637841165066
training loss 0.00016089199925772843 mae 0.00959297105231706
training loss 0.00016400739487196027 mae 0.009707111667030222
training loss 0.0001652509733762388 mae 0.00969175711607203
training loss 0.0001666274448275793 mae 0.009724849570931782
Epoch 38, training: loss: 0.0001660, mae: 0.0097151 test: loss0.0001879, mae:0.0103406
training loss 0.00017888100410345942 mae 0.009127385914325714
training loss 0.00016037049214241518 mae 0.009615172549863068
training loss 0.0001668729526157916 mae 0.009716767439674028
training loss 0.0001669570261366599 mae 0.009782790400655259
training loss 0.00016691630859818753 mae 0.009742938294839945
Epoch 39, training: loss: 0.0001657, mae: 0.0097065 test: loss0.0001643, mae:0.0096300
training loss 0.00012314371997490525 mae 0.009115740656852722
training loss 0.000170518591938376 mae 0.009779977246972858
training loss 0.00016628055473660478 mae 0.009683925709180015
training loss 0.00016381793979452057 mae 0.009617597907041475
training loss 0.0001593817401383725 mae 0.009512319199425811
Epoch 40, training: loss: 0.0001589, mae: 0.0095061 test: loss0.0001631, mae:0.0095506
training loss 0.00013700388080906123 mae 0.00946279987692833
training loss 0.00016108351130741553 mae 0.00953506450990544
training loss 0.00016108193971610873 mae 0.009567827617952435
training loss 0.00015898279737587093 mae 0.009479959948691507
training loss 0.00015589538551878595 mae 0.00938413985101367
Epoch 41, training: loss: 0.0001542, mae: 0.0093395 test: loss0.0001581, mae:0.0093745
training loss 0.00011308345710858703 mae 0.00793957244604826
training loss 0.00015618579060423611 mae 0.009507790637001688
training loss 0.00015334737726883729 mae 0.009363730728515596
training loss 0.00015036641389389055 mae 0.009269569599569233
training loss 0.00015349055598988038 mae 0.009367566634507019
Epoch 42, training: loss: 0.0001533, mae: 0.0093643 test: loss0.0001606, mae:0.0095368
training loss 0.00015430449275299907 mae 0.009453009814023972
training loss 0.00015533050221468633 mae 0.009271600740213022
training loss 0.00015157679805306187 mae 0.009221005705323547
training loss 0.0001468611568415272 mae 0.009129839640924864
training loss 0.00014577192267106458 mae 0.009093639327315101
Epoch 43, training: loss: 0.0001468, mae: 0.0091176 test: loss0.0001586, mae:0.0095239
training loss 0.000155493980855681 mae 0.009662995114922523
training loss 0.00014400286868205476 mae 0.009067811236223756
training loss 0.00014692007690509046 mae 0.00919683042010016
training loss 0.0001442248497712611 mae 0.009090000609343023
training loss 0.00014621153584911967 mae 0.009125285686934324
Epoch 44, training: loss: 0.0001464, mae: 0.0091438 test: loss0.0001683, mae:0.0098680
training loss 0.00015678278577979654 mae 0.009288125671446323
training loss 0.0001393616458859897 mae 0.008956615292631526
training loss 0.0001401573243507156 mae 0.008990725165804719
training loss 0.00014229395883797737 mae 0.009039315453650346
training loss 0.000142540569985934 mae 0.009038883159446776
Epoch 45, training: loss: 0.0001441, mae: 0.0090727 test: loss0.0001507, mae:0.0092371
training loss 0.00010113862663274631 mae 0.008273781277239323
training loss 0.000140752576151178 mae 0.008996123822369409
training loss 0.0001407104069171105 mae 0.008992842232605604
training loss 0.0001377698973128464 mae 0.008878849900099418
training loss 0.0001367828044025355 mae 0.008853088116820035
Epoch 46, training: loss: 0.0001368, mae: 0.0088437 test: loss0.0001530, mae:0.0092068
training loss 0.00019418929878156632 mae 0.010573199950158596
training loss 0.00013735390601533586 mae 0.008789295011072182
training loss 0.00013235599447461305 mae 0.008701788708752042
training loss 0.00013441437954046074 mae 0.008762767930077206
training loss 0.00013507928037396235 mae 0.008783608802422807
Epoch 47, training: loss: 0.0001347, mae: 0.0087763 test: loss0.0001498, mae:0.0090728
training loss 0.00012408483598846942 mae 0.008722083643078804
training loss 0.00014311099834694507 mae 0.008934454965021677
training loss 0.00013715416614581735 mae 0.008829395752400162
training loss 0.00013462570548907163 mae 0.00875067023486392
training loss 0.0001352188816387902 mae 0.008750528453928026
Epoch 48, training: loss: 0.0001356, mae: 0.0087579 test: loss0.0001541, mae:0.0092829
training loss 0.00012085497291991487 mae 0.00826896633952856
training loss 0.00011953044304391369 mae 0.008205184265605956
training loss 0.0001227227440173046 mae 0.008291697456953255
training loss 0.000124170900207238 mae 0.008357660064708125
training loss 0.00012531974032898864 mae 0.008431483280210206
Epoch 49, training: loss: 0.0001254, mae: 0.0084351 test: loss0.0001376, mae:0.0087740
training loss 0.00011556423123693094 mae 0.008373766206204891
training loss 0.00013745487362315297 mae 0.00883343474318584
training loss 0.00013361571069650055 mae 0.008677067949740899
training loss 0.00012779550228671902 mae 0.008495479035777173
training loss 0.00012967386873020799 mae 0.008565660387703882
Epoch 50, training: loss: 0.0001300, mae: 0.0085698 test: loss0.0001433, mae:0.0091774
training loss 0.0001592296321177855 mae 0.009792736731469631
training loss 0.00012677261424958524 mae 0.0085224342057664
training loss 0.00012512285770798826 mae 0.008503629756851653
training loss 0.00012530069401667346 mae 0.008414380819789619
training loss 0.0001233421064524064 mae 0.008358373561531159
Epoch 51, training: loss: 0.0001235, mae: 0.0083670 test: loss0.0001404, mae:0.0090720
training loss 8.447107393294573e-05 mae 0.008085456676781178
training loss 0.00012477084522948174 mae 0.008481820287438583
training loss 0.00012703956073694067 mae 0.008525146535680725
training loss 0.00012431449330271534 mae 0.008452359907674472
training loss 0.0001269868360269081 mae 0.008508248729584967
Epoch 52, training: loss: 0.0001267, mae: 0.0084989 test: loss0.0001484, mae:0.0091006
training loss 0.00016342972230631858 mae 0.010133757255971432
training loss 0.00011656766704012018 mae 0.008226198381653014
training loss 0.00011958748179702272 mae 0.008262956026240739
training loss 0.00011874254000186614 mae 0.008246360251075583
training loss 0.0001191682249147779 mae 0.008239463430398438
Epoch 53, training: loss: 0.0001190, mae: 0.0082386 test: loss0.0001288, mae:0.0084595
training loss 0.00010276940884068608 mae 0.00808758195489645
training loss 0.00013680225774627544 mae 0.00885486181861922
training loss 0.00012814169448493787 mae 0.008597835737031576
training loss 0.0001265871546127901 mae 0.008454255772051427
training loss 0.000125062125131189 mae 0.008406287853823816
Epoch 54, training: loss: 0.0001241, mae: 0.0083811 test: loss0.0001323, mae:0.0086694
training loss 0.00011148442717967555 mae 0.008192437700927258
training loss 0.00011445874049825886 mae 0.008033297033797876
training loss 0.00011724915283692927 mae 0.00815833207348933
training loss 0.00011616770161889393 mae 0.008120696097365668
training loss 0.00011621787662667667 mae 0.00813597835489173
Epoch 55, training: loss: 0.0001166, mae: 0.0081491 test: loss0.0001318, mae:0.0086395
training loss 0.00013773345563095063 mae 0.00842053908854723
training loss 0.00011531469841857494 mae 0.008042262629697136
training loss 0.0001207736859214492 mae 0.008241625161527995
training loss 0.00011947545598204005 mae 0.008190693848747885
training loss 0.00011849243433356043 mae 0.008187214209146762
Epoch 56, training: loss: 0.0001187, mae: 0.0081907 test: loss0.0001665, mae:0.0099162
training loss 0.00014330142585095018 mae 0.009607244282960892
training loss 0.00011364316200271396 mae 0.008263738040684487
training loss 0.00011669509107792189 mae 0.008237138934580996
training loss 0.00011535693349456263 mae 0.008143039512762565
training loss 0.00011689015212399301 mae 0.008161670134509382
Epoch 57, training: loss: 0.0001171, mae: 0.0081569 test: loss0.0001281, mae:0.0085627
training loss 0.0001367618824588135 mae 0.008570757694542408
training loss 0.00011767411157123599 mae 0.008243398567405984
training loss 0.00011042492150237998 mae 0.008005384475656662
training loss 0.00011491807288392325 mae 0.008165945265047398
training loss 0.00011659594262406846 mae 0.008183091767688291
Epoch 58, training: loss: 0.0001169, mae: 0.0081835 test: loss0.0001343, mae:0.0088231
training loss 0.00011498649109853432 mae 0.007487661670893431
training loss 0.00010855667884849636 mae 0.007943256721630985
training loss 0.00011261755578671726 mae 0.00793224669033938
training loss 0.00011443273820228178 mae 0.008009096508882691
training loss 0.00011378186082308747 mae 0.008028477006383351
Epoch 59, training: loss: 0.0001141, mae: 0.0080437 test: loss0.0001290, mae:0.0085161
training loss 0.0001103597242035903 mae 0.007772988174110651
training loss 0.00011471799128533652 mae 0.008034066607554756
training loss 0.00011186300987829592 mae 0.007939985575209748
training loss 0.00011221743329017213 mae 0.007951300036137466
training loss 0.00011148208895476941 mae 0.00793992478252198
Epoch 60, training: loss: 0.0001116, mae: 0.0079594 test: loss0.0001281, mae:0.0084973
training loss 9.547769877826795e-05 mae 0.007401806768029928
training loss 0.00010562386887613683 mae 0.00767916606227849
training loss 0.00010592235455555738 mae 0.007731096422539491
training loss 0.00010833968060027142 mae 0.007830706600790585
training loss 0.00010931450552602445 mae 0.007846630071822685
Epoch 61, training: loss: 0.0001098, mae: 0.0078654 test: loss0.0001290, mae:0.0085806
training loss 0.0001142643959610723 mae 0.008122320286929607
training loss 0.00012026312626902851 mae 0.008253443949654991
training loss 0.00011587264287137968 mae 0.008124084493126906
training loss 0.00011546504041966397 mae 0.008098245789574474
training loss 0.00011439159899412548 mae 0.008063114746311559
Epoch 62, training: loss: 0.0001153, mae: 0.0080890 test: loss0.0001368, mae:0.0087358
training loss 0.00012149282702011988 mae 0.009105737321078777
training loss 0.00011351114510177343 mae 0.008027224363211322
training loss 0.0001153816563889297 mae 0.008084292399079197
training loss 0.0001144712910201441 mae 0.008059209018378265
training loss 0.00011111096603088132 mae 0.007971847541074254
Epoch 63, training: loss: 0.0001108, mae: 0.0079578 test: loss0.0001265, mae:0.0082958
training loss 7.358210859820247e-05 mae 0.0059924074448645115
training loss 9.809394018078571e-05 mae 0.0075837919470287994
training loss 0.00010410400842590776 mae 0.00771810738185402
training loss 0.0001089511334546071 mae 0.007892170103747913
training loss 0.0001098336918706385 mae 0.007901183862368858
Epoch 64, training: loss: 0.0001103, mae: 0.0079162 test: loss0.0001198, mae:0.0081146
training loss 8.973647345555946e-05 mae 0.007359223905950785
training loss 0.00010170065733665765 mae 0.0075888430754489766
training loss 0.00010333196758546195 mae 0.007712425834656058
training loss 0.00010534496580879668 mae 0.007735459590392397
training loss 0.00010677409051791233 mae 0.007800920523554829
Epoch 65, training: loss: 0.0001080, mae: 0.0078315 test: loss0.0001232, mae:0.0083628
training loss 0.00011502153938636184 mae 0.008401871658861637
training loss 0.00011391175483403654 mae 0.008040946816989022
training loss 0.00010797181070896353 mae 0.007827260748178948
training loss 0.00010581885227707634 mae 0.007769990323839204
training loss 0.00010322862098703566 mae 0.007703137681332988
Epoch 66, training: loss: 0.0001026, mae: 0.0076873 test: loss0.0001285, mae:0.0084510
training loss 0.00012336349755059928 mae 0.008262037299573421
training loss 0.00010037308660970416 mae 0.00755080095875789
training loss 9.953719729472345e-05 mae 0.00757029700544801
training loss 9.995450124812211e-05 mae 0.007588222030772279
training loss 9.95966832699956e-05 mae 0.007558440518523774
Epoch 67, training: loss: 0.0001005, mae: 0.0075988 test: loss0.0001227, mae:0.0081731
training loss 0.0002418098010821268 mae 0.011433216743171215
training loss 9.623534281797449e-05 mae 0.007480434498147053
training loss 0.00010137173999417897 mae 0.00764461957139544
training loss 0.00010427815586868953 mae 0.00774914778719675
training loss 0.00010577470633715037 mae 0.007788098324553592
Epoch 68, training: loss: 0.0001047, mae: 0.0077520 test: loss0.0001142, mae:0.0080174
training loss 5.2726114518009126e-05 mae 0.005604317877441645
training loss 9.48062288265575e-05 mae 0.007304315857004887
training loss 9.590319659944742e-05 mae 0.007395308192745593
training loss 9.816242619001285e-05 mae 0.007492808887229257
training loss 0.00010099329663726118 mae 0.0075753133683766575
Epoch 69, training: loss: 0.0001010, mae: 0.0075862 test: loss0.0001173, mae:0.0081555
training loss 9.439847053727135e-05 mae 0.007911617867648602
training loss 9.877737523079394e-05 mae 0.007614732747349669
training loss 0.00010020556903039657 mae 0.0075948333902524255
training loss 9.970279293969756e-05 mae 0.007556070080212014
training loss 9.990785581442585e-05 mae 0.00755703260204685
Epoch 70, training: loss: 0.0000999, mae: 0.0075585 test: loss0.0001096, mae:0.0077762
training loss 7.227060268633068e-05 mae 0.006507655140012503
training loss 9.441862515386139e-05 mae 0.0073068063505286095
training loss 9.344480209205858e-05 mae 0.00731984687747784
training loss 9.760189990725808e-05 mae 0.007470948818997044
training loss 9.422742821254758e-05 mae 0.007378021278302767
Epoch 71, training: loss: 0.0000949, mae: 0.0073926 test: loss0.0001145, mae:0.0079753
training loss 6.883956666570157e-05 mae 0.006560046691447496
training loss 9.985315737853229e-05 mae 0.007493048011526174
training loss 9.865009781038895e-05 mae 0.0075177296415192656
training loss 9.983459182834528e-05 mae 0.00753727535538326
training loss 0.00010064481196861918 mae 0.00758827488582155
Epoch 72, training: loss: 0.0000997, mae: 0.0075540 test: loss0.0001161, mae:0.0079586
training loss 5.254734060144983e-05 mae 0.005595156457275152
training loss 9.984122951601761e-05 mae 0.007555014682093671
training loss 9.595556220669057e-05 mae 0.007410386466596386
training loss 9.391257211347014e-05 mae 0.007358350112590173
training loss 9.617428283368707e-05 mae 0.007431777796490275
Epoch 73, training: loss: 0.0000969, mae: 0.0074569 test: loss0.0001177, mae:0.0081388
training loss 5.174949546926655e-05 mae 0.005678270012140274
training loss 9.254542366012605e-05 mae 0.007174038054311978
training loss 9.144206331365068e-05 mae 0.0072040677826740954
training loss 9.482209794284986e-05 mae 0.007332487133584474
training loss 9.351998531527178e-05 mae 0.007291423983927539
Epoch 74, training: loss: 0.0000940, mae: 0.0073195 test: loss0.0001150, mae:0.0079842
training loss 8.710718975635245e-05 mae 0.007130714599043131
training loss 9.096658479892555e-05 mae 0.007217028808286962
training loss 8.877272647632614e-05 mae 0.00715201502979392
training loss 9.126843541412653e-05 mae 0.007277848544401048
training loss 9.041865634562833e-05 mae 0.0072404097867619956
Epoch 75, training: loss: 0.0000910, mae: 0.0072538 test: loss0.0001201, mae:0.0081799
training loss 0.00010408237722003832 mae 0.008302208967506886
training loss 8.89982876804995e-05 mae 0.0072482693027339715
training loss 9.3870334456027e-05 mae 0.007379513215458037
training loss 9.153831670359887e-05 mae 0.007307336893894814
training loss 9.279727756751784e-05 mae 0.0073303783808218595
Epoch 76, training: loss: 0.0000932, mae: 0.0073378 test: loss0.0001070, mae:0.0078276
training loss 7.376595749519765e-05 mae 0.0072202011942863464
training loss 9.413306272072788e-05 mae 0.007265352123581312
training loss 8.972769209511096e-05 mae 0.007130180310347295
training loss 9.02409056226248e-05 mae 0.007177569243489512
training loss 8.93257341370223e-05 mae 0.007176523886861939
Epoch 77, training: loss: 0.0000888, mae: 0.0071603 test: loss0.0001042, mae:0.0075751
training loss 6.12486110185273e-05 mae 0.006435208022594452
training loss 8.14596216300475e-05 mae 0.0069215547969090005
training loss 8.458357349287088e-05 mae 0.007013985186372652
training loss 8.737848356939747e-05 mae 0.007106147617076999
training loss 8.81326434941526e-05 mae 0.0071358168567183305
Epoch 78, training: loss: 0.0000876, mae: 0.0071219 test: loss0.0001066, mae:0.0076371
training loss 7.769009243929759e-05 mae 0.00741930166259408
training loss 7.768561223273021e-05 mae 0.006778684718643919
training loss 8.656968105305449e-05 mae 0.007113970124559236
training loss 8.938551925424186e-05 mae 0.0071862252638829486
training loss 8.68059944864467e-05 mae 0.007102285208301021
Epoch 79, training: loss: 0.0000877, mae: 0.0071079 test: loss0.0001248, mae:0.0084804
training loss 0.00013207283336669207 mae 0.009317022748291492
training loss 8.300100607277056e-05 mae 0.00692278235310725
training loss 8.801006715477323e-05 mae 0.007147857527442202
training loss 8.618337794558597e-05 mae 0.0070831287214347454
training loss 8.626434110688274e-05 mae 0.007098915650791939
Epoch 80, training: loss: 0.0000863, mae: 0.0071082 test: loss0.0001301, mae:0.0082659
training loss 0.00011921642726520076 mae 0.00900768581777811
training loss 8.94196400979269e-05 mae 0.007120197748436648
training loss 8.537248623445588e-05 mae 0.006994238372265113
training loss 8.404281651269684e-05 mae 0.006975050392677846
training loss 8.538727706763895e-05 mae 0.007020589791293909
Epoch 81, training: loss: 0.0000848, mae: 0.0069984 test: loss0.0001232, mae:0.0080102
training loss 5.535857053473592e-05 mae 0.0060348547995090485
training loss 9.223000333626185e-05 mae 0.0071837465717073746
training loss 8.772027066166481e-05 mae 0.007042470337398865
training loss 8.417743941250914e-05 mae 0.00695236949620164
training loss 8.505942049900768e-05 mae 0.006995803632527885
Epoch 82, training: loss: 0.0000858, mae: 0.0070190 test: loss0.0001063, mae:0.0075720
training loss 6.83915859553963e-05 mae 0.006397528108209372
training loss 7.73273501431058e-05 mae 0.006813722443492974
training loss 7.97648713745333e-05 mae 0.006845722795639298
training loss 7.972956733066516e-05 mae 0.0068274362641849275
training loss 7.898339853866432e-05 mae 0.006792322929429622
Epoch 83, training: loss: 0.0000794, mae: 0.0068066 test: loss0.0000985, mae:0.0073693
training loss 8.826478733681142e-05 mae 0.006752400193363428
training loss 8.034235496797561e-05 mae 0.0068892382779249944
training loss 7.994919518985101e-05 mae 0.006833469771807736
training loss 8.063732372906789e-05 mae 0.006880192505504122
training loss 8.040938844000199e-05 mae 0.0068712914223546424
Epoch 84, training: loss: 0.0000801, mae: 0.0068556 test: loss0.0000972, mae:0.0073179
training loss 5.3941843361826614e-05 mae 0.005368390586227179
training loss 7.637000410742237e-05 mae 0.006700813122020631
training loss 7.537878386441173e-05 mae 0.006647368265998245
training loss 7.446708344327845e-05 mae 0.006619784034270522
training loss 7.68351422795685e-05 mae 0.006731431173688886
Epoch 85, training: loss: 0.0000772, mae: 0.0067403 test: loss0.0001042, mae:0.0075971
training loss 5.558374687097967e-05 mae 0.005640290677547455
training loss 7.772251610374808e-05 mae 0.006815069355070592
training loss 7.825796857636448e-05 mae 0.006771179635335904
training loss 7.856537703392309e-05 mae 0.006756223661097275
training loss 7.86771003247542e-05 mae 0.006762885085459969
Epoch 86, training: loss: 0.0000785, mae: 0.0067617 test: loss0.0001064, mae:0.0076480
training loss 6.658936035819352e-05 mae 0.006669366266578436
training loss 7.468393981552627e-05 mae 0.006681153952491049
training loss 7.370322775458778e-05 mae 0.006638806018203792
training loss 7.569215664044884e-05 mae 0.0066586686575797645
training loss 7.54760389534214e-05 mae 0.006640132213131854
Epoch 87, training: loss: 0.0000755, mae: 0.0066341 test: loss0.0000956, mae:0.0072884
training loss 8.529797196388245e-05 mae 0.0069367121905088425
training loss 6.982176401994791e-05 mae 0.006463533093897152
training loss 7.649470299469664e-05 mae 0.006625420529174037
training loss 7.695780476938395e-05 mae 0.006696035943629331
training loss 7.831919162585722e-05 mae 0.006727916580064234
Epoch 88, training: loss: 0.0000780, mae: 0.0067152 test: loss0.0001025, mae:0.0075508
training loss 4.550923404167406e-05 mae 0.00528094032779336
training loss 7.659280256085172e-05 mae 0.006698573090355186
training loss 7.413216850848286e-05 mae 0.006618196974591452
training loss 7.587824002739402e-05 mae 0.006674627856727664
training loss 7.767828274561693e-05 mae 0.006736028147166345
Epoch 89, training: loss: 0.0000778, mae: 0.0067454 test: loss0.0001170, mae:0.0077972
training loss 4.783153417520225e-05 mae 0.005371699575334787
training loss 8.526893768302057e-05 mae 0.007094758297955873
training loss 8.049654433766549e-05 mae 0.006873209762506851
training loss 8.001657198406816e-05 mae 0.00686271356369387
training loss 7.8589396076468e-05 mae 0.0067835198089814
Epoch 90, training: loss: 0.0000783, mae: 0.0067701 test: loss0.0000952, mae:0.0072865
training loss 4.182998964097351e-05 mae 0.005439744796603918
training loss 7.12159634727583e-05 mae 0.006411973907447913
training loss 7.479236700248186e-05 mae 0.0065718969966441715
training loss 7.452989324375318e-05 mae 0.00657191336932068
training loss 7.46585991244012e-05 mae 0.006590444833244109
Epoch 91, training: loss: 0.0000746, mae: 0.0065900 test: loss0.0001001, mae:0.0073595
training loss 3.983297210652381e-05 mae 0.005354650784283876
training loss 6.29812825982477e-05 mae 0.006181895532006142
training loss 6.692309682758864e-05 mae 0.00633744159621177
training loss 6.628234621903718e-05 mae 0.006286618830204402
training loss 6.96590260616434e-05 mae 0.0063952154466020514
Epoch 92, training: loss: 0.0000698, mae: 0.0063976 test: loss0.0001040, mae:0.0075266
training loss 0.00011564812302822247 mae 0.008272543549537659
training loss 7.549679474893719e-05 mae 0.00659841507234994
training loss 7.252161749690488e-05 mae 0.006538036735820592
training loss 7.264988737054394e-05 mae 0.006551198547224925
training loss 7.15735562456151e-05 mae 0.006488876709186316
Epoch 93, training: loss: 0.0000715, mae: 0.0064944 test: loss0.0001001, mae:0.0074679
training loss 8.548425830667838e-05 mae 0.0075847189873456955
training loss 6.994594328787965e-05 mae 0.006469339232745709
training loss 7.088289055558346e-05 mae 0.006496636160086879
training loss 7.090764428606093e-05 mae 0.0064631203522547975
training loss 7.09381457812046e-05 mae 0.00644668215996961
Epoch 94, training: loss: 0.0000709, mae: 0.0064414 test: loss0.0001110, mae:0.0076971
training loss 6.72784517519176e-05 mae 0.0067514986731112
training loss 7.496062101506371e-05 mae 0.006661391059192373
training loss 7.72740690675587e-05 mae 0.006677650284058977
training loss 7.472280928493473e-05 mae 0.0065987012657454085
training loss 7.362488987151914e-05 mae 0.006566121747986003
Epoch 95, training: loss: 0.0000735, mae: 0.0065635 test: loss0.0000928, mae:0.0071720
training loss 7.751046359771863e-05 mae 0.006091488059610128
training loss 6.8827157163326e-05 mae 0.006321715638406722
training loss 6.748617274166861e-05 mae 0.00629403377597285
training loss 6.7840140862799e-05 mae 0.0063561402070058505
training loss 6.757760278542355e-05 mae 0.006340922859603936
Epoch 96, training: loss: 0.0000677, mae: 0.0063396 test: loss0.0000902, mae:0.0070356
training loss 8.179082215065137e-05 mae 0.006014648824930191
training loss 6.932531965396584e-05 mae 0.006284723953143056
training loss 6.912805631397737e-05 mae 0.006280080498297615
training loss 6.818563924030327e-05 mae 0.006272411849719799
training loss 6.800398551237786e-05 mae 0.006263980421872429
Epoch 97, training: loss: 0.0000681, mae: 0.0062738 test: loss0.0000993, mae:0.0073525
training loss 8.727368549443781e-05 mae 0.007354299072176218
training loss 7.181694102245295e-05 mae 0.006462700016723544
training loss 6.910290132941782e-05 mae 0.006353171920348512
training loss 6.772413232000641e-05 mae 0.006313806085150367
training loss 6.674695788970708e-05 mae 0.006298545794329833
Epoch 98, training: loss: 0.0000665, mae: 0.0062900 test: loss0.0000896, mae:0.0070849
training loss 3.820806523435749e-05 mae 0.0048523470759391785
training loss 6.227137583111613e-05 mae 0.006010739488855881
training loss 6.409662073745511e-05 mae 0.006131416763003806
training loss 6.590221364483611e-05 mae 0.006236702280846849
training loss 6.675897136449913e-05 mae 0.006300666483017192
Epoch 99, training: loss: 0.0000671, mae: 0.0063155 test: loss0.0000902, mae:0.0070261
current learning rate: 0.00025
training loss 9.763194975676015e-05 mae 0.007559934165328741
training loss 5.6615178490413685e-05 mae 0.0058339511628682705
training loss 5.497016801281561e-05 mae 0.005743677043217685
training loss 5.423492354825401e-05 mae 0.005709134288971001
training loss 5.360955502008222e-05 mae 0.005672278932170642
Epoch 100, training: loss: 0.0000533, mae: 0.0056496 test: loss0.0000820, mae:0.0066758
training loss 4.717388947028667e-05 mae 0.00533381849527359
training loss 4.8198389063769606e-05 mae 0.005412120527277389
training loss 4.848592164640955e-05 mae 0.005409719519280265
training loss 4.965082957854167e-05 mae 0.0054811807318485735
training loss 4.9718321132791034e-05 mae 0.005484186638776791
Epoch 101, training: loss: 0.0000498, mae: 0.0054839 test: loss0.0000842, mae:0.0067792
training loss 3.7177767808316275e-05 mae 0.005025162361562252
training loss 4.982595023380673e-05 mae 0.005497137468089075
training loss 5.019539423643651e-05 mae 0.0054843921656950855
training loss 4.9678015880963585e-05 mae 0.005478753455427309
training loss 5.0024810856421335e-05 mae 0.005501445540370631
Epoch 102, training: loss: 0.0000499, mae: 0.0054915 test: loss0.0000815, mae:0.0066378
training loss 4.802109833690338e-05 mae 0.005151058547198772
training loss 4.6477639960928586e-05 mae 0.005352895478626678
training loss 4.7383352079357277e-05 mae 0.005397607249641184
training loss 4.775094176567044e-05 mae 0.005375228680374213
training loss 4.798714594946788e-05 mae 0.005387342891734631
Epoch 103, training: loss: 0.0000480, mae: 0.0053868 test: loss0.0000823, mae:0.0066819
training loss 3.361429844517261e-05 mae 0.004731783177703619
training loss 4.619750339485359e-05 mae 0.0053171033003166605
training loss 4.6457957140963044e-05 mae 0.005335121535428681
training loss 4.755200000286307e-05 mae 0.0053667862042291285
training loss 4.739140597880423e-05 mae 0.005353454180729032
Epoch 104, training: loss: 0.0000472, mae: 0.0053384 test: loss0.0000838, mae:0.0067272
training loss 3.6408877349458635e-05 mae 0.004854539409279823
training loss 4.66319310291118e-05 mae 0.00539721108461712
training loss 4.7594197444550895e-05 mae 0.005415924050208956
training loss 4.821716746972988e-05 mae 0.0054203265389760604
training loss 4.797925969828449e-05 mae 0.0053978608110317235
Epoch 105, training: loss: 0.0000481, mae: 0.0054059 test: loss0.0000867, mae:0.0068061
training loss 3.616325557231903e-05 mae 0.0048451810143888
training loss 4.4335937655334576e-05 mae 0.005159908170610957
training loss 4.485711013168641e-05 mae 0.0052100111393829675
training loss 4.597400697551448e-05 mae 0.00528272739549889
training loss 4.6036458600008965e-05 mae 0.005281337447677604
Epoch 106, training: loss: 0.0000460, mae: 0.0052847 test: loss0.0000809, mae:0.0066035
training loss 4.07295701734256e-05 mae 0.004982864949852228
training loss 4.3416970940740486e-05 mae 0.005132871976706622
training loss 4.3467182345777084e-05 mae 0.005123821479074732
training loss 4.453873361552609e-05 mae 0.005180618879909551
training loss 4.505862403195931e-05 mae 0.005212483978575441
Epoch 107, training: loss: 0.0000451, mae: 0.0052099 test: loss0.0000847, mae:0.0067493
training loss 4.332931712269783e-05 mae 0.004820352885872126
training loss 4.3908521868562437e-05 mae 0.005214187805084327
training loss 4.5461059926310554e-05 mae 0.00528540346992783
training loss 4.539347425471707e-05 mae 0.005268855669554199
training loss 4.583997511121315e-05 mae 0.005281315379734358
Epoch 108, training: loss: 0.0000458, mae: 0.0052844 test: loss0.0000839, mae:0.0067379
training loss 5.3663778089685366e-05 mae 0.005718491971492767
training loss 4.359815701558118e-05 mae 0.005104429572455438
training loss 4.1745899219641064e-05 mae 0.005031623333545012
training loss 4.341284233341625e-05 mae 0.005125667878260085
training loss 4.3920329947138106e-05 mae 0.005153596083823572
Epoch 109, training: loss: 0.0000439, mae: 0.0051563 test: loss0.0000815, mae:0.0066058
training loss 4.613087003235705e-05 mae 0.005391110200434923
training loss 4.1803669827298594e-05 mae 0.005017972884553613
training loss 4.4849247649043454e-05 mae 0.005224917104725939
training loss 4.431052293993892e-05 mae 0.005191336116068905
training loss 4.4505273962300174e-05 mae 0.00518827653832882
Epoch 110, training: loss: 0.0000445, mae: 0.0051864 test: loss0.0000873, mae:0.0067959
training loss 4.3869786168215796e-05 mae 0.005314567591995001
training loss 4.330420800697004e-05 mae 0.005066987565335105
training loss 4.212381685251199e-05 mae 0.005058263829623413
training loss 4.255206679332838e-05 mae 0.005112491839485155
training loss 4.3272497350699255e-05 mae 0.005155976376361882
Epoch 111, training: loss: 0.0000432, mae: 0.0051496 test: loss0.0000810, mae:0.0065924
training loss 3.383550210855901e-05 mae 0.004650090355426073
training loss 4.1369435184249446e-05 mae 0.0050328380939568965
training loss 4.176100019952236e-05 mae 0.005071568305115447
training loss 4.17600546662645e-05 mae 0.005054811914667292
training loss 4.1905126482420904e-05 mae 0.0050609710589236585
Epoch 112, training: loss: 0.0000419, mae: 0.0050518 test: loss0.0000826, mae:0.0066807
training loss 4.368154986877926e-05 mae 0.004987589083611965
training loss 4.274887696203009e-05 mae 0.005065020671406505
training loss 4.2465864212452754e-05 mae 0.0050748277197379885
training loss 4.2517095710157064e-05 mae 0.005091806858259912
training loss 4.222006750231104e-05 mae 0.005074598412932965
Epoch 113, training: loss: 0.0000423, mae: 0.0050779 test: loss0.0000825, mae:0.0065751
training loss 4.263335722498596e-05 mae 0.004883375484496355
training loss 3.636154278687101e-05 mae 0.004767323547389868
training loss 3.802963735801997e-05 mae 0.00483867571006833
training loss 3.981156590192685e-05 mae 0.004918350082581602
training loss 4.1132600804914234e-05 mae 0.004988716783196622
Epoch 114, training: loss: 0.0000414, mae: 0.0050097 test: loss0.0000819, mae:0.0066132
training loss 3.60096491931472e-05 mae 0.005006567109376192
training loss 4.0283537993102576e-05 mae 0.005009133706563244
training loss 4.1640407235348966e-05 mae 0.005035522158746377
training loss 4.107989371545686e-05 mae 0.005012523270341637
training loss 4.162736566928714e-05 mae 0.005050614815603811
Epoch 115, training: loss: 0.0000414, mae: 0.0050385 test: loss0.0000787, mae:0.0065637
training loss 3.6467517929850146e-05 mae 0.004912737291306257
training loss 3.697213786853907e-05 mae 0.0047456035847026935
training loss 3.9541857728577853e-05 mae 0.00492375426566099
training loss 4.082179261093561e-05 mae 0.005010099520302369
training loss 4.05871802892458e-05 mae 0.004984242166276666
Epoch 116, training: loss: 0.0000408, mae: 0.0049972 test: loss0.0000855, mae:0.0068197
training loss 4.944613829138689e-05 mae 0.005636641290038824
training loss 3.909724203849156e-05 mae 0.004918557893046561
training loss 4.007250768919753e-05 mae 0.004922530463998125
training loss 3.9221803689127276e-05 mae 0.004885496659549759
training loss 3.9855774414704743e-05 mae 0.004924457516771425
Epoch 117, training: loss: 0.0000399, mae: 0.0049283 test: loss0.0000801, mae:0.0065455
training loss 3.7316382076824084e-05 mae 0.004742534831166267
training loss 3.670800327324518e-05 mae 0.004703172998429805
training loss 3.7064312756670134e-05 mae 0.004741185593723068
training loss 3.8314334630818924e-05 mae 0.004830870652472634
training loss 3.871796312225016e-05 mae 0.004862141015879176
Epoch 118, training: loss: 0.0000386, mae: 0.0048610 test: loss0.0000790, mae:0.0065097
training loss 2.8890521207358688e-05 mae 0.004200553055852652
training loss 3.9469244417405746e-05 mae 0.0049680666313232725
training loss 3.795557961776729e-05 mae 0.004842149232062372
training loss 3.8179012552437234e-05 mae 0.0048369761241834255
training loss 3.813060934431513e-05 mae 0.004844869579656498
Epoch 119, training: loss: 0.0000383, mae: 0.0048557 test: loss0.0000827, mae:0.0067089
training loss 4.0801303839543834e-05 mae 0.005200165323913097
training loss 3.6170188632597424e-05 mae 0.004678319377240306
training loss 3.694405041633308e-05 mae 0.004722701436332842
training loss 3.800235922818014e-05 mae 0.004785176344980663
training loss 3.77726882654306e-05 mae 0.004779874374951001
Epoch 120, training: loss: 0.0000378, mae: 0.0047823 test: loss0.0000821, mae:0.0066315
training loss 3.707599171320908e-05 mae 0.00449674716219306
training loss 3.7516981975903604e-05 mae 0.004742347165102175
training loss 3.757273151506417e-05 mae 0.004761260763585271
training loss 3.741817568723324e-05 mae 0.004757844673358641
training loss 3.7022999556450216e-05 mae 0.0047472629994987975
Epoch 121, training: loss: 0.0000371, mae: 0.0047588 test: loss0.0000794, mae:0.0064990
training loss 2.4517628844478168e-05 mae 0.004037606529891491
training loss 3.7382325043675815e-05 mae 0.004723511211683645
training loss 3.764362747352778e-05 mae 0.0047386978697585
training loss 3.782134513582323e-05 mae 0.004766028801546683
training loss 3.775355396295648e-05 mae 0.004773646788856946
Epoch 122, training: loss: 0.0000377, mae: 0.0047719 test: loss0.0000800, mae:0.0065436
training loss 2.4519054932170548e-05 mae 0.003991889767348766
training loss 3.463405818026741e-05 mae 0.00462564404633846
training loss 3.522999903376186e-05 mae 0.004644386984389462
training loss 3.556677604848533e-05 mae 0.0046704877044586165
training loss 3.5882642357558034e-05 mae 0.004704101694590269
Epoch 123, training: loss: 0.0000361, mae: 0.0047163 test: loss0.0000798, mae:0.0065056
training loss 3.589892003219575e-05 mae 0.004574211314320564
training loss 3.543331837920727e-05 mae 0.004657422758492769
training loss 3.521342751801147e-05 mae 0.004641003491781137
training loss 3.5181979705916485e-05 mae 0.004631671035316906
training loss 3.517274710057832e-05 mae 0.004624010849076871
Epoch 124, training: loss: 0.0000351, mae: 0.0046160 test: loss0.0000792, mae:0.0065079
training loss 3.755615034606308e-05 mae 0.004659838508814573
training loss 3.657982033830338e-05 mae 0.004647330193798621
training loss 3.771804757087751e-05 mae 0.004745216267921104
training loss 3.6804686853718414e-05 mae 0.0047107597146898664
training loss 3.7446999244391346e-05 mae 0.004754813053223549
Epoch 125, training: loss: 0.0000376, mae: 0.0047704 test: loss0.0000802, mae:0.0065883
training loss 2.64963000518037e-05 mae 0.004316818434745073
training loss 3.223106168483283e-05 mae 0.004475167217463546
training loss 3.299849433262514e-05 mae 0.004497763903645595
training loss 3.3534120114725255e-05 mae 0.004525865694902688
training loss 3.410281914425895e-05 mae 0.004560173427760232
Epoch 126, training: loss: 0.0000344, mae: 0.0045859 test: loss0.0000808, mae:0.0065814
training loss 3.3395557693438604e-05 mae 0.004350435920059681
training loss 3.647861922217997e-05 mae 0.004738968450064752
training loss 3.5052533060632686e-05 mae 0.0046499950354826625
training loss 3.5183281548200185e-05 mae 0.00463957665642316
training loss 3.4657463384155114e-05 mae 0.004603333819879963
Epoch 127, training: loss: 0.0000345, mae: 0.0045916 test: loss0.0000844, mae:0.0067322
training loss 2.9699505830649287e-05 mae 0.004000736866146326
training loss 3.38391450450039e-05 mae 0.004478025852757342
training loss 3.372927568478298e-05 mae 0.004528435763192945
training loss 3.3894739281336016e-05 mae 0.004538026242054357
training loss 3.408068057716666e-05 mae 0.0045596322285669354
Epoch 128, training: loss: 0.0000341, mae: 0.0045673 test: loss0.0000822, mae:0.0066496
training loss 3.5860070056514814e-05 mae 0.0042165606282651424
training loss 3.537459548062865e-05 mae 0.004620064803234796
training loss 3.4688317226586034e-05 mae 0.004600108815541508
training loss 3.534985604491285e-05 mae 0.004658347533843077
training loss 3.484110175801351e-05 mae 0.004627966642870904
Epoch 129, training: loss: 0.0000349, mae: 0.0046330 test: loss0.0000813, mae:0.0065720
training loss 4.615854049916379e-05 mae 0.005112247075885534
training loss 3.623943463454077e-05 mae 0.004716313393859595
training loss 3.4226155350714376e-05 mae 0.004579141000654585
training loss 3.373157363099705e-05 mae 0.004538800248210104
training loss 3.350830244849405e-05 mae 0.004521789552819966
Epoch 130, training: loss: 0.0000336, mae: 0.0045229 test: loss0.0000786, mae:0.0064882
training loss 3.461391679593362e-05 mae 0.004872528836131096
training loss 3.1729611888009696e-05 mae 0.004374256412334302
training loss 3.151356107235475e-05 mae 0.004387753349406145
training loss 3.290170582881758e-05 mae 0.0044715679142722795
training loss 3.26760596927575e-05 mae 0.004460102009739892
Epoch 131, training: loss: 0.0000326, mae: 0.0044586 test: loss0.0000781, mae:0.0064450
training loss 3.299695526948199e-05 mae 0.004476283676922321
training loss 2.935283284351303e-05 mae 0.004245704446243597
training loss 3.0051540631062054e-05 mae 0.004313762726188445
training loss 3.129045400168711e-05 mae 0.004373678841955024
training loss 3.173064583709343e-05 mae 0.004398200664419069
Epoch 132, training: loss: 0.0000319, mae: 0.0044082 test: loss0.0000815, mae:0.0065919
training loss 4.1491424781270325e-05 mae 0.004975845105946064
training loss 3.4161696734657856e-05 mae 0.004545823826144138
training loss 3.376664898325436e-05 mae 0.004538940662278396
training loss 3.281703106498128e-05 mae 0.004471426855908421
training loss 3.3123653926637586e-05 mae 0.004500394158843737
Epoch 133, training: loss: 0.0000330, mae: 0.0044911 test: loss0.0000769, mae:0.0063713
training loss 3.051850580959581e-05 mae 0.0043816049583256245
training loss 3.134518056849987e-05 mae 0.004352857933982331
training loss 3.1655340472473715e-05 mae 0.004389687219985052
training loss 3.205874539867032e-05 mae 0.004419710300554402
training loss 3.1979862321906636e-05 mae 0.004406771160411967
Epoch 134, training: loss: 0.0000321, mae: 0.0044214 test: loss0.0000770, mae:0.0064064
training loss 1.655278538237326e-05 mae 0.0031178444623947144
training loss 2.8604159881212455e-05 mae 0.004208316063216213
training loss 3.16081716459173e-05 mae 0.004399438234193638
training loss 3.1473925554751346e-05 mae 0.0043895273725348
training loss 3.159552098754175e-05 mae 0.004396869981689236
Epoch 135, training: loss: 0.0000315, mae: 0.0043866 test: loss0.0000783, mae:0.0064632
training loss 3.4868193324655294e-05 mae 0.004877118859440088
training loss 3.425537322661565e-05 mae 0.004601174490708932
training loss 3.3115187239438984e-05 mae 0.004483304034900103
training loss 3.210635235072854e-05 mae 0.0044387646377185295
training loss 3.193367371944923e-05 mae 0.004414111960781229
Epoch 136, training: loss: 0.0000319, mae: 0.0044148 test: loss0.0000838, mae:0.0066550
training loss 3.77100586774759e-05 mae 0.004729485604912043
training loss 3.538636723105276e-05 mae 0.00458539898196856
training loss 3.3825958015236974e-05 mae 0.004522055018656325
training loss 3.268582125183577e-05 mae 0.004448086735699947
training loss 3.275212220071795e-05 mae 0.0044575051001426065
Epoch 137, training: loss: 0.0000328, mae: 0.0044655 test: loss0.0000775, mae:0.0064212
training loss 4.779982555191964e-05 mae 0.004922051448374987
training loss 3.171970988036014e-05 mae 0.004346840864266543
training loss 3.184990894417095e-05 mae 0.004362904759455878
training loss 3.0655165796845834e-05 mae 0.0043020591460513746
training loss 3.0655056040963425e-05 mae 0.004317706404485511
Epoch 138, training: loss: 0.0000307, mae: 0.0043176 test: loss0.0000752, mae:0.0063029
training loss 1.9838758817058988e-05 mae 0.003574295900762081
training loss 2.79132982363009e-05 mae 0.004115514849842181
training loss 2.8440795140701117e-05 mae 0.004173543809055546
training loss 2.8915287302217193e-05 mae 0.004213321648710811
training loss 2.957477886298531e-05 mae 0.004246516727763622
Epoch 139, training: loss: 0.0000296, mae: 0.0042515 test: loss0.0000799, mae:0.0065168
training loss 2.091273017867934e-05 mae 0.003129528136923909
training loss 2.8067297463684212e-05 mae 0.00412843535270761
training loss 3.0536125333389575e-05 mae 0.004294729215268157
training loss 3.0434478750542597e-05 mae 0.004294494447600565
training loss 2.9999455219509415e-05 mae 0.004272296959858627
Epoch 140, training: loss: 0.0000302, mae: 0.0042876 test: loss0.0000790, mae:0.0065181
training loss 3.095703141298145e-05 mae 0.0042787822894752026
training loss 2.8334409102635442e-05 mae 0.00412869405494455
training loss 2.944064040588778e-05 mae 0.004242892102753319
training loss 2.9081508396995196e-05 mae 0.004202349933283712
training loss 2.939013444771643e-05 mae 0.004231372616480833
Epoch 141, training: loss: 0.0000295, mae: 0.0042442 test: loss0.0000798, mae:0.0065936
training loss 3.688377546495758e-05 mae 0.004820451606065035
training loss 3.171064824581502e-05 mae 0.004378560994404787
training loss 3.040675664583386e-05 mae 0.004292639516441539
training loss 2.9416442065974234e-05 mae 0.0042248746142753485
training loss 2.9391862270457025e-05 mae 0.004220588269425709
Epoch 142, training: loss: 0.0000295, mae: 0.0042279 test: loss0.0000781, mae:0.0064580
training loss 2.8599301003850996e-05 mae 0.004052754025906324
training loss 2.756749328160795e-05 mae 0.004113106152919285
training loss 2.779824819191333e-05 mae 0.004124119525111401
training loss 2.85465038064555e-05 mae 0.004177563623309335
training loss 2.9288523061647233e-05 mae 0.00423555876897528
Epoch 143, training: loss: 0.0000293, mae: 0.0042407 test: loss0.0000772, mae:0.0064291
training loss 1.8839207768905908e-05 mae 0.0035030338913202286
training loss 2.8910103805281004e-05 mae 0.004184422813648102
training loss 2.9148873605069218e-05 mae 0.004216131173295551
training loss 2.982238262445768e-05 mae 0.004282548582637743
training loss 3.0113679174937913e-05 mae 0.004294369594467367
Epoch 144, training: loss: 0.0000300, mae: 0.0042826 test: loss0.0000787, mae:0.0064772
training loss 2.3596696337335743e-05 mae 0.003788087284192443
training loss 2.6921981913676743e-05 mae 0.004055303822764578
training loss 2.709436502849677e-05 mae 0.00406892721006407
training loss 2.766225417197601e-05 mae 0.004109029010530342
training loss 2.8007024683126902e-05 mae 0.0041405711339239926
Epoch 145, training: loss: 0.0000281, mae: 0.0041538 test: loss0.0000757, mae:0.0063796
training loss 2.996959119627718e-05 mae 0.004334618337452412
training loss 2.6982462277017342e-05 mae 0.0040301137293378515
training loss 2.7192182039137755e-05 mae 0.004037977104066031
training loss 2.7560423368654394e-05 mae 0.004088223171697942
training loss 2.7761898218611815e-05 mae 0.00410783242211858
Epoch 146, training: loss: 0.0000278, mae: 0.0041166 test: loss0.0000762, mae:0.0063749
training loss 3.026966624020133e-05 mae 0.004270464647561312
training loss 2.9577309653498506e-05 mae 0.004228104896587776
training loss 2.8260265043902092e-05 mae 0.004128536799609071
training loss 2.74978579150741e-05 mae 0.004088585383709003
training loss 2.777452346207388e-05 mae 0.004112716817496283
Epoch 147, training: loss: 0.0000278, mae: 0.0041146 test: loss0.0000772, mae:0.0064506
training loss 2.4096387278405018e-05 mae 0.003861069679260254
training loss 2.7558751289867428e-05 mae 0.004069562243553354
training loss 2.7958551036495952e-05 mae 0.00412058828186502
training loss 2.829022458846995e-05 mae 0.004156502017701997
training loss 2.8540939796702663e-05 mae 0.004178806523148396
Epoch 148, training: loss: 0.0000283, mae: 0.0041648 test: loss0.0000796, mae:0.0064928
training loss 3.197987098246813e-05 mae 0.004500971641391516
training loss 2.6661624342879948e-05 mae 0.004039585736452363
training loss 2.6795269303253773e-05 mae 0.004048779648313722
training loss 2.7038958190837402e-05 mae 0.0040765493146039
training loss 2.7124984124167154e-05 mae 0.004075862203182567
Epoch 149, training: loss: 0.0000269, mae: 0.0040638 test: loss0.0000785, mae:0.0064450
training loss 1.9747034457395785e-05 mae 0.0036660274490714073
training loss 2.748984782859528e-05 mae 0.004082512649177921
training loss 2.695693628183026e-05 mae 0.004072530063717523
training loss 2.677308070785227e-05 mae 0.004044308502917848
training loss 2.7083855185530895e-05 mae 0.004064461196638382
Epoch 150, training: loss: 0.0000272, mae: 0.0040721 test: loss0.0000764, mae:0.0063461
training loss 3.2850675779627636e-05 mae 0.004076881799846888
training loss 2.782484429475267e-05 mae 0.004080591600059585
training loss 2.6972104987549702e-05 mae 0.0040591785356881875
training loss 2.7488790819072928e-05 mae 0.004101251992119464
training loss 2.728918389491365e-05 mae 0.004095388876403967
Epoch 151, training: loss: 0.0000277, mae: 0.0041178 test: loss0.0000770, mae:0.0064209
training loss 3.162532448186539e-05 mae 0.004213600419461727
training loss 2.6389173274533364e-05 mae 0.004007267656133456
training loss 2.6001561435454158e-05 mae 0.003961884167672388
training loss 2.611874868982088e-05 mae 0.00399181777488021
training loss 2.644303022766982e-05 mae 0.0040131945566121326
Epoch 152, training: loss: 0.0000264, mae: 0.0040128 test: loss0.0000776, mae:0.0064485
training loss 1.4162327715894207e-05 mae 0.00296134315431118
training loss 2.436582824028685e-05 mae 0.003908785238095068
training loss 2.574499632990607e-05 mae 0.003981768829047238
training loss 2.566044492553958e-05 mae 0.003967985587922348
training loss 2.5592381864758928e-05 mae 0.003960435738583183
Epoch 153, training: loss: 0.0000256, mae: 0.0039649 test: loss0.0000810, mae:0.0065509
training loss 2.4437196771032177e-05 mae 0.003582206554710865
training loss 2.650766465176631e-05 mae 0.0039921879320971515
training loss 2.614582303571711e-05 mae 0.0039647848548201615
training loss 2.596084792963932e-05 mae 0.003970905068324297
training loss 2.6484528615723533e-05 mae 0.004026147539250143
Epoch 154, training: loss: 0.0000265, mae: 0.0040260 test: loss0.0000806, mae:0.0065113
training loss 2.513091021683067e-05 mae 0.00400580745190382
training loss 2.4764701854163673e-05 mae 0.0039021754454748305
training loss 2.5692234768291315e-05 mae 0.0039523039013147345
training loss 2.5849997287902162e-05 mae 0.003966758211161815
training loss 2.681491011811597e-05 mae 0.0040440859808239015
Epoch 155, training: loss: 0.0000268, mae: 0.0040484 test: loss0.0000782, mae:0.0065257
training loss 1.813312883314211e-05 mae 0.0035747906658798456
training loss 2.7831067164501572e-05 mae 0.00413885915779746
training loss 2.744866588448593e-05 mae 0.004124103798669309
training loss 2.6807891683817193e-05 mae 0.0040617579708356905
training loss 2.6316974126258604e-05 mae 0.00402069559079868
Epoch 156, training: loss: 0.0000264, mae: 0.0040267 test: loss0.0000758, mae:0.0063513
training loss 2.7464770028018393e-05 mae 0.003799852915108204
training loss 2.5199170504849603e-05 mae 0.003921883847272279
training loss 2.4784902097512672e-05 mae 0.003875885834216628
training loss 2.5167636799447576e-05 mae 0.003908359174252741
training loss 2.4809789424783853e-05 mae 0.003886529514380728
Epoch 157, training: loss: 0.0000250, mae: 0.0038999 test: loss0.0000785, mae:0.0063905
training loss 2.44227940129349e-05 mae 0.0035859504714608192
training loss 2.4344304192174392e-05 mae 0.003881458581114808
training loss 2.4992736954199353e-05 mae 0.003926848616948959
training loss 2.5065781060310963e-05 mae 0.003929159044294266
training loss 2.5235943072194584e-05 mae 0.003940321268764003
Epoch 158, training: loss: 0.0000253, mae: 0.0039390 test: loss0.0000804, mae:0.0064954
training loss 2.5382119929417968e-05 mae 0.004138958174735308
training loss 2.3671119254454518e-05 mae 0.0038227065530258646
training loss 2.4251315324471594e-05 mae 0.0038787357158737604
training loss 2.4549718744804564e-05 mae 0.003894533248100158
training loss 2.4344875113166935e-05 mae 0.0038726384165367815
Epoch 159, training: loss: 0.0000245, mae: 0.0038841 test: loss0.0000772, mae:0.0064068
training loss 3.265199120505713e-05 mae 0.004364387597888708
training loss 2.3964719641263206e-05 mae 0.003839196500313632
training loss 2.365665247029623e-05 mae 0.003814438274960116
training loss 2.4107805147168803e-05 mae 0.003854244276696186
training loss 2.4400167857135747e-05 mae 0.0038671769845804453
Epoch 160, training: loss: 0.0000244, mae: 0.0038686 test: loss0.0000787, mae:0.0064170
training loss 1.8023731172434054e-05 mae 0.0030988624785095453
training loss 2.406369643651095e-05 mae 0.0038189078343338232
training loss 2.401129652095939e-05 mae 0.0038409002106821175
training loss 2.461213690842831e-05 mae 0.0038895039412112815
training loss 2.4799562351430915e-05 mae 0.0039069845343462735
Epoch 161, training: loss: 0.0000248, mae: 0.0039057 test: loss0.0000782, mae:0.0064205
training loss 3.366475721122697e-05 mae 0.00427069840952754
training loss 2.3315259711000857e-05 mae 0.0037925889277282882
training loss 2.320013412611275e-05 mae 0.0037918369986559494
training loss 2.3398071054592022e-05 mae 0.003795601009693466
training loss 2.321191254058571e-05 mae 0.0037895834990846595
Epoch 162, training: loss: 0.0000232, mae: 0.0037827 test: loss0.0000815, mae:0.0065052
training loss 2.279861109855119e-05 mae 0.0037225608248263597
training loss 2.3031910952305024e-05 mae 0.003769880156124047
training loss 2.3263078597521213e-05 mae 0.003782153514555038
training loss 2.4359425620088383e-05 mae 0.0038722300584372498
training loss 2.4877905537264547e-05 mae 0.003899205121587017
Epoch 163, training: loss: 0.0000249, mae: 0.0039008 test: loss0.0000836, mae:0.0066725
training loss 2.74001376965316e-05 mae 0.004086754284799099
training loss 2.3391417230681435e-05 mae 0.0037991521998728605
training loss 2.2631283722112496e-05 mae 0.0037354482153412133
training loss 2.3391517308048767e-05 mae 0.00380016463833386
training loss 2.3661765257953322e-05 mae 0.0038211722893000967
Epoch 164, training: loss: 0.0000239, mae: 0.0038353 test: loss0.0000790, mae:0.0064313
training loss 3.4282671549590304e-05 mae 0.004324767272919416
training loss 2.4804084419242018e-05 mae 0.003870749033476208
training loss 2.4367825359816075e-05 mae 0.0038409909108967173
training loss 2.487664160854298e-05 mae 0.0038878061632866296
training loss 2.4854808738319074e-05 mae 0.003895857811567202
Epoch 165, training: loss: 0.0000249, mae: 0.0038962 test: loss0.0000801, mae:0.0065134
training loss 2.5986688342527486e-05 mae 0.003722480731084943
training loss 2.25344505737417e-05 mae 0.0037092511113002613
training loss 2.2717535778223697e-05 mae 0.003720047731542646
training loss 2.3058552490312166e-05 mae 0.0037538915154036903
training loss 2.3333629031013477e-05 mae 0.003773755168041855
Epoch 166, training: loss: 0.0000232, mae: 0.0037658 test: loss0.0000808, mae:0.0064970
training loss 2.4903974917833693e-05 mae 0.00380584504455328
training loss 2.1475881023398675e-05 mae 0.0036278429952468355
training loss 2.2421207096059415e-05 mae 0.0037141945558867534
training loss 2.2471611216874025e-05 mae 0.0037041379552063174
training loss 2.2766120667288148e-05 mae 0.0037322164900063437
Epoch 167, training: loss: 0.0000226, mae: 0.0037189 test: loss0.0000776, mae:0.0063962
training loss 1.9864459318341687e-05 mae 0.003541915910318494
training loss 2.3631759070079116e-05 mae 0.003801534445408513
training loss 2.3006880675695523e-05 mae 0.0037550066487107526
training loss 2.354474748166383e-05 mae 0.0037882467022548054
training loss 2.450365435431389e-05 mae 0.0038280659644813523
Epoch 168, training: loss: 0.0000250, mae: 0.0038622 test: loss0.0000941, mae:0.0070231
training loss 4.791747414856218e-05 mae 0.0051892721094191074
training loss 3.544491946562558e-05 mae 0.00457144035574268
training loss 3.19505564373973e-05 mae 0.004376300308613641
training loss 2.9728647173245093e-05 mae 0.004216151799241833
training loss 2.8822519817906163e-05 mae 0.004159812079927888
Epoch 169, training: loss: 0.0000290, mae: 0.0041690 test: loss0.0000846, mae:0.0067515
training loss 2.637975376273971e-05 mae 0.003911932464689016
training loss 2.470439886012316e-05 mae 0.00388925905595077
training loss 2.4538965743265032e-05 mae 0.003872018620961962
training loss 2.3717126765067542e-05 mae 0.003811967256941542
training loss 2.3754853751072815e-05 mae 0.0038114936260126566
Epoch 170, training: loss: 0.0000239, mae: 0.0038198 test: loss0.0000798, mae:0.0065108
training loss 2.991678957187105e-05 mae 0.00437499163672328
training loss 2.2876065985674e-05 mae 0.0037452229298651214
training loss 2.2615897467061367e-05 mae 0.0037216306710154707
training loss 2.1942608321639895e-05 mae 0.003666906267564917
training loss 2.2437203804303812e-05 mae 0.0037022707288834594
Epoch 171, training: loss: 0.0000225, mae: 0.0037079 test: loss0.0000791, mae:0.0064432
training loss 2.4091677914839238e-05 mae 0.003956066910177469
training loss 2.1901086898544552e-05 mae 0.0036570574522164535
training loss 2.1559938386867578e-05 mae 0.0036314547748885844
training loss 2.129037882045638e-05 mae 0.0036122533292293753
training loss 2.1679102183359874e-05 mae 0.0036444220152122902
Epoch 172, training: loss: 0.0000219, mae: 0.0036544 test: loss0.0000784, mae:0.0064029
training loss 1.8401527995592915e-05 mae 0.0034728439059108496
training loss 2.1199256908028683e-05 mae 0.0036498670027974777
training loss 2.159116211440068e-05 mae 0.0036423404834758826
training loss 2.1685643374764925e-05 mae 0.0036400306849610924
training loss 2.183553233091281e-05 mae 0.0036571754226044045
Epoch 173, training: loss: 0.0000218, mae: 0.0036546 test: loss0.0000779, mae:0.0064045
training loss 1.9462904674583115e-05 mae 0.0034772956278175116
training loss 2.0266732373748512e-05 mae 0.003503709775852222
training loss 2.1028760252958443e-05 mae 0.003573166047416554
training loss 2.1090710425590615e-05 mae 0.0035801911803690213
training loss 2.1509770419076774e-05 mae 0.0036079427644388007
Epoch 174, training: loss: 0.0000216, mae: 0.0036134 test: loss0.0000793, mae:0.0064581
training loss 2.1737118004239164e-05 mae 0.003496125340461731
training loss 2.3093269031532516e-05 mae 0.003757567697332478
training loss 2.2573394798569944e-05 mae 0.0037184353562828044
training loss 2.2446676961865984e-05 mae 0.00372109779107403
training loss 2.2356236739830106e-05 mae 0.0037143939401750553
Epoch 175, training: loss: 0.0000224, mae: 0.0037170 test: loss0.0000783, mae:0.0064188
training loss 2.4334391127922572e-05 mae 0.0036003945861011744
training loss 2.1197337232211693e-05 mae 0.0035570835028135898
training loss 2.1672927543099065e-05 mae 0.0036150831498648265
training loss 2.1339489070886356e-05 mae 0.0036054634248578773
training loss 2.1859257821464548e-05 mae 0.003651843357608833
Epoch 176, training: loss: 0.0000220, mae: 0.0036625 test: loss0.0000825, mae:0.0065887
training loss 2.1445923266583122e-05 mae 0.0038154239300638437
training loss 2.4709490871293178e-05 mae 0.003861041967848352
training loss 2.346113090218093e-05 mae 0.003776370012892946
training loss 2.3309655159589252e-05 mae 0.0037663627835299008
training loss 2.3106238699600103e-05 mae 0.003761042574355703
Epoch 177, training: loss: 0.0000229, mae: 0.0037418 test: loss0.0000781, mae:0.0064419
training loss 1.433294983144151e-05 mae 0.0028369061183184385
training loss 1.9878318822770334e-05 mae 0.003470025175963254
training loss 2.0201996989325758e-05 mae 0.0035105604346435844
training loss 2.0332062214212844e-05 mae 0.0035299493172606883
training loss 2.0766163148228906e-05 mae 0.003561746466914143
Epoch 178, training: loss: 0.0000208, mae: 0.0035735 test: loss0.0000792, mae:0.0064127
training loss 2.1333733457140625e-05 mae 0.0038047833368182182
training loss 2.0844546716212865e-05 mae 0.003602328541341657
training loss 2.0484544017229986e-05 mae 0.0035452623574966833
training loss 2.1214893223675018e-05 mae 0.003597628943535763
training loss 2.1102861245170028e-05 mae 0.003597077611249994
Epoch 179, training: loss: 0.0000212, mae: 0.0036045 test: loss0.0000796, mae:0.0064681
training loss 1.7443519027438015e-05 mae 0.0034460362512618303
training loss 1.9689578021272746e-05 mae 0.0034636074092750453
training loss 2.031807870669509e-05 mae 0.0035265278748909733
training loss 2.0352917680617458e-05 mae 0.0035236093450016943
training loss 2.0482798494802293e-05 mae 0.003530166906522431
Epoch 180, training: loss: 0.0000206, mae: 0.0035418 test: loss0.0000796, mae:0.0064929
training loss 1.9151877495460212e-05 mae 0.00337012205272913
training loss 2.0438577848835848e-05 mae 0.003562829489180562
training loss 1.9986628833048778e-05 mae 0.00350045727907062
training loss 1.989817235953042e-05 mae 0.003490854486428348
training loss 1.9837847164004122e-05 mae 0.003494304209827116
Epoch 181, training: loss: 0.0000197, mae: 0.0034909 test: loss0.0000787, mae:0.0064594
training loss 1.977692409127485e-05 mae 0.0037389732897281647
training loss 1.825252638809616e-05 mae 0.0033462479272309475
training loss 1.8812510578493735e-05 mae 0.0033961048391232705
training loss 1.91968780766289e-05 mae 0.003416539200686462
training loss 2.0078380797921326e-05 mae 0.0035029335019285267
Epoch 182, training: loss: 0.0000202, mae: 0.0035107 test: loss0.0000795, mae:0.0064422
training loss 2.7224106815992855e-05 mae 0.0041346075013279915
training loss 1.99849963809909e-05 mae 0.003476524634250239
training loss 1.9711738958102443e-05 mae 0.0034618767918926643
training loss 2.0583867892778677e-05 mae 0.0035399829857791485
training loss 2.1030220984778396e-05 mae 0.00357335314863542
Epoch 183, training: loss: 0.0000209, mae: 0.0035634 test: loss0.0000800, mae:0.0065576
training loss 2.0483674234128557e-05 mae 0.0036181847099214792
training loss 1.9325554336212325e-05 mae 0.0034371166371320394
training loss 2.013288219068902e-05 mae 0.00348077215355738
training loss 2.018689437985608e-05 mae 0.00348669974694633
training loss 2.007197184214948e-05 mae 0.0034788751636460353
Epoch 184, training: loss: 0.0000200, mae: 0.0034757 test: loss0.0000773, mae:0.0063994
training loss 2.3745407816022635e-05 mae 0.003349674167111516
training loss 1.8882110470811172e-05 mae 0.003380002230223195
training loss 1.906850611523605e-05 mae 0.003401226626875082
training loss 1.942086577557516e-05 mae 0.0034478964462925656
training loss 1.995158963022256e-05 mae 0.0034851852169640327
Epoch 185, training: loss: 0.0000199, mae: 0.0034827 test: loss0.0000812, mae:0.0065031
training loss 2.2760263163945638e-05 mae 0.003476059762760997
training loss 2.0158003637477e-05 mae 0.0034837548246643714
training loss 2.025589400554874e-05 mae 0.003497527476566116
training loss 2.0892182404571684e-05 mae 0.003553256097971307
training loss 2.0458886367793025e-05 mae 0.0035147645003370835
Epoch 186, training: loss: 0.0000205, mae: 0.0035191 test: loss0.0000785, mae:0.0063482
training loss 1.9930075723095797e-05 mae 0.0033242919016629457
training loss 1.884936775205265e-05 mae 0.0033743978414100175
training loss 1.8893384378208062e-05 mae 0.0033609787241952243
training loss 1.9033939649386795e-05 mae 0.003386865396188288
training loss 1.9261521688055494e-05 mae 0.003414537373059127
Epoch 187, training: loss: 0.0000193, mae: 0.0034178 test: loss0.0000794, mae:0.0064368
training loss 1.5988100130925886e-05 mae 0.0030695011373609304
training loss 2.0355577719016632e-05 mae 0.003509080658356349
training loss 1.979301766305701e-05 mae 0.0034713318368585987
training loss 2.0281560948856377e-05 mae 0.003501714585003573
training loss 2.0506754331855416e-05 mae 0.0035255252735791214
Epoch 188, training: loss: 0.0000205, mae: 0.0035219 test: loss0.0000781, mae:0.0063550
training loss 1.216676628246205e-05 mae 0.0026627008337527514
training loss 1.832612205736737e-05 mae 0.0033523297969105773
training loss 1.8565575698117432e-05 mae 0.003366896301729253
training loss 1.8245699443800044e-05 mae 0.0033332595142184303
training loss 1.837831361391448e-05 mae 0.0033429032654978736
Epoch 189, training: loss: 0.0000183, mae: 0.0033402 test: loss0.0000803, mae:0.0064614
training loss 2.0330555344116874e-05 mae 0.003486025147140026
training loss 1.8104015854238676e-05 mae 0.003317027457752356
training loss 1.89021104215679e-05 mae 0.003392701016480822
training loss 1.8784059347012037e-05 mae 0.0033900479150824983
training loss 1.891229395771722e-05 mae 0.0033998643893711454
Epoch 190, training: loss: 0.0000193, mae: 0.0034299 test: loss0.0000790, mae:0.0064295
training loss 2.2643003831035458e-05 mae 0.003956382628530264
training loss 2.0122467682157024e-05 mae 0.0034640387577169083
training loss 1.9815804917736453e-05 mae 0.003450812283202563
training loss 1.977130861801406e-05 mae 0.0034518224782378267
training loss 1.9690842263890532e-05 mae 0.0034534420160141155
Epoch 191, training: loss: 0.0000198, mae: 0.0034634 test: loss0.0000795, mae:0.0064059
training loss 1.6372703612432815e-05 mae 0.003324649529531598
training loss 2.1185111394266174e-05 mae 0.003578183195535459
training loss 2.0132104275557362e-05 mae 0.0034949363296906846
training loss 1.9539588991404555e-05 mae 0.0034483993003477918
training loss 1.9556494502901477e-05 mae 0.003445767777837554
Epoch 192, training: loss: 0.0000197, mae: 0.0034585 test: loss0.0000783, mae:0.0064295
training loss 2.071212475129869e-05 mae 0.003407728625461459
training loss 1.9504848162742908e-05 mae 0.0034627486202938886
training loss 1.9285407706594904e-05 mae 0.00343677163409936
training loss 1.9002967445771143e-05 mae 0.003404083354892025
training loss 1.94238196148836e-05 mae 0.003449385414659902
Epoch 193, training: loss: 0.0000196, mae: 0.0034650 test: loss0.0000804, mae:0.0065166
training loss 2.0573106667143293e-05 mae 0.0036500443238765
training loss 2.0111462403934537e-05 mae 0.0034915677353958867
training loss 1.9333064787727808e-05 mae 0.003423956273547788
training loss 1.902647272800095e-05 mae 0.00341454896897028
training loss 1.952047799235361e-05 mae 0.0034549759587374916
Epoch 194, training: loss: 0.0000196, mae: 0.0034490 test: loss0.0000823, mae:0.0065607
training loss 1.9256707673775963e-05 mae 0.0034023476764559746
training loss 2.0604451813543333e-05 mae 0.0035212270495500048
training loss 2.143740549750999e-05 mae 0.0035965444616936505
training loss 2.1643618371149542e-05 mae 0.0036320188113593103
training loss 2.11803046918769e-05 mae 0.0035852567415536204
Epoch 195, training: loss: 0.0000212, mae: 0.0035865 test: loss0.0000787, mae:0.0063717
training loss 1.0490260137885343e-05 mae 0.0024903500452637672
training loss 1.774676453422608e-05 mae 0.0032883522685105892
training loss 1.8544738670225515e-05 mae 0.003347369616316392
training loss 1.8794624049514097e-05 mae 0.003381995806495185
training loss 1.9035271206894985e-05 mae 0.0034049793936553143
Epoch 196, training: loss: 0.0000190, mae: 0.0034032 test: loss0.0000803, mae:0.0064452
training loss 1.806027466955129e-05 mae 0.003642302006483078
training loss 1.8247987973867847e-05 mae 0.0033175935420919865
training loss 1.8426252113791218e-05 mae 0.0033426042208171427
training loss 1.8803402311681293e-05 mae 0.003401944487793556
training loss 1.839982077901585e-05 mae 0.003363730607146573
Epoch 197, training: loss: 0.0000185, mae: 0.0033696 test: loss0.0000797, mae:0.0064433
training loss 1.5936611816869117e-05 mae 0.003034742549061775
training loss 1.734363578697073e-05 mae 0.0032885387841173827
training loss 1.7621240253406944e-05 mae 0.0032918323937914146
training loss 1.7678952464234066e-05 mae 0.0032693482071950736
training loss 1.8264116305323323e-05 mae 0.0033215293313251498
Epoch 198, training: loss: 0.0000183, mae: 0.0033216 test: loss0.0000794, mae:0.0064756
training loss 1.4165759239403997e-05 mae 0.00285437167622149
training loss 1.8285783649023906e-05 mae 0.00333754181423608
training loss 1.7896685406385862e-05 mae 0.003282665710730276
training loss 1.7816111270177558e-05 mae 0.003285831179691862
training loss 1.7648699090997668e-05 mae 0.003281977497949381
Epoch 199, training: loss: 0.0000178, mae: 0.0032977 test: loss0.0000799, mae:0.0064444
current learning rate: 0.000125
training loss 1.6342941307811998e-05 mae 0.0031190242152661085
training loss 1.5893806413063718e-05 mae 0.0030747678258693685
training loss 1.5310915868852106e-05 mae 0.003008352652524071
training loss 1.4732037437694231e-05 mae 0.0029594429910478996
training loss 1.4530193689481974e-05 mae 0.0029351431297240274
Epoch 200, training: loss: 0.0000145, mae: 0.0029331 test: loss0.0000789, mae:0.0064019
training loss 1.477197656640783e-05 mae 0.002890567295253277
training loss 1.1923545990416018e-05 mae 0.0026795248245345613
training loss 1.2635038810102662e-05 mae 0.002737383969072807
training loss 1.2726823693593902e-05 mae 0.0027580726521694125
training loss 1.3021078076457142e-05 mae 0.0027724292166345756
Epoch 201, training: loss: 0.0000131, mae: 0.0027792 test: loss0.0000771, mae:0.0063611
training loss 9.639169547881465e-06 mae 0.0023930829484015703
training loss 1.2595796021082162e-05 mae 0.0027300082215601026
training loss 1.2720746556147106e-05 mae 0.0027599298651099636
training loss 1.2771053176845653e-05 mae 0.0027604693529930053
training loss 1.320309235521943e-05 mae 0.0028024098166352647
Epoch 202, training: loss: 0.0000132, mae: 0.0028009 test: loss0.0000782, mae:0.0063560
training loss 1.8619486581883393e-05 mae 0.003301349701359868
training loss 1.2787511556833872e-05 mae 0.0027431447707189642
training loss 1.2629930709651785e-05 mae 0.0027311387135287623
training loss 1.2922886718952688e-05 mae 0.002757062498689782
training loss 1.30108897720127e-05 mae 0.002771537581838628
Epoch 203, training: loss: 0.0000130, mae: 0.0027703 test: loss0.0000767, mae:0.0063129
training loss 9.80239474301925e-06 mae 0.0023310489486902952
training loss 1.2203442075391935e-05 mae 0.0026969351285301597
training loss 1.2630498504492634e-05 mae 0.0027142044071495402
training loss 1.2771418317887412e-05 mae 0.0027431615841087727
training loss 1.3047208323036131e-05 mae 0.0027678788811042548
Epoch 204, training: loss: 0.0000130, mae: 0.0027624 test: loss0.0000781, mae:0.0063623
training loss 1.0641808330547065e-05 mae 0.0025666540022939444
training loss 1.2442943969682686e-05 mae 0.002705414738396511
training loss 1.2582179770466641e-05 mae 0.0027221615027433416
training loss 1.2556195353127556e-05 mae 0.0027197893695066998
training loss 1.2692624777492477e-05 mae 0.002744929503014691
Epoch 205, training: loss: 0.0000127, mae: 0.0027464 test: loss0.0000777, mae:0.0063690
training loss 1.6283762306557037e-05 mae 0.0031421107705682516
training loss 1.18523064563359e-05 mae 0.0026413735285328306
training loss 1.2419920305618873e-05 mae 0.0027003500260191385
training loss 1.2452362806175638e-05 mae 0.0027098617029209806
training loss 1.2593831345373069e-05 mae 0.002725983410378667
Epoch 206, training: loss: 0.0000126, mae: 0.0027329 test: loss0.0000787, mae:0.0063995
training loss 1.4614211067964789e-05 mae 0.003073735162615776
training loss 1.2272933221823883e-05 mae 0.0026603939614313486
training loss 1.2494155685495157e-05 mae 0.0027180323077568604
training loss 1.2778768366192493e-05 mae 0.0027456940151751033
training loss 1.2751804855661565e-05 mae 0.0027476517096699307
Epoch 207, training: loss: 0.0000128, mae: 0.0027503 test: loss0.0000775, mae:0.0063582
training loss 1.1617920790740754e-05 mae 0.0024563863407820463
training loss 1.2590276253154028e-05 mae 0.002702694021019281
training loss 1.2543851925788612e-05 mae 0.0027206032513871345
training loss 1.2580082990543872e-05 mae 0.002731850867264516
training loss 1.2707908487542382e-05 mae 0.0027433046329747987
Epoch 208, training: loss: 0.0000127, mae: 0.0027410 test: loss0.0000785, mae:0.0063772
training loss 1.2977206097275484e-05 mae 0.0029407578986138105
training loss 1.1876643553727725e-05 mae 0.002650910075388702
training loss 1.2163219158573703e-05 mae 0.0026751732255452043
training loss 1.2396392788327857e-05 mae 0.0027013829961690487
training loss 1.2651562695766495e-05 mae 0.002729346046678315
Epoch 209, training: loss: 0.0000126, mae: 0.0027318 test: loss0.0000769, mae:0.0063187
training loss 1.3321729056769982e-05 mae 0.0027648203540593386
training loss 1.194399385663108e-05 mae 0.0026766906159144694
training loss 1.2204012098591914e-05 mae 0.002690540158678547
training loss 1.2331959362127714e-05 mae 0.002698024719435449
training loss 1.2426607988902048e-05 mae 0.002709622720756858
Epoch 210, training: loss: 0.0000124, mae: 0.0027062 test: loss0.0000782, mae:0.0063915
training loss 1.1668690603983123e-05 mae 0.0027989305090159178
training loss 1.1370459177152384e-05 mae 0.0026149816751735756
training loss 1.1927999983725228e-05 mae 0.0026457341881192253
training loss 1.2123263115053675e-05 mae 0.002672395861664454
training loss 1.2400290531209682e-05 mae 0.0026964673358919242
Epoch 211, training: loss: 0.0000124, mae: 0.0026981 test: loss0.0000784, mae:0.0063808
training loss 9.937974027707241e-06 mae 0.002530251629650593
training loss 1.1978783197050419e-05 mae 0.002606697990011205
training loss 1.16998013155434e-05 mae 0.00260939012456796
training loss 1.200361117563616e-05 mae 0.002657670547741631
training loss 1.2075396105374866e-05 mae 0.0026737132204687247
Epoch 212, training: loss: 0.0000122, mae: 0.0026822 test: loss0.0000779, mae:0.0063662
training loss 1.0670781193766743e-05 mae 0.0026597415562719107
training loss 1.1205043196526936e-05 mae 0.0025732055689920395
training loss 1.1797361574029099e-05 mae 0.0026312432745194967
training loss 1.1877628207460749e-05 mae 0.002648981855503771
training loss 1.2121549775995965e-05 mae 0.0026809094677591214
Epoch 213, training: loss: 0.0000122, mae: 0.0026896 test: loss0.0000794, mae:0.0064500
training loss 2.0246841813786887e-05 mae 0.003516663797199726
training loss 1.2717792590895744e-05 mae 0.0027012765937137833
training loss 1.2699302458551558e-05 mae 0.002718621044244507
training loss 1.262536959273842e-05 mae 0.002729519894840011
training loss 1.269061234625811e-05 mae 0.0027356548350656517
Epoch 214, training: loss: 0.0000126, mae: 0.0027282 test: loss0.0000785, mae:0.0064035
training loss 1.1728951903933194e-05 mae 0.0025797768030315638
training loss 1.227558992718068e-05 mae 0.0026585646064988533
training loss 1.2339116762119047e-05 mae 0.002673650563851294
training loss 1.2264693965178416e-05 mae 0.002681539138001914
training loss 1.2245285670450803e-05 mae 0.002693802705590627
Epoch 215, training: loss: 0.0000122, mae: 0.0026863 test: loss0.0000780, mae:0.0063736
training loss 9.676962690718938e-06 mae 0.00247967173345387
training loss 1.130253813065721e-05 mae 0.002562118817449493
training loss 1.1445445894910018e-05 mae 0.002582979556818556
training loss 1.1623632679474642e-05 mae 0.0026245821816291625
training loss 1.195139509773931e-05 mae 0.002661923933496226
Epoch 216, training: loss: 0.0000121, mae: 0.0026739 test: loss0.0000782, mae:0.0063887
training loss 1.7093479982577264e-05 mae 0.0025492676068097353
training loss 1.2728134972313256e-05 mae 0.0027238494104833578
training loss 1.2586155685087283e-05 mae 0.002716239108381294
training loss 1.2206579949091946e-05 mae 0.0026795035289810193
training loss 1.2342815375519075e-05 mae 0.002695560603592526
Epoch 217, training: loss: 0.0000123, mae: 0.0026917 test: loss0.0000778, mae:0.0063477
training loss 9.396246241522022e-06 mae 0.0024058108683675528
training loss 1.2121475550634832e-05 mae 0.0026813962875737577
training loss 1.1720291272152703e-05 mae 0.0026433052308396376
training loss 1.193869525647359e-05 mae 0.0026604897806741623
training loss 1.1915783258035535e-05 mae 0.0026459597805236915
Epoch 218, training: loss: 0.0000119, mae: 0.0026497 test: loss0.0000785, mae:0.0063883
training loss 8.815487490210216e-06 mae 0.0022797195706516504
training loss 1.2610213964174313e-05 mae 0.0027225416349977547
training loss 1.2534141403941052e-05 mae 0.0027208492392674084
training loss 1.224180874451363e-05 mae 0.0026967407979798935
training loss 1.211471427625871e-05 mae 0.0026859245488920548
Epoch 219, training: loss: 0.0000121, mae: 0.0026849 test: loss0.0000798, mae:0.0064367
training loss 1.2703819265880156e-05 mae 0.002890821546316147
training loss 1.1048694565996651e-05 mae 0.0025573777327058365
training loss 1.1394652763484184e-05 mae 0.002584921465817802
training loss 1.1428879257220234e-05 mae 0.002592177251323022
training loss 1.1464242292258943e-05 mae 0.0025976542615104665
Epoch 220, training: loss: 0.0000115, mae: 0.0026037 test: loss0.0000799, mae:0.0064830
training loss 7.863329301471822e-06 mae 0.002171507803723216
training loss 1.188350627083256e-05 mae 0.002620286077680979
training loss 1.1867914860102807e-05 mae 0.002639054278105423
training loss 1.1907957979555617e-05 mae 0.0026486635112530545
training loss 1.1870802550514221e-05 mae 0.0026479560328156347
Epoch 221, training: loss: 0.0000119, mae: 0.0026505 test: loss0.0000804, mae:0.0064628
training loss 7.4876843427773565e-06 mae 0.002022315515205264
training loss 1.1249443310125749e-05 mae 0.0025769942599878294
training loss 1.142563779818501e-05 mae 0.002609873090243649
training loss 1.1348676677596291e-05 mae 0.0025952748209546435
training loss 1.138722675828442e-05 mae 0.0025952039877258568
Epoch 222, training: loss: 0.0000114, mae: 0.0026021 test: loss0.0000784, mae:0.0064038
training loss 7.443148660968291e-06 mae 0.002082493156194687
training loss 1.0750050701151882e-05 mae 0.002503977376310265
training loss 1.1408999947853222e-05 mae 0.0025920654280516923
training loss 1.1472488160916629e-05 mae 0.0026066855231411026
training loss 1.1603769876703733e-05 mae 0.0026177464849987436
Epoch 223, training: loss: 0.0000116, mae: 0.0026222 test: loss0.0000809, mae:0.0064341
training loss 9.111895451496821e-06 mae 0.002268560929223895
training loss 1.1164418127936743e-05 mae 0.00255942049746712
training loss 1.1633339945186857e-05 mae 0.002601241928036555
training loss 1.1757782660111509e-05 mae 0.002619194717220469
training loss 1.1764367150248639e-05 mae 0.0026214845213269923
Epoch 224, training: loss: 0.0000118, mae: 0.0026264 test: loss0.0000821, mae:0.0065261
training loss 8.929925570555497e-06 mae 0.002172033069655299
training loss 1.2090144367653456e-05 mae 0.0026407881420763097
training loss 1.1599902367973724e-05 mae 0.0026068623952272504
training loss 1.1826097990393125e-05 mae 0.0026443166356065026
training loss 1.2090632822602753e-05 mae 0.0026786535473271693
Epoch 225, training: loss: 0.0000121, mae: 0.0026740 test: loss0.0000792, mae:0.0064172
training loss 1.4322209608508274e-05 mae 0.0025399522855877876
training loss 1.0714544616491237e-05 mae 0.0025071142008528113
training loss 1.095919286412801e-05 mae 0.002532822340450221
training loss 1.1036727620188111e-05 mae 0.002549051132904278
training loss 1.1273810617407505e-05 mae 0.0025745292277589635
Epoch 226, training: loss: 0.0000113, mae: 0.0025768 test: loss0.0000798, mae:0.0064536
training loss 7.769535841362085e-06 mae 0.0020652273669838905
training loss 1.0732195661665745e-05 mae 0.002538462507301101
training loss 1.073182420441299e-05 mae 0.00253171845578985
training loss 1.0889088523090202e-05 mae 0.0025436741188908656
training loss 1.113336062660358e-05 mae 0.0025678821665637967
Epoch 227, training: loss: 0.0000112, mae: 0.0025706 test: loss0.0000796, mae:0.0064618
training loss 5.649089871440083e-06 mae 0.0018929491052404046
training loss 1.1246957857271327e-05 mae 0.0025516856900032825
training loss 1.142465746195037e-05 mae 0.0025857265803520342
training loss 1.0971966438599286e-05 mae 0.0025350563297516964
training loss 1.1119360783708886e-05 mae 0.0025537068074087234
Epoch 228, training: loss: 0.0000112, mae: 0.0025586 test: loss0.0000795, mae:0.0064348
training loss 1.0639288120728452e-05 mae 0.002482926705852151
training loss 1.0566202856317236e-05 mae 0.002466368769733783
training loss 1.0464765898294639e-05 mae 0.0024681405387911026
training loss 1.0674801569922352e-05 mae 0.0024917043779440072
training loss 1.093972903038942e-05 mae 0.002529184272586928
Epoch 229, training: loss: 0.0000110, mae: 0.0025299 test: loss0.0000809, mae:0.0064577
training loss 8.722369784663897e-06 mae 0.002362583065405488
training loss 1.1420249411101253e-05 mae 0.002560974121568542
training loss 1.0941090245564333e-05 mae 0.0025408976241061
training loss 1.0984531125973717e-05 mae 0.002544110485149043
training loss 1.1014934520587406e-05 mae 0.002554174871716778
Epoch 230, training: loss: 0.0000110, mae: 0.0025585 test: loss0.0000799, mae:0.0064807
training loss 6.66126425130642e-06 mae 0.002071536146104336
training loss 1.1178768064008394e-05 mae 0.002532230651316544
training loss 1.05167275232996e-05 mae 0.002476584393355886
training loss 1.0441636668896155e-05 mae 0.0024767950160231524
training loss 1.0674517499810738e-05 mae 0.0024988482230162224
Epoch 231, training: loss: 0.0000108, mae: 0.0025091 test: loss0.0000815, mae:0.0065330
training loss 1.0723370905907359e-05 mae 0.0025805102195590734
training loss 1.097857583255402e-05 mae 0.002546972590589932
training loss 1.0718618661017126e-05 mae 0.0025219499746724806
training loss 1.0939273085526697e-05 mae 0.0025433628428233974
training loss 1.1077416182170008e-05 mae 0.0025630457203983516
Epoch 232, training: loss: 0.0000111, mae: 0.0025675 test: loss0.0000811, mae:0.0065233
training loss 1.6464842701680027e-05 mae 0.00323387049138546
training loss 1.1207025859310339e-05 mae 0.0026036660488256633
training loss 1.1314470918144857e-05 mae 0.002606187624358895
training loss 1.1118672354982758e-05 mae 0.0025733731901396496
training loss 1.1137901830933795e-05 mae 0.0025782546947075444
Epoch 233, training: loss: 0.0000111, mae: 0.0025737 test: loss0.0000810, mae:0.0065049
training loss 1.4492569789581466e-05 mae 0.002909195376560092
training loss 1.0288535764523067e-05 mae 0.0024564649097110138
training loss 1.0191921449917895e-05 mae 0.002444048259997428
training loss 1.0564235741570737e-05 mae 0.0024949684554081888
training loss 1.084779408650397e-05 mae 0.002530733607037559
Epoch 234, training: loss: 0.0000109, mae: 0.0025382 test: loss0.0000808, mae:0.0065128
training loss 6.256321739783743e-06 mae 0.0018252882873639464
training loss 1.1673884844140941e-05 mae 0.0026047770162204308
training loss 1.1505543634394594e-05 mae 0.0026073957606689013
training loss 1.1310334889608903e-05 mae 0.0025947446146955258
training loss 1.1159825725602546e-05 mae 0.0025679845112695616
Epoch 235, training: loss: 0.0000111, mae: 0.0025594 test: loss0.0000798, mae:0.0064507
training loss 1.5546496797469445e-05 mae 0.00277110212482512
training loss 1.0239090059650492e-05 mae 0.002492967865192422
training loss 1.0169973771409407e-05 mae 0.0024690642713172616
training loss 1.0303378801848566e-05 mae 0.002480426235802916
training loss 1.0434378502884394e-05 mae 0.002498236116466682
Epoch 236, training: loss: 0.0000105, mae: 0.0025028 test: loss0.0000798, mae:0.0064614
training loss 1.2229600542923436e-05 mae 0.0027396806981414557
training loss 9.656897810939468e-06 mae 0.002399720411345947
training loss 9.912041618782933e-06 mae 0.0024218413135092143
training loss 1.0324393131680374e-05 mae 0.0024596556939419025
training loss 1.0627785182416244e-05 mae 0.0024911024027378915
Epoch 237, training: loss: 0.0000106, mae: 0.0024972 test: loss0.0000822, mae:0.0065180
training loss 8.800921932561323e-06 mae 0.002381255617365241
training loss 1.022384115939905e-05 mae 0.002457151810328166
training loss 1.0338526372284867e-05 mae 0.002470024915568826
training loss 1.0307634529333468e-05 mae 0.002466080152886051
training loss 1.0298394300748004e-05 mae 0.002460319991918533
Epoch 238, training: loss: 0.0000104, mae: 0.0024671 test: loss0.0000817, mae:0.0065147
training loss 1.1336650459270459e-05 mae 0.002694051479920745
training loss 9.546694355758876e-06 mae 0.0023973755488245216
training loss 1.021210708149558e-05 mae 0.0024635044565092376
training loss 1.035170612189574e-05 mae 0.002472912366902889
training loss 1.0475662028470854e-05 mae 0.0024823675120935157
Epoch 239, training: loss: 0.0000105, mae: 0.0024848 test: loss0.0000824, mae:0.0065451
training loss 1.605525176273659e-05 mae 0.003033082000911236
training loss 1.069298263709974e-05 mae 0.0025463380928461743
training loss 1.0530915194904802e-05 mae 0.002497000358122779
training loss 1.0308309350309612e-05 mae 0.002469428701616132
training loss 1.0259304112131846e-05 mae 0.0024589760111996995
Epoch 240, training: loss: 0.0000103, mae: 0.0024675 test: loss0.0000812, mae:0.0064895
training loss 6.7620608206198085e-06 mae 0.0019851624965667725
training loss 9.973496634854966e-06 mae 0.0023717470444264074
training loss 9.82197898248151e-06 mae 0.0023775482677052366
training loss 1.002534698827068e-05 mae 0.0024141271287168296
training loss 1.0231727766714621e-05 mae 0.0024418843074919504
Epoch 241, training: loss: 0.0000103, mae: 0.0024533 test: loss0.0000814, mae:0.0065005
training loss 1.6814901755424216e-05 mae 0.0026594677474349737
training loss 1.118550077687056e-05 mae 0.0025512671312682483
training loss 1.066578239927069e-05 mae 0.002515331895026093
training loss 1.0566660174369475e-05 mae 0.0024903860152853253
training loss 1.0412681909493486e-05 mae 0.0024770311930156622
Epoch 242, training: loss: 0.0000105, mae: 0.0024833 test: loss0.0000825, mae:0.0065480
training loss 7.547713266831124e-06 mae 0.0021830995101481676
training loss 1.0442838040890978e-05 mae 0.0024624547199802666
training loss 1.020966869018054e-05 mae 0.0024471562563229605
training loss 1.0401507272084006e-05 mae 0.002475613020422147
training loss 1.0488305871325826e-05 mae 0.0024895085895257933
Epoch 243, training: loss: 0.0000105, mae: 0.0024803 test: loss0.0000822, mae:0.0065199
training loss 1.0162218131881673e-05 mae 0.002445347374305129
training loss 1.0056015659810166e-05 mae 0.0024053086891916457
training loss 9.656680657205317e-06 mae 0.00238123827494017
training loss 9.921324609898134e-06 mae 0.0024150483948579492
training loss 1.014229950334849e-05 mae 0.0024444368014584737
Epoch 244, training: loss: 0.0000101, mae: 0.0024452 test: loss0.0000811, mae:0.0065113
training loss 7.624103545822436e-06 mae 0.0021482864394783974
training loss 1.056664197284251e-05 mae 0.0024828009019770164
training loss 1.0353531499529003e-05 mae 0.002466178937160437
training loss 1.0342389077476465e-05 mae 0.002474603487567247
training loss 1.0209992852375318e-05 mae 0.0024522825390962304
Epoch 245, training: loss: 0.0000102, mae: 0.0024519 test: loss0.0000803, mae:0.0064859
training loss 7.791485586494673e-06 mae 0.0023760392796248198
training loss 9.539795312365664e-06 mae 0.0023770212428644295
training loss 9.650251371621754e-06 mae 0.0023697780140924576
training loss 9.689810119517459e-06 mae 0.0023857134992614493
training loss 9.8635491237804e-06 mae 0.0024090777209204673
Epoch 246, training: loss: 0.0000100, mae: 0.0024221 test: loss0.0000818, mae:0.0065218
training loss 5.958695055596763e-06 mae 0.0018373969942331314
training loss 9.501424301561196e-06 mae 0.002355974163020067
training loss 9.533493868171842e-06 mae 0.002370073237585476
training loss 9.951676333340273e-06 mae 0.0024127218970991914
training loss 1.0090767372913881e-05 mae 0.002441035690184553
Epoch 247, training: loss: 0.0000101, mae: 0.0024431 test: loss0.0000811, mae:0.0065151
training loss 9.457516171096358e-06 mae 0.0023164323065429926
training loss 9.03534699176032e-06 mae 0.0023372966188973956
training loss 9.568818480300537e-06 mae 0.0023828458518140245
training loss 9.880571302732132e-06 mae 0.002412169562677379
training loss 9.985761051266039e-06 mae 0.0024282794702909323
Epoch 248, training: loss: 0.0000100, mae: 0.0024352 test: loss0.0000811, mae:0.0064893
training loss 1.017397426039679e-05 mae 0.002087180269882083
training loss 9.393587480885526e-06 mae 0.0023581426814381105
training loss 9.516266699041678e-06 mae 0.002372840761741864
training loss 9.778054103457935e-06 mae 0.002409434015400066
training loss 9.918576122220821e-06 mae 0.002423160525840424
Epoch 249, training: loss: 0.0000099, mae: 0.0024188 test: loss0.0000805, mae:0.0064838
training loss 1.0530260624364018e-05 mae 0.00255416426807642
training loss 9.272531894798614e-06 mae 0.002307474172637597
training loss 9.324323859755615e-06 mae 0.0023218419674857715
training loss 9.64665591197523e-06 mae 0.0023757160305113395
training loss 9.715418825782479e-06 mae 0.0023939780304810753
Epoch 250, training: loss: 0.0000097, mae: 0.0023927 test: loss0.0000802, mae:0.0064656
training loss 7.1928930083231535e-06 mae 0.0020244556944817305
training loss 9.701946126762211e-06 mae 0.0023944917244507983
training loss 9.492143169271655e-06 mae 0.0023811441260657407
training loss 9.742373371987712e-06 mae 0.0024038406839865627
training loss 9.708504591405625e-06 mae 0.0024005491321267046
Epoch 251, training: loss: 0.0000098, mae: 0.0024048 test: loss0.0000805, mae:0.0064835
training loss 5.844601673743455e-06 mae 0.0018309202278032899
training loss 9.712226956788288e-06 mae 0.0023646376337673448
training loss 9.708754361673702e-06 mae 0.002378663850376511
training loss 9.779903399814902e-06 mae 0.0023982929835244028
training loss 9.753698557335103e-06 mae 0.0024064753154774583
Epoch 252, training: loss: 0.0000097, mae: 0.0024048 test: loss0.0000822, mae:0.0065077
training loss 8.034722668526229e-06 mae 0.0021757252980023623
training loss 8.989438539156613e-06 mae 0.0023083087563624274
training loss 9.089668132633457e-06 mae 0.0023086904963054273
training loss 9.398953001653722e-06 mae 0.002342056281038172
training loss 9.485006642793064e-06 mae 0.00235220373883862
Epoch 253, training: loss: 0.0000095, mae: 0.0023587 test: loss0.0000820, mae:0.0065463
training loss 7.841606020519976e-06 mae 0.002301913918927312
training loss 1.0180379662165697e-05 mae 0.0024273054966447404
training loss 9.84076049381557e-06 mae 0.0023920639720053797
training loss 9.801610339977834e-06 mae 0.0023929052145855606
training loss 9.799955061942958e-06 mae 0.0023903604046631574
Epoch 254, training: loss: 0.0000098, mae: 0.0023970 test: loss0.0000818, mae:0.0065275
training loss 1.4895415915816557e-05 mae 0.0028161443769931793
training loss 1.1612146578294878e-05 mae 0.0026019129756034583
training loss 1.0697703105911879e-05 mae 0.0024965056725362736
training loss 1.0269583267683436e-05 mae 0.002446906676589949
training loss 1.0022622674697951e-05 mae 0.00242508911245053
Epoch 255, training: loss: 0.0000100, mae: 0.0024202 test: loss0.0000825, mae:0.0065284
training loss 5.4255701797956135e-06 mae 0.0018348408630117774
training loss 9.76182804258903e-06 mae 0.0023845191702575362
training loss 9.418732867680496e-06 mae 0.002347227992803448
training loss 9.259632533096782e-06 mae 0.002325999266807211
training loss 9.180938511022915e-06 mae 0.002323742321720207
Epoch 256, training: loss: 0.0000092, mae: 0.0023341 test: loss0.0000815, mae:0.0065130
training loss 5.5299046834988985e-06 mae 0.0018175089498981833
training loss 9.37562303007843e-06 mae 0.002340799587888314
training loss 9.057620229556263e-06 mae 0.0023112648252010494
training loss 8.982712627988537e-06 mae 0.002299518137861858
training loss 9.360329275395502e-06 mae 0.002334602181197364
Epoch 257, training: loss: 0.0000094, mae: 0.0023379 test: loss0.0000849, mae:0.0066675
training loss 7.43187274565571e-06 mae 0.0020772498100996017
training loss 9.573790298329153e-06 mae 0.0023558743197616993
training loss 9.32186530647387e-06 mae 0.0023327297450384445
training loss 9.23975876684086e-06 mae 0.002326555850002792
training loss 9.240583861342274e-06 mae 0.002325220485287372
Epoch 258, training: loss: 0.0000093, mae: 0.0023268 test: loss0.0000830, mae:0.0065511
training loss 7.614791684318334e-06 mae 0.0021946895867586136
training loss 8.4059988515875e-06 mae 0.0022289031322168945
training loss 8.783962485803482e-06 mae 0.0022895932958304573
training loss 9.361504501594146e-06 mae 0.002348785162194081
training loss 9.530232546239853e-06 mae 0.0023703387485520507
Epoch 259, training: loss: 0.0000095, mae: 0.0023736 test: loss0.0000824, mae:0.0065418
training loss 6.946755547687644e-06 mae 0.002083570696413517
training loss 9.908585659158078e-06 mae 0.00240991262983823
training loss 9.654707175268595e-06 mae 0.0023717361209582137
training loss 9.77047306608539e-06 mae 0.0023896119862622193
training loss 9.667756850957907e-06 mae 0.0023841907928202006
Epoch 260, training: loss: 0.0000097, mae: 0.0023903 test: loss0.0000839, mae:0.0066074
training loss 6.6662378230830655e-06 mae 0.0020494451746344566
training loss 8.709442654970403e-06 mae 0.00225640694364248
training loss 8.764184402403625e-06 mae 0.0022548083757686584
training loss 9.183083348516267e-06 mae 0.002319024163390824
training loss 9.109330531642641e-06 mae 0.0023123177703219784
Epoch 261, training: loss: 0.0000091, mae: 0.0023159 test: loss0.0000828, mae:0.0065716
training loss 8.424778570770286e-06 mae 0.002186315134167671
training loss 9.096293961549376e-06 mae 0.0023262434040981477
training loss 9.323765209388724e-06 mae 0.0023508919864007735
training loss 9.039014662599185e-06 mae 0.0023227504466888514
training loss 9.028388213569558e-06 mae 0.002317326579507051
Epoch 262, training: loss: 0.0000091, mae: 0.0023245 test: loss0.0000828, mae:0.0065359
training loss 4.70348868475412e-06 mae 0.0017152741784229875
training loss 8.568572607770215e-06 mae 0.0022452484642831133
training loss 8.543400065601675e-06 mae 0.0022415641354083414
training loss 8.759697007218878e-06 mae 0.00228351167332233
training loss 9.010356259397613e-06 mae 0.0023047077065259597
Epoch 263, training: loss: 0.0000091, mae: 0.0023106 test: loss0.0000835, mae:0.0065901
training loss 1.068027540895855e-05 mae 0.002532895654439926
training loss 9.388214689976187e-06 mae 0.002315510345129844
training loss 9.028787397003514e-06 mae 0.0022926730965955734
training loss 8.821558873288618e-06 mae 0.002273760829275069
training loss 8.994076808756564e-06 mae 0.00229386781256609
Epoch 264, training: loss: 0.0000090, mae: 0.0022960 test: loss0.0000820, mae:0.0065242
training loss 7.977738277986646e-06 mae 0.0021053412929177284
training loss 9.093019076210432e-06 mae 0.0022858555369334767
training loss 9.019705151394464e-06 mae 0.002284668120878315
training loss 8.975399751514918e-06 mae 0.0022835940861186343
training loss 8.968247150176521e-06 mae 0.0022852290601270324
Epoch 265, training: loss: 0.0000089, mae: 0.0022846 test: loss0.0000836, mae:0.0065987
training loss 7.879371878516395e-06 mae 0.002224825555458665
training loss 8.414255039408057e-06 mae 0.0022081575885999436
training loss 8.446801188040247e-06 mae 0.0022261640746238645
training loss 8.773628306753026e-06 mae 0.0022607187143779835
training loss 8.724654491915374e-06 mae 0.0022556630822723324
Epoch 266, training: loss: 0.0000087, mae: 0.0022575 test: loss0.0000841, mae:0.0066462
training loss 8.690626600582618e-06 mae 0.002473666565492749
training loss 8.013654491258096e-06 mae 0.002191335442202056
training loss 8.410395907102979e-06 mae 0.002231806465931752
training loss 8.540730408676366e-06 mae 0.0022411722588099986
training loss 8.814224794824708e-06 mae 0.0022779813126674793
Epoch 267, training: loss: 0.0000088, mae: 0.0022779 test: loss0.0000819, mae:0.0065506
training loss 1.0872237908188254e-05 mae 0.0023616293910890818
training loss 8.602373737613008e-06 mae 0.002242192500453516
training loss 8.433454979132139e-06 mae 0.002227253051207282
training loss 8.697580807882405e-06 mae 0.002266945138524305
training loss 9.100683519815239e-06 mae 0.0023145827004889884
Epoch 268, training: loss: 0.0000091, mae: 0.0023139 test: loss0.0000823, mae:0.0065143
training loss 8.360792890016455e-06 mae 0.0021833719220012426
training loss 8.737929277355991e-06 mae 0.0022863430637574083
training loss 9.16236472176474e-06 mae 0.0023210849659056356
training loss 9.273400710841107e-06 mae 0.0023340604735217645
training loss 9.294757365746616e-06 mae 0.00234262606323655
Epoch 269, training: loss: 0.0000094, mae: 0.0023542 test: loss0.0000828, mae:0.0065743
training loss 8.123530278680846e-06 mae 0.002265221206471324
training loss 9.155577822193484e-06 mae 0.002301811265722648
training loss 8.921764057760972e-06 mae 0.00227933541968428
training loss 8.772631476467296e-06 mae 0.0022655613651030417
training loss 9.376997557645795e-06 mae 0.002320025959492323
Epoch 270, training: loss: 0.0000097, mae: 0.0023533 test: loss0.0000850, mae:0.0066328
training loss 1.858716132119298e-05 mae 0.0033789591398090124
training loss 1.1519364743791952e-05 mae 0.002614473549685642
training loss 1.0707185515682363e-05 mae 0.002521637661179694
training loss 1.0127008246119705e-05 mae 0.002454652932059292
training loss 9.902482420035647e-06 mae 0.0024268833870550073
Epoch 271, training: loss: 0.0000099, mae: 0.0024230 test: loss0.0000834, mae:0.0066141
training loss 8.74762099556392e-06 mae 0.0022466976661235094
training loss 8.589550611313649e-06 mae 0.0022582959945258853
training loss 8.214547222559131e-06 mae 0.0021981937836136416
training loss 8.360524851086184e-06 mae 0.002213863484000184
training loss 8.612196153030952e-06 mae 0.002244494795748288
Epoch 272, training: loss: 0.0000087, mae: 0.0022554 test: loss0.0000821, mae:0.0065223
training loss 8.199394869734533e-06 mae 0.002230890793725848
training loss 8.119994876609534e-06 mae 0.0021857258010034757
training loss 8.311068522923977e-06 mae 0.0021988532130173097
training loss 8.287603390910579e-06 mae 0.0022048952677811433
training loss 8.382969282586184e-06 mae 0.002212451988094446
Epoch 273, training: loss: 0.0000084, mae: 0.0022151 test: loss0.0000821, mae:0.0065513
training loss 9.349411811854225e-06 mae 0.0022905729711055756
training loss 8.348515763703056e-06 mae 0.002214333606317785
training loss 8.531985308268703e-06 mae 0.002223448869357309
training loss 8.54043805732646e-06 mae 0.0022245512092609375
training loss 8.546879417731376e-06 mae 0.0022314714206456755
Epoch 274, training: loss: 0.0000086, mae: 0.0022361 test: loss0.0000826, mae:0.0065853
training loss 8.775818969297688e-06 mae 0.0023287415970116854
training loss 8.116841337480358e-06 mae 0.002170754194844002
training loss 8.711788158246123e-06 mae 0.002245908364478387
training loss 8.821945873824732e-06 mae 0.002266133096167781
training loss 8.646606075124522e-06 mae 0.0022478702713258168
Epoch 275, training: loss: 0.0000086, mae: 0.0022473 test: loss0.0000818, mae:0.0065494
training loss 6.860859230073402e-06 mae 0.00209683901630342
training loss 8.096108968839318e-06 mae 0.0021663132173867497
training loss 8.061715271372186e-06 mae 0.0021620338038071114
training loss 8.101789309261677e-06 mae 0.0021771187061406915
training loss 8.130210574957192e-06 mae 0.0021796328396726373
Epoch 276, training: loss: 0.0000082, mae: 0.0021839 test: loss0.0000825, mae:0.0065441
training loss 6.888819825690007e-06 mae 0.0018968799849972129
training loss 8.978729297001232e-06 mae 0.002250247650925873
training loss 8.836624102054927e-06 mae 0.0022662219217987637
training loss 8.635516338315767e-06 mae 0.002247506015570155
training loss 8.513523532124433e-06 mae 0.0022365083027195503
Epoch 277, training: loss: 0.0000085, mae: 0.0022355 test: loss0.0000841, mae:0.0066306
training loss 6.866750027256785e-06 mae 0.0020763210486620665
training loss 8.297953689750512e-06 mae 0.0022085289146714644
training loss 8.605745360099638e-06 mae 0.002254176905636888
training loss 8.377985142147576e-06 mae 0.00222105918365796
training loss 8.393611466741655e-06 mae 0.0022208237920920224
Epoch 278, training: loss: 0.0000084, mae: 0.0022178 test: loss0.0000821, mae:0.0065398
training loss 7.66491939430125e-06 mae 0.002095721662044525
training loss 8.066067966508715e-06 mae 0.0021677934838568464
training loss 8.293435446758668e-06 mae 0.002198196976390291
training loss 8.433716699218118e-06 mae 0.0022214784314670915
training loss 8.391157386463808e-06 mae 0.0022255687827281473
Epoch 279, training: loss: 0.0000084, mae: 0.0022247 test: loss0.0000849, mae:0.0066387
training loss 1.141879602073459e-05 mae 0.0025664118584245443
training loss 8.756788862656856e-06 mae 0.0022639830290869453
training loss 8.80938371818335e-06 mae 0.002270832604793185
training loss 8.696289177838865e-06 mae 0.002266350490602337
training loss 8.778558935208013e-06 mae 0.002278017860128362
Epoch 280, training: loss: 0.0000087, mae: 0.0022700 test: loss0.0000830, mae:0.0065448
training loss 7.564298812212655e-06 mae 0.0021233258303254843
training loss 7.44656968962255e-06 mae 0.002119849447398355
training loss 7.819144337929068e-06 mae 0.0021538419595984086
training loss 8.015810235158504e-06 mae 0.002180805493035161
training loss 8.000782398637021e-06 mae 0.002178925205374238
Epoch 281, training: loss: 0.0000081, mae: 0.0021936 test: loss0.0000845, mae:0.0066126
training loss 9.604637853044551e-06 mae 0.0021925605833530426
training loss 7.689327646278566e-06 mae 0.0021002519551628074
training loss 8.017390657619438e-06 mae 0.0021671710298442766
training loss 8.278907076625264e-06 mae 0.0022019563367689002
training loss 8.14376102339768e-06 mae 0.0021872800868699576
Epoch 282, training: loss: 0.0000082, mae: 0.0021965 test: loss0.0000849, mae:0.0066191
training loss 6.28914631306543e-06 mae 0.001980123808607459
training loss 7.812505444627617e-06 mae 0.0021623114543948695
training loss 8.286950312797467e-06 mae 0.0022136147645772392
training loss 8.201829523808172e-06 mae 0.0022015559189453326
training loss 8.28472437758106e-06 mae 0.002212398347852574
Epoch 283, training: loss: 0.0000083, mae: 0.0022177 test: loss0.0000847, mae:0.0066274
training loss 1.045109183905879e-05 mae 0.0025242662522941828
training loss 7.647779342483314e-06 mae 0.002132145416301985
training loss 7.980466590998909e-06 mae 0.0021743678734499478
training loss 8.179287215773235e-06 mae 0.0022020850319797733
training loss 8.399493400876549e-06 mae 0.0022230363990740853
Epoch 284, training: loss: 0.0000084, mae: 0.0022277 test: loss0.0000826, mae:0.0065640
training loss 5.400531790655805e-06 mae 0.0018177516758441925
training loss 7.790334460450512e-06 mae 0.0021519849884926393
training loss 7.852461199981555e-06 mae 0.0021487981655017134
training loss 7.850688848972537e-06 mae 0.0021472069834180133
training loss 8.05631070399604e-06 mae 0.00216968243696092
Epoch 285, training: loss: 0.0000081, mae: 0.0021825 test: loss0.0000834, mae:0.0065932
training loss 8.39006679598242e-06 mae 0.0023427552077919245
training loss 8.377454931035695e-06 mae 0.002241381425775734
training loss 8.149383462512214e-06 mae 0.0022086444907904695
training loss 8.255764237068828e-06 mae 0.0022196718183228885
training loss 8.363127687349263e-06 mae 0.002225977139876442
Epoch 286, training: loss: 0.0000083, mae: 0.0022205 test: loss0.0000840, mae:0.0065991
training loss 7.236238161567599e-06 mae 0.0021028537303209305
training loss 8.067747687306897e-06 mae 0.002185169025324284
training loss 8.317460483205185e-06 mae 0.0022125316364229606
training loss 8.63314677635632e-06 mae 0.0022518324036619915
training loss 8.55009241590846e-06 mae 0.0022423146889123374
Epoch 287, training: loss: 0.0000085, mae: 0.0022408 test: loss0.0000849, mae:0.0066227
training loss 6.561161626450485e-06 mae 0.001995905302464962
training loss 7.871201343696874e-06 mae 0.00216185758072956
training loss 7.93462897652915e-06 mae 0.002174258817651189
training loss 8.0879869968561e-06 mae 0.0021930486981491778
training loss 8.063694024971106e-06 mae 0.002188752870427547
Epoch 288, training: loss: 0.0000081, mae: 0.0021902 test: loss0.0000827, mae:0.0065558
training loss 8.03165039542364e-06 mae 0.001980380155146122
training loss 7.955828033457279e-06 mae 0.0021210529558433615
training loss 7.71420608282707e-06 mae 0.0021096119302149744
training loss 7.955302874729595e-06 mae 0.002143461388282922
training loss 7.934915223343144e-06 mae 0.0021490330086205494
Epoch 289, training: loss: 0.0000080, mae: 0.0021532 test: loss0.0000849, mae:0.0066216
training loss 1.1266393812547904e-05 mae 0.00233918777666986
training loss 7.341362320670777e-06 mae 0.0020605428966090954
training loss 7.729356765243082e-06 mae 0.0021117164207032258
training loss 7.77573000622523e-06 mae 0.0021341926833022616
training loss 7.981057662157156e-06 mae 0.0021599312921040765
Epoch 290, training: loss: 0.0000079, mae: 0.0021551 test: loss0.0000844, mae:0.0066278
training loss 5.408927336247871e-06 mae 0.001889988430775702
training loss 7.520836860231953e-06 mae 0.00209072662526559
training loss 7.588889504836742e-06 mae 0.002117195551624835
training loss 7.801025913369017e-06 mae 0.002141587383773745
training loss 8.008232575391151e-06 mae 0.002177705912415253
Epoch 291, training: loss: 0.0000080, mae: 0.0021679 test: loss0.0000838, mae:0.0066058
training loss 6.498453785752645e-06 mae 0.0019479446345940232
training loss 7.752595584130247e-06 mae 0.0021196466975608
training loss 7.856155882631589e-06 mae 0.0021447055583524678
training loss 8.056317376440648e-06 mae 0.0021800077342989535
training loss 7.990685851348362e-06 mae 0.00216879187191174
Epoch 292, training: loss: 0.0000080, mae: 0.0021724 test: loss0.0000837, mae:0.0066007
training loss 5.302808403939707e-06 mae 0.0016954479506239295
training loss 7.809076712018239e-06 mae 0.0021510264566898638
training loss 7.735199374965849e-06 mae 0.0021332180805246135
training loss 7.685953153288922e-06 mae 0.00213204119632033
training loss 7.82397593517922e-06 mae 0.002147391195228296
Epoch 293, training: loss: 0.0000078, mae: 0.0021413 test: loss0.0000823, mae:0.0065675
training loss 8.430004527326673e-06 mae 0.0022409784141927958
training loss 8.022555171990853e-06 mae 0.0021595295299501978
training loss 8.220140691516667e-06 mae 0.0021889166477698794
training loss 8.027191344676162e-06 mae 0.002168952335529523
training loss 7.860275855135798e-06 mae 0.0021465904373601454
Epoch 294, training: loss: 0.0000079, mae: 0.0021457 test: loss0.0000833, mae:0.0066195
training loss 6.078644219087437e-06 mae 0.0019687723834067583
training loss 7.2164405662310766e-06 mae 0.0020668341358210524
training loss 7.0995197617803295e-06 mae 0.002037343498789659
training loss 7.406921201213631e-06 mae 0.0020764561351512044
training loss 7.5085099253094555e-06 mae 0.0020950349935084872
Epoch 295, training: loss: 0.0000075, mae: 0.0020966 test: loss0.0000851, mae:0.0066323
training loss 7.124246621970087e-06 mae 0.0019651411566883326
training loss 7.517557882166846e-06 mae 0.002100495468624228
training loss 8.24851026258744e-06 mae 0.0021962747946012735
training loss 8.234588089457827e-06 mae 0.002205274886581184
training loss 8.090859041228545e-06 mae 0.002187425173374256
Epoch 296, training: loss: 0.0000082, mae: 0.0021981 test: loss0.0000848, mae:0.0066798
training loss 8.032806363189593e-06 mae 0.0022868516389280558
training loss 7.688376894400468e-06 mae 0.0021206618391671297
training loss 8.045941411741168e-06 mae 0.002165719585511648
training loss 8.114257091782568e-06 mae 0.002178169799265484
training loss 8.048418538834736e-06 mae 0.0021707508504501933
Epoch 297, training: loss: 0.0000080, mae: 0.0021595 test: loss0.0000853, mae:0.0066877
training loss 9.891956324281637e-06 mae 0.0021821267437189817
training loss 7.38304250119779e-06 mae 0.0020570914722139056
training loss 7.441007980587259e-06 mae 0.002073272511750313
training loss 7.382125424302883e-06 mae 0.0020777021661178767
training loss 7.362836260735597e-06 mae 0.002076447863752644
Epoch 298, training: loss: 0.0000074, mae: 0.0020752 test: loss0.0000858, mae:0.0066874
training loss 1.1796284525189549e-05 mae 0.0024673689622431993
training loss 8.103667520723926e-06 mae 0.0021966674465539984
training loss 7.492309339453423e-06 mae 0.002098300058560649
training loss 7.5120727533972985e-06 mae 0.0020986261348924706
training loss 7.4931797416639914e-06 mae 0.0021020013361761542
Epoch 299, training: loss: 0.0000075, mae: 0.0021021 test: loss0.0000837, mae:0.0066504
current learning rate: 6.25e-05
training loss 7.758520041534211e-06 mae 0.002192934975028038
training loss 6.369227611346513e-06 mae 0.0019038522469938973
training loss 6.429420765097447e-06 mae 0.001890218778660909
training loss 6.263936533865272e-06 mae 0.0018666632741763697
training loss 6.187446290855245e-06 mae 0.0018550553816644038
Epoch 300, training: loss: 0.0000062, mae: 0.0018519 test: loss0.0000837, mae:0.0066115
training loss 3.5096875308227027e-06 mae 0.0014189844951033592
training loss 5.319178309910772e-06 mae 0.001700251297020883
training loss 5.548160018379618e-06 mae 0.001734687378491727
training loss 5.672338537631681e-06 mae 0.001758782421269559
training loss 5.741940194593778e-06 mae 0.0017695233656732895
Epoch 301, training: loss: 0.0000058, mae: 0.0017726 test: loss0.0000838, mae:0.0066219
training loss 3.746180254893261e-06 mae 0.0014604944735765457
training loss 5.437580895494309e-06 mae 0.0017262937126717732
training loss 5.4137369535947264e-06 mae 0.001716853715341059
training loss 5.425015509996193e-06 mae 0.0017274749667278882
training loss 5.6631286687620775e-06 mae 0.001757141438993946
Epoch 302, training: loss: 0.0000057, mae: 0.0017652 test: loss0.0000840, mae:0.0066237
training loss 5.644862540066242e-06 mae 0.0018038222333416343
training loss 5.898226542437137e-06 mae 0.001766674177191567
training loss 5.78208683502525e-06 mae 0.0017685660386992858
training loss 5.687168848585946e-06 mae 0.0017577742052184329
training loss 5.670092376709562e-06 mae 0.0017595362759181361
Epoch 303, training: loss: 0.0000057, mae: 0.0017559 test: loss0.0000846, mae:0.0066478
training loss 8.857835382514168e-06 mae 0.002331505762413144
training loss 5.759818122896831e-06 mae 0.001763174871859305
training loss 5.575882657314362e-06 mae 0.001738937414887518
training loss 5.6052788400751545e-06 mae 0.0017449817001905957
training loss 5.66100569772265e-06 mae 0.0017569719747737489
Epoch 304, training: loss: 0.0000057, mae: 0.0017626 test: loss0.0000849, mae:0.0066427
training loss 5.4500146688951645e-06 mae 0.0017666475614532828
training loss 5.622824752279878e-06 mae 0.0017656799401247908
training loss 5.73720147051315e-06 mae 0.0017788471967572033
training loss 5.788893009136422e-06 mae 0.0017795106704464022
training loss 5.731686745894516e-06 mae 0.0017798874476248052
Epoch 305, training: loss: 0.0000058, mae: 0.0017870 test: loss0.0000847, mae:0.0066512
training loss 4.910104962618789e-06 mae 0.0015368834137916565
training loss 5.346202567356977e-06 mae 0.0017242354395634988
training loss 5.519033776982895e-06 mae 0.0017556352955796339
training loss 5.650983746589506e-06 mae 0.0017687577587731228
training loss 5.613175384930954e-06 mae 0.0017524612633122215
Epoch 306, training: loss: 0.0000056, mae: 0.0017538 test: loss0.0000849, mae:0.0066605
training loss 4.584625912684714e-06 mae 0.0017183385789394379
training loss 5.543265151751327e-06 mae 0.0017453958234731467
training loss 5.623079286493468e-06 mae 0.0017648978553030006
training loss 5.593939189361511e-06 mae 0.0017592754709959912
training loss 5.607757448070056e-06 mae 0.0017630150147820284
Epoch 307, training: loss: 0.0000056, mae: 0.0017653 test: loss0.0000888, mae:0.0066908
training loss 5.969246558379382e-06 mae 0.0019566931296139956
training loss 5.839060002151255e-06 mae 0.0017929480059583687
training loss 5.56824076990986e-06 mae 0.0017522019622294178
training loss 5.625485226914109e-06 mae 0.0017626531782948614
training loss 5.7331515696902145e-06 mae 0.0017784348218724947
Epoch 308, training: loss: 0.0000057, mae: 0.0017780 test: loss0.0000842, mae:0.0066429
training loss 3.981411282438785e-06 mae 0.0014415672048926353
training loss 5.806708018366517e-06 mae 0.0017768330232915924
training loss 5.6628475524103315e-06 mae 0.001763906532732567
training loss 5.650059215109433e-06 mae 0.0017614646387650377
training loss 5.711138299067617e-06 mae 0.001775576079157714
Epoch 309, training: loss: 0.0000057, mae: 0.0017752 test: loss0.0000857, mae:0.0066906
training loss 5.843115559400758e-06 mae 0.001689685508608818
training loss 4.967503338205052e-06 mae 0.0016718627245841074
training loss 5.241436950893674e-06 mae 0.001709100906737149
training loss 5.438712102677408e-06 mae 0.0017320755568474829
training loss 5.584637714926077e-06 mae 0.0017501298866387622
Epoch 310, training: loss: 0.0000056, mae: 0.0017530 test: loss0.0000848, mae:0.0066436
training loss 7.5860311881115194e-06 mae 0.0020642264280468225
training loss 5.246869437435046e-06 mae 0.001697097521494417
training loss 5.416726154261696e-06 mae 0.0017330770168711644
training loss 5.560810337399577e-06 mae 0.0017589752304842633
training loss 5.635348726621068e-06 mae 0.00176498440061858
Epoch 311, training: loss: 0.0000056, mae: 0.0017646 test: loss0.0000851, mae:0.0066656
training loss 5.327525286702439e-06 mae 0.0017227655043825507
training loss 5.259760156132317e-06 mae 0.0017207602481814283
training loss 5.360940491246395e-06 mae 0.0017236347856590202
training loss 5.482300353013976e-06 mae 0.0017464648537485806
training loss 5.643897048092419e-06 mae 0.0017687197356480193
Epoch 312, training: loss: 0.0000056, mae: 0.0017665 test: loss0.0000846, mae:0.0066335
training loss 5.1696260925382376e-06 mae 0.0016084378585219383
training loss 5.4029346399914296e-06 mae 0.0017321278322853296
training loss 5.391386402701718e-06 mae 0.0017202820405271824
training loss 5.56422638143e-06 mae 0.0017485347783057308
training loss 5.60352730563361e-06 mae 0.0017570484011429383
Epoch 313, training: loss: 0.0000056, mae: 0.0017567 test: loss0.0000854, mae:0.0066527
training loss 4.112868737138342e-06 mae 0.0014505963772535324
training loss 5.536122149779285e-06 mae 0.0017406089007671848
training loss 5.694171013691641e-06 mae 0.0017789223318974865
training loss 5.685910138034416e-06 mae 0.0017773892779816064
training loss 5.641003863998797e-06 mae 0.0017752756853472089
Epoch 314, training: loss: 0.0000056, mae: 0.0017752 test: loss0.0000846, mae:0.0066614
training loss 4.675205218518386e-06 mae 0.0017733970889821649
training loss 5.2221603522518906e-06 mae 0.0016903579075291167
training loss 5.462068280970618e-06 mae 0.0017241395094736232
training loss 5.460315250521964e-06 mae 0.0017292308096854107
training loss 5.556410647542762e-06 mae 0.0017490373178389834
Epoch 315, training: loss: 0.0000055, mae: 0.0017410 test: loss0.0000855, mae:0.0066904
training loss 5.448477168101817e-06 mae 0.0018080422887578607
training loss 5.154001570974964e-06 mae 0.0017036455592104032
training loss 5.410483148501376e-06 mae 0.0017397915863773166
training loss 5.441526705691637e-06 mae 0.0017490411302173465
training loss 5.449258293961221e-06 mae 0.0017431615127843625
Epoch 316, training: loss: 0.0000055, mae: 0.0017456 test: loss0.0000850, mae:0.0066566
training loss 5.134168986842269e-06 mae 0.0016594994813203812
training loss 5.367191295358769e-06 mae 0.0017042136922314328
training loss 5.488376609494004e-06 mae 0.0017361404751150842
training loss 5.455525176077439e-06 mae 0.001735617243607732
training loss 5.478577081341417e-06 mae 0.0017377181955733664
Epoch 317, training: loss: 0.0000055, mae: 0.0017386 test: loss0.0000861, mae:0.0067187
training loss 5.418834916781634e-06 mae 0.0016606319695711136
training loss 5.47984798097733e-06 mae 0.00172468115730832
training loss 5.411053399428161e-06 mae 0.001722845208536059
training loss 5.3384289054785174e-06 mae 0.0017176842199798843
training loss 5.439102398749628e-06 mae 0.0017376858144488867
Epoch 318, training: loss: 0.0000054, mae: 0.0017359 test: loss0.0000857, mae:0.0066878
training loss 4.636389803636121e-06 mae 0.0016730433562770486
training loss 4.920033305489933e-06 mae 0.0016507222339985708
training loss 5.28091905240703e-06 mae 0.0017104548930033747
training loss 5.387433080818633e-06 mae 0.001722987869982638
training loss 5.471177660387073e-06 mae 0.001738876286217599
Epoch 319, training: loss: 0.0000055, mae: 0.0017388 test: loss0.0000869, mae:0.0067291
training loss 3.794350050156936e-06 mae 0.00160127691924572
training loss 5.3360521695710705e-06 mae 0.0017182095108699861
training loss 5.574340685589139e-06 mae 0.0017511089848778632
training loss 5.475673480988604e-06 mae 0.0017363401368840146
training loss 5.476479693234113e-06 mae 0.0017388237255685658
Epoch 320, training: loss: 0.0000055, mae: 0.0017360 test: loss0.0000851, mae:0.0066547
training loss 6.733827831340022e-06 mae 0.0019746702164411545
training loss 5.239173917431673e-06 mae 0.0016995084645482254
training loss 5.2351581484124285e-06 mae 0.0017038935283660006
training loss 5.338252547830388e-06 mae 0.0017180523020986296
training loss 5.411535264794287e-06 mae 0.001734304492042136
Epoch 321, training: loss: 0.0000054, mae: 0.0017326 test: loss0.0000860, mae:0.0067085
training loss 3.6592862215911737e-06 mae 0.0014548791805282235
training loss 5.133553860800662e-06 mae 0.0016785351806046336
training loss 5.133557070252263e-06 mae 0.0016755671136652922
training loss 5.161832917032286e-06 mae 0.0016816666254169297
training loss 5.297257490808678e-06 mae 0.0017073053868941553
Epoch 322, training: loss: 0.0000053, mae: 0.0017116 test: loss0.0000855, mae:0.0067035
training loss 3.873806690535275e-06 mae 0.001603511511348188
training loss 5.032376609996589e-06 mae 0.0016679056125747805
training loss 5.206503218876534e-06 mae 0.0016849744998121469
training loss 5.256622213609025e-06 mae 0.0016980975479294626
training loss 5.288595170285004e-06 mae 0.0017084390369813843
Epoch 323, training: loss: 0.0000053, mae: 0.0017134 test: loss0.0000874, mae:0.0067551
training loss 2.521022224755143e-06 mae 0.0011959448456764221
training loss 4.958460343843006e-06 mae 0.0016481626491226691
training loss 5.1772426221498655e-06 mae 0.0016815440872186185
training loss 5.245398618334372e-06 mae 0.001702117977281477
training loss 5.284958767256669e-06 mae 0.001707828289189779
Epoch 324, training: loss: 0.0000053, mae: 0.0017104 test: loss0.0000862, mae:0.0067098
training loss 4.4744833758159075e-06 mae 0.0016157816862687469
training loss 5.149209514724241e-06 mae 0.0016826339152769423
training loss 5.409264006873243e-06 mae 0.0017255545296420403
training loss 5.286433013571508e-06 mae 0.0017079794047005624
training loss 5.3669167397824984e-06 mae 0.0017222834065259989
Epoch 325, training: loss: 0.0000053, mae: 0.0017215 test: loss0.0000859, mae:0.0067028
training loss 5.238560788711766e-06 mae 0.001580422162078321
training loss 5.547203633849516e-06 mae 0.0017658546589789732
training loss 5.52644996112054e-06 mae 0.0017541663970171229
training loss 5.381833892632683e-06 mae 0.0017252404679118701
training loss 5.29948435012475e-06 mae 0.0017121416025456808
Epoch 326, training: loss: 0.0000053, mae: 0.0017121 test: loss0.0000866, mae:0.0067206
training loss 4.439493295649299e-06 mae 0.0015691164880990982
training loss 5.418339750416207e-06 mae 0.0017237327855956898
training loss 5.086534780401612e-06 mae 0.001660170706708242
training loss 5.1259111739834435e-06 mae 0.0016667883425321899
training loss 5.105947894522405e-06 mae 0.0016730655794407227
Epoch 327, training: loss: 0.0000051, mae: 0.0016783 test: loss0.0000874, mae:0.0067338
training loss 6.7727869463851675e-06 mae 0.0017842467641457915
training loss 5.009582782447835e-06 mae 0.0016593496570838431
training loss 5.0822531247291355e-06 mae 0.0016727831080961637
training loss 5.128007452360229e-06 mae 0.0016813776864429675
training loss 5.20544663186455e-06 mae 0.001692539485824753
Epoch 328, training: loss: 0.0000052, mae: 0.0016947 test: loss0.0000875, mae:0.0067602
training loss 3.3871931464091176e-06 mae 0.001301057985983789
training loss 4.777526167826555e-06 mae 0.0016175054319604648
training loss 5.112456354182353e-06 mae 0.001678768764388296
training loss 5.148227570461076e-06 mae 0.0016898374028491562
training loss 5.181718273689598e-06 mae 0.00169362719356087
Epoch 329, training: loss: 0.0000052, mae: 0.0016951 test: loss0.0000867, mae:0.0067258
training loss 9.786782356968615e-06 mae 0.0023030005395412445
training loss 5.5425383896933855e-06 mae 0.00174485034464548
training loss 5.278249368034674e-06 mae 0.0017007770464008702
training loss 5.24394234565724e-06 mae 0.0016957833940496317
training loss 5.179174376285095e-06 mae 0.0016883562419758135
Epoch 330, training: loss: 0.0000052, mae: 0.0016934 test: loss0.0000871, mae:0.0067533
training loss 4.277635980542982e-06 mae 0.0015559798339381814
training loss 5.083439387532302e-06 mae 0.0016801782892834325
training loss 5.14951052582096e-06 mae 0.0016935053535063966
training loss 5.13938596289137e-06 mae 0.0016890467801408857
training loss 5.181810753690041e-06 mae 0.001700325017628163
Epoch 331, training: loss: 0.0000052, mae: 0.0016988 test: loss0.0000864, mae:0.0067169
training loss 5.196767688175896e-06 mae 0.00171767920255661
training loss 5.169638459882858e-06 mae 0.0016965573977715538
training loss 5.069181893889949e-06 mae 0.001671362520739584
training loss 5.010532406580847e-06 mae 0.0016635570371375476
training loss 5.114950386426365e-06 mae 0.0016790951210634774
Epoch 332, training: loss: 0.0000051, mae: 0.0016796 test: loss0.0000865, mae:0.0067118
training loss 4.096731117897434e-06 mae 0.0014188360655680299
training loss 4.867784359614199e-06 mae 0.0016464660304835906
training loss 5.143363021506779e-06 mae 0.001682116421824782
training loss 5.028942987096435e-06 mae 0.0016606442664213333
training loss 5.1262742436480965e-06 mae 0.0016825943351236742
Epoch 333, training: loss: 0.0000051, mae: 0.0016862 test: loss0.0000870, mae:0.0067261
training loss 3.444381945882924e-06 mae 0.0015433846274390817
training loss 4.868432798128411e-06 mae 0.0016219230213512976
training loss 5.129871496056593e-06 mae 0.001678044846659471
training loss 5.1222910266933315e-06 mae 0.0016795816516738066
training loss 5.169254527753765e-06 mae 0.0016887237912099537
Epoch 334, training: loss: 0.0000052, mae: 0.0016852 test: loss0.0000866, mae:0.0067091
training loss 6.121349088061834e-06 mae 0.0017980557167902589
training loss 4.8911295460343615e-06 mae 0.0016503856384980625
training loss 4.927998903926575e-06 mae 0.0016524460994278884
training loss 5.007791186538129e-06 mae 0.0016620969507744575
training loss 5.066183352914889e-06 mae 0.001672921373870279
Epoch 335, training: loss: 0.0000051, mae: 0.0016795 test: loss0.0000872, mae:0.0067506
training loss 6.0942543314013164e-06 mae 0.002012861194089055
training loss 5.226295610662097e-06 mae 0.001719885850416533
training loss 5.131271759952629e-06 mae 0.0016922135226483835
training loss 5.089296487824788e-06 mae 0.0016825772383996587
training loss 5.11312550566908e-06 mae 0.0016891055981355222
Epoch 336, training: loss: 0.0000052, mae: 0.0016917 test: loss0.0000867, mae:0.0067230
training loss 5.097929260955425e-06 mae 0.0016486886888742447
training loss 5.170217763424814e-06 mae 0.0016770163190313707
training loss 4.938462278023327e-06 mae 0.001642962659497072
training loss 4.9999072859241056e-06 mae 0.0016576866349452073
training loss 5.02206296063993e-06 mae 0.0016598186521693608
Epoch 337, training: loss: 0.0000050, mae: 0.0016537 test: loss0.0000870, mae:0.0067359
training loss 4.095116764801787e-06 mae 0.0015113446861505508
training loss 4.906705026216658e-06 mae 0.0016423158858921015
training loss 4.952605408026005e-06 mae 0.001643477336245377
training loss 4.908530418738672e-06 mae 0.0016409849820485877
training loss 5.031716758781135e-06 mae 0.0016612883735985609
Epoch 338, training: loss: 0.0000050, mae: 0.0016612 test: loss0.0000878, mae:0.0067651
training loss 4.9222458073927555e-06 mae 0.001729130162857473
training loss 5.09205308256188e-06 mae 0.0016733480060436563
training loss 5.2224210240538995e-06 mae 0.0016928049159411452
training loss 5.17976935868578e-06 mae 0.0016874314689799056
training loss 5.166069967123094e-06 mae 0.0016878201054010664
Epoch 339, training: loss: 0.0000051, mae: 0.0016848 test: loss0.0000879, mae:0.0067792
training loss 8.446647370874416e-06 mae 0.0019389800727367401
training loss 4.854096259270377e-06 mae 0.0016258227018018563
training loss 5.013910158415437e-06 mae 0.0016582617628397326
training loss 5.0141789066305175e-06 mae 0.0016582863658222535
training loss 4.963780997176721e-06 mae 0.0016502979343562426
Epoch 340, training: loss: 0.0000050, mae: 0.0016515 test: loss0.0000874, mae:0.0067640
training loss 4.675341187976301e-06 mae 0.0016278460389003158
training loss 4.88336706225442e-06 mae 0.0016485627503225616
training loss 4.848924767979004e-06 mae 0.001629768707097904
training loss 4.848899190818468e-06 mae 0.0016311857169370677
training loss 4.849320711699289e-06 mae 0.0016331224706932089
Epoch 341, training: loss: 0.0000048, mae: 0.0016315 test: loss0.0000876, mae:0.0067776
training loss 3.6985084079788066e-06 mae 0.0013599619269371033
training loss 4.940810332995811e-06 mae 0.001631079937842693
training loss 4.84920578495767e-06 mae 0.001623022497982008
training loss 4.855716301579571e-06 mae 0.0016257717584056274
training loss 4.948594291976526e-06 mae 0.0016498719394179202
Epoch 342, training: loss: 0.0000049, mae: 0.0016461 test: loss0.0000879, mae:0.0067796
training loss 4.854836788581451e-06 mae 0.0014391569420695305
training loss 4.644053567116155e-06 mae 0.0015904403259685519
training loss 4.801108720004219e-06 mae 0.0016241280643250985
training loss 4.814288232163087e-06 mae 0.0016285338152249333
training loss 4.840163282734915e-06 mae 0.0016338844541506612
Epoch 343, training: loss: 0.0000049, mae: 0.0016372 test: loss0.0000869, mae:0.0067312
training loss 4.5867404878663365e-06 mae 0.001530274748802185
training loss 4.614202368304855e-06 mae 0.0015910635131648652
training loss 4.833032810235842e-06 mae 0.0016371949280768925
training loss 4.828864974789868e-06 mae 0.0016355591005836891
training loss 4.8662872816282785e-06 mae 0.0016425900376721547
Epoch 344, training: loss: 0.0000049, mae: 0.0016489 test: loss0.0000884, mae:0.0068112
training loss 3.419489075895399e-06 mae 0.0014885741984471679
training loss 5.094711192285498e-06 mae 0.0016734689296058875
training loss 4.939881359905036e-06 mae 0.0016393704307588312
training loss 4.910479486212681e-06 mae 0.0016370856739728657
training loss 4.8670724176353464e-06 mae 0.0016323083818227562
Epoch 345, training: loss: 0.0000048, mae: 0.0016314 test: loss0.0000885, mae:0.0067956
training loss 4.1552675611455925e-06 mae 0.0015270992880687118
training loss 4.836730215852274e-06 mae 0.0016227861405258958
training loss 4.7182715553561245e-06 mae 0.0015969578393430696
training loss 4.775161926694552e-06 mae 0.001612497622375407
training loss 4.803693031615117e-06 mae 0.0016183453491559964
Epoch 346, training: loss: 0.0000048, mae: 0.0016212 test: loss0.0000886, mae:0.0068094
training loss 2.3774550754751544e-06 mae 0.0012402919819578528
training loss 5.067201862081728e-06 mae 0.0016619730603826392
training loss 4.9971253909209535e-06 mae 0.0016635861455250789
training loss 4.82232025091591e-06 mae 0.0016269538696509912
training loss 4.89650838673371e-06 mae 0.0016372722407932672
Epoch 347, training: loss: 0.0000049, mae: 0.0016391 test: loss0.0000883, mae:0.0067942
training loss 3.2158466183318524e-06 mae 0.0014561926946043968
training loss 4.4640113468203195e-06 mae 0.0015826295015822145
training loss 4.576773617958285e-06 mae 0.0015869711428917574
training loss 4.738138170448311e-06 mae 0.0016072712906494435
training loss 4.8019495206565104e-06 mae 0.0016177845105241557
Epoch 348, training: loss: 0.0000048, mae: 0.0016196 test: loss0.0000877, mae:0.0067546
training loss 5.945497377979336e-06 mae 0.0016832882538437843
training loss 4.698986784063108e-06 mae 0.0015941939211687915
training loss 4.69382628508167e-06 mae 0.0015924120550089987
training loss 4.651041218187176e-06 mae 0.0015906115973812363
training loss 4.708413194721423e-06 mae 0.0016040198072855972
Epoch 349, training: loss: 0.0000047, mae: 0.0016083 test: loss0.0000878, mae:0.0067738
training loss 2.698901653275243e-06 mae 0.0012809043982997537
training loss 4.673993441660881e-06 mae 0.001603948050543812
training loss 4.871708968872412e-06 mae 0.0016351340702817877
training loss 4.913239545143167e-06 mae 0.0016442567984333001
training loss 4.876828165720891e-06 mae 0.0016372373897526689
Epoch 350, training: loss: 0.0000048, mae: 0.0016327 test: loss0.0000886, mae:0.0068033
training loss 5.023337052989518e-06 mae 0.001615283079445362
training loss 4.471704472712638e-06 mae 0.0015841696694420247
training loss 4.589666825340108e-06 mae 0.0016026693525364496
training loss 4.7631979861690115e-06 mae 0.0016272777100926304
training loss 4.741999634972845e-06 mae 0.001627653869058918
Epoch 351, training: loss: 0.0000048, mae: 0.0016335 test: loss0.0000882, mae:0.0067931
training loss 3.998653937742347e-06 mae 0.0014514349168166518
training loss 4.579021847080987e-06 mae 0.0015798596911789737
training loss 4.544598839244584e-06 mae 0.001579277032967841
training loss 4.627091540884753e-06 mae 0.001595417168669846
training loss 4.65335981075373e-06 mae 0.0016008610617072863
Epoch 352, training: loss: 0.0000047, mae: 0.0016005 test: loss0.0000884, mae:0.0067850
training loss 4.1202488318958785e-06 mae 0.0015121223405003548
training loss 4.810220077036088e-06 mae 0.0016336153357234947
training loss 4.741950162119027e-06 mae 0.0016116324079103103
training loss 4.722711549443405e-06 mae 0.0016166912206750828
training loss 4.715649666238516e-06 mae 0.0016178376283450281
Epoch 353, training: loss: 0.0000047, mae: 0.0016172 test: loss0.0000875, mae:0.0067631
training loss 4.638948212232208e-06 mae 0.0015573067357763648
training loss 4.638906785640205e-06 mae 0.001592829478813299
training loss 4.506184342683963e-06 mae 0.001575361087798408
training loss 4.5284023814706375e-06 mae 0.0015758664959375525
training loss 4.629934741347116e-06 mae 0.0015851871385030554
Epoch 354, training: loss: 0.0000046, mae: 0.0015871 test: loss0.0000891, mae:0.0068253
training loss 6.74591274218983e-06 mae 0.0017674366245046258
training loss 4.34133705187146e-06 mae 0.00155378881084057
training loss 4.471676276699497e-06 mae 0.0015640339957052234
training loss 4.658536050821013e-06 mae 0.001600077210783662
training loss 4.799891532674163e-06 mae 0.0016311373120285013
Epoch 355, training: loss: 0.0000048, mae: 0.0016320 test: loss0.0000888, mae:0.0068162
training loss 5.939369657426141e-06 mae 0.0017022844403982162
training loss 5.029583159436394e-06 mae 0.0016506950430316377
training loss 4.775156628624029e-06 mae 0.001606164103613632
training loss 4.652781123713673e-06 mae 0.001590175696784347
training loss 4.6629481405477715e-06 mae 0.0015913858628641834
Epoch 356, training: loss: 0.0000047, mae: 0.0015933 test: loss0.0000882, mae:0.0067902
training loss 5.467375103762606e-06 mae 0.0015452370280399919
training loss 4.910264587855562e-06 mae 0.0016743322724805157
training loss 4.780802148143444e-06 mae 0.001636970693725023
training loss 4.866764920011791e-06 mae 0.001639179781307497
training loss 4.789150121967995e-06 mae 0.0016271611386029733
Epoch 357, training: loss: 0.0000048, mae: 0.0016227 test: loss0.0000880, mae:0.0067928
training loss 6.083806965762051e-06 mae 0.0016236907104030252
training loss 4.512006345227264e-06 mae 0.001586295630070655
training loss 4.420063647774097e-06 mae 0.0015637580014875915
training loss 4.465953158753397e-06 mae 0.0015709919289112187
training loss 4.57240617490143e-06 mae 0.001588628139110532
Epoch 358, training: loss: 0.0000046, mae: 0.0015912 test: loss0.0000891, mae:0.0068250
training loss 4.169559815636603e-06 mae 0.0015595793956890702
training loss 4.45401626308797e-06 mae 0.0015649631363796248
training loss 4.516015041649511e-06 mae 0.001573133047227517
training loss 4.546300977453464e-06 mae 0.0015776886917737052
training loss 4.5737380925621745e-06 mae 0.0015769837000198775
Epoch 359, training: loss: 0.0000046, mae: 0.0015743 test: loss0.0000895, mae:0.0068309
training loss 3.3217595500900643e-06 mae 0.0014673465630039573
training loss 4.406841726984778e-06 mae 0.0015387357025425513
training loss 4.52293053094403e-06 mae 0.001577128613561318
training loss 4.639250971574456e-06 mae 0.0015976305415742844
training loss 4.5724127246207375e-06 mae 0.0015895996492967675
Epoch 360, training: loss: 0.0000046, mae: 0.0015926 test: loss0.0000883, mae:0.0067825
training loss 2.3869449705671286e-06 mae 0.0012171441921964288
training loss 4.230923909120726e-06 mae 0.001532867470957047
training loss 4.752953695024551e-06 mae 0.0016123382086487544
training loss 4.737097521039133e-06 mae 0.0016102491436982592
training loss 4.653047115389381e-06 mae 0.0016022155377596842
Epoch 361, training: loss: 0.0000047, mae: 0.0016022 test: loss0.0000883, mae:0.0068026
training loss 5.681954007741297e-06 mae 0.0017897995421662927
training loss 4.721510986557547e-06 mae 0.0016148147223444256
training loss 4.566774762652399e-06 mae 0.0015987547501774124
training loss 4.61378265344746e-06 mae 0.001601733029079901
training loss 4.645895920878307e-06 mae 0.0016048082708849094
Epoch 362, training: loss: 0.0000046, mae: 0.0016054 test: loss0.0000888, mae:0.0068114
training loss 3.4384702303214e-06 mae 0.0013572847237810493
training loss 4.845497958776119e-06 mae 0.0016277424452424633
training loss 4.572998423640511e-06 mae 0.0015892896669833817
training loss 4.532103147786551e-06 mae 0.0015767152438137602
training loss 4.545813170857155e-06 mae 0.0015772986098364068
Epoch 363, training: loss: 0.0000045, mae: 0.0015777 test: loss0.0000893, mae:0.0068237
training loss 5.233310275798431e-06 mae 0.0015881964936852455
training loss 4.481943303254214e-06 mae 0.0015747128237111896
training loss 4.5316317658097685e-06 mae 0.0015852781252400712
training loss 4.580615035779643e-06 mae 0.0015885114025275247
training loss 4.529966898176169e-06 mae 0.001585168492938601
Epoch 364, training: loss: 0.0000046, mae: 0.0015908 test: loss0.0000883, mae:0.0067882
training loss 4.302686647861265e-06 mae 0.0015223728260025382
training loss 4.311523160176163e-06 mae 0.001532600316054681
training loss 4.353812219861621e-06 mae 0.0015455472806357956
training loss 4.4308669236146535e-06 mae 0.001556916931792108
training loss 4.459660985460446e-06 mae 0.0015662895261879966
Epoch 365, training: loss: 0.0000045, mae: 0.0015680 test: loss0.0000898, mae:0.0068437
training loss 6.052501248632325e-06 mae 0.0016349836951121688
training loss 4.709117369131098e-06 mae 0.00159730981149729
training loss 4.618600194202604e-06 mae 0.0015890700039499909
training loss 4.620876345462412e-06 mae 0.0016000001006667185
training loss 4.532766750466477e-06 mae 0.0015816000106035546
Epoch 366, training: loss: 0.0000045, mae: 0.0015776 test: loss0.0000889, mae:0.0068168
training loss 4.218873982608784e-06 mae 0.0016945786774158478
training loss 4.094087983628247e-06 mae 0.0015241092016153477
training loss 4.2252129355576905e-06 mae 0.0015283979586150384
training loss 4.3667780558683925e-06 mae 0.001543361933389987
training loss 4.399890351114711e-06 mae 0.0015494198647248957
Epoch 367, training: loss: 0.0000044, mae: 0.0015519 test: loss0.0000890, mae:0.0068373
training loss 3.323054670545389e-06 mae 0.001319004804827273
training loss 4.2207800784215635e-06 mae 0.0015099650778460738
training loss 4.4082715491937705e-06 mae 0.001550168892838946
training loss 4.402996416158671e-06 mae 0.0015508901398776978
training loss 4.460120888877066e-06 mae 0.0015580076874877832
Epoch 368, training: loss: 0.0000045, mae: 0.0015663 test: loss0.0000890, mae:0.0068281
training loss 2.785061951726675e-06 mae 0.0013409635284915566
training loss 4.1997831449050715e-06 mae 0.001540337198450431
training loss 4.325643417495042e-06 mae 0.0015528913296869787
training loss 4.471335450522406e-06 mae 0.0015712119739144947
training loss 4.45295039305586e-06 mae 0.001567134591959306
Epoch 369, training: loss: 0.0000045, mae: 0.0015705 test: loss0.0000885, mae:0.0068043
training loss 2.5221841042366577e-06 mae 0.001230384805239737
training loss 4.173340990817843e-06 mae 0.0015272070475689625
training loss 4.29153786144795e-06 mae 0.0015369407512192234
training loss 4.40715878341013e-06 mae 0.0015559566634343258
training loss 4.4461102813659775e-06 mae 0.0015624283473540223
Epoch 370, training: loss: 0.0000045, mae: 0.0015669 test: loss0.0000886, mae:0.0068054
training loss 3.2807595289341407e-06 mae 0.001467771246097982
training loss 4.4884338475479614e-06 mae 0.001577027368030566
training loss 4.525656673775862e-06 mae 0.0015835356518238933
training loss 4.4314135269188476e-06 mae 0.0015631691338371069
training loss 4.465441395552434e-06 mae 0.001571201111783102
Epoch 371, training: loss: 0.0000045, mae: 0.0015713 test: loss0.0000894, mae:0.0068276
training loss 3.4444649372744607e-06 mae 0.0013583622639998794
training loss 4.037298817909723e-06 mae 0.0014829603297745481
training loss 4.2238682206264264e-06 mae 0.0015212562578352223
training loss 4.2547290148235664e-06 mae 0.001523789303648649
training loss 4.368929501027588e-06 mae 0.0015412044609475197
Epoch 372, training: loss: 0.0000044, mae: 0.0015424 test: loss0.0000890, mae:0.0068261
training loss 3.6942401493433863e-06 mae 0.0014604140305891633
training loss 4.2523058531443495e-06 mae 0.0015371985044147745
training loss 4.372887117527414e-06 mae 0.0015483512935035003
training loss 4.261884747477521e-06 mae 0.0015209303075595696
training loss 4.239563648849949e-06 mae 0.0015174960034125627
Epoch 373, training: loss: 0.0000042, mae: 0.0015138 test: loss0.0000896, mae:0.0068461
training loss 3.2949540127447108e-06 mae 0.0013865726068615913
training loss 4.18766718194449e-06 mae 0.0014963237564170771
training loss 4.255409256002886e-06 mae 0.001512775452355716
training loss 4.221428552203645e-06 mae 0.0015151221120100944
training loss 4.259506168021641e-06 mae 0.0015307338873563874
Epoch 374, training: loss: 0.0000043, mae: 0.0015286 test: loss0.0000884, mae:0.0068128
training loss 3.828046828857623e-06 mae 0.0016023451462388039
training loss 4.413137891306714e-06 mae 0.001557500909685212
training loss 4.236079398156516e-06 mae 0.0015237701176158563
training loss 4.2549069625816425e-06 mae 0.0015227246940111281
training loss 4.319164571965512e-06 mae 0.00153866237833336
Epoch 375, training: loss: 0.0000043, mae: 0.0015336 test: loss0.0000889, mae:0.0068112
training loss 4.436093604454072e-06 mae 0.0014333970611914992
training loss 3.940554667197454e-06 mae 0.001474484899446514
training loss 4.196701857016713e-06 mae 0.0015182099063041625
training loss 4.336574662127962e-06 mae 0.0015421137942266423
training loss 4.2685243045919184e-06 mae 0.0015297640032199812
Epoch 376, training: loss: 0.0000042, mae: 0.0015281 test: loss0.0000890, mae:0.0068365
training loss 3.96389532397734e-06 mae 0.001456310972571373
training loss 4.086381277731703e-06 mae 0.0015003868657182538
training loss 4.103778655016775e-06 mae 0.0015106121698246883
training loss 4.118294525039724e-06 mae 0.0015089145517716838
training loss 4.241396177733699e-06 mae 0.0015275618125133175
Epoch 377, training: loss: 0.0000042, mae: 0.0015268 test: loss0.0000891, mae:0.0068336
training loss 4.0687446016818285e-06 mae 0.0014105290174484253
training loss 4.069346700549323e-06 mae 0.0014985896003268217
training loss 4.1745465773376505e-06 mae 0.001515775671145777
training loss 4.161529027110881e-06 mae 0.0015135055164257616
training loss 4.281100795631219e-06 mae 0.0015303411669287802
Epoch 378, training: loss: 0.0000043, mae: 0.0015328 test: loss0.0000900, mae:0.0068643
training loss 4.601202363119228e-06 mae 0.0017163759330287576
training loss 4.656835333698124e-06 mae 0.0016046873951221213
training loss 4.3511619027737155e-06 mae 0.001555267426342067
training loss 4.319652607471246e-06 mae 0.0015521738585701453
training loss 4.367200973541715e-06 mae 0.0015590846954395452
Epoch 379, training: loss: 0.0000044, mae: 0.0015590 test: loss0.0000889, mae:0.0068156
training loss 3.304026904515922e-06 mae 0.001358403475023806
training loss 4.0900290648149924e-06 mae 0.0014892215520947002
training loss 4.208166006215419e-06 mae 0.0015213026245594911
training loss 4.188840600381686e-06 mae 0.0015147340126236935
training loss 4.2608276315837614e-06 mae 0.0015277254936141673
Epoch 380, training: loss: 0.0000043, mae: 0.0015287 test: loss0.0000893, mae:0.0068396
training loss 2.6717571017798036e-06 mae 0.0012719184160232544
training loss 4.177960229700307e-06 mae 0.001506755139916113
training loss 3.981875847379756e-06 mae 0.0014781612086819838
training loss 4.057599717602204e-06 mae 0.0014936393550874674
training loss 4.240019676167571e-06 mae 0.0015215132824511297
Epoch 381, training: loss: 0.0000042, mae: 0.0015199 test: loss0.0000888, mae:0.0068157
training loss 3.338008355058264e-06 mae 0.0014661367749795318
training loss 3.896220529860133e-06 mae 0.001467974162569233
training loss 3.961087269216126e-06 mae 0.0014808540218648049
training loss 4.15913285866694e-06 mae 0.0015204518143741403
training loss 4.256244174776742e-06 mae 0.0015394783049441916
Epoch 382, training: loss: 0.0000043, mae: 0.0015417 test: loss0.0000894, mae:0.0068405
training loss 3.0638359476142796e-06 mae 0.0015379259129986167
training loss 4.109733214029213e-06 mae 0.001522635332281318
training loss 4.090546533664355e-06 mae 0.0015048636302195197
training loss 4.158555079570849e-06 mae 0.0015114235937663213
training loss 4.2725751063246615e-06 mae 0.0015309179217360025
Epoch 383, training: loss: 0.0000043, mae: 0.0015307 test: loss0.0000899, mae:0.0068570
training loss 3.952010501961922e-06 mae 0.001363991410471499
training loss 3.7772953548625854e-06 mae 0.001441018596547199
training loss 3.916341131683803e-06 mae 0.0014649771677219482
training loss 4.1022438656891745e-06 mae 0.0014923756606013373
training loss 4.181614896983253e-06 mae 0.001516917601125825
Epoch 384, training: loss: 0.0000042, mae: 0.0015190 test: loss0.0000900, mae:0.0068584
training loss 3.887025741278194e-06 mae 0.0015885786851868033
training loss 4.1983252984602e-06 mae 0.0015232735838485408
training loss 4.12099360337469e-06 mae 0.0015125363779318803
training loss 4.058380017376961e-06 mae 0.0014948644521121945
training loss 4.152806085567225e-06 mae 0.0015102347141289288
Epoch 385, training: loss: 0.0000041, mae: 0.0015099 test: loss0.0000899, mae:0.0068442
training loss 2.2428378088079626e-06 mae 0.0010690264170989394
training loss 4.204682918319961e-06 mae 0.0015286196059748236
training loss 4.029919986044995e-06 mae 0.0014992524860518996
training loss 4.1091237632062035e-06 mae 0.0015030866373169996
training loss 4.180616769534127e-06 mae 0.0015189749784118947
Epoch 386, training: loss: 0.0000042, mae: 0.0015197 test: loss0.0000895, mae:0.0068410
training loss 4.635880486603128e-06 mae 0.0014772620052099228
training loss 3.837182498054793e-06 mae 0.0014509499648257215
training loss 3.990386570120167e-06 mae 0.001476912833012567
training loss 4.13734075953362e-06 mae 0.0015126664279374162
training loss 4.1670168299892236e-06 mae 0.001520421872591942
Epoch 387, training: loss: 0.0000042, mae: 0.0015220 test: loss0.0000901, mae:0.0068897
training loss 2.4750713691901183e-06 mae 0.0012268883874639869
training loss 3.826847038159458e-06 mae 0.0014247694554939579
training loss 3.78460766560328e-06 mae 0.0014270541470239664
training loss 3.92927528844437e-06 mae 0.0014559309212614274
training loss 4.036628154300791e-06 mae 0.0014751157236855424
Epoch 388, training: loss: 0.0000040, mae: 0.0014724 test: loss0.0000896, mae:0.0068589
training loss 3.8765010685892776e-06 mae 0.0014046666910871863
training loss 3.772148593449018e-06 mae 0.0014336812241441189
training loss 3.905683471283836e-06 mae 0.00145503179294675
training loss 4.080103273483342e-06 mae 0.0014911884702557064
training loss 4.124813504922395e-06 mae 0.001502055237509903
Epoch 389, training: loss: 0.0000041, mae: 0.0014999 test: loss0.0000888, mae:0.0068190
training loss 4.675523541664006e-06 mae 0.0016036262968555093
training loss 3.7466051360431316e-06 mae 0.0014549346375918274
training loss 3.85352545845758e-06 mae 0.0014596108812838793
training loss 3.892190892692937e-06 mae 0.0014568229224409487
training loss 3.983030959585195e-06 mae 0.001475128055890716
Epoch 390, training: loss: 0.0000040, mae: 0.0014758 test: loss0.0000898, mae:0.0068578
training loss 2.706287659748341e-06 mae 0.0012694488978013396
training loss 3.975725812600941e-06 mae 0.0014874622358155309
training loss 4.089085473199226e-06 mae 0.0015043380939964172
training loss 4.053892435750573e-06 mae 0.0014997664646288717
training loss 4.097970277279501e-06 mae 0.0015055766579375339
Epoch 391, training: loss: 0.0000041, mae: 0.0015043 test: loss0.0000903, mae:0.0068762
training loss 3.0206811061361805e-06 mae 0.00131416751537472
training loss 3.959975733956597e-06 mae 0.001468209124069807
training loss 4.040872030069151e-06 mae 0.001482944740189959
training loss 4.064186997245227e-06 mae 0.0014918316136398012
training loss 4.074952318176348e-06 mae 0.001495202078046138
Epoch 392, training: loss: 0.0000041, mae: 0.0014986 test: loss0.0000905, mae:0.0068761
training loss 4.243189323460683e-06 mae 0.0014943020651116967
training loss 3.898119548109973e-06 mae 0.0014603884443275487
training loss 3.912721462085648e-06 mae 0.0014743467039050587
training loss 4.066759545924091e-06 mae 0.001506923182115008
training loss 4.116659447141839e-06 mae 0.001514850233551764
Epoch 393, training: loss: 0.0000041, mae: 0.0015133 test: loss0.0000895, mae:0.0068508
training loss 4.234317657392239e-06 mae 0.0015208361437544227
training loss 3.997120275551636e-06 mae 0.0014564308826355084
training loss 3.936362654622814e-06 mae 0.001464466310779352
training loss 3.974353771448156e-06 mae 0.001478008950585561
training loss 3.998477411494885e-06 mae 0.0014787609011296236
Epoch 394, training: loss: 0.0000040, mae: 0.0014742 test: loss0.0000894, mae:0.0068562
training loss 4.574719696393004e-06 mae 0.0016031855484470725
training loss 4.205016707333818e-06 mae 0.0014958804308929867
training loss 4.062070648516139e-06 mae 0.0014931949392643455
training loss 4.103257179813469e-06 mae 0.001499877232651146
training loss 4.08117031682521e-06 mae 0.0015036344692685559
Epoch 395, training: loss: 0.0000041, mae: 0.0015073 test: loss0.0000914, mae:0.0068951
training loss 2.153253490178031e-06 mae 0.001153800985775888
training loss 3.971986621300126e-06 mae 0.0014883015537634488
training loss 3.874391073523919e-06 mae 0.0014593957158008423
training loss 3.979601844988287e-06 mae 0.0014788123265537887
training loss 4.017962976320461e-06 mae 0.001487188393931224
Epoch 396, training: loss: 0.0000040, mae: 0.0014876 test: loss0.0000888, mae:0.0068300
training loss 4.317684670240851e-06 mae 0.0014881727984175086
training loss 3.7207924124766805e-06 mae 0.0014251137275577465
training loss 3.7597837652554366e-06 mae 0.0014331696778628051
training loss 3.917936451608025e-06 mae 0.001459896488158365
training loss 3.97129194744363e-06 mae 0.001471998242994965
Epoch 397, training: loss: 0.0000040, mae: 0.0014769 test: loss0.0000905, mae:0.0069063
training loss 4.030432137369644e-06 mae 0.0015061106532812119
training loss 4.394487262413581e-06 mae 0.0015452972786756704
training loss 4.257300196792976e-06 mae 0.0015377143056749705
training loss 4.2957344492799115e-06 mae 0.001547762670011463
training loss 4.162576087493554e-06 mae 0.0015207876611167367
Epoch 398, training: loss: 0.0000042, mae: 0.0015207 test: loss0.0000906, mae:0.0068995
training loss 4.82485711472691e-06 mae 0.0017774443840608
training loss 4.093682406949251e-06 mae 0.0015089442387369333
training loss 4.072215494605837e-06 mae 0.0015051299729153954
training loss 3.993290040654261e-06 mae 0.0014875152075835907
training loss 3.9994135230568395e-06 mae 0.0014852528919034932
Epoch 399, training: loss: 0.0000040, mae: 0.0014841 test: loss0.0000905, mae:0.0068930
current learning rate: 3.125e-05
training loss 3.1892016068013618e-06 mae 0.001471123076044023
training loss 3.6933613835251044e-06 mae 0.0014019687431773135
training loss 3.552906753944932e-06 mae 0.0013675257001109053
training loss 3.4260442679931415e-06 mae 0.0013411646170714347
training loss 3.427945367023208e-06 mae 0.0013361398460209107
Epoch 400, training: loss: 0.0000034, mae: 0.0013369 test: loss0.0000901, mae:0.0068720
training loss 3.687015123432502e-06 mae 0.0013209948083385825
training loss 2.9369038554482264e-06 mae 0.0012191121989185464
training loss 3.1285270709547446e-06 mae 0.0012597148577535135
training loss 3.257653237060909e-06 mae 0.0012818288766556962
training loss 3.2948064240394957e-06 mae 0.0012867714255811887
Epoch 401, training: loss: 0.0000033, mae: 0.0012846 test: loss0.0000902, mae:0.0068801
training loss 2.5166461909975624e-06 mae 0.0011039344826713204
training loss 3.1697580456524266e-06 mae 0.0012579120663633824
training loss 3.2603074075295534e-06 mae 0.0012666334497216753
training loss 3.226937738324841e-06 mae 0.0012672533453072096
training loss 3.2555445456741304e-06 mae 0.001280740449907008
Epoch 402, training: loss: 0.0000033, mae: 0.0012789 test: loss0.0000913, mae:0.0069189
training loss 3.2397213090007426e-06 mae 0.0012979930033907294
training loss 3.0908873494648577e-06 mae 0.001236662377064646
training loss 3.1563144596088246e-06 mae 0.001243487009110095
training loss 3.1699388301266593e-06 mae 0.0012523764694288423
training loss 3.1938563947461704e-06 mae 0.0012589730397654488
Epoch 403, training: loss: 0.0000032, mae: 0.0012604 test: loss0.0000901, mae:0.0068926
training loss 2.431557959425845e-06 mae 0.001081273891031742
training loss 3.314085762105927e-06 mae 0.0012775271726936544
training loss 3.2431710618009976e-06 mae 0.001267425392611451
training loss 3.1624849273895296e-06 mae 0.0012590490472009599
training loss 3.2523927342056e-06 mae 0.0012746854182518097
Epoch 404, training: loss: 0.0000032, mae: 0.0012707 test: loss0.0000905, mae:0.0068795
training loss 1.8380047777100117e-06 mae 0.000984219484962523
training loss 3.194545588128534e-06 mae 0.001246583889987247
training loss 3.185468532894159e-06 mae 0.0012591308280180144
training loss 3.259735388154099e-06 mae 0.0012775704600780414
training loss 3.294969577660568e-06 mae 0.0012820076179431187
Epoch 405, training: loss: 0.0000033, mae: 0.0012794 test: loss0.0000910, mae:0.0069078
training loss 3.0648491247120546e-06 mae 0.0012849426129832864
training loss 3.1464104272255965e-06 mae 0.0012495221293933104
training loss 3.024312808529339e-06 mae 0.001234116661477612
training loss 3.18006615113054e-06 mae 0.0012618855801441809
training loss 3.2552985629904895e-06 mae 0.0012767371902270106
Epoch 406, training: loss: 0.0000032, mae: 0.0012706 test: loss0.0000902, mae:0.0068702
training loss 5.7205420489481185e-06 mae 0.0015478810528293252
training loss 3.393780360181386e-06 mae 0.0013017071726019766
training loss 3.3277074293221064e-06 mae 0.0012844035743117925
training loss 3.2887556403392297e-06 mae 0.0012763969986452819
training loss 3.241578987845106e-06 mae 0.0012723822641853642
Epoch 407, training: loss: 0.0000032, mae: 0.0012725 test: loss0.0000912, mae:0.0069132
training loss 3.014581352545065e-06 mae 0.0013712808722630143
training loss 3.252352268877896e-06 mae 0.0012499227384835773
training loss 3.1667900691107858e-06 mae 0.0012422716089450557
training loss 3.220908283102766e-06 mae 0.0012655783774614922
training loss 3.272761416689777e-06 mae 0.0012799467551475627
Epoch 408, training: loss: 0.0000032, mae: 0.0012769 test: loss0.0000909, mae:0.0069059
training loss 4.901732609141618e-06 mae 0.0015222480287775397
training loss 3.125996557833325e-06 mae 0.0012419275202167532
training loss 3.126301860470223e-06 mae 0.001252365007530898
training loss 3.2207007376164485e-06 mae 0.0012738399769623599
training loss 3.2307677075274583e-06 mae 0.001274513655263393
Epoch 409, training: loss: 0.0000032, mae: 0.0012739 test: loss0.0000913, mae:0.0069117
training loss 3.1281326755561167e-06 mae 0.0012663115048781037
training loss 3.23293823064031e-06 mae 0.0012797266267714838
training loss 3.2065050552971853e-06 mae 0.001266073971516499
training loss 3.1264360425527846e-06 mae 0.00125295830241386
training loss 3.2055730414062436e-06 mae 0.0012713078451713558
Epoch 410, training: loss: 0.0000032, mae: 0.0012755 test: loss0.0000906, mae:0.0068988
training loss 3.94856215280015e-06 mae 0.0013326151529327035
training loss 3.0428323823655907e-06 mae 0.0012456543570128727
training loss 3.1456396583854424e-06 mae 0.0012597093412809372
training loss 3.1576635107637614e-06 mae 0.0012592422106728843
training loss 3.2241194228628218e-06 mae 0.0012751746235243901
Epoch 411, training: loss: 0.0000032, mae: 0.0012710 test: loss0.0000919, mae:0.0069343
training loss 3.2414607176178833e-06 mae 0.0014487681910395622
training loss 2.939283450252654e-06 mae 0.0012190830288911424
training loss 3.0788080831632337e-06 mae 0.0012422292378021056
training loss 3.1623314945792895e-06 mae 0.0012537579530917903
training loss 3.198182946166231e-06 mae 0.0012661484701429215
Epoch 412, training: loss: 0.0000032, mae: 0.0012695 test: loss0.0000911, mae:0.0069200
training loss 1.581795572747069e-06 mae 0.0009530183742754161
training loss 3.1875820508610235e-06 mae 0.0012793675254500815
training loss 3.215338845638873e-06 mae 0.0012842374894131746
training loss 3.1535529749226946e-06 mae 0.0012643479789138479
training loss 3.2002451705387045e-06 mae 0.0012750642881520194
Epoch 413, training: loss: 0.0000032, mae: 0.0012738 test: loss0.0000919, mae:0.0069352
training loss 3.562898200470954e-06 mae 0.0013067774707451463
training loss 3.2356830120881893e-06 mae 0.0012757721991625197
training loss 3.186061005658078e-06 mae 0.0012639709414065921
training loss 3.1098576601949245e-06 mae 0.0012596564187008323
training loss 3.1858715151406735e-06 mae 0.0012694962409009875
Epoch 414, training: loss: 0.0000032, mae: 0.0012712 test: loss0.0000913, mae:0.0069208
training loss 1.511382833996322e-06 mae 0.0008838961948640645
training loss 2.880118766851784e-06 mae 0.0011916921266318097
training loss 3.009703106619043e-06 mae 0.0012294154609599622
training loss 3.084206102225202e-06 mae 0.0012430457551716922
training loss 3.190817073009928e-06 mae 0.0012641266620815243
Epoch 415, training: loss: 0.0000032, mae: 0.0012635 test: loss0.0000910, mae:0.0069145
training loss 2.4489793304383056e-06 mae 0.0011921394616365433
training loss 2.9640357074551476e-06 mae 0.0012266448854158327
training loss 2.963285886259052e-06 mae 0.00122213679985927
training loss 3.077372076217854e-06 mae 0.0012368431977082708
training loss 3.1151189291019705e-06 mae 0.0012475041850975048
Epoch 416, training: loss: 0.0000031, mae: 0.0012495 test: loss0.0000917, mae:0.0069483
training loss 3.123903297819197e-06 mae 0.0013563655083999038
training loss 3.175297561653018e-06 mae 0.0012643456118930062
training loss 3.1654311230637843e-06 mae 0.0012560552014913961
training loss 3.119683187441785e-06 mae 0.0012459443347830788
training loss 3.1491304331275075e-06 mae 0.0012566320038057712
Epoch 417, training: loss: 0.0000032, mae: 0.0012623 test: loss0.0000920, mae:0.0069405
training loss 3.9400779314746615e-06 mae 0.001349543803371489
training loss 3.121462292060017e-06 mae 0.0012406870864295199
training loss 3.0804902937936527e-06 mae 0.0012412019181157466
training loss 3.1108479290184522e-06 mae 0.0012529719376547983
training loss 3.1407437935700026e-06 mae 0.0012576884525800267
Epoch 418, training: loss: 0.0000031, mae: 0.0012580 test: loss0.0000923, mae:0.0069583
training loss 2.4353782919206424e-06 mae 0.0012280041119083762
training loss 3.066747077426436e-06 mae 0.0012597089962047689
training loss 3.125011222202897e-06 mae 0.0012611638394986642
training loss 3.1113026763693374e-06 mae 0.0012549783140945168
training loss 3.1334233176607426e-06 mae 0.001258925346261475
Epoch 419, training: loss: 0.0000032, mae: 0.0012659 test: loss0.0000917, mae:0.0069454
training loss 1.619587123968813e-06 mae 0.0008972318028099835
training loss 3.1881468158626735e-06 mae 0.001264749751106708
training loss 3.133920667195445e-06 mae 0.001265521370460012
training loss 3.191404385652517e-06 mae 0.001271469792619422
training loss 3.1548993614144894e-06 mae 0.0012696373284521024
Epoch 420, training: loss: 0.0000032, mae: 0.0012698 test: loss0.0000918, mae:0.0069404
training loss 3.932122581318254e-06 mae 0.0012706220149993896
training loss 3.3167852917340856e-06 mae 0.0012921397825337803
training loss 3.1341749182403052e-06 mae 0.0012603098841264177
training loss 3.1163298977928864e-06 mae 0.001259522500990686
training loss 3.1514759255071525e-06 mae 0.0012647833638078534
Epoch 421, training: loss: 0.0000031, mae: 0.0012621 test: loss0.0000924, mae:0.0069691
training loss 3.327224703753018e-06 mae 0.0012126486981287599
training loss 3.128521315125445e-06 mae 0.0012613649836138766
training loss 3.02846286774751e-06 mae 0.001242097407823779
training loss 3.059318244999039e-06 mae 0.0012478291204304041
training loss 3.11177134324282e-06 mae 0.0012533852957369775
Epoch 422, training: loss: 0.0000031, mae: 0.0012536 test: loss0.0000920, mae:0.0069459
training loss 2.585963784440537e-06 mae 0.0011760598281398416
training loss 3.09888497329167e-06 mae 0.0012611762937797483
training loss 3.2097103521304204e-06 mae 0.001269822642852616
training loss 3.0994164803242586e-06 mae 0.0012479179568853944
training loss 3.137718404693048e-06 mae 0.0012597451730751762
Epoch 423, training: loss: 0.0000031, mae: 0.0012591 test: loss0.0000924, mae:0.0069767
training loss 2.7645262434816686e-06 mae 0.0011685285717248917
training loss 2.83529177923971e-06 mae 0.0012069256649827403
training loss 2.992645129392492e-06 mae 0.0012415864246685315
training loss 3.120001079932628e-06 mae 0.0012563958321543812
training loss 3.0977467669564506e-06 mae 0.001249127588362261
Epoch 424, training: loss: 0.0000031, mae: 0.0012525 test: loss0.0000917, mae:0.0069326
training loss 1.4146481817078893e-06 mae 0.0008668508380651474
training loss 2.901005308219692e-06 mae 0.0012047755687187114
training loss 2.9788413005710278e-06 mae 0.0012287305262271724
training loss 3.091565601595681e-06 mae 0.0012479857576314045
training loss 3.099293109305258e-06 mae 0.0012556000129639448
Epoch 425, training: loss: 0.0000031, mae: 0.0012557 test: loss0.0000919, mae:0.0069449
training loss 1.7461716197431087e-06 mae 0.0009675522451288998
training loss 3.0745362374884784e-06 mae 0.0012423848441126299
training loss 3.1029890608059043e-06 mae 0.0012457285878608134
training loss 3.059013860913773e-06 mae 0.001248374474173202
training loss 3.0669030946282594e-06 mae 0.0012536129592199672
Epoch 426, training: loss: 0.0000031, mae: 0.0012602 test: loss0.0000920, mae:0.0069680
training loss 2.5692390863696346e-06 mae 0.001254095695912838
training loss 2.8863953057047113e-06 mae 0.0012039811003441904
training loss 2.880486021024241e-06 mae 0.0012084367087959208
training loss 3.025482075694804e-06 mae 0.001234526641509613
training loss 3.032638042371115e-06 mae 0.0012403633790113845
Epoch 427, training: loss: 0.0000031, mae: 0.0012444 test: loss0.0000919, mae:0.0069530
training loss 2.7690255137713393e-06 mae 0.0012539249146357179
training loss 2.9301311939229445e-06 mae 0.0012162631268010419
training loss 3.0137440321010613e-06 mae 0.001229874417416309
training loss 3.1044727942848685e-06 mae 0.001253735844974117
training loss 3.0649112344898497e-06 mae 0.0012480725079024252
Epoch 428, training: loss: 0.0000031, mae: 0.0012528 test: loss0.0000916, mae:0.0069392
training loss 4.005715254606912e-06 mae 0.0014202514430508018
training loss 3.227358935392284e-06 mae 0.00126661544245686
training loss 3.0424650114384525e-06 mae 0.0012313683942596584
training loss 3.05863116240807e-06 mae 0.001241650919211004
training loss 3.0846811617705125e-06 mae 0.0012456052901862718
Epoch 429, training: loss: 0.0000031, mae: 0.0012444 test: loss0.0000924, mae:0.0069536
training loss 3.016157279489562e-06 mae 0.0012678488856181502
training loss 2.977127834645787e-06 mae 0.0012229263266621561
training loss 3.0963079757609665e-06 mae 0.001245528482601489
training loss 3.0848213076305847e-06 mae 0.0012511641503348695
training loss 3.1047487319388458e-06 mae 0.0012590464671369428
Epoch 430, training: loss: 0.0000031, mae: 0.0012559 test: loss0.0000921, mae:0.0069538
training loss 1.983690708584618e-06 mae 0.00098235288169235
training loss 3.0893557648453813e-06 mae 0.0012501463049785324
training loss 3.086215656367132e-06 mae 0.0012482796331157553
training loss 2.9999844971356147e-06 mae 0.0012279940543184363
training loss 3.0055687826705842e-06 mae 0.0012345661311317702
Epoch 431, training: loss: 0.0000030, mae: 0.0012358 test: loss0.0000921, mae:0.0069605
training loss 2.736772103162366e-06 mae 0.0010863147908821702
training loss 2.9351404856581703e-06 mae 0.0012144983686762406
training loss 2.9960212435134323e-06 mae 0.00122787402527831
training loss 2.9048911085960777e-06 mae 0.0012132735874361997
training loss 3.0174799432679535e-06 mae 0.001230117999566302
Epoch 432, training: loss: 0.0000030, mae: 0.0012288 test: loss0.0000923, mae:0.0069692
training loss 3.758097591344267e-06 mae 0.0012609431287273765
training loss 2.821831950920266e-06 mae 0.0012124158583544926
training loss 3.0397667398916604e-06 mae 0.0012446788622591971
training loss 3.135128361235134e-06 mae 0.001269670249200395
training loss 3.114433297923825e-06 mae 0.0012620308900377093
Epoch 433, training: loss: 0.0000031, mae: 0.0012610 test: loss0.0000922, mae:0.0069635
training loss 2.0230008885846473e-06 mae 0.0011022776598110795
training loss 3.0108198362701353e-06 mae 0.001218169374053604
training loss 2.983283701552755e-06 mae 0.0012177209943846456
training loss 3.0313836851740563e-06 mae 0.0012262280211654424
training loss 3.016389744751637e-06 mae 0.0012335617857433814
Epoch 434, training: loss: 0.0000030, mae: 0.0012352 test: loss0.0000927, mae:0.0069762
training loss 2.9405534860416083e-06 mae 0.0012107603251934052
training loss 2.9823646494097526e-06 mae 0.0012140271502693056
training loss 2.94396586063572e-06 mae 0.0012117855470987163
training loss 2.963721613766951e-06 mae 0.0012162168046138833
training loss 2.9799969669696995e-06 mae 0.0012183786709839247
Epoch 435, training: loss: 0.0000030, mae: 0.0012204 test: loss0.0000926, mae:0.0069748
training loss 1.9854762740578735e-06 mae 0.0010610632598400116
training loss 2.9506733979217763e-06 mae 0.0011968007961781146
training loss 3.031556421675384e-06 mae 0.0012301613569794469
training loss 2.9698916453989947e-06 mae 0.0012235452844267904
training loss 2.976153790207794e-06 mae 0.0012266160094929837
Epoch 436, training: loss: 0.0000030, mae: 0.0012266 test: loss0.0000920, mae:0.0069510
training loss 2.0495024273259332e-06 mae 0.0011134172091260552
training loss 2.9249165748421777e-06 mae 0.0012061477983461729
training loss 2.993649598753866e-06 mae 0.0012236475820035333
training loss 3.01576195346305e-06 mae 0.0012259302226771032
training loss 2.99531411904929e-06 mae 0.0012256798692932575
Epoch 437, training: loss: 0.0000030, mae: 0.0012250 test: loss0.0000926, mae:0.0069738
training loss 3.6115461625740863e-06 mae 0.0012745748972520232
training loss 2.7395506451427424e-06 mae 0.0011806400485939402
training loss 2.90036387306629e-06 mae 0.001209839452911021
training loss 2.8990541041092608e-06 mae 0.0012091870827520188
training loss 2.941006442117793e-06 mae 0.001222090209505304
Epoch 438, training: loss: 0.0000030, mae: 0.0012268 test: loss0.0000926, mae:0.0069940
training loss 3.0141636671032757e-06 mae 0.0012810345506295562
training loss 2.7626112265958613e-06 mae 0.0011719336924964892
training loss 2.88874396389501e-06 mae 0.0011946225681775575
training loss 2.9877794998578816e-06 mae 0.0012189814464701025
training loss 3.003884141697704e-06 mae 0.001227503054861016
Epoch 439, training: loss: 0.0000030, mae: 0.0012254 test: loss0.0000931, mae:0.0069762
training loss 1.8873000726671307e-06 mae 0.0011018881341442466
training loss 2.8496196544919632e-06 mae 0.0012163922633500952
training loss 2.997080822287988e-06 mae 0.0012433352150567028
training loss 3.0058080575892e-06 mae 0.0012417890479177582
training loss 3.011195142855412e-06 mae 0.0012388600393277193
Epoch 440, training: loss: 0.0000030, mae: 0.0012388 test: loss0.0000924, mae:0.0069799
training loss 2.585403535704245e-06 mae 0.00105204526335001
training loss 2.9641676622072814e-06 mae 0.0012103093653351214
training loss 2.967792550695623e-06 mae 0.0012158154599866502
training loss 2.9896623360906483e-06 mae 0.0012264551883193334
training loss 2.9823460837536008e-06 mae 0.0012267726144535624
Epoch 441, training: loss: 0.0000030, mae: 0.0012268 test: loss0.0000923, mae:0.0069668
training loss 2.2986191652307753e-06 mae 0.0011161727597936988
training loss 2.944987580335959e-06 mae 0.001197590395652999
training loss 2.913589195720693e-06 mae 0.0012042917660437524
training loss 2.950911029291466e-06 mae 0.0012158078722816136
training loss 2.9578057391160626e-06 mae 0.0012190768990980746
Epoch 442, training: loss: 0.0000029, mae: 0.0012161 test: loss0.0000923, mae:0.0069593
training loss 1.793714432096749e-06 mae 0.000978784984908998
training loss 2.9225722519364424e-06 mae 0.0012013427274045986
training loss 2.8763549666433796e-06 mae 0.0012001566759467421
training loss 2.9625368297756536e-06 mae 0.0012179043927285456
training loss 2.9525382450090495e-06 mae 0.0012219182333783876
Epoch 443, training: loss: 0.0000030, mae: 0.0012272 test: loss0.0000925, mae:0.0069841
training loss 1.5360566294475575e-06 mae 0.0008360997890122235
training loss 2.7880665677858975e-06 mae 0.001200060357850995
training loss 2.896843428439607e-06 mae 0.001211579342713893
training loss 2.9005806254644986e-06 mae 0.0012104575065834244
training loss 2.952954603538793e-06 mae 0.0012196793825601912
Epoch 444, training: loss: 0.0000030, mae: 0.0012216 test: loss0.0000931, mae:0.0070024
training loss 3.0644898743048543e-06 mae 0.0014121650019660592
training loss 2.784210964506985e-06 mae 0.0012095998162312395
training loss 2.9241994807892807e-06 mae 0.0012164464892386135
training loss 2.9465092289014964e-06 mae 0.0012228275515873061
training loss 2.9479202084555595e-06 mae 0.001219689250061872
Epoch 445, training: loss: 0.0000029, mae: 0.0012210 test: loss0.0000929, mae:0.0069906
training loss 2.24382142732793e-06 mae 0.001160909072495997
training loss 2.8763361204291154e-06 mae 0.0012076886521433204
training loss 2.9122695222852564e-06 mae 0.001214354298432533
training loss 2.9716312211583745e-06 mae 0.0012295289745319954
training loss 2.953287599639361e-06 mae 0.00122755780626107
Epoch 446, training: loss: 0.0000030, mae: 0.0012310 test: loss0.0000938, mae:0.0070271
training loss 2.481394403730519e-06 mae 0.0011863369727507234
training loss 2.9990458990462626e-06 mae 0.001231792813176106
training loss 3.057192724788753e-06 mae 0.001236056923949261
training loss 2.9931601156871494e-06 mae 0.0012216471029223492
training loss 2.902051650566118e-06 mae 0.0012070819094373998
Epoch 447, training: loss: 0.0000029, mae: 0.0012057 test: loss0.0000924, mae:0.0069714
training loss 2.4953874344646465e-06 mae 0.0011639203876256943
training loss 2.6942347573162357e-06 mae 0.0011549020144978868
training loss 2.87783487773694e-06 mae 0.0011927303564181493
training loss 2.948626323262979e-06 mae 0.0012065390940595245
training loss 2.8902040626868726e-06 mae 0.0011992336633680066
Epoch 448, training: loss: 0.0000029, mae: 0.0012015 test: loss0.0000934, mae:0.0070027
training loss 3.6655872008850565e-06 mae 0.0012686960399150848
training loss 2.8431687211445484e-06 mae 0.001211132589435461
training loss 2.845820679993104e-06 mae 0.001201243897079315
training loss 2.8479625044136333e-06 mae 0.0012016970323497397
training loss 2.9131116915420493e-06 mae 0.0012117348378524184
Epoch 449, training: loss: 0.0000029, mae: 0.0012081 test: loss0.0000929, mae:0.0069878
training loss 1.3685785233974457e-06 mae 0.000868525356054306
training loss 2.7897210657101815e-06 mae 0.0011737104939004662
training loss 2.8098310212503698e-06 mae 0.001186427046512576
training loss 2.8486952785728644e-06 mae 0.0011980714829983986
training loss 2.8721528210621376e-06 mae 0.0012048373693271316
Epoch 450, training: loss: 0.0000029, mae: 0.0012058 test: loss0.0000936, mae:0.0070040
training loss 4.6829359234834556e-06 mae 0.0013166818534955382
training loss 2.764041699033531e-06 mae 0.0011739413568969156
training loss 2.7526362065046745e-06 mae 0.001168114176979012
training loss 2.790783706369975e-06 mae 0.0011838196914809578
training loss 2.882507048335396e-06 mae 0.0012042564221203735
Epoch 451, training: loss: 0.0000029, mae: 0.0012064 test: loss0.0000932, mae:0.0070007
training loss 2.167274033126887e-06 mae 0.0011293332790955901
training loss 2.787706929505957e-06 mae 0.0011919773658554931
training loss 2.8568908041196123e-06 mae 0.0012012286051775855
training loss 2.888291255236639e-06 mae 0.0012026862534293475
training loss 2.879761844346446e-06 mae 0.0012076824601735016
Epoch 452, training: loss: 0.0000029, mae: 0.0012085 test: loss0.0000969, mae:0.0070528
training loss 1.8453338270774111e-06 mae 0.0010456396266818047
training loss 2.6680621602941295e-06 mae 0.0011634588431037377
training loss 2.7365532565625595e-06 mae 0.0011692175377704351
training loss 2.7878218536789793e-06 mae 0.0011760686514189847
training loss 2.8349657051437503e-06 mae 0.0011899176387310916
Epoch 453, training: loss: 0.0000028, mae: 0.0011912 test: loss0.0000934, mae:0.0069961
training loss 2.01591024051595e-06 mae 0.0011562686413526535
training loss 2.8247651448955546e-06 mae 0.0011946359927784286
training loss 2.898562071761874e-06 mae 0.0012106640843010626
training loss 2.832409350711239e-06 mae 0.0011998697130829838
training loss 2.864196979959587e-06 mae 0.0012048551058674705
Epoch 454, training: loss: 0.0000029, mae: 0.0012064 test: loss0.0000944, mae:0.0070274
training loss 3.347754727656138e-06 mae 0.0013730026548728347
training loss 2.5306643019397523e-06 mae 0.0011347369016969902
training loss 2.6060849477019443e-06 mae 0.0011438888354724879
training loss 2.805114976527326e-06 mae 0.001181334249801712
training loss 2.8043372982771796e-06 mae 0.0011871960561430613
Epoch 455, training: loss: 0.0000028, mae: 0.0011947 test: loss0.0000943, mae:0.0070240
training loss 2.1465182271640515e-06 mae 0.0011194782564416528
training loss 2.935843576333176e-06 mae 0.0012102621954445752
training loss 2.9854936262228586e-06 mae 0.0012192104108594733
training loss 2.9221201403014384e-06 mae 0.0012100883667042716
training loss 2.857632028312608e-06 mae 0.0011983873358515289
Epoch 456, training: loss: 0.0000028, mae: 0.0011967 test: loss0.0000931, mae:0.0069928
training loss 4.802785952051636e-06 mae 0.0015466395998373628
training loss 3.0391888908614463e-06 mae 0.0012213970892880044
training loss 2.8702961662029242e-06 mae 0.0011967652411624282
training loss 2.8402632748918593e-06 mae 0.001200597456869183
training loss 2.8932591088399096e-06 mae 0.0012161468986227229
Epoch 457, training: loss: 0.0000029, mae: 0.0012152 test: loss0.0000937, mae:0.0070156
training loss 1.7417532944818959e-06 mae 0.0008940569241531193
training loss 2.539151334228033e-06 mae 0.0011189160663086705
training loss 2.694286147761706e-06 mae 0.0011541579307130733
training loss 2.7619979581331116e-06 mae 0.0011694789625124524
training loss 2.8358118154742497e-06 mae 0.001190040402163042
Epoch 458, training: loss: 0.0000028, mae: 0.0011906 test: loss0.0000946, mae:0.0070391
training loss 2.1594203190034023e-06 mae 0.0010857501765713096
training loss 2.594785889735196e-06 mae 0.0011378475533360072
training loss 2.783419967101401e-06 mae 0.001166085665930284
training loss 2.80748368094169e-06 mae 0.0011786728062734802
training loss 2.8294381721916653e-06 mae 0.0011903759785951
Epoch 459, training: loss: 0.0000028, mae: 0.0011925 test: loss0.0000933, mae:0.0070016
training loss 2.6122768304048805e-06 mae 0.0011732559651136398
training loss 2.7190544375009587e-06 mae 0.0011700112851994004
training loss 2.768871623440718e-06 mae 0.001180481291860158
training loss 2.8128336984479665e-06 mae 0.0011841131849152761
training loss 2.8313914144384413e-06 mae 0.0011974622745099322
Epoch 460, training: loss: 0.0000028, mae: 0.0011982 test: loss0.0000931, mae:0.0070058
training loss 2.4103685518639395e-06 mae 0.0010383371263742447
training loss 2.7337576966601765e-06 mae 0.0011701244488815027
training loss 2.8164884380464282e-06 mae 0.0011819125346765663
training loss 2.860706923149566e-06 mae 0.0011883942014846137
training loss 2.8021565737585828e-06 mae 0.0011857865309917292
Epoch 461, training: loss: 0.0000028, mae: 0.0011902 test: loss0.0000933, mae:0.0070033
training loss 1.6754987655076548e-06 mae 0.0009525104542262852
training loss 2.516077440269132e-06 mae 0.0011447019413953613
training loss 2.7653148451760647e-06 mae 0.0011802608536851439
training loss 2.7790807351097614e-06 mae 0.0011797952777952796
training loss 2.790556080827488e-06 mae 0.0011845145401178827
Epoch 462, training: loss: 0.0000028, mae: 0.0011820 test: loss0.0000937, mae:0.0070137
training loss 1.2770057082889252e-06 mae 0.0008858597720973194
training loss 2.7251804126268208e-06 mae 0.0011715454633767696
training loss 2.659808379882299e-06 mae 0.0011699673515689183
training loss 2.760197456769216e-06 mae 0.0011839503965907984
training loss 2.797496403222772e-06 mae 0.0011881958876866201
Epoch 463, training: loss: 0.0000028, mae: 0.0011855 test: loss0.0000941, mae:0.0070231
training loss 2.273200834679301e-06 mae 0.0010820735478773713
training loss 2.488140946262123e-06 mae 0.0011415734588095514
training loss 2.741906844604082e-06 mae 0.0011877884245754242
training loss 2.798797563085802e-06 mae 0.0011901717235192357
training loss 2.8000051578198697e-06 mae 0.0011910393881027703
Epoch 464, training: loss: 0.0000028, mae: 0.0011903 test: loss0.0000936, mae:0.0070067
training loss 2.105951125486172e-06 mae 0.0009897425770759583
training loss 2.843160526775618e-06 mae 0.0011931572180223088
training loss 2.890790797479922e-06 mae 0.0012048309522514958
training loss 2.772828603511452e-06 mae 0.0011826166686515587
training loss 2.772889872201052e-06 mae 0.0011794640258677418
Epoch 465, training: loss: 0.0000028, mae: 0.0011856 test: loss0.0000942, mae:0.0070435
training loss 2.6507516395213315e-06 mae 0.0011786507675424218
training loss 2.6255464249783586e-06 mae 0.0011683843125972677
training loss 2.8095626246363057e-06 mae 0.0011934148514237588
training loss 2.7436162661205953e-06 mae 0.0011818592566904304
training loss 2.7900247520996513e-06 mae 0.001188741951418661
Epoch 466, training: loss: 0.0000028, mae: 0.0011887 test: loss0.0000943, mae:0.0070336
training loss 2.761215682767215e-06 mae 0.0011936522787436843
training loss 2.65382824832783e-06 mae 0.001137247471157096
training loss 2.7561038126409003e-06 mae 0.0011646904530796674
training loss 2.7479814875823047e-06 mae 0.0011667282195663123
training loss 2.758385026434337e-06 mae 0.0011737637172003661
Epoch 467, training: loss: 0.0000027, mae: 0.0011714 test: loss0.0000945, mae:0.0070413
training loss 3.3050473575713113e-06 mae 0.0013005031505599618
training loss 2.7436392718871007e-06 mae 0.0011660916532170683
training loss 2.673502369208731e-06 mae 0.001162374596166803
training loss 2.6990490237913286e-06 mae 0.0011703883203684003
training loss 2.7278643502898414e-06 mae 0.0011750873698347915
Epoch 468, training: loss: 0.0000027, mae: 0.0011762 test: loss0.0000939, mae:0.0070279
training loss 2.458403741911752e-06 mae 0.0010948587441816926
training loss 2.937444960212708e-06 mae 0.0012053893262739565
training loss 2.7847404363447e-06 mae 0.00117769214269327
training loss 2.724766443331843e-06 mae 0.0011665607885062844
training loss 2.737770000975393e-06 mae 0.0011689882610799443
Epoch 469, training: loss: 0.0000028, mae: 0.0011716 test: loss0.0000938, mae:0.0070238
training loss 2.104295390381594e-06 mae 0.001147300354205072
training loss 2.6167836943065645e-06 mae 0.0011317906878413812
training loss 2.773381025929098e-06 mae 0.0011619349156672331
training loss 2.704386179250632e-06 mae 0.0011538998913852535
training loss 2.7316477240986634e-06 mae 0.001164498292685797
Epoch 470, training: loss: 0.0000027, mae: 0.0011634 test: loss0.0000942, mae:0.0070398
training loss 4.826789790968178e-06 mae 0.0014973325887694955
training loss 2.758728085466952e-06 mae 0.0011836294753604806
training loss 2.778898547013783e-06 mae 0.00117974449352518
training loss 2.733777030198628e-06 mae 0.0011805308480062426
training loss 2.765762426644136e-06 mae 0.0011873482911852163
Epoch 471, training: loss: 0.0000028, mae: 0.0011854 test: loss0.0000942, mae:0.0070357
training loss 4.752138011099305e-06 mae 0.0014440022641792893
training loss 2.7598034069110416e-06 mae 0.0011998865109704
training loss 2.7107658934705833e-06 mae 0.0011843162275260628
training loss 2.7603824719335526e-06 mae 0.001182802708816425
training loss 2.767151267474205e-06 mae 0.0011801552317054274
Epoch 472, training: loss: 0.0000028, mae: 0.0011791 test: loss0.0000947, mae:0.0070463
training loss 2.3623024389962666e-06 mae 0.0010565416887402534
training loss 2.770011408257215e-06 mae 0.0011808590086943966
training loss 2.780479296081178e-06 mae 0.001182141525919853
training loss 2.68488793906558e-06 mae 0.0011675571321219892
training loss 2.712943580944517e-06 mae 0.0011712279609770544
Epoch 473, training: loss: 0.0000027, mae: 0.0011735 test: loss0.0000942, mae:0.0070276
training loss 2.6370071282144636e-06 mae 0.001269534812308848
training loss 2.5522785549850697e-06 mae 0.0011477920582846682
training loss 2.6070975417325703e-06 mae 0.0011527145553454018
training loss 2.72309512861314e-06 mae 0.001173901523922922
training loss 2.7225198986777824e-06 mae 0.0011780828044087217
Epoch 474, training: loss: 0.0000027, mae: 0.0011773 test: loss0.0000940, mae:0.0070314
training loss 1.3735119637203752e-06 mae 0.000863750756252557
training loss 2.838068818770505e-06 mae 0.001186955170066771
training loss 2.7635313181866037e-06 mae 0.0011714845734399433
training loss 2.7174680630593767e-06 mae 0.001165003296826651
training loss 2.6973092898157996e-06 mae 0.001163681764016623
Epoch 475, training: loss: 0.0000027, mae: 0.0011644 test: loss0.0000938, mae:0.0070158
training loss 3.6067513065063395e-06 mae 0.0012998670572414994
training loss 2.5470564303438605e-06 mae 0.001145226417529379
training loss 2.677509635507498e-06 mae 0.0011603502132514916
training loss 2.7014922352464063e-06 mae 0.0011672865381982449
training loss 2.7274158149913863e-06 mae 0.0011693790279310291
Epoch 476, training: loss: 0.0000027, mae: 0.0011713 test: loss0.0000940, mae:0.0070210
training loss 2.6768973384605488e-06 mae 0.0012111170217394829
training loss 2.6953541803367463e-06 mae 0.0011600253124283075
training loss 2.628115996933298e-06 mae 0.0011489331004172927
training loss 2.7373160592948936e-06 mae 0.0011724416021603027
training loss 2.68943514373547e-06 mae 0.001166420319495231
Epoch 477, training: loss: 0.0000027, mae: 0.0011696 test: loss0.0000948, mae:0.0070552
training loss 3.4967840747412993e-06 mae 0.0012375038350000978
training loss 2.6645587610676323e-06 mae 0.0011461406338996454
training loss 2.698158999073082e-06 mae 0.0011597058613609407
training loss 2.748341467542788e-06 mae 0.001173881492777943
training loss 2.725679092437889e-06 mae 0.0011674565175057046
Epoch 478, training: loss: 0.0000027, mae: 0.0011703 test: loss0.0000948, mae:0.0070423
training loss 3.1160900562099414e-06 mae 0.001322485855780542
training loss 2.688823796608416e-06 mae 0.0011505392863981282
training loss 2.673782115371159e-06 mae 0.0011472928655877853
training loss 2.6423744170853557e-06 mae 0.0011495015050426419
training loss 2.6513146059952115e-06 mae 0.0011523927380076013
Epoch 479, training: loss: 0.0000026, mae: 0.0011490 test: loss0.0000948, mae:0.0070564
training loss 2.8830090741394088e-06 mae 0.001230916939675808
training loss 2.544199651756749e-06 mae 0.0011185312771475786
training loss 2.6115165356074635e-06 mae 0.0011448352563075057
training loss 2.686120235120328e-06 mae 0.0011630124217423528
training loss 2.704665275610859e-06 mae 0.0011656769126797302
Epoch 480, training: loss: 0.0000027, mae: 0.0011669 test: loss0.0000940, mae:0.0070234
training loss 4.723087386082625e-06 mae 0.0015816412633284926
training loss 2.73018290990163e-06 mae 0.0011759175320941151
training loss 2.705401628434017e-06 mae 0.0011695468530870305
training loss 2.642475731632984e-06 mae 0.0011557777657401778
training loss 2.6397214139084017e-06 mae 0.0011564056065850965
Epoch 481, training: loss: 0.0000027, mae: 0.0011587 test: loss0.0000946, mae:0.0070408
training loss 1.6737017176637892e-06 mae 0.0009380849078297615
training loss 2.724322654352827e-06 mae 0.0011645606022748143
training loss 2.664695802925035e-06 mae 0.001150908779080883
training loss 2.6566501915022547e-06 mae 0.0011551565855328263
training loss 2.6724617486369747e-06 mae 0.0011536706773568846
Epoch 482, training: loss: 0.0000027, mae: 0.0011514 test: loss0.0000942, mae:0.0070354
training loss 2.2838394215796143e-06 mae 0.0011697029694914818
training loss 2.7261034828745113e-06 mae 0.0011868198030153473
training loss 2.69932476867055e-06 mae 0.001167190149956669
training loss 2.6841211552253332e-06 mae 0.0011673565188404299
training loss 2.7022595755869286e-06 mae 0.001169740756688667
Epoch 483, training: loss: 0.0000027, mae: 0.0011723 test: loss0.0000946, mae:0.0070526
training loss 4.369881480670301e-06 mae 0.0013924777740612626
training loss 2.793801531522917e-06 mae 0.001171539603805571
training loss 2.720989141830659e-06 mae 0.001169588472140898
training loss 2.6439963216974424e-06 mae 0.0011565381339566161
training loss 2.6664247773293772e-06 mae 0.0011656181261277025
Epoch 484, training: loss: 0.0000027, mae: 0.0011656 test: loss0.0000952, mae:0.0070812
training loss 2.289041049152729e-06 mae 0.0010338578140363097
training loss 2.5916344125731726e-06 mae 0.0011473566558504226
training loss 2.6446700037850674e-06 mae 0.0011566974998378004
training loss 2.593168387376852e-06 mae 0.0011490038291563514
training loss 2.604014434724033e-06 mae 0.0011460308943509094
Epoch 485, training: loss: 0.0000026, mae: 0.0011505 test: loss0.0000949, mae:0.0070644
training loss 1.7330313539787312e-06 mae 0.0009562217746861279
training loss 2.740156629425102e-06 mae 0.0011872207362423925
training loss 2.6346259967948125e-06 mae 0.001155284499071796
training loss 2.6987950248090673e-06 mae 0.001169105484427896
training loss 2.712170653263564e-06 mae 0.0011740353344175127
Epoch 486, training: loss: 0.0000027, mae: 0.0011690 test: loss0.0000946, mae:0.0070538
training loss 4.010275461041601e-06 mae 0.001357130124233663
training loss 2.505332335489335e-06 mae 0.0011207650589081003
training loss 2.4990736041091964e-06 mae 0.0011162271269016188
training loss 2.549357996723838e-06 mae 0.0011267585140986008
training loss 2.5843150345720103e-06 mae 0.0011355280965932683
Epoch 487, training: loss: 0.0000026, mae: 0.0011395 test: loss0.0000944, mae:0.0070580
training loss 1.7017115396811278e-06 mae 0.0009017701377160847
training loss 2.4948351883900675e-06 mae 0.0011329810790188023
training loss 2.53974594152799e-06 mae 0.0011349562520972722
training loss 2.542210886953013e-06 mae 0.0011375545615034293
training loss 2.617826901913944e-06 mae 0.0011494967948056912
Epoch 488, training: loss: 0.0000026, mae: 0.0011565 test: loss0.0000950, mae:0.0070728
training loss 2.8247661703062477e-06 mae 0.001218823716044426
training loss 2.4579916338124715e-06 mae 0.001123415565291675
training loss 2.496083323980074e-06 mae 0.0011195074674664155
training loss 2.5930006789631007e-06 mae 0.0011374096818644532
training loss 2.6204536323402587e-06 mae 0.001143168977436271
Epoch 489, training: loss: 0.0000026, mae: 0.0011386 test: loss0.0000948, mae:0.0070583
training loss 1.6456424418720417e-06 mae 0.000943730294238776
training loss 2.5629536063812943e-06 mae 0.0011177123136197529
training loss 2.6097057158976625e-06 mae 0.0011383875285846982
training loss 2.517759688071359e-06 mae 0.001123958905242902
training loss 2.5768161155760203e-06 mae 0.0011340711020808945
Epoch 490, training: loss: 0.0000026, mae: 0.0011357 test: loss0.0000950, mae:0.0070686
training loss 1.356444954581093e-06 mae 0.000860266387462616
training loss 2.6177872492624764e-06 mae 0.0011425506319010666
training loss 2.7604855318659788e-06 mae 0.0011692045384437066
training loss 2.6750815661686424e-06 mae 0.0011480021362180598
training loss 2.598187159637022e-06 mae 0.0011385515377398086
Epoch 491, training: loss: 0.0000026, mae: 0.0011412 test: loss0.0000951, mae:0.0070740
training loss 2.0450122519832803e-06 mae 0.0009407720644958317
training loss 2.5400657219221356e-06 mae 0.0011132227521681907
training loss 2.4625227578695513e-06 mae 0.001105859757534754
training loss 2.540095170385657e-06 mae 0.0011181665433336368
training loss 2.5793055434107054e-06 mae 0.0011292722505350495
Epoch 492, training: loss: 0.0000026, mae: 0.0011278 test: loss0.0000947, mae:0.0070607
training loss 3.6168955830362393e-06 mae 0.0014313813298940659
training loss 2.666963922035625e-06 mae 0.0011562056412609911
training loss 2.6644958412848306e-06 mae 0.001166890747614647
training loss 2.6275806948469602e-06 mae 0.0011568048129333116
training loss 2.6358611487704545e-06 mae 0.0011582712825869595
Epoch 493, training: loss: 0.0000026, mae: 0.0011576 test: loss0.0000946, mae:0.0070465
training loss 2.1882012788410066e-06 mae 0.0010025255614891648
training loss 2.3846293547024944e-06 mae 0.0010864100414419583
training loss 2.4185626139880307e-06 mae 0.0010973087242241986
training loss 2.4829617559770356e-06 mae 0.0011097732908246633
training loss 2.5512386715282568e-06 mae 0.0011261428242424194
Epoch 494, training: loss: 0.0000026, mae: 0.0011321 test: loss0.0000941, mae:0.0070379
training loss 1.5804743043190683e-06 mae 0.0009415959939360619
training loss 2.486993600862608e-06 mae 0.001116701951889577
training loss 2.480895903831294e-06 mae 0.001117341853747384
training loss 2.498135008923988e-06 mae 0.0011239171186955457
training loss 2.5363783770761315e-06 mae 0.0011301420009880328
Epoch 495, training: loss: 0.0000025, mae: 0.0011309 test: loss0.0000959, mae:0.0071087
training loss 2.9422342322504846e-06 mae 0.0013176094507798553
training loss 2.5503517882302106e-06 mae 0.0011537094536584374
training loss 2.6113509163971483e-06 mae 0.0011614128448170524
training loss 2.5891640568761126e-06 mae 0.0011538003697769328
training loss 2.5921278802569534e-06 mae 0.0011506110860674238
Epoch 496, training: loss: 0.0000026, mae: 0.0011506 test: loss0.0000958, mae:0.0071093
training loss 3.948301127820741e-06 mae 0.0011919079115614295
training loss 2.5243189000713173e-06 mae 0.001133540798179513
training loss 2.6090694905854046e-06 mae 0.0011431833465196336
training loss 2.547218559510725e-06 mae 0.0011311315176607605
training loss 2.529980247854568e-06 mae 0.001126945187020305
Epoch 497, training: loss: 0.0000025, mae: 0.0011296 test: loss0.0000951, mae:0.0070691
training loss 1.5259657857313869e-06 mae 0.000978877185843885
training loss 2.3097311396650544e-06 mae 0.0010913977417729647
training loss 2.4000731236180335e-06 mae 0.001099714939021459
training loss 2.4621155358509228e-06 mae 0.0011156849067392976
training loss 2.5416452733374335e-06 mae 0.0011318960818994916
Epoch 498, training: loss: 0.0000025, mae: 0.0011342 test: loss0.0000962, mae:0.0070834
training loss 2.47349998971913e-06 mae 0.0010162309044972062
training loss 2.5913494821496917e-06 mae 0.001133368609646591
training loss 2.6304870800123525e-06 mae 0.001152234849875037
training loss 2.601894959677537e-06 mae 0.0011474948179050794
training loss 2.572768565795881e-06 mae 0.001138179035931455
Epoch 499, training: loss: 0.0000026, mae: 0.0011378 test: loss0.0000951, mae:0.0070761
current learning rate: 1.5625e-05
training loss 2.2934821117814863e-06 mae 0.0010325852781534195
training loss 2.3282253934274773e-06 mae 0.001050327817781591
training loss 2.335521998301744e-06 mae 0.0010487768351847286
training loss 2.358861630810433e-06 mae 0.0010498003519158689
training loss 2.3248912859131612e-06 mae 0.0010489605254822628
Epoch 500, training: loss: 0.0000023, mae: 0.0010440 test: loss0.0000956, mae:0.0070941
training loss 2.2491954041470308e-06 mae 0.001145139685831964
training loss 2.199267350542689e-06 mae 0.0010049586097582003
training loss 2.2913950257395405e-06 mae 0.0010214477632250086
training loss 2.2860688441834615e-06 mae 0.0010200111856159839
training loss 2.2534997951178273e-06 mae 0.0010183419312920368
Epoch 501, training: loss: 0.0000023, mae: 0.0010260 test: loss0.0000951, mae:0.0070745
training loss 1.9782883100560866e-06 mae 0.0009470156510360539
training loss 2.314569827581626e-06 mae 0.0010163011177735152
training loss 2.216371165172184e-06 mae 0.0010065768544543724
training loss 2.25304272860752e-06 mae 0.0010193746459102942
training loss 2.287742986272773e-06 mae 0.0010263351048806226
Epoch 502, training: loss: 0.0000023, mae: 0.0010258 test: loss0.0000950, mae:0.0070735
training loss 2.6526931833359413e-06 mae 0.0011713780695572495
training loss 2.2174066160397496e-06 mae 0.0009911354407485505
training loss 2.2489400736400183e-06 mae 0.0010004384341732702
training loss 2.2251465241617493e-06 mae 0.0010048100333167422
training loss 2.2818536547163437e-06 mae 0.001023335158778586
Epoch 503, training: loss: 0.0000023, mae: 0.0010214 test: loss0.0000952, mae:0.0070800
training loss 2.9361035558395088e-06 mae 0.0011552307987585664
training loss 2.209792803435395e-06 mae 0.0010173076492113373
training loss 2.2516900603162375e-06 mae 0.0010250448704568103
training loss 2.302138221582466e-06 mae 0.0010314775397230116
training loss 2.290347289990722e-06 mae 0.001027672304551752
Epoch 504, training: loss: 0.0000023, mae: 0.0010234 test: loss0.0000947, mae:0.0070532
training loss 3.0962039545556763e-06 mae 0.0011155869578942657
training loss 2.1432173588098026e-06 mae 0.0009969129733850847
training loss 2.3304872652576283e-06 mae 0.0010310592167863232
training loss 2.2936602210351016e-06 mae 0.0010300162787987095
training loss 2.267037759216393e-06 mae 0.0010219622934491959
Epoch 505, training: loss: 0.0000023, mae: 0.0010202 test: loss0.0000957, mae:0.0070917
training loss 1.937391971296165e-06 mae 0.0009445103933103383
training loss 2.4182100223608315e-06 mae 0.0010356627080553012
training loss 2.2842184765716297e-06 mae 0.0010172979488950406
training loss 2.2996105374189425e-06 mae 0.0010236186466475842
training loss 2.281137663779267e-06 mae 0.0010247307641904305
Epoch 506, training: loss: 0.0000023, mae: 0.0010219 test: loss0.0000955, mae:0.0070889
training loss 1.5496570995310321e-06 mae 0.0009072416578419507
training loss 2.276666252470807e-06 mae 0.0010284274969450836
training loss 2.2303077498285754e-06 mae 0.0010162125379630912
training loss 2.2870070899853387e-06 mae 0.0010248094994500772
training loss 2.276864950429747e-06 mae 0.001024209041106723
Epoch 507, training: loss: 0.0000023, mae: 0.0010241 test: loss0.0000960, mae:0.0071075
training loss 2.552952309997636e-06 mae 0.0011991815408691764
training loss 2.079815254885534e-06 mae 0.0009998654554068457
training loss 2.1684639347254823e-06 mae 0.0010103328207664484
training loss 2.204152914697225e-06 mae 0.0010183874038911564
training loss 2.2512089445350766e-06 mae 0.0010245648877051153
Epoch 508, training: loss: 0.0000023, mae: 0.0010287 test: loss0.0000951, mae:0.0070682
training loss 2.181872787332395e-06 mae 0.0011507576564326882
training loss 2.1748700938659052e-06 mae 0.0010165486178890456
training loss 2.1671157759835235e-06 mae 0.0009996945333510343
training loss 2.221541209239499e-06 mae 0.0010130029161682737
training loss 2.247767180160194e-06 mae 0.0010189273112583264
Epoch 509, training: loss: 0.0000023, mae: 0.0010210 test: loss0.0000958, mae:0.0071009
training loss 3.286448190920055e-06 mae 0.0011851321905851364
training loss 2.3253937530064242e-06 mae 0.0010245599973417235
training loss 2.2883987387806233e-06 mae 0.0010264161098195192
training loss 2.28361890826487e-06 mae 0.0010264624019665337
training loss 2.2626489614246504e-06 mae 0.0010254897897720196
Epoch 510, training: loss: 0.0000023, mae: 0.0010232 test: loss0.0000956, mae:0.0070916
training loss 1.668766913098807e-06 mae 0.0009588440880179405
training loss 2.260698766697126e-06 mae 0.0010201620901295664
training loss 2.2680642202187905e-06 mae 0.0010216546747464654
training loss 2.2464888702596614e-06 mae 0.0010193687210642791
training loss 2.246382627190406e-06 mae 0.0010193865094681397
Epoch 511, training: loss: 0.0000022, mae: 0.0010197 test: loss0.0000953, mae:0.0070782
training loss 3.6506535252556205e-06 mae 0.0012201378121972084
training loss 2.2031907056039997e-06 mae 0.0010098319480140857
training loss 2.164648083293069e-06 mae 0.0010028683556253003
training loss 2.2244334667634828e-06 mae 0.0010120567679867793
training loss 2.2338044842983227e-06 mae 0.0010152007968614427
Epoch 512, training: loss: 0.0000022, mae: 0.0010134 test: loss0.0000958, mae:0.0070969
training loss 1.1207361012566253e-06 mae 0.0008591329678893089
training loss 2.3209787759340402e-06 mae 0.001039865425577862
training loss 2.18796236820276e-06 mae 0.0010144538580356998
training loss 2.182916343661311e-06 mae 0.001012735010975885
training loss 2.2435642272601644e-06 mae 0.001019050673563126
Epoch 513, training: loss: 0.0000022, mae: 0.0010194 test: loss0.0000958, mae:0.0071101
training loss 1.9300352960271994e-06 mae 0.000941811129450798
training loss 2.1141284581098457e-06 mae 0.000994899346694058
training loss 2.212008552652322e-06 mae 0.0010155374368671145
training loss 2.2029290725011323e-06 mae 0.001014923271322191
training loss 2.234690344581187e-06 mae 0.0010205381332702046
Epoch 514, training: loss: 0.0000022, mae: 0.0010195 test: loss0.0000961, mae:0.0071114
training loss 1.4482403685178724e-06 mae 0.0007707641343586147
training loss 2.1357676832492735e-06 mae 0.0009945558849722145
training loss 2.204249368571597e-06 mae 0.0010100740790689859
training loss 2.2180891513269803e-06 mae 0.0010161791266626763
training loss 2.2453459381274525e-06 mae 0.0010218650267102664
Epoch 515, training: loss: 0.0000022, mae: 0.0010212 test: loss0.0000959, mae:0.0071096
training loss 2.1366201963246567e-06 mae 0.0009511150419712067
training loss 2.2674845824582796e-06 mae 0.0010367957338709018
training loss 2.157349702919968e-06 mae 0.001004322996768768
training loss 2.224015671022876e-06 mae 0.0010164654971754172
training loss 2.22411442684316e-06 mae 0.0010146210291561908
Epoch 516, training: loss: 0.0000022, mae: 0.0010169 test: loss0.0000955, mae:0.0070946
training loss 1.4105786476648063e-06 mae 0.0009248778223991394
training loss 2.3656285123911843e-06 mae 0.0010228601030950596
training loss 2.2387790114173694e-06 mae 0.0010124782606323092
training loss 2.2444205242118785e-06 mae 0.0010236112986049854
training loss 2.233773375959645e-06 mae 0.0010171598866025905
Epoch 517, training: loss: 0.0000022, mae: 0.0010152 test: loss0.0000959, mae:0.0071073
training loss 1.9546871499187546e-06 mae 0.0009279251098632812
training loss 2.3440807048442073e-06 mae 0.001029529121439612
training loss 2.2861882833843414e-06 mae 0.0010272681558652233
training loss 2.295065483331621e-06 mae 0.00103411951683622
training loss 2.2377575685176043e-06 mae 0.0010216833927450858
Epoch 518, training: loss: 0.0000023, mae: 0.0010248 test: loss0.0000959, mae:0.0071051
training loss 1.8842598592527793e-06 mae 0.0009345067664980888
training loss 2.066295263937397e-06 mae 0.0009817083254365212
training loss 2.1249039493773346e-06 mae 0.0009969893593968142
training loss 2.207562400653479e-06 mae 0.0010141376147681956
training loss 2.226244787809842e-06 mae 0.0010177414793062457
Epoch 519, training: loss: 0.0000022, mae: 0.0010150 test: loss0.0000961, mae:0.0071180
training loss 3.5819584809360094e-06 mae 0.001233853050507605
training loss 2.071475268719807e-06 mae 0.0009813514485608277
training loss 2.1631268003210893e-06 mae 0.001004298244681357
training loss 2.209122215688527e-06 mae 0.0010130127139438855
training loss 2.2250460754897267e-06 mae 0.0010152853986558805
Epoch 520, training: loss: 0.0000022, mae: 0.0010142 test: loss0.0000956, mae:0.0070883
training loss 2.107409045493114e-06 mae 0.0010650750482454896
training loss 2.0301048123780905e-06 mae 0.0009697255091832987
training loss 2.1087737535492475e-06 mae 0.0009916087757740727
training loss 2.186988964189104e-06 mae 0.0010099935211204244
training loss 2.2225835009385963e-06 mae 0.0010166951731445074
Epoch 521, training: loss: 0.0000022, mae: 0.0010143 test: loss0.0000961, mae:0.0071052
training loss 2.0689658413175493e-06 mae 0.0010312352096661925
training loss 2.1279505138712307e-06 mae 0.0009888114017324852
training loss 2.1946758129116004e-06 mae 0.0010067257006643431
training loss 2.1913694923412376e-06 mae 0.0010085164311816853
training loss 2.201002199858498e-06 mae 0.0010110774608583777
Epoch 522, training: loss: 0.0000022, mae: 0.0010139 test: loss0.0000966, mae:0.0071296
training loss 2.407201691312366e-06 mae 0.0010187994921579957
training loss 2.2349044537109227e-06 mae 0.0010095059531558233
training loss 2.201115795188424e-06 mae 0.001001920248497056
training loss 2.1690265601510494e-06 mae 0.0010052138630343933
training loss 2.1979188162713485e-06 mae 0.0010087370515722246
Epoch 523, training: loss: 0.0000022, mae: 0.0010085 test: loss0.0000957, mae:0.0070908
training loss 7.925801241981389e-07 mae 0.0006741887773387134
training loss 2.2637460476432227e-06 mae 0.0010157323558814824
training loss 2.212351240800808e-06 mae 0.001008975816922871
training loss 2.194827638146273e-06 mae 0.001001717075967656
training loss 2.2159983871289076e-06 mae 0.0010118460889664753
Epoch 524, training: loss: 0.0000022, mae: 0.0010091 test: loss0.0000957, mae:0.0070909
training loss 2.9834252472937806e-06 mae 0.0011136956745758653
training loss 2.305513155132597e-06 mae 0.0010197139185323723
training loss 2.183275669402533e-06 mae 0.0010062731879689536
training loss 2.1683291369516532e-06 mae 0.001007028628334971
training loss 2.209513281820423e-06 mae 0.0010134363384796908
Epoch 525, training: loss: 0.0000022, mae: 0.0010117 test: loss0.0000959, mae:0.0071057
training loss 2.4564110390201677e-06 mae 0.0011644050246104598
training loss 2.129563610838779e-06 mae 0.000984456402697034
training loss 2.194102129430243e-06 mae 0.0010015322477556763
training loss 2.217393946526704e-06 mae 0.0010098730262510778
training loss 2.215721800627276e-06 mae 0.0010142650673008386
Epoch 526, training: loss: 0.0000022, mae: 0.0010104 test: loss0.0000964, mae:0.0071291
training loss 3.783486363317934e-06 mae 0.0012397943064570427
training loss 2.2090850293085063e-06 mae 0.0009997351364433474
training loss 2.1803243005634414e-06 mae 0.0009967307600487797
training loss 2.22719622707393e-06 mae 0.0010049356620246846
training loss 2.2211548380413783e-06 mae 0.0010128158537916544
Epoch 527, training: loss: 0.0000022, mae: 0.0010099 test: loss0.0000969, mae:0.0071334
training loss 3.7691061152145267e-06 mae 0.0011116923997178674
training loss 2.132501610869404e-06 mae 0.0009646784587233674
training loss 2.2483514977457253e-06 mae 0.001003690453541308
training loss 2.2159490233345724e-06 mae 0.0010129942517073838
training loss 2.1803426631745582e-06 mae 0.0010061150863280158
Epoch 528, training: loss: 0.0000022, mae: 0.0010072 test: loss0.0000963, mae:0.0071241
training loss 8.44166436309024e-07 mae 0.000675564631819725
training loss 2.026296859488259e-06 mae 0.000965472878020841
training loss 2.0846249205785363e-06 mae 0.0009896046082065032
training loss 2.1421538839519855e-06 mae 0.0009983483623306189
training loss 2.191949767193228e-06 mae 0.001008216560116288
Epoch 529, training: loss: 0.0000022, mae: 0.0010089 test: loss0.0000964, mae:0.0071358
training loss 1.7161445384772378e-06 mae 0.0010090941796079278
training loss 2.1406488610641564e-06 mae 0.0009897697863041185
training loss 2.1086893354441788e-06 mae 0.0009853844617327468
training loss 2.143505160434895e-06 mae 0.001001955111378446
training loss 2.178712706701836e-06 mae 0.0010061253578082395
Epoch 530, training: loss: 0.0000022, mae: 0.0010087 test: loss0.0000960, mae:0.0071019
training loss 1.4054512575967237e-06 mae 0.0008493252098560333
training loss 2.1802786738375925e-06 mae 0.0010026041737885452
training loss 2.1659108305771454e-06 mae 0.000997450264945332
training loss 2.196556394080849e-06 mae 0.0010051401108441675
training loss 2.186146041401401e-06 mae 0.0010053638906903871
Epoch 531, training: loss: 0.0000022, mae: 0.0010053 test: loss0.0000963, mae:0.0071330
training loss 2.498452204235946e-06 mae 0.001019359566271305
training loss 2.250255384555488e-06 mae 0.0010192498037427228
training loss 2.2073889583522698e-06 mae 0.0010022140479194802
training loss 2.156140277934626e-06 mae 0.0009932119005347383
training loss 2.1634438132708463e-06 mae 0.00099745523723176
Epoch 532, training: loss: 0.0000022, mae: 0.0009964 test: loss0.0000960, mae:0.0071026
training loss 3.066432327614166e-06 mae 0.0012532426044344902
training loss 2.231489511760789e-06 mae 0.0010146532554690744
training loss 2.1819869953285907e-06 mae 0.0009984883484384511
training loss 2.157102009479653e-06 mae 0.0009947213045913507
training loss 2.1508728159641583e-06 mae 0.0009964127600336655
Epoch 533, training: loss: 0.0000022, mae: 0.0009988 test: loss0.0000999, mae:0.0071674
training loss 1.6849022586029605e-06 mae 0.0007650051265954971
training loss 2.0594770920105666e-06 mae 0.0009640885982662438
training loss 2.1107751850010603e-06 mae 0.0009824744066094409
training loss 2.085781190336568e-06 mae 0.0009837200616899609
training loss 2.15307770346062e-06 mae 0.000998689887456744
Epoch 534, training: loss: 0.0000022, mae: 0.0010013 test: loss0.0000962, mae:0.0071154
training loss 1.9870958567480557e-06 mae 0.0010759610449895263
training loss 2.0212245328598243e-06 mae 0.000958131993755552
training loss 2.1692431921497473e-06 mae 0.0009984640452992354
training loss 2.1803017011132172e-06 mae 0.0010061940210655534
training loss 2.13852751559346e-06 mae 0.0009995035903032554
Epoch 535, training: loss: 0.0000022, mae: 0.0010004 test: loss0.0000961, mae:0.0071154
training loss 2.2790850380260963e-06 mae 0.0009326876024715602
training loss 2.1852727047867214e-06 mae 0.0010204366225676212
training loss 2.1928723856602815e-06 mae 0.0010063397262411392
training loss 2.170697543973296e-06 mae 0.0010020890977042447
training loss 2.1678479121231734e-06 mae 0.001003899899675089
Epoch 536, training: loss: 0.0000022, mae: 0.0010054 test: loss0.0000963, mae:0.0071245
training loss 3.3264452667935984e-06 mae 0.0011370088905096054
training loss 2.182828356411597e-06 mae 0.000992515765806185
training loss 2.133931500924122e-06 mae 0.0009894763185218489
training loss 2.1103528581157746e-06 mae 0.0009874777863764306
training loss 2.1332395196772436e-06 mae 0.0009935740896950087
Epoch 537, training: loss: 0.0000021, mae: 0.0009968 test: loss0.0000980, mae:0.0071672
training loss 1.7836601955423248e-06 mae 0.0008850982412695885
training loss 2.135962787696033e-06 mae 0.0009899606458002737
training loss 2.128230938402646e-06 mae 0.0009909159524624442
training loss 2.0681877648357175e-06 mae 0.00098332622865622
training loss 2.1215381237388053e-06 mae 0.0009949563519417576
Epoch 538, training: loss: 0.0000021, mae: 0.0009995 test: loss0.0000961, mae:0.0071091
training loss 1.1671437505356153e-06 mae 0.0008035146747715771
training loss 2.1265893636547274e-06 mae 0.0010138421305729188
training loss 2.2067059064446068e-06 mae 0.0010214055067684395
training loss 2.211236619210552e-06 mae 0.0010180854514574278
training loss 2.1857916945167876e-06 mae 0.0010093070479197917
Epoch 539, training: loss: 0.0000022, mae: 0.0010057 test: loss0.0000967, mae:0.0071381
training loss 1.6416521475548507e-06 mae 0.000895094417501241
training loss 2.0595125065750936e-06 mae 0.0009798260745775029
training loss 2.0785595550046085e-06 mae 0.0009768413855823328
training loss 2.1345197371332366e-06 mae 0.0009896124097123885
training loss 2.1334901920843416e-06 mae 0.0009888688476515268
Epoch 540, training: loss: 0.0000021, mae: 0.0009860 test: loss0.0000969, mae:0.0071352
training loss 2.8999309051869204e-06 mae 0.0011951414635404944
training loss 2.05568406664836e-06 mae 0.0009913258357247445
training loss 2.1517093854175857e-06 mae 0.0010011344502615454
training loss 2.1588275693835953e-06 mae 0.0009985996926024506
training loss 2.152352514070601e-06 mae 0.0009968608460824626
Epoch 541, training: loss: 0.0000021, mae: 0.0009940 test: loss0.0000966, mae:0.0071344
training loss 1.7945179706657655e-06 mae 0.00086172204464674
training loss 2.046304641725346e-06 mae 0.0009729204900270585
training loss 2.111686737322034e-06 mae 0.00098085124967712
training loss 2.105659245823715e-06 mae 0.0009827877179756513
training loss 2.121145573551867e-06 mae 0.0009922170580077487
Epoch 542, training: loss: 0.0000021, mae: 0.0009954 test: loss0.0000969, mae:0.0071459
training loss 4.082721716258675e-06 mae 0.00122048647608608
training loss 2.1560894473551846e-06 mae 0.0009830705981299862
training loss 2.1955772786211608e-06 mae 0.0009953975763937254
training loss 2.137929707576117e-06 mae 0.0009907054593817426
training loss 2.145712936347159e-06 mae 0.000989889931362307
Epoch 543, training: loss: 0.0000021, mae: 0.0009896 test: loss0.0000969, mae:0.0071458
training loss 3.0186081403371645e-06 mae 0.0010413232957944274
training loss 2.08965260704716e-06 mae 0.0009669699710702487
training loss 2.2099639736687576e-06 mae 0.0010036885096359593
training loss 2.1580833779789955e-06 mae 0.0009985781546114688
training loss 2.1343918876428046e-06 mae 0.0009976979431960342
Epoch 544, training: loss: 0.0000021, mae: 0.0009993 test: loss0.0000964, mae:0.0071290
training loss 1.745282474985288e-06 mae 0.0009881012374535203
training loss 2.0485275325988954e-06 mae 0.0009840505925810656
training loss 2.0515022953023244e-06 mae 0.0009817962568151848
training loss 2.0722469247666353e-06 mae 0.0009892281362268802
training loss 2.131063217915997e-06 mae 0.0009997264593283625
Epoch 545, training: loss: 0.0000021, mae: 0.0009978 test: loss0.0000971, mae:0.0071524
training loss 1.563328964948596e-06 mae 0.0008095189114101231
training loss 2.1207991637186513e-06 mae 0.0009897109109670945
training loss 2.091925977094776e-06 mae 0.0009844229945055402
training loss 2.1224602563726617e-06 mae 0.0009925111454436616
training loss 2.119303856961687e-06 mae 0.000989474380789649
Epoch 546, training: loss: 0.0000021, mae: 0.0009911 test: loss0.0001003, mae:0.0071919
training loss 2.4096104880300118e-06 mae 0.0010780900483950973
training loss 2.1014563837222036e-06 mae 0.0009619005779991403
training loss 2.1182892833347075e-06 mae 0.000976132690021307
training loss 2.1024087591918353e-06 mae 0.0009818951771224521
training loss 2.1098218998693358e-06 mae 0.0009849482767320984
Epoch 547, training: loss: 0.0000021, mae: 0.0009878 test: loss0.0000966, mae:0.0071272
training loss 2.620625991767156e-06 mae 0.001216916018165648
training loss 1.9862398974010815e-06 mae 0.0009520097313832276
training loss 2.0516014397925228e-06 mae 0.0009627392663076372
training loss 2.078391842148045e-06 mae 0.0009711483760971254
training loss 2.07876084445007e-06 mae 0.0009747644694086478
Epoch 548, training: loss: 0.0000021, mae: 0.0009815 test: loss0.0000968, mae:0.0071415
training loss 1.975005261556362e-06 mae 0.0009726916323415935
training loss 1.9342790462324765e-06 mae 0.0009642944057179872
training loss 2.0211318786609227e-06 mae 0.0009710641144452122
training loss 2.028959865119819e-06 mae 0.0009727265866201939
training loss 2.1074122044035034e-06 mae 0.0009931100286839911
Epoch 549, training: loss: 0.0000021, mae: 0.0009929 test: loss0.0000968, mae:0.0071401
training loss 5.769558356405469e-06 mae 0.0016068710247054696
training loss 2.283939648787411e-06 mae 0.0010115492064049288
training loss 2.0776204786499764e-06 mae 0.0009773402863255893
training loss 2.0988330322102197e-06 mae 0.0009827958909480133
training loss 2.101876153705487e-06 mae 0.0009888182504250854
Epoch 550, training: loss: 0.0000021, mae: 0.0009887 test: loss0.0000968, mae:0.0071363
training loss 1.5033268709885306e-06 mae 0.0008566559408791363
training loss 1.9229248831673902e-06 mae 0.0009433788980138213
training loss 2.0220626232336453e-06 mae 0.0009657888636361845
training loss 2.10503931902655e-06 mae 0.0009854136891385527
training loss 2.107792814644453e-06 mae 0.0009871364252025548
Epoch 551, training: loss: 0.0000021, mae: 0.0009883 test: loss0.0000969, mae:0.0071433
training loss 1.8792488845065236e-06 mae 0.0008911211043596268
training loss 2.0367920059220973e-06 mae 0.0009683851636580977
training loss 2.041676373435189e-06 mae 0.0009738207831609959
training loss 2.063108073924988e-06 mae 0.000974574054218212
training loss 2.0756601643438096e-06 mae 0.0009817276787086949
Epoch 552, training: loss: 0.0000021, mae: 0.0009867 test: loss0.0000971, mae:0.0071505
training loss 1.7339812075078953e-06 mae 0.0009019511635415256
training loss 1.942371206237737e-06 mae 0.0009473589004711339
training loss 2.039205245376135e-06 mae 0.0009679951518312983
training loss 2.08183237741018e-06 mae 0.0009794014853746046
training loss 2.094463950617585e-06 mae 0.0009846978447868018
Epoch 553, training: loss: 0.0000021, mae: 0.0009884 test: loss0.0000978, mae:0.0071760
training loss 3.7959616747684777e-06 mae 0.0013551419833675027
training loss 2.114895639182328e-06 mae 0.0009901474969571124
training loss 2.0512802508381013e-06 mae 0.0009759648560112955
training loss 2.0828397478209613e-06 mae 0.0009850167552940552
training loss 2.09221513305645e-06 mae 0.0009879974248142573
Epoch 554, training: loss: 0.0000021, mae: 0.0009898 test: loss0.0000992, mae:0.0072013
training loss 1.956339247044525e-06 mae 0.0010311314836144447
training loss 1.936499714639806e-06 mae 0.0009528949249572324
training loss 2.0426308459142405e-06 mae 0.0009738411247213882
training loss 2.0754838277036398e-06 mae 0.0009820844410665776
training loss 2.100192544681044e-06 mae 0.0009858500054893801
Epoch 555, training: loss: 0.0000021, mae: 0.0009854 test: loss0.0000967, mae:0.0071274
training loss 2.4867620140867075e-06 mae 0.0011553047224879265
training loss 2.2694720234326e-06 mae 0.0010144885312126694
training loss 2.08025526060035e-06 mae 0.000980553225300616
training loss 2.062109528925543e-06 mae 0.0009785828335574576
training loss 2.0824069600673476e-06 mae 0.0009847170677944202
Epoch 556, training: loss: 0.0000021, mae: 0.0009862 test: loss0.0000969, mae:0.0071458
training loss 1.3228027455625124e-06 mae 0.000821591995190829
training loss 2.0776956741393198e-06 mae 0.0009689927785931265
training loss 2.095486891984464e-06 mae 0.0009831659499288418
training loss 2.086964716620167e-06 mae 0.0009848221759496886
training loss 2.073594990618026e-06 mae 0.00098266954005543
Epoch 557, training: loss: 0.0000021, mae: 0.0009890 test: loss0.0000969, mae:0.0071501
training loss 1.9834428712783847e-06 mae 0.0009848146000877023
training loss 1.9499446689313153e-06 mae 0.0009616082408648056
training loss 1.9920575426220977e-06 mae 0.0009716784634562853
training loss 2.1115151027503776e-06 mae 0.0009914301651906187
training loss 2.0852423270511124e-06 mae 0.0009844510280874672
Epoch 558, training: loss: 0.0000021, mae: 0.0009849 test: loss0.0000974, mae:0.0071679
training loss 2.2276051367953187e-06 mae 0.0011147555196657777
training loss 2.0219306484963735e-06 mae 0.0009768179859783424
training loss 2.021788446037845e-06 mae 0.0009837036717240464
training loss 2.0313095236008456e-06 mae 0.0009806848104907473
training loss 2.0623938133719273e-06 mae 0.0009772617744040943
Epoch 559, training: loss: 0.0000021, mae: 0.0009814 test: loss0.0000967, mae:0.0071285
training loss 2.1318780909496127e-06 mae 0.0009692950989119709
training loss 1.9159478294265373e-06 mae 0.0009522840081641049
training loss 2.0078381705595993e-06 mae 0.0009656606580932987
training loss 2.0597577213115463e-06 mae 0.0009750666155399671
training loss 2.0525703297069357e-06 mae 0.0009748337805428685
Epoch 560, training: loss: 0.0000021, mae: 0.0009766 test: loss0.0000973, mae:0.0071613
training loss 2.260982455482008e-06 mae 0.0009358159149996936
training loss 2.0761366174298803e-06 mae 0.0009725093065450587
training loss 2.064895883481452e-06 mae 0.0009692162684496893
training loss 2.0230289549285986e-06 mae 0.0009707075548327423
training loss 2.069338020633109e-06 mae 0.0009784617247546453
Epoch 561, training: loss: 0.0000021, mae: 0.0009795 test: loss0.0000975, mae:0.0071706
training loss 3.6309450024418766e-06 mae 0.001077397377230227
training loss 1.9431420052266435e-06 mae 0.0009524887518993781
training loss 2.020287300269101e-06 mae 0.0009642925463838153
training loss 2.003706282606957e-06 mae 0.0009637334918301081
training loss 2.065526175341512e-06 mae 0.0009784634814097252
Epoch 562, training: loss: 0.0000021, mae: 0.0009802 test: loss0.0000974, mae:0.0071728
training loss 1.888310293907125e-06 mae 0.0009742360562086105
training loss 2.0482022712124435e-06 mae 0.0009731395448576294
training loss 2.083067892663797e-06 mae 0.0009795928357796064
training loss 2.0715378076337048e-06 mae 0.000982434050155347
training loss 2.0634266933602115e-06 mae 0.0009841302682562219
Epoch 563, training: loss: 0.0000021, mae: 0.0009836 test: loss0.0000971, mae:0.0071494
training loss 1.44536386414984e-06 mae 0.0008134174277074635
training loss 2.0429806994013477e-06 mae 0.0009692547986667384
training loss 2.0497140019077693e-06 mae 0.0009725691006048631
training loss 2.0679150639922074e-06 mae 0.0009742226418483977
training loss 2.065303399761329e-06 mae 0.0009756875650109306
Epoch 564, training: loss: 0.0000021, mae: 0.0009754 test: loss0.0000969, mae:0.0071459
training loss 3.308510940769338e-06 mae 0.0012325827265158296
training loss 2.0868369373976533e-06 mae 0.0009850027409436953
training loss 2.109406414047494e-06 mae 0.000985713562084417
training loss 2.0559794947322674e-06 mae 0.0009712946504070757
training loss 2.0481504445039426e-06 mae 0.0009722295593683467
Epoch 565, training: loss: 0.0000021, mae: 0.0009747 test: loss0.0000973, mae:0.0071641
training loss 1.6146403822858701e-06 mae 0.0009021383593790233
training loss 1.861980490279986e-06 mae 0.0009331438569443336
training loss 1.9700056234139293e-06 mae 0.0009588861254753892
training loss 2.0545735567143113e-06 mae 0.0009703743533656887
training loss 2.044862483431934e-06 mae 0.0009689204784605964
Epoch 566, training: loss: 0.0000020, mae: 0.0009669 test: loss0.0000970, mae:0.0071572
training loss 1.3919024013375747e-06 mae 0.0007987127755768597
training loss 1.9137610274365036e-06 mae 0.0009428855812396199
training loss 1.955777684155926e-06 mae 0.0009495005896776031
training loss 2.003265814247597e-06 mae 0.0009576489771901328
training loss 2.0167854866336526e-06 mae 0.0009616582691484125
Epoch 567, training: loss: 0.0000020, mae: 0.0009664 test: loss0.0000974, mae:0.0071557
training loss 3.1575698358210502e-06 mae 0.001196580589748919
training loss 2.0597994855908083e-06 mae 0.0009705619315854181
training loss 2.0026449079739575e-06 mae 0.0009642731730300601
training loss 2.021237458164565e-06 mae 0.0009657212051976198
training loss 2.0561548098071828e-06 mae 0.000973099264355645
Epoch 568, training: loss: 0.0000020, mae: 0.0009722 test: loss0.0000988, mae:0.0071958
training loss 3.632421567090205e-06 mae 0.0010367054492235184
training loss 1.9657925071103323e-06 mae 0.0009405522301391351
training loss 1.985669922701749e-06 mae 0.0009560007267714579
training loss 2.0571921978769408e-06 mae 0.0009739048260924041
training loss 2.0384198240375627e-06 mae 0.0009719132566222205
Epoch 569, training: loss: 0.0000020, mae: 0.0009729 test: loss0.0000976, mae:0.0071729
training loss 1.2047748896293342e-06 mae 0.0007027775864116848
training loss 1.933237855360394e-06 mae 0.0009384255082456067
training loss 1.9801999255292628e-06 mae 0.0009551849285380382
training loss 1.9437081232655246e-06 mae 0.000949654940959831
training loss 2.0175025460007614e-06 mae 0.0009645733261823803
Epoch 570, training: loss: 0.0000020, mae: 0.0009693 test: loss0.0000978, mae:0.0071816
training loss 1.1227333516217186e-06 mae 0.000841184810269624
training loss 1.973267096806975e-06 mae 0.0009479545874009822
training loss 2.045704635216537e-06 mae 0.0009696888587539003
training loss 2.0167475282650185e-06 mae 0.0009651654781557818
training loss 2.0368824949056842e-06 mae 0.0009703109607295092
Epoch 571, training: loss: 0.0000020, mae: 0.0009692 test: loss0.0000968, mae:0.0071388
training loss 2.24126165448979e-06 mae 0.0009529199451208115
training loss 2.1586862899044613e-06 mae 0.0009936632465242464
training loss 1.9929670024298906e-06 mae 0.0009569811028437601
training loss 1.994978564779624e-06 mae 0.0009548261269020375
training loss 2.014937633782269e-06 mae 0.0009639427715690058
Epoch 572, training: loss: 0.0000020, mae: 0.0009621 test: loss0.0000999, mae:0.0072114
training loss 1.6308534895870253e-06 mae 0.000893566117156297
training loss 1.9171424585193543e-06 mae 0.0009297047440400895
training loss 1.9770622988163614e-06 mae 0.00094599184859083
training loss 1.9753478237145455e-06 mae 0.0009462449565492817
training loss 2.0027455810063946e-06 mae 0.0009570118848961868
Epoch 573, training: loss: 0.0000020, mae: 0.0009581 test: loss0.0000978, mae:0.0071745
training loss 1.1690788142004749e-06 mae 0.0007593873888254166
training loss 2.0148646704714627e-06 mae 0.0009390977256949625
training loss 2.019227200898878e-06 mae 0.0009628556032508315
training loss 2.034301155843735e-06 mae 0.0009695930801183972
training loss 2.0228842119527746e-06 mae 0.0009671029813161389
Epoch 574, training: loss: 0.0000020, mae: 0.0009660 test: loss0.0000973, mae:0.0071700
training loss 1.4614070096286014e-06 mae 0.0007988695870153606
training loss 2.038238947347161e-06 mae 0.0009629020042827025
training loss 2.050154111234091e-06 mae 0.0009650103851581123
training loss 2.0489951144940142e-06 mae 0.0009678644494350928
training loss 2.032035333277284e-06 mae 0.0009677108051139511
Epoch 575, training: loss: 0.0000020, mae: 0.0009652 test: loss0.0000975, mae:0.0071671
training loss 2.257071400890709e-06 mae 0.0009280673111788929
training loss 2.070742803372174e-06 mae 0.0009740353710310278
training loss 2.071595583901858e-06 mae 0.0009684190891756871
training loss 2.0322400383213118e-06 mae 0.0009632036043639415
training loss 2.0341896702889957e-06 mae 0.0009667171264966183
Epoch 576, training: loss: 0.0000020, mae: 0.0009624 test: loss0.0000982, mae:0.0071970
training loss 2.0987938569305697e-06 mae 0.0008622088353149593
training loss 2.0000323132205497e-06 mae 0.000943535225529808
training loss 2.076984956845377e-06 mae 0.0009754512817506669
training loss 2.0895657905312447e-06 mae 0.0009832544335540362
training loss 2.0407545158848194e-06 mae 0.0009731106467044627
Epoch 577, training: loss: 0.0000020, mae: 0.0009729 test: loss0.0000976, mae:0.0071754
training loss 2.599120989543735e-06 mae 0.0011804386740550399
training loss 1.944955944482525e-06 mae 0.0009515030850546767
training loss 2.0698476369529596e-06 mae 0.0009793371600698274
training loss 2.0115057123947687e-06 mae 0.0009658729020715845
training loss 1.997433368945335e-06 mae 0.0009633068088557926
Epoch 578, training: loss: 0.0000020, mae: 0.0009631 test: loss0.0000975, mae:0.0071620
training loss 3.3620956401136937e-06 mae 0.0011092998320236802
training loss 1.8406409124366416e-06 mae 0.0009167134606589875
training loss 1.9770876768702546e-06 mae 0.0009476367919014232
training loss 2.03485873519802e-06 mae 0.0009641525249262124
training loss 2.016959479248332e-06 mae 0.0009662429582031067
Epoch 579, training: loss: 0.0000020, mae: 0.0009647 test: loss0.0000979, mae:0.0071864
training loss 2.083660319840419e-06 mae 0.000980727723799646
training loss 1.8503776316845015e-06 mae 0.0009296900597309657
training loss 1.9977389854938158e-06 mae 0.0009555188808728489
training loss 2.0414170096395618e-06 mae 0.000966887593821357
training loss 1.9968286661102643e-06 mae 0.0009592175457640477
Epoch 580, training: loss: 0.0000020, mae: 0.0009600 test: loss0.0000978, mae:0.0071792
training loss 2.2161873403092613e-06 mae 0.0010688500478863716
training loss 1.8147767143679683e-06 mae 0.0009248659822761137
training loss 1.952364120352464e-06 mae 0.0009523373740347157
training loss 1.9716019997662464e-06 mae 0.0009562853470888756
training loss 1.9997960823356638e-06 mae 0.000961718874327857
Epoch 581, training: loss: 0.0000020, mae: 0.0009627 test: loss0.0000982, mae:0.0071868
training loss 1.3474215165842907e-06 mae 0.0007948248530738056
training loss 1.933072983810431e-06 mae 0.0009370366300857973
training loss 1.9500914152216656e-06 mae 0.0009479908702553871
training loss 1.9882911477371277e-06 mae 0.0009597195185171999
training loss 1.9891222168824714e-06 mae 0.0009572406511392395
Epoch 582, training: loss: 0.0000020, mae: 0.0009595 test: loss0.0000977, mae:0.0071656
training loss 2.866007662305492e-06 mae 0.0011893619084730744
training loss 2.2015225195615384e-06 mae 0.0010054913810545615
training loss 2.05856372979337e-06 mae 0.0009740417377128832
training loss 2.0405858033834203e-06 mae 0.0009700833574640539
training loss 1.998325198352783e-06 mae 0.0009575322816560782
Epoch 583, training: loss: 0.0000020, mae: 0.0009566 test: loss0.0000976, mae:0.0071711
training loss 9.662902584750555e-07 mae 0.0007149313460104167
training loss 1.9503753559119475e-06 mae 0.0009433249554907286
training loss 1.9374361014874152e-06 mae 0.0009413409590868669
training loss 1.9855046543581895e-06 mae 0.0009600798755927344
training loss 1.9907911721825765e-06 mae 0.0009614903985439286
Epoch 584, training: loss: 0.0000020, mae: 0.0009615 test: loss0.0000981, mae:0.0071932
training loss 1.0748293561846367e-06 mae 0.0007031876593828201
training loss 1.9436998799142647e-06 mae 0.000951029791715829
training loss 1.9142507261955806e-06 mae 0.0009345084233993279
training loss 1.9695354373218997e-06 mae 0.000943583743924288
training loss 1.980261790256791e-06 mae 0.0009483402517892943
Epoch 585, training: loss: 0.0000020, mae: 0.0009487 test: loss0.0000980, mae:0.0071813
training loss 2.0786003460671054e-06 mae 0.0009805249283090234
training loss 1.956251943355193e-06 mae 0.0009479202791208439
training loss 1.8929750061231816e-06 mae 0.0009312021072336131
training loss 1.9741321714881288e-06 mae 0.0009474100834930576
training loss 1.9797321080084812e-06 mae 0.0009486611679993313
Epoch 586, training: loss: 0.0000020, mae: 0.0009479 test: loss0.0000984, mae:0.0072044
training loss 1.9598303424572805e-06 mae 0.0010115168988704681
training loss 1.9800494462341547e-06 mae 0.0009589592752722549
training loss 2.0290857705866538e-06 mae 0.0009702638265363283
training loss 1.973163295365308e-06 mae 0.0009579195560801106
training loss 1.9764097312157817e-06 mae 0.0009570415369211811
Epoch 587, training: loss: 0.0000020, mae: 0.0009560 test: loss0.0000982, mae:0.0071849
training loss 1.1253119964749203e-06 mae 0.0007385931094177067
training loss 1.991543065153511e-06 mae 0.000961997650116317
training loss 1.985786329581484e-06 mae 0.0009567284533560348
training loss 1.9994298596002235e-06 mae 0.0009628875155637149
training loss 1.984125789526595e-06 mae 0.0009629944595971619
Epoch 588, training: loss: 0.0000020, mae: 0.0009653 test: loss0.0000983, mae:0.0072045
training loss 1.1177756960023544e-06 mae 0.0007199129904620349
training loss 2.0381024373194782e-06 mae 0.0009682838937413753
training loss 1.9684716030136723e-06 mae 0.0009529256782872548
training loss 1.9946937980734567e-06 mae 0.0009599128758861677
training loss 1.954768228995872e-06 mae 0.0009553147449774379
Epoch 589, training: loss: 0.0000020, mae: 0.0009585 test: loss0.0000983, mae:0.0071999
training loss 5.583509391726693e-06 mae 0.0014635954285040498
training loss 1.965088275108642e-06 mae 0.0009566102982681319
training loss 2.013750356627047e-06 mae 0.000961481527273202
training loss 2.0115120803634664e-06 mae 0.0009597302040560108
training loss 1.9620511195275343e-06 mae 0.0009544980211243663
Epoch 590, training: loss: 0.0000020, mae: 0.0009553 test: loss0.0000979, mae:0.0071719
training loss 1.2168583225502516e-06 mae 0.0007632207125425339
training loss 1.870514890969407e-06 mae 0.0009328159788970416
training loss 1.8017672760329832e-06 mae 0.0009163409750447561
training loss 1.8451014252063007e-06 mae 0.000927875185036871
training loss 1.93446753293747e-06 mae 0.0009442878445721603
Epoch 591, training: loss: 0.0000020, mae: 0.0009490 test: loss0.0000983, mae:0.0071994
training loss 1.2307849601711496e-06 mae 0.0007157167419791222
training loss 1.9514048214356685e-06 mae 0.0009571087966654815
training loss 1.8954059273124987e-06 mae 0.0009426021116260107
training loss 1.9668356275216017e-06 mae 0.0009551732812465377
training loss 1.9740454216848757e-06 mae 0.000956709022327924
Epoch 592, training: loss: 0.0000020, mae: 0.0009573 test: loss0.0000983, mae:0.0072054
training loss 1.833588044064527e-06 mae 0.0009206754039041698
training loss 1.6377895469625346e-06 mae 0.0008879834797470742
training loss 1.8443900512916157e-06 mae 0.0009229572133695949
training loss 1.9162384740935658e-06 mae 0.0009432033549328121
training loss 1.9677047798247813e-06 mae 0.000953501062027517
Epoch 593, training: loss: 0.0000020, mae: 0.0009537 test: loss0.0000978, mae:0.0071755
training loss 2.2655888187728124e-06 mae 0.0009094889392144978
training loss 2.005088937690459e-06 mae 0.0009525807200036212
training loss 1.9832348615105593e-06 mae 0.0009506225014234536
training loss 1.974500050801601e-06 mae 0.000955266372642992
training loss 1.952276024883359e-06 mae 0.0009514744786212029
Epoch 594, training: loss: 0.0000020, mae: 0.0009548 test: loss0.0000992, mae:0.0072173
training loss 1.7983323914450011e-06 mae 0.0008559515699744225
training loss 2.0226296743096416e-06 mae 0.0009560444501831251
training loss 1.8940092299188943e-06 mae 0.0009334440260092811
training loss 1.918397028388403e-06 mae 0.0009393165287382854
training loss 1.956268372350905e-06 mae 0.000944997950813812
Epoch 595, training: loss: 0.0000020, mae: 0.0009458 test: loss0.0000987, mae:0.0072030
training loss 1.262419232261891e-06 mae 0.0008702144841663539
training loss 1.8637642813469118e-06 mae 0.0009321151540943371
training loss 1.919984602900153e-06 mae 0.0009456009657057651
training loss 1.9434227595152374e-06 mae 0.0009427638427625779
training loss 1.939678451157335e-06 mae 0.000944940592231813
Epoch 596, training: loss: 0.0000019, mae: 0.0009425 test: loss0.0000984, mae:0.0072007
training loss 1.2967863085577847e-06 mae 0.000781766779255122
training loss 1.9088477453503092e-06 mae 0.0009402408499690685
training loss 1.8708923785242865e-06 mae 0.0009235984162833214
training loss 1.890949415640637e-06 mae 0.0009312463503343283
training loss 1.9409569054616113e-06 mae 0.000944561175874606
Epoch 597, training: loss: 0.0000019, mae: 0.0009472 test: loss0.0000993, mae:0.0072319
training loss 2.752932914518169e-06 mae 0.0010608943412080407
training loss 1.763933892998823e-06 mae 0.0009211566500073554
training loss 1.7976056646562434e-06 mae 0.0009261881419697243
training loss 1.9121145092771287e-06 mae 0.000946058057917788
training loss 1.9655386100494423e-06 mae 0.0009553710007993735
Epoch 598, training: loss: 0.0000020, mae: 0.0009543 test: loss0.0000987, mae:0.0072243
training loss 2.1056148398201913e-06 mae 0.0009280064259655774
training loss 1.9026203325862007e-06 mae 0.0009345674668164813
training loss 1.9459599972680446e-06 mae 0.000941527793575816
training loss 1.977149180741297e-06 mae 0.0009499327584157472
training loss 1.942708550685175e-06 mae 0.0009438514295937633
Epoch 599, training: loss: 0.0000019, mae: 0.0009419 test: loss0.0000984, mae:0.0072133
current learning rate: 7.8125e-06
training loss 1.1216151278858888e-06 mae 0.0008046235889196396
training loss 1.684298768743568e-06 mae 0.0008612752141084013
training loss 1.842819719202831e-06 mae 0.000888502149782743
training loss 1.8391587476744822e-06 mae 0.0008950252760516245
training loss 1.8256350244882926e-06 mae 0.0008963334958523105
Epoch 600, training: loss: 0.0000018, mae: 0.0008991 test: loss0.0000982, mae:0.0072030
training loss 2.066201659545186e-06 mae 0.001064587733708322
training loss 1.7408087429947617e-06 mae 0.000863074269100074
training loss 1.6935508300109476e-06 mae 0.0008637701186563559
training loss 1.7327359910569727e-06 mae 0.0008721291438355693
training loss 1.8008777294483877e-06 mae 0.0008864614994390243
Epoch 601, training: loss: 0.0000018, mae: 0.0008884 test: loss0.0000991, mae:0.0072323
training loss 1.4020866956343525e-06 mae 0.0008013223414309323
training loss 1.6692383166859962e-06 mae 0.0008480846448162316
training loss 1.7218405940472467e-06 mae 0.0008666055077585473
training loss 1.7869436914486947e-06 mae 0.000880144155639785
training loss 1.8322771679426696e-06 mae 0.0008902545955810528
Epoch 602, training: loss: 0.0000018, mae: 0.0008876 test: loss0.0000983, mae:0.0071943
training loss 2.633543090269086e-06 mae 0.0011165799805894494
training loss 1.830196655352184e-06 mae 0.0008843377410598538
training loss 1.7561650360905764e-06 mae 0.0008674480117820573
training loss 1.8539719321960252e-06 mae 0.000893112075814225
training loss 1.8204599232783733e-06 mae 0.0008889675158914639
Epoch 603, training: loss: 0.0000018, mae: 0.0008882 test: loss0.0000993, mae:0.0072250
training loss 8.536906648259901e-07 mae 0.0006941771134734154
training loss 1.7984513725227142e-06 mae 0.0008888180029815901
training loss 1.783398704559443e-06 mae 0.0008836168032785009
training loss 1.8033084293225537e-06 mae 0.000886609319856526
training loss 1.8091546366036236e-06 mae 0.0008900864808519014
Epoch 604, training: loss: 0.0000018, mae: 0.0008917 test: loss0.0000983, mae:0.0072030
training loss 8.365855705960712e-07 mae 0.0006174693698994815
training loss 1.8335756956677124e-06 mae 0.000877644931760562
training loss 1.7947395676991177e-06 mae 0.000880255871613759
training loss 1.7934592051460636e-06 mae 0.0008782932094594355
training loss 1.8184661675109256e-06 mae 0.000887434324655052
Epoch 605, training: loss: 0.0000018, mae: 0.0008877 test: loss0.0000986, mae:0.0072155
training loss 1.2448202824089094e-06 mae 0.000769532285630703
training loss 1.7698176549731895e-06 mae 0.000882067561250034
training loss 1.7780807156938885e-06 mae 0.0008755216264341139
training loss 1.8198297130308392e-06 mae 0.0008899794185275075
training loss 1.8275620234577509e-06 mae 0.0008922104248244186
Epoch 606, training: loss: 0.0000018, mae: 0.0008877 test: loss0.0000987, mae:0.0072237
training loss 9.85162159850006e-07 mae 0.00072427251143381
training loss 1.8406011242725928e-06 mae 0.0008965276898451004
training loss 1.8327153017284813e-06 mae 0.0008887614043991976
training loss 1.8369351810721145e-06 mae 0.00089285311609852
training loss 1.8338908786511215e-06 mae 0.0008897473398521914
Epoch 607, training: loss: 0.0000018, mae: 0.0008842 test: loss0.0000990, mae:0.0072283
training loss 2.3448283172911033e-06 mae 0.0010731179499998689
training loss 1.7864503402676087e-06 mae 0.0008822369569565588
training loss 1.8002310561863813e-06 mae 0.0008875784345459893
training loss 1.7801610517037734e-06 mae 0.0008813837714283572
training loss 1.809003772190345e-06 mae 0.0008863216519138001
Epoch 608, training: loss: 0.0000018, mae: 0.0008858 test: loss0.0000989, mae:0.0072207
training loss 2.1480834675458027e-06 mae 0.0009083344484679401
training loss 1.8159756758209638e-06 mae 0.0008815471162818665
training loss 1.8117606462303482e-06 mae 0.0008925542207711406
training loss 1.7779660585187688e-06 mae 0.0008830257875702977
training loss 1.8094549502835719e-06 mae 0.0008881920488175945
Epoch 609, training: loss: 0.0000018, mae: 0.0008895 test: loss0.0000993, mae:0.0072187
training loss 1.5095596381797804e-06 mae 0.0008663184125907719
training loss 1.7611987871924951e-06 mae 0.0008881418422485393
training loss 1.7636354837437955e-06 mae 0.0008794416449871836
training loss 1.8051491908848625e-06 mae 0.000888564604232766
training loss 1.8060357103255157e-06 mae 0.0008903643228136816
Epoch 610, training: loss: 0.0000018, mae: 0.0008891 test: loss0.0000987, mae:0.0072178
training loss 2.0877089355053613e-06 mae 0.0009076052228920162
training loss 1.8037276377339475e-06 mae 0.000881916814821535
training loss 1.8362738365110572e-06 mae 0.0008959035840955116
training loss 1.823700679083322e-06 mae 0.0008925212117756984
training loss 1.8205985691846275e-06 mae 0.0008896947209954968
Epoch 611, training: loss: 0.0000018, mae: 0.0008882 test: loss0.0000989, mae:0.0072146
training loss 1.088280100702832e-06 mae 0.0007633498753421009
training loss 1.6716468091241645e-06 mae 0.0008417373721706954
training loss 1.7539012227337813e-06 mae 0.0008694775699923681
training loss 1.7715999280086021e-06 mae 0.0008812308302210852
training loss 1.8100236112802498e-06 mae 0.000887594564264149
Epoch 612, training: loss: 0.0000018, mae: 0.0008863 test: loss0.0001001, mae:0.0072505
training loss 2.336592615392874e-06 mae 0.0010415982687845826
training loss 1.7571645228189503e-06 mae 0.0008741704445770559
training loss 1.7366655619847702e-06 mae 0.0008714324760130877
training loss 1.7286253787972702e-06 mae 0.0008732218018560272
training loss 1.8090069899237767e-06 mae 0.000886140764512548
Epoch 613, training: loss: 0.0000018, mae: 0.0008865 test: loss0.0000985, mae:0.0072067
training loss 1.5299034430427128e-06 mae 0.0008362845401279628
training loss 1.943939549601411e-06 mae 0.000909995935930341
training loss 1.8239391489251012e-06 mae 0.0008899398571073935
training loss 1.8174107970552963e-06 mae 0.0008889957930929714
training loss 1.795530128824047e-06 mae 0.0008842133075944079
Epoch 614, training: loss: 0.0000018, mae: 0.0008871 test: loss0.0000988, mae:0.0072229
training loss 1.5129652410905692e-06 mae 0.0008121204446069896
training loss 1.7395627943259052e-06 mae 0.0008641894445663282
training loss 1.8000714848907104e-06 mae 0.0008847159224027528
training loss 1.8187956469177487e-06 mae 0.000887888297515718
training loss 1.7933868055782418e-06 mae 0.0008829506206794165
Epoch 615, training: loss: 0.0000018, mae: 0.0008842 test: loss0.0000987, mae:0.0072155
training loss 4.095987605978735e-06 mae 0.001262995763681829
training loss 1.8552119816680113e-06 mae 0.0008920025635127197
training loss 1.8021811248254019e-06 mae 0.0008813633962700348
training loss 1.7955839082438524e-06 mae 0.0008869944425571145
training loss 1.8082035217727266e-06 mae 0.0008905386521518047
Epoch 616, training: loss: 0.0000018, mae: 0.0008894 test: loss0.0000987, mae:0.0072196
training loss 1.5287085943782586e-06 mae 0.0008526907186023891
training loss 1.7295965413526856e-06 mae 0.0008524253863987385
training loss 1.763904115610821e-06 mae 0.0008676378333037442
training loss 1.7852653317222934e-06 mae 0.0008749692085575286
training loss 1.7985231740784732e-06 mae 0.0008819723989471645
Epoch 617, training: loss: 0.0000018, mae: 0.0008809 test: loss0.0000985, mae:0.0072136
training loss 1.7359683397444314e-06 mae 0.0008741030469536781
training loss 1.670555953822651e-06 mae 0.0008622648607592519
training loss 1.7952205592625278e-06 mae 0.0008860240744583604
training loss 1.8005067888665001e-06 mae 0.0008807267877273262
training loss 1.7917825615614533e-06 mae 0.0008820317916118015
Epoch 618, training: loss: 0.0000018, mae: 0.0008849 test: loss0.0000993, mae:0.0072455
training loss 1.174152089333802e-06 mae 0.0007534328033216298
training loss 1.7109283890824696e-06 mae 0.0008655335359257079
training loss 1.69801970764413e-06 mae 0.0008635818405021536
training loss 1.7867735491202118e-06 mae 0.0008785670950071303
training loss 1.791923661915629e-06 mae 0.000881563370238841
Epoch 619, training: loss: 0.0000018, mae: 0.0008840 test: loss0.0000995, mae:0.0072469
training loss 1.8195688653577236e-06 mae 0.0008711681584827602
training loss 1.691501298035505e-06 mae 0.0008584845790584734
training loss 1.759118307777183e-06 mae 0.0008766632878664181
training loss 1.7591751721913214e-06 mae 0.0008790543228110285
training loss 1.801246633621489e-06 mae 0.0008873407933420836
Epoch 620, training: loss: 0.0000018, mae: 0.0008851 test: loss0.0000985, mae:0.0072135
training loss 1.983660013138433e-06 mae 0.0009891735389828682
training loss 1.7045399244232295e-06 mae 0.0008829278111293473
training loss 1.7642759464233864e-06 mae 0.0008905610216016154
training loss 1.804384648406971e-06 mae 0.000891142322786695
training loss 1.8021749717752137e-06 mae 0.0008899322545864798
Epoch 621, training: loss: 0.0000018, mae: 0.0008910 test: loss0.0000993, mae:0.0072329
training loss 3.744546802408877e-06 mae 0.0012303156545385718
training loss 1.7115612948538342e-06 mae 0.0008756163342874131
training loss 1.7810233841130578e-06 mae 0.0008830173398681573
training loss 1.7734999633282642e-06 mae 0.0008815895678901069
training loss 1.7909846589749975e-06 mae 0.000883666277209769
Epoch 622, training: loss: 0.0000018, mae: 0.0008851 test: loss0.0000984, mae:0.0071955
training loss 1.1219888165214797e-06 mae 0.0006986530497670174
training loss 1.70192030215249e-06 mae 0.0008668142300117395
training loss 1.7749021994274183e-06 mae 0.00088556192925956
training loss 1.779026925362736e-06 mae 0.0008806347745217826
training loss 1.8009997473820106e-06 mae 0.000883937584565239
Epoch 623, training: loss: 0.0000018, mae: 0.0008811 test: loss0.0000990, mae:0.0072210
training loss 1.9589135717978934e-06 mae 0.0009791083866730332
training loss 1.7100125691571268e-06 mae 0.0008642988167611846
training loss 1.7932015446216844e-06 mae 0.0008817526510695348
training loss 1.7762000656274788e-06 mae 0.000875315975493425
training loss 1.7723464021509613e-06 mae 0.0008766454643342848
Epoch 624, training: loss: 0.0000018, mae: 0.0008810 test: loss0.0000998, mae:0.0072356
training loss 1.1057985602747067e-06 mae 0.000771339691709727
training loss 1.8042628486160972e-06 mae 0.0008886308094267457
training loss 1.9037630309948436e-06 mae 0.0008991688413840561
training loss 1.796311188166672e-06 mae 0.0008816826523479002
training loss 1.7899737735719728e-06 mae 0.0008817408700121467
Epoch 625, training: loss: 0.0000018, mae: 0.0008816 test: loss0.0000986, mae:0.0072171
training loss 1.034225874718686e-06 mae 0.000719929113984108
training loss 1.5678727636976176e-06 mae 0.0008285418141395878
training loss 1.7176530468757417e-06 mae 0.0008636917336843909
training loss 1.7342204234367854e-06 mae 0.0008666085631321399
training loss 1.7818869670705833e-06 mae 0.0008796174869761425
Epoch 626, training: loss: 0.0000018, mae: 0.0008793 test: loss0.0000994, mae:0.0072467
training loss 2.457898972352268e-06 mae 0.0010693460935726762
training loss 1.758364112893283e-06 mae 0.0008750989583467004
training loss 1.7146972448127855e-06 mae 0.0008596015631885148
training loss 1.7020843703951847e-06 mae 0.0008624692132159921
training loss 1.7590797901518374e-06 mae 0.0008769985531637474
Epoch 627, training: loss: 0.0000018, mae: 0.0008795 test: loss0.0000992, mae:0.0072402
training loss 1.9644242001959356e-06 mae 0.0008876317297108471
training loss 1.848643949763166e-06 mae 0.0008784981236756577
training loss 1.783876515142621e-06 mae 0.0008746501419112307
training loss 1.7475034493771771e-06 mae 0.0008686299371033511
training loss 1.7829785928940394e-06 mae 0.000878192056074683
Epoch 628, training: loss: 0.0000018, mae: 0.0008755 test: loss0.0000987, mae:0.0072147
training loss 2.5240522063540993e-06 mae 0.0011429270962253213
training loss 1.6784309318430974e-06 mae 0.000850260020324997
training loss 1.7458721055782641e-06 mae 0.0008626428258715956
training loss 1.766439103846175e-06 mae 0.0008748393865851131
training loss 1.7793659269421316e-06 mae 0.000877546445658866
Epoch 629, training: loss: 0.0000018, mae: 0.0008778 test: loss0.0000989, mae:0.0072211
training loss 1.3078270058031194e-06 mae 0.0007629987667314708
training loss 1.6787819611315224e-06 mae 0.0008482058639820738
training loss 1.742403024889985e-06 mae 0.0008746819526860768
training loss 1.7745611373525595e-06 mae 0.0008751672100112315
training loss 1.7742815051254484e-06 mae 0.0008775682896039735
Epoch 630, training: loss: 0.0000018, mae: 0.0008760 test: loss0.0000987, mae:0.0072130
training loss 1.458936480958073e-06 mae 0.0007720912690274417
training loss 1.564978282319727e-06 mae 0.0008243413872596825
training loss 1.6899668755268252e-06 mae 0.0008570434269725829
training loss 1.7461717093373718e-06 mae 0.0008708875088964814
training loss 1.779959150234621e-06 mae 0.0008776148107014039
Epoch 631, training: loss: 0.0000018, mae: 0.0008769 test: loss0.0000994, mae:0.0072402
training loss 6.325668095996662e-07 mae 0.0005250650574453175
training loss 1.801734631677462e-06 mae 0.0008758856987982405
training loss 1.753494183895503e-06 mae 0.0008792898509337907
training loss 1.785785438464368e-06 mae 0.0008828073723815284
training loss 1.7869689980700363e-06 mae 0.000884833057302129
Epoch 632, training: loss: 0.0000018, mae: 0.0008838 test: loss0.0000992, mae:0.0072360
training loss 1.1190535360583453e-06 mae 0.0006997219170443714
training loss 1.7091271582317217e-06 mae 0.0008606640485060565
training loss 1.7853080367281608e-06 mae 0.0008748837966831528
training loss 1.7646588718666835e-06 mae 0.000870330954611992
training loss 1.766571019449309e-06 mae 0.0008742921268660113
Epoch 633, training: loss: 0.0000018, mae: 0.0008756 test: loss0.0000991, mae:0.0072235
training loss 1.2853764701503678e-06 mae 0.0008050259202718735
training loss 1.7240247265871243e-06 mae 0.0008567158091703759
training loss 1.7229512128652811e-06 mae 0.0008666172023115698
training loss 1.7552655042406416e-06 mae 0.0008741994257337902
training loss 1.7662906443762328e-06 mae 0.0008780084378364041
Epoch 634, training: loss: 0.0000018, mae: 0.0008773 test: loss0.0000988, mae:0.0072215
training loss 1.2019701216559042e-06 mae 0.0007466403767466545
training loss 1.6888686775696532e-06 mae 0.0008581072644458389
training loss 1.7367227903630213e-06 mae 0.0008658806800307466
training loss 1.7517465162139862e-06 mae 0.0008758351539064274
training loss 1.767180960645808e-06 mae 0.0008777478977398417
Epoch 635, training: loss: 0.0000018, mae: 0.0008777 test: loss0.0001005, mae:0.0072564
training loss 1.541183678455127e-06 mae 0.0007974809850566089
training loss 1.7639979432715336e-06 mae 0.0008614879813702667
training loss 1.7297654198505326e-06 mae 0.0008556940304668678
training loss 1.7358244682984023e-06 mae 0.0008627662742787986
training loss 1.7572963674775118e-06 mae 0.0008728515556017263
Epoch 636, training: loss: 0.0000018, mae: 0.0008709 test: loss0.0001000, mae:0.0072609
training loss 3.993623977294192e-06 mae 0.0012079061707481742
training loss 1.766527592441408e-06 mae 0.0008750499748880519
training loss 1.7360866427180093e-06 mae 0.000865946579747603
training loss 1.7898184082606042e-06 mae 0.0008776810340928717
training loss 1.746033937629748e-06 mae 0.0008689669787360175
Epoch 637, training: loss: 0.0000018, mae: 0.0008718 test: loss0.0000989, mae:0.0072288
training loss 1.3708962569580763e-06 mae 0.0008425426785834134
training loss 1.6872495652308978e-06 mae 0.0008580822465649132
training loss 1.7152576010450488e-06 mae 0.0008570631242145108
training loss 1.733644948193169e-06 mae 0.0008653871127595469
training loss 1.755744820420841e-06 mae 0.0008712315335939292
Epoch 638, training: loss: 0.0000018, mae: 0.0008724 test: loss0.0000989, mae:0.0072167
training loss 2.0412182948348345e-06 mae 0.000857887149322778
training loss 1.8032544786882344e-06 mae 0.0008822501347144591
training loss 1.7806620738354306e-06 mae 0.0008761278263726594
training loss 1.764312094637235e-06 mae 0.0008722027082964533
training loss 1.7566975460980738e-06 mae 0.0008714633735489863
Epoch 639, training: loss: 0.0000018, mae: 0.0008736 test: loss0.0000994, mae:0.0072404
training loss 2.3026964299788233e-06 mae 0.0010360240703448653
training loss 1.7370069537148583e-06 mae 0.0008611641093796374
training loss 1.6955196175817634e-06 mae 0.0008583318662257997
training loss 1.729436230592103e-06 mae 0.0008653402899003388
training loss 1.7635993238477183e-06 mae 0.0008729417100015902
Epoch 640, training: loss: 0.0000017, mae: 0.0008706 test: loss0.0000994, mae:0.0072399
training loss 1.5745334849270876e-06 mae 0.0008439105004072189
training loss 1.6400340372206744e-06 mae 0.0008478127956372118
training loss 1.708924278971446e-06 mae 0.0008702180884308227
training loss 1.7032253439212925e-06 mae 0.0008694802057028854
training loss 1.748798632972502e-06 mae 0.0008743032775425462
Epoch 641, training: loss: 0.0000018, mae: 0.0008762 test: loss0.0000994, mae:0.0072337
training loss 1.375162696604093e-06 mae 0.0007639331743121147
training loss 1.634870206062795e-06 mae 0.0008394196857789569
training loss 1.632390607672206e-06 mae 0.0008418358102242583
training loss 1.7108524846945564e-06 mae 0.0008647570038439445
training loss 1.7521177626202477e-06 mae 0.0008713033095764375
Epoch 642, training: loss: 0.0000017, mae: 0.0008706 test: loss0.0000995, mae:0.0072496
training loss 1.4383685993379913e-06 mae 0.0008397835190407932
training loss 1.572843849967909e-06 mae 0.0008382579419013186
training loss 1.6811463347669875e-06 mae 0.0008507444889810266
training loss 1.689926767729602e-06 mae 0.0008565572368764374
training loss 1.7636448955842622e-06 mae 0.0008754948911660197
Epoch 643, training: loss: 0.0000018, mae: 0.0008755 test: loss0.0000992, mae:0.0072389
training loss 1.0101169891640893e-06 mae 0.0007930835708975792
training loss 1.7602501886768529e-06 mae 0.0008791809212233799
training loss 1.7682531346742894e-06 mae 0.0008815297111177282
training loss 1.7595370456776129e-06 mae 0.0008737626674412349
training loss 1.7661712620259038e-06 mae 0.0008763543301753107
Epoch 644, training: loss: 0.0000017, mae: 0.0008721 test: loss0.0000991, mae:0.0072331
training loss 1.3818599882142735e-06 mae 0.0007587109575979412
training loss 1.8225592939683443e-06 mae 0.000881634358152309
training loss 1.7662398747260978e-06 mae 0.0008691039621036979
training loss 1.7819100152855332e-06 mae 0.0008767867647419804
training loss 1.7571690099389545e-06 mae 0.0008744359747923692
Epoch 645, training: loss: 0.0000017, mae: 0.0008709 test: loss0.0000995, mae:0.0072563
training loss 1.2501996025093831e-06 mae 0.0007753784884698689
training loss 1.6873376268324814e-06 mae 0.0008318998540441196
training loss 1.7391388827364958e-06 mae 0.0008580685722282029
training loss 1.7245690538848121e-06 mae 0.0008624190759894421
training loss 1.7408571602459792e-06 mae 0.0008697528085110721
Epoch 646, training: loss: 0.0000017, mae: 0.0008691 test: loss0.0000997, mae:0.0072598
training loss 1.5302706515285536e-06 mae 0.0008351473952643573
training loss 1.5588459351580668e-06 mae 0.0008150231841878565
training loss 1.7064537536777546e-06 mae 0.0008518311338001253
training loss 1.7579028421824957e-06 mae 0.0008706666497569159
training loss 1.7541981989453952e-06 mae 0.0008712780351440107
Epoch 647, training: loss: 0.0000017, mae: 0.0008687 test: loss0.0000993, mae:0.0072447
training loss 1.1146462384203915e-06 mae 0.0007625945727340877
training loss 1.5918533425100772e-06 mae 0.0008332982588577651
training loss 1.6555118765321063e-06 mae 0.0008484328697011392
training loss 1.713381301585545e-06 mae 0.0008633941352607942
training loss 1.7302169602369038e-06 mae 0.0008675913194389042
Epoch 648, training: loss: 0.0000017, mae: 0.0008697 test: loss0.0000993, mae:0.0072420
training loss 1.553509378027229e-06 mae 0.0008502313867211342
training loss 1.948532511220287e-06 mae 0.000898257699137663
training loss 1.763594905703417e-06 mae 0.0008684504710340707
training loss 1.726855410066455e-06 mae 0.0008629615467183587
training loss 1.7255577180022006e-06 mae 0.0008622279934543396
Epoch 649, training: loss: 0.0000017, mae: 0.0008657 test: loss0.0001003, mae:0.0072774
training loss 1.1029269444406964e-06 mae 0.0007087259436957538
training loss 1.6217143328789214e-06 mae 0.0008479749245167364
training loss 1.6311778254719071e-06 mae 0.0008448945072544092
training loss 1.705492013081239e-06 mae 0.0008630055574922268
training loss 1.7239905274104164e-06 mae 0.0008652730244304516
Epoch 650, training: loss: 0.0000017, mae: 0.0008701 test: loss0.0000992, mae:0.0072427
training loss 2.5395249849680113e-06 mae 0.0010868454119190574
training loss 1.6080221409235187e-06 mae 0.0008351073689375293
training loss 1.709437596430392e-06 mae 0.000865616490442011
training loss 1.7128231327160704e-06 mae 0.0008628993007564138
training loss 1.7365987584290728e-06 mae 0.0008701645999113607
Epoch 651, training: loss: 0.0000017, mae: 0.0008707 test: loss0.0000991, mae:0.0072296
training loss 1.447745944460621e-06 mae 0.0009020582656376064
training loss 1.5291690622947782e-06 mae 0.00082575159224992
training loss 1.65653310429013e-06 mae 0.0008531988052061146
training loss 1.6823219007300397e-06 mae 0.000857117192261538
training loss 1.7440631357156018e-06 mae 0.0008691463208831477
Epoch 652, training: loss: 0.0000017, mae: 0.0008654 test: loss0.0000989, mae:0.0072236
training loss 1.0642733059285092e-06 mae 0.0007635485380887985
training loss 1.6524951913161918e-06 mae 0.0008393915977292493
training loss 1.7309792024387224e-06 mae 0.0008674513123293253
training loss 1.709457483576256e-06 mae 0.0008611978391734332
training loss 1.7098668437748567e-06 mae 0.0008615628487212742
Epoch 653, training: loss: 0.0000017, mae: 0.0008655 test: loss0.0001008, mae:0.0072690
training loss 1.1343204278091434e-06 mae 0.0007510647992603481
training loss 1.6224698298423858e-06 mae 0.0008488391473542387
training loss 1.7364252313866635e-06 mae 0.000863600804673342
training loss 1.720283834076676e-06 mae 0.0008634927247755773
training loss 1.72604022035426e-06 mae 0.0008661198792909621
Epoch 654, training: loss: 0.0000017, mae: 0.0008677 test: loss0.0000995, mae:0.0072516
training loss 1.410849108651746e-06 mae 0.0008809010614641011
training loss 1.6377690777588602e-06 mae 0.0008313989040770513
training loss 1.6664812715877663e-06 mae 0.0008436737721555377
training loss 1.6977148805950333e-06 mae 0.000853645032817584
training loss 1.7382600490587334e-06 mae 0.000866719853742261
Epoch 655, training: loss: 0.0000017, mae: 0.0008666 test: loss0.0000994, mae:0.0072391
training loss 8.192188829525548e-07 mae 0.0006804115255363286
training loss 1.4393324029108905e-06 mae 0.0007870481908777913
training loss 1.7343856641916615e-06 mae 0.000864485912670446
training loss 1.7097969746840356e-06 mae 0.000857353697697453
training loss 1.7258007305465062e-06 mae 0.00086536605533129
Epoch 656, training: loss: 0.0000017, mae: 0.0008672 test: loss0.0000994, mae:0.0072457
training loss 1.38782297653961e-06 mae 0.0008783775265328586
training loss 1.6804230863628763e-06 mae 0.00086011456932836
training loss 1.7181101889666995e-06 mae 0.0008659257068261873
training loss 1.6948715789949828e-06 mae 0.0008619094432473481
training loss 1.731872361916652e-06 mae 0.0008666936634803444
Epoch 657, training: loss: 0.0000017, mae: 0.0008653 test: loss0.0000999, mae:0.0072673
training loss 1.2888268656752189e-06 mae 0.0007637550006620586
training loss 1.6436122457301222e-06 mae 0.0008537600583889905
training loss 1.730716697279198e-06 mae 0.0008707265934381303
training loss 1.7291875974780054e-06 mae 0.0008648136994548588
training loss 1.720473876638838e-06 mae 0.0008668951723210297
Epoch 658, training: loss: 0.0000017, mae: 0.0008680 test: loss0.0000995, mae:0.0072446
training loss 1.6117355698952451e-06 mae 0.0008479359676130116
training loss 1.7595607259328757e-06 mae 0.0008599920730179576
training loss 1.715765025923841e-06 mae 0.0008570540230721236
training loss 1.7136238972509563e-06 mae 0.0008587101327921482
training loss 1.7162521801163448e-06 mae 0.0008623028751485869
Epoch 659, training: loss: 0.0000017, mae: 0.0008636 test: loss0.0000992, mae:0.0072369
training loss 1.621128831175156e-06 mae 0.000851484015583992
training loss 1.6980285776673344e-06 mae 0.000860168451361139
training loss 1.7330285996055448e-06 mae 0.0008613553726907339
training loss 1.699330431355165e-06 mae 0.0008521996417687282
training loss 1.7187214319309715e-06 mae 0.0008627385774451258
Epoch 660, training: loss: 0.0000017, mae: 0.0008618 test: loss0.0000999, mae:0.0072560
training loss 1.2561373523567454e-06 mae 0.0007942238007672131
training loss 1.63668741437255e-06 mae 0.0008307309186670416
training loss 1.6404333295402402e-06 mae 0.0008404290886817147
training loss 1.6763486156522908e-06 mae 0.0008532650057929537
training loss 1.7154680935915057e-06 mae 0.0008634709827231234
Epoch 661, training: loss: 0.0000017, mae: 0.0008661 test: loss0.0000997, mae:0.0072553
training loss 1.1005657825080561e-06 mae 0.0007188279996626079
training loss 1.594067471137797e-06 mae 0.0008287305804426033
training loss 1.6801688021510784e-06 mae 0.0008504993125501248
training loss 1.7183960037166485e-06 mae 0.0008613036522203872
training loss 1.7172942504976845e-06 mae 0.0008649474423406508
Epoch 662, training: loss: 0.0000017, mae: 0.0008629 test: loss0.0000995, mae:0.0072543
training loss 1.693133413027681e-06 mae 0.0009264958207495511
training loss 1.6757833949953897e-06 mae 0.0008479221358749211
training loss 1.7223982015343829e-06 mae 0.0008592586584001927
training loss 1.6964756727946414e-06 mae 0.0008564418511748902
training loss 1.7156089668548086e-06 mae 0.0008612941969093396
Epoch 663, training: loss: 0.0000017, mae: 0.0008588 test: loss0.0001003, mae:0.0072810
training loss 2.7590042463998543e-06 mae 0.0009858660632744431
training loss 1.756514339991982e-06 mae 0.0008533072415921909
training loss 1.6806200660134399e-06 mae 0.0008465426525626665
training loss 1.667257135330219e-06 mae 0.000848633603004751
training loss 1.7056325792007338e-06 mae 0.0008583500091130132
Epoch 664, training: loss: 0.0000017, mae: 0.0008617 test: loss0.0001004, mae:0.0072885
training loss 1.358541226181842e-06 mae 0.0008009162847883999
training loss 1.8101965792993724e-06 mae 0.0008719433464255986
training loss 1.801645406556976e-06 mae 0.0008861313091065405
training loss 1.7567616676339656e-06 mae 0.0008719231746782434
training loss 1.7263985861534289e-06 mae 0.0008645174029834605
Epoch 665, training: loss: 0.0000017, mae: 0.0008597 test: loss0.0000998, mae:0.0072568
training loss 1.8620561377247213e-06 mae 0.0009174393489956856
training loss 1.7652084205324442e-06 mae 0.0008706243357201126
training loss 1.768548718482544e-06 mae 0.0008694110997251725
training loss 1.7493981355805441e-06 mae 0.0008666154979388939
training loss 1.7072343672141837e-06 mae 0.0008587524235776778
Epoch 666, training: loss: 0.0000017, mae: 0.0008587 test: loss0.0000996, mae:0.0072483
training loss 1.8849735852199956e-06 mae 0.0008582419832237065
training loss 1.6966955716306276e-06 mae 0.0008610371048288311
training loss 1.7722680742717976e-06 mae 0.0008705642430508271
training loss 1.7029547959752168e-06 mae 0.0008583980795945848
training loss 1.7099230760562646e-06 mae 0.0008608056996851713
Epoch 667, training: loss: 0.0000017, mae: 0.0008607 test: loss0.0001010, mae:0.0072770
training loss 1.6608604482826195e-06 mae 0.0007532260497100651
training loss 1.6211408039595758e-06 mae 0.0008279351407990737
training loss 1.6553109581214849e-06 mae 0.0008410130227491774
training loss 1.6629840717318953e-06 mae 0.0008441451185271886
training loss 1.7127952557996135e-06 mae 0.0008578482483610957
Epoch 668, training: loss: 0.0000017, mae: 0.0008573 test: loss0.0001001, mae:0.0072647
training loss 1.8206031882073148e-06 mae 0.0008651104872114956
training loss 1.649212393596256e-06 mae 0.0008351008462555266
training loss 1.6824912556852955e-06 mae 0.0008488539923305188
training loss 1.6746025077935461e-06 mae 0.000850477834488111
training loss 1.697506109143324e-06 mae 0.000855807439697471
Epoch 669, training: loss: 0.0000017, mae: 0.0008574 test: loss0.0000998, mae:0.0072477
training loss 7.335684131248854e-07 mae 0.0005939693073742092
training loss 1.6675124256567764e-06 mae 0.000843310274421146
training loss 1.7022193619010461e-06 mae 0.0008598431883757877
training loss 1.7073875643586235e-06 mae 0.0008644315256854358
training loss 1.7041505595638165e-06 mae 0.0008604023927030386
Epoch 670, training: loss: 0.0000017, mae: 0.0008603 test: loss0.0001000, mae:0.0072542
training loss 1.4538906043526367e-06 mae 0.0007515422184951603
training loss 1.508058722256643e-06 mae 0.0008197094007924783
training loss 1.7290791171746773e-06 mae 0.0008586957326878118
training loss 1.683581385820214e-06 mae 0.0008536152779093838
training loss 1.693721171716278e-06 mae 0.0008559174996694154
Epoch 671, training: loss: 0.0000017, mae: 0.0008565 test: loss0.0000995, mae:0.0072408
training loss 1.4971100199545617e-06 mae 0.000788222998380661
training loss 1.6319585083737385e-06 mae 0.0008231856764348992
training loss 1.7327706078697584e-06 mae 0.0008538763988578676
training loss 1.7068632894128704e-06 mae 0.0008568608141134165
training loss 1.6961764288791427e-06 mae 0.0008537409669575998
Epoch 672, training: loss: 0.0000017, mae: 0.0008536 test: loss0.0000999, mae:0.0072655
training loss 1.8703826754062902e-06 mae 0.0008885335992090404
training loss 1.7355360121066894e-06 mae 0.000872279629202596
training loss 1.7029021819325867e-06 mae 0.0008558529223970109
training loss 1.707570467658194e-06 mae 0.0008581376527374398
training loss 1.7016905831245712e-06 mae 0.0008571004684416771
Epoch 673, training: loss: 0.0000017, mae: 0.0008560 test: loss0.0000997, mae:0.0072528
training loss 1.247788532054983e-06 mae 0.0007065602694638073
training loss 1.6604928597260767e-06 mae 0.0008286493442331752
training loss 1.6703862520939366e-06 mae 0.000845164560832747
training loss 1.6480934122655715e-06 mae 0.0008411205482410131
training loss 1.6895020444597562e-06 mae 0.0008544103258094792
Epoch 674, training: loss: 0.0000017, mae: 0.0008561 test: loss0.0001005, mae:0.0072799
training loss 2.2775604975322494e-06 mae 0.001017210423015058
training loss 1.6362757565623135e-06 mae 0.0008363692117307117
training loss 1.6876649407783975e-06 mae 0.00085007113036884
training loss 1.6896676071013977e-06 mae 0.0008550454114196785
training loss 1.7012724746093756e-06 mae 0.0008590791013325796
Epoch 675, training: loss: 0.0000017, mae: 0.0008583 test: loss0.0001000, mae:0.0072641
training loss 2.1315938738553086e-06 mae 0.0009567439556121826
training loss 1.7418960149317332e-06 mae 0.000873632813511672
training loss 1.676647881109008e-06 mae 0.0008553627758554306
training loss 1.6844325275527301e-06 mae 0.0008518065592668804
training loss 1.6780048100028485e-06 mae 0.0008513155362605867
Epoch 676, training: loss: 0.0000017, mae: 0.0008526 test: loss0.0001003, mae:0.0072807
training loss 1.7931814682015101e-06 mae 0.0009753459016792476
training loss 1.7222985084296015e-06 mae 0.0008610358607827447
training loss 1.6934261076706808e-06 mae 0.0008507749068062052
training loss 1.7162081496509607e-06 mae 0.0008558311583690149
training loss 1.7060417406749212e-06 mae 0.0008587116124780281
Epoch 677, training: loss: 0.0000017, mae: 0.0008566 test: loss0.0000997, mae:0.0072591
training loss 1.9494334537739633e-06 mae 0.000981380813755095
training loss 1.743897029222816e-06 mae 0.0008629051781277737
training loss 1.7165600199702325e-06 mae 0.0008514961014538633
training loss 1.6867010840271167e-06 mae 0.0008515237891485311
training loss 1.676354917327077e-06 mae 0.0008505416717797298
Epoch 678, training: loss: 0.0000017, mae: 0.0008528 test: loss0.0000996, mae:0.0072473
training loss 1.5491365275011049e-06 mae 0.0008634871919639409
training loss 1.532018680282978e-06 mae 0.0008207054114809223
training loss 1.6485902639069179e-06 mae 0.0008500040869578942
training loss 1.6605821797956323e-06 mae 0.0008524850979200659
training loss 1.6976645976448067e-06 mae 0.000859681927524878
Epoch 679, training: loss: 0.0000017, mae: 0.0008603 test: loss0.0001004, mae:0.0072802
training loss 8.059167271312617e-07 mae 0.000653648457955569
training loss 1.7919699220614098e-06 mae 0.0008609377238077714
training loss 1.7239912377851515e-06 mae 0.0008493449457109638
training loss 1.7027382899332769e-06 mae 0.0008517215425450825
training loss 1.6816046443306563e-06 mae 0.000853463709099561
Epoch 680, training: loss: 0.0000017, mae: 0.0008545 test: loss0.0001001, mae:0.0072792
training loss 1.4213652548278333e-06 mae 0.0009308326989412308
training loss 1.6225181879877606e-06 mae 0.0008423972164974639
training loss 1.6892560806539198e-06 mae 0.000847364098091822
training loss 1.673668156330533e-06 mae 0.0008530147255719055
training loss 1.6962032288543183e-06 mae 0.0008579940350948652
Epoch 681, training: loss: 0.0000017, mae: 0.0008569 test: loss0.0001005, mae:0.0072796
training loss 1.6822104953462258e-06 mae 0.0009375773370265961
training loss 1.6317069121571107e-06 mae 0.000825556715283835
training loss 1.6760207755072368e-06 mae 0.0008453329544431428
training loss 1.6587066838928275e-06 mae 0.0008457144448650841
training loss 1.6795659997961e-06 mae 0.0008510304698882747
Epoch 682, training: loss: 0.0000017, mae: 0.0008543 test: loss0.0001009, mae:0.0072963
training loss 2.193598675148678e-06 mae 0.0009386462043039501
training loss 1.7526316368917721e-06 mae 0.0008632541832733243
training loss 1.711469222430937e-06 mae 0.0008558796881237023
training loss 1.6995979466483778e-06 mae 0.0008547227183628264
training loss 1.6710417997305567e-06 mae 0.0008512747911875374
Epoch 683, training: loss: 0.0000017, mae: 0.0008520 test: loss0.0001002, mae:0.0072765
training loss 2.5211313641193556e-06 mae 0.0009369384497404099
training loss 1.690580972990574e-06 mae 0.0008535050654637756
training loss 1.6941952672274092e-06 mae 0.0008550058775996369
training loss 1.6740070560408886e-06 mae 0.0008500204668287808
training loss 1.6707357485417428e-06 mae 0.000848970779644057
Epoch 684, training: loss: 0.0000017, mae: 0.0008538 test: loss0.0000999, mae:0.0072656
training loss 2.106186684613931e-06 mae 0.000927269458770752
training loss 1.6473210714994978e-06 mae 0.0008384029885443548
training loss 1.6664942563693328e-06 mae 0.000843256732707124
training loss 1.6704466349968041e-06 mae 0.0008457543213531031
training loss 1.6727995837605534e-06 mae 0.0008501394588699503
Epoch 685, training: loss: 0.0000017, mae: 0.0008514 test: loss0.0001007, mae:0.0072780
training loss 2.427012759653735e-06 mae 0.0010207034647464752
training loss 1.598600064488553e-06 mae 0.0008367049797693742
training loss 1.6351447461692276e-06 mae 0.0008399897620386195
training loss 1.6643965508839328e-06 mae 0.0008500496612473187
training loss 1.682465381519201e-06 mae 0.0008510319549076035
Epoch 686, training: loss: 0.0000017, mae: 0.0008519 test: loss0.0001006, mae:0.0072870
training loss 1.4254395637181005e-06 mae 0.0007786474307067692
training loss 1.534886142089104e-06 mae 0.0008147499448729351
training loss 1.6589183349082083e-06 mae 0.0008390960658181201
training loss 1.7099865813661193e-06 mae 0.0008537342655472457
training loss 1.6847488246409069e-06 mae 0.0008510559437036589
Epoch 687, training: loss: 0.0000017, mae: 0.0008489 test: loss0.0001002, mae:0.0072733
training loss 1.1423627483964083e-06 mae 0.0007855224539525807
training loss 1.6383766848651016e-06 mae 0.0008297276651194575
training loss 1.6443436361621956e-06 mae 0.0008362720669754367
training loss 1.648660773205257e-06 mae 0.0008403768952602879
training loss 1.681379416423574e-06 mae 0.0008503079793151858
Epoch 688, training: loss: 0.0000017, mae: 0.0008495 test: loss0.0001001, mae:0.0072670
training loss 1.6629206811558106e-06 mae 0.0008564237505197525
training loss 1.7365969800302277e-06 mae 0.0008641116631527741
training loss 1.67694807289243e-06 mae 0.0008529069347048897
training loss 1.6191068010321268e-06 mae 0.0008410501219825634
training loss 1.6698762866536837e-06 mae 0.0008538928078672853
Epoch 689, training: loss: 0.0000017, mae: 0.0008544 test: loss0.0001002, mae:0.0072797
training loss 1.2845630408264697e-06 mae 0.0007521649822592735
training loss 1.698653076185592e-06 mae 0.0008542558826579183
training loss 1.6843703736070678e-06 mae 0.0008551469032357605
training loss 1.6839569359836596e-06 mae 0.0008533544420566882
training loss 1.6724138841953989e-06 mae 0.0008522513185266348
Epoch 690, training: loss: 0.0000017, mae: 0.0008527 test: loss0.0001008, mae:0.0072859
training loss 1.7988741092267446e-06 mae 0.0007888025720603764
training loss 1.7291310539393652e-06 mae 0.0008585049437505064
training loss 1.654937316480205e-06 mae 0.0008369294916726832
training loss 1.666854230871342e-06 mae 0.0008441629988798217
training loss 1.6688710201236316e-06 mae 0.0008489263296968288
Epoch 691, training: loss: 0.0000017, mae: 0.0008490 test: loss0.0001002, mae:0.0072816
training loss 1.4915402744009043e-06 mae 0.000875061668921262
training loss 1.6660001778309022e-06 mae 0.0008472818127997658
training loss 1.6616183483399645e-06 mae 0.0008431691036670293
training loss 1.6810673708049169e-06 mae 0.0008497972543265004
training loss 1.6766523269022102e-06 mae 0.0008506939936989907
Epoch 692, training: loss: 0.0000017, mae: 0.0008474 test: loss0.0000999, mae:0.0072543
training loss 1.24562859582511e-06 mae 0.0007687633042223752
training loss 1.6595234620920152e-06 mae 0.0008405973694250718
training loss 1.6238190900511121e-06 mae 0.0008371310086118631
training loss 1.656475275910229e-06 mae 0.0008451467100839249
training loss 1.6642911329984764e-06 mae 0.0008516552358800867
Epoch 693, training: loss: 0.0000017, mae: 0.0008521 test: loss0.0001001, mae:0.0072698
training loss 1.6146642565217917e-06 mae 0.0009001161088235676
training loss 1.6693994822679664e-06 mae 0.000851740534462984
training loss 1.6432973007786227e-06 mae 0.00084621119912308
training loss 1.6358322461610747e-06 mae 0.0008441431868809046
training loss 1.6451239396778494e-06 mae 0.0008442797466408259
Epoch 694, training: loss: 0.0000017, mae: 0.0008484 test: loss0.0001010, mae:0.0072896
training loss 1.2811450460503693e-06 mae 0.0007770955562591553
training loss 1.5651072405320268e-06 mae 0.000819291783651958
training loss 1.6482911040285294e-06 mae 0.0008462680645499118
training loss 1.6876041461996191e-06 mae 0.0008531014623108091
training loss 1.669993966388076e-06 mae 0.0008490799916954479
Epoch 695, training: loss: 0.0000017, mae: 0.0008473 test: loss0.0001004, mae:0.0072826
training loss 1.7660266848906758e-06 mae 0.0009029926732182503
training loss 1.7574275284014045e-06 mae 0.0008586513006365765
training loss 1.631735311674324e-06 mae 0.0008364150578405083
training loss 1.6611660516780425e-06 mae 0.0008430663756070204
training loss 1.6592038745255115e-06 mae 0.0008449217733760264
Epoch 696, training: loss: 0.0000016, mae: 0.0008414 test: loss0.0001001, mae:0.0072626
training loss 1.5336116803155164e-06 mae 0.0008725470979698002
training loss 1.5776906653788019e-06 mae 0.0008305067324316968
training loss 1.5848465723408503e-06 mae 0.0008347091146302179
training loss 1.5884420967846165e-06 mae 0.0008329958392812911
training loss 1.6562618043846572e-06 mae 0.0008442543730237031
Epoch 697, training: loss: 0.0000017, mae: 0.0008462 test: loss0.0001010, mae:0.0072900
training loss 2.1201424260652857e-06 mae 0.0009743453119881451
training loss 1.6667093161998436e-06 mae 0.000840883733996866
training loss 1.657952007480892e-06 mae 0.0008480801226313016
training loss 1.648590492152756e-06 mae 0.0008450462737287216
training loss 1.660474209694796e-06 mae 0.0008457153355030907
Epoch 698, training: loss: 0.0000017, mae: 0.0008447 test: loss0.0001002, mae:0.0072701
training loss 3.248347411499708e-06 mae 0.001026253798045218
training loss 1.6482661903062443e-06 mae 0.0008343223861290838
training loss 1.584340774534572e-06 mae 0.0008301146917441619
training loss 1.6480104555071102e-06 mae 0.0008430466632549925
training loss 1.6511115847552258e-06 mae 0.0008446867765965336
Epoch 699, training: loss: 0.0000017, mae: 0.0008445 test: loss0.0001011, mae:0.0072965
current learning rate: 3.90625e-06
training loss 1.0967611387968645e-06 mae 0.0007159877568483353
training loss 1.4953235416428746e-06 mae 0.0007957553219360611
training loss 1.5594579455711656e-06 mae 0.0008090939689750351
training loss 1.6112478535496433e-06 mae 0.0008227175193382366
training loss 1.6117139751935419e-06 mae 0.0008226615247613203
Epoch 700, training: loss: 0.0000016, mae: 0.0008229 test: loss0.0001006, mae:0.0072900
training loss 1.4648776414105669e-06 mae 0.0008137229015119374
training loss 1.481047147580302e-06 mae 0.000780611851380444
training loss 1.573520127168545e-06 mae 0.0008020519145238812
training loss 1.599388464635031e-06 mae 0.0008148899225175799
training loss 1.6069702306918519e-06 mae 0.0008185725149163393
Epoch 701, training: loss: 0.0000016, mae: 0.0008167 test: loss0.0001002, mae:0.0072734
training loss 1.2831924323108979e-06 mae 0.0007580090314149857
training loss 1.7292160877929623e-06 mae 0.0008498492585632076
training loss 1.6898084796517517e-06 mae 0.0008416242236478976
training loss 1.6175083821631926e-06 mae 0.0008222102567272896
training loss 1.6014202474173485e-06 mae 0.0008174375602245722
Epoch 702, training: loss: 0.0000016, mae: 0.0008173 test: loss0.0001005, mae:0.0072860
training loss 9.098360465031874e-07 mae 0.0007481500506401062
training loss 1.6052457383733889e-06 mae 0.0008079822046234838
training loss 1.617750804357591e-06 mae 0.0008191450773899691
training loss 1.588483137168364e-06 mae 0.0008149925487914696
training loss 1.6061595323972865e-06 mae 0.0008158155232471005
Epoch 703, training: loss: 0.0000016, mae: 0.0008159 test: loss0.0001001, mae:0.0072715
training loss 1.469720132263319e-06 mae 0.0008419140358455479
training loss 1.6657071522358283e-06 mae 0.0008305891293703633
training loss 1.6838564972816373e-06 mae 0.0008385361352068656
training loss 1.603738215652094e-06 mae 0.0008161751112425761
training loss 1.588106805773585e-06 mae 0.0008131240999015666
Epoch 704, training: loss: 0.0000016, mae: 0.0008171 test: loss0.0001007, mae:0.0072983
training loss 1.1736331089196028e-06 mae 0.0007601666147820652
training loss 1.5609754629452912e-06 mae 0.0007928359926011703
training loss 1.6212929879337487e-06 mae 0.0008201718121181508
training loss 1.597417114164381e-06 mae 0.0008161955899312252
training loss 1.5954051554527533e-06 mae 0.0008153082082970116
Epoch 705, training: loss: 0.0000016, mae: 0.0008149 test: loss0.0001004, mae:0.0072876
training loss 1.1450570127635729e-06 mae 0.0007541120867244899
training loss 1.611902030274495e-06 mae 0.0008146009560875304
training loss 1.633719199052132e-06 mae 0.000818916832760788
training loss 1.6155838838453106e-06 mae 0.0008156071363066306
training loss 1.616864117334928e-06 mae 0.000819951001918453
Epoch 706, training: loss: 0.0000016, mae: 0.0008159 test: loss0.0001006, mae:0.0072903
training loss 7.665606176487927e-07 mae 0.0006101808394305408
training loss 1.5309254208392198e-06 mae 0.0007960603506231278
training loss 1.5703103245094575e-06 mae 0.0008074498901346533
training loss 1.5777764904380209e-06 mae 0.0008134766491839283
training loss 1.5975239043836162e-06 mae 0.0008175588622388075
Epoch 707, training: loss: 0.0000016, mae: 0.0008145 test: loss0.0001008, mae:0.0072896
training loss 1.3628053920911043e-06 mae 0.0007524133543483913
training loss 1.6005113185629478e-06 mae 0.0008162776522986663
training loss 1.5790806978567156e-06 mae 0.0008081865086598266
training loss 1.5797324555409288e-06 mae 0.000811614850165927
training loss 1.595556499075849e-06 mae 0.000815973003494521
Epoch 708, training: loss: 0.0000016, mae: 0.0008155 test: loss0.0001008, mae:0.0072916
training loss 9.89618456515018e-07 mae 0.00070290855364874
training loss 1.6924579081294763e-06 mae 0.0008318875585317466
training loss 1.5596963980852248e-06 mae 0.0008006083939252145
training loss 1.6106322173012612e-06 mae 0.000814183267444027
training loss 1.6076246795783115e-06 mae 0.0008189670063337471
Epoch 709, training: loss: 0.0000016, mae: 0.0008138 test: loss0.0001024, mae:0.0073123
training loss 2.356182221774361e-06 mae 0.0010256875539198518
training loss 1.4724204576993416e-06 mae 0.0007925163680577978
training loss 1.5740033772729445e-06 mae 0.000810691075962
training loss 1.5797235115489506e-06 mae 0.0008130522771732303
training loss 1.595010489191777e-06 mae 0.0008166352552199619
Epoch 710, training: loss: 0.0000016, mae: 0.0008174 test: loss0.0001004, mae:0.0072861
training loss 2.2196193185664015e-06 mae 0.0008877866785041988
training loss 1.5746185655929117e-06 mae 0.0008101100775449739
training loss 1.5747096919276735e-06 mae 0.0008096843364298123
training loss 1.5842654360484397e-06 mae 0.0008143695146122961
training loss 1.603000270764443e-06 mae 0.0008166654951133726
Epoch 711, training: loss: 0.0000016, mae: 0.0008146 test: loss0.0001013, mae:0.0073185
training loss 1.153129005615483e-06 mae 0.0008021807298064232
training loss 1.6468725501748422e-06 mae 0.0008049586755862715
training loss 1.594295508935505e-06 mae 0.0008029847790251976
training loss 1.5727031070763297e-06 mae 0.0008035825639855216
training loss 1.6058963832839526e-06 mae 0.0008148820377221395
Epoch 712, training: loss: 0.0000016, mae: 0.0008154 test: loss0.0001003, mae:0.0072793
training loss 1.409739525115583e-06 mae 0.0007748783682473004
training loss 1.6239156087638047e-06 mae 0.000822037760871371
training loss 1.5757119773198684e-06 mae 0.0008041272003268173
training loss 1.5990520617059701e-06 mae 0.0008140377799012011
training loss 1.610460982923071e-06 mae 0.0008185207087949694
Epoch 713, training: loss: 0.0000016, mae: 0.0008130 test: loss0.0001005, mae:0.0072871
training loss 1.5623005538145662e-06 mae 0.0007485502283088863
training loss 1.6762462748561924e-06 mae 0.0008172880425620093
training loss 1.5913746092945363e-06 mae 0.0008044653294935342
training loss 1.5970731241446898e-06 mae 0.0008095102132196286
training loss 1.5992438206571413e-06 mae 0.0008142465505162857
Epoch 714, training: loss: 0.0000016, mae: 0.0008114 test: loss0.0001004, mae:0.0072780
training loss 1.783296283974778e-06 mae 0.0008758797193877399
training loss 1.4832858158463163e-06 mae 0.0007770010190285449
training loss 1.5130025976286742e-06 mae 0.000790351760310886
training loss 1.5284696515612917e-06 mae 0.0008002556247531823
training loss 1.5893105494432807e-06 mae 0.0008126259813275171
Epoch 715, training: loss: 0.0000016, mae: 0.0008116 test: loss0.0001048, mae:0.0073396
training loss 1.4856019561193534e-06 mae 0.0008002615650184453
training loss 1.6333491580954773e-06 mae 0.0008067892922554163
training loss 1.6429838900862836e-06 mae 0.0008132978106761294
training loss 1.6096570325000641e-06 mae 0.0008127839693457305
training loss 1.5988646490354558e-06 mae 0.0008148210977762472
Epoch 716, training: loss: 0.0000016, mae: 0.0008151 test: loss0.0001004, mae:0.0072813
training loss 1.5721803947599255e-06 mae 0.000751278072129935
training loss 1.4988290897735189e-06 mae 0.0007916576766600726
training loss 1.5415735483296857e-06 mae 0.0008065611533797578
training loss 1.6194066822527932e-06 mae 0.0008212438349331425
training loss 1.5914722380624323e-06 mae 0.0008135063523618938
Epoch 717, training: loss: 0.0000016, mae: 0.0008162 test: loss0.0001003, mae:0.0072821
training loss 2.874570554922684e-06 mae 0.000912414223421365
training loss 1.5805565010173178e-06 mae 0.0008122623731455236
training loss 1.5800110085059137e-06 mae 0.0008130900136490198
training loss 1.5879466337350733e-06 mae 0.0008161398643837468
training loss 1.5829261317875949e-06 mae 0.0008129498273231535
Epoch 718, training: loss: 0.0000016, mae: 0.0008136 test: loss0.0001005, mae:0.0072874
training loss 1.188003579954966e-06 mae 0.000745998986531049
training loss 1.534084012345214e-06 mae 0.0007987542856283778
training loss 1.5037083077760766e-06 mae 0.0007875105585109918
training loss 1.5678030479570138e-06 mae 0.0008053967936599737
training loss 1.582808193712589e-06 mae 0.0008120890576461913
Epoch 719, training: loss: 0.0000016, mae: 0.0008133 test: loss0.0001003, mae:0.0072719
training loss 1.1375279882486211e-06 mae 0.0007672446663491428
training loss 1.5231390461047022e-06 mae 0.000787190832745503
training loss 1.5920100357034967e-06 mae 0.0008104122848950769
training loss 1.5470138321806319e-06 mae 0.0008062511067282386
training loss 1.5756905040134867e-06 mae 0.000809403802430378
Epoch 720, training: loss: 0.0000016, mae: 0.0008119 test: loss0.0001011, mae:0.0073099
training loss 1.867996957116702e-06 mae 0.0009721445967443287
training loss 1.5423468118355877e-06 mae 0.0007962432881707654
training loss 1.5430431614781315e-06 mae 0.0007992064508858439
training loss 1.5679284925359477e-06 mae 0.0008085164236096012
training loss 1.5851620379741609e-06 mae 0.0008106826938079454
Epoch 721, training: loss: 0.0000016, mae: 0.0008092 test: loss0.0001008, mae:0.0073031
training loss 1.8155611769543611e-06 mae 0.0008033579215407372
training loss 1.6696544105120578e-06 mae 0.0008292489059671177
training loss 1.5620651331371142e-06 mae 0.0008076924405932094
training loss 1.582786653565659e-06 mae 0.0008106388219875836
training loss 1.6015809396618229e-06 mae 0.0008156208979691017
Epoch 722, training: loss: 0.0000016, mae: 0.0008113 test: loss0.0001005, mae:0.0072922
training loss 8.094315830931009e-07 mae 0.0006737231160514057
training loss 1.6674613278815274e-06 mae 0.0008279887968491688
training loss 1.5679986799483174e-06 mae 0.0008018054266916676
training loss 1.5822678071664078e-06 mae 0.0008101733966147684
training loss 1.585380375242824e-06 mae 0.0008137093398509203
Epoch 723, training: loss: 0.0000016, mae: 0.0008160 test: loss0.0001011, mae:0.0073169
training loss 2.0458501239772886e-06 mae 0.0009944718331098557
training loss 1.6366277990010868e-06 mae 0.0008134033288989289
training loss 1.6547812095376516e-06 mae 0.0008218732767964312
training loss 1.637117251499414e-06 mae 0.000819650339309607
training loss 1.5870331986069016e-06 mae 0.0008125143304909691
Epoch 724, training: loss: 0.0000016, mae: 0.0008133 test: loss0.0001013, mae:0.0072993
training loss 2.305297130078543e-06 mae 0.0010632757330313325
training loss 1.688082848512054e-06 mae 0.0008392398275307142
training loss 1.620076929076178e-06 mae 0.0008168317334920096
training loss 1.5737543449839434e-06 mae 0.0008083662755990909
training loss 1.5808150865540039e-06 mae 0.0008079280395672395
Epoch 725, training: loss: 0.0000016, mae: 0.0008097 test: loss0.0001004, mae:0.0072841
training loss 1.1341412573528942e-06 mae 0.0007382240146398544
training loss 1.6091334509392081e-06 mae 0.0008125429626042937
training loss 1.5803314765056863e-06 mae 0.0008039780573154071
training loss 1.589437103563573e-06 mae 0.0008122610793270122
training loss 1.5912004464870057e-06 mae 0.0008137364543404486
Epoch 726, training: loss: 0.0000016, mae: 0.0008115 test: loss0.0001007, mae:0.0072927
training loss 1.368511789223703e-06 mae 0.0007764079491607845
training loss 1.5596477294143282e-06 mae 0.0008052308454780898
training loss 1.5566235216544667e-06 mae 0.0008078903232996198
training loss 1.571408369278854e-06 mae 0.0008098197289872056
training loss 1.5800530711126542e-06 mae 0.0008125026948953892
Epoch 727, training: loss: 0.0000016, mae: 0.0008124 test: loss0.0001004, mae:0.0072791
training loss 7.57600616907439e-07 mae 0.0005642508040182292
training loss 1.5177034599361707e-06 mae 0.0007934483434712769
training loss 1.5657374827958052e-06 mae 0.0008070198502068838
training loss 1.5816932480053066e-06 mae 0.0008118533940264176
training loss 1.555864793401306e-06 mae 0.00080607186386649
Epoch 728, training: loss: 0.0000016, mae: 0.0008113 test: loss0.0001006, mae:0.0072928
training loss 7.273005735441984e-07 mae 0.0006272935424931347
training loss 1.57785274157005e-06 mae 0.0008190168078084859
training loss 1.5640848771746987e-06 mae 0.0008127489009074458
training loss 1.5511508299507393e-06 mae 0.0008067357500734434
training loss 1.5944902453548215e-06 mae 0.0008141532925766108
Epoch 729, training: loss: 0.0000016, mae: 0.0008120 test: loss0.0001008, mae:0.0073063
training loss 1.4399701058209757e-06 mae 0.000751430809032172
training loss 1.5951626314923558e-06 mae 0.0008001119433673941
training loss 1.5867461212506569e-06 mae 0.0008105183401177316
training loss 1.593196731427938e-06 mae 0.0008143391820605202
training loss 1.5678215480766625e-06 mae 0.0008069165918306536
Epoch 730, training: loss: 0.0000016, mae: 0.0008105 test: loss0.0001012, mae:0.0073092
training loss 9.398924589731905e-07 mae 0.0006706251879222691
training loss 1.5175147676499739e-06 mae 0.0007897211231893916
training loss 1.494685917724283e-06 mae 0.0007873626459288494
training loss 1.5439058510587194e-06 mae 0.0008010671390004655
training loss 1.5660677461968204e-06 mae 0.000808133144251335
Epoch 731, training: loss: 0.0000016, mae: 0.0008116 test: loss0.0001011, mae:0.0073016
training loss 1.1253927141297027e-06 mae 0.0007076726178638637
training loss 1.4753788667464915e-06 mae 0.0007843928407037669
training loss 1.5152845499455502e-06 mae 0.0008022307267292673
training loss 1.583374373484013e-06 mae 0.0008137512602072384
training loss 1.58278589681084e-06 mae 0.0008107705148335415
Epoch 732, training: loss: 0.0000016, mae: 0.0008086 test: loss0.0001004, mae:0.0072754
training loss 1.464797946937324e-06 mae 0.0008579101413488388
training loss 1.5981023056173953e-06 mae 0.0008101301386441084
training loss 1.533450904051926e-06 mae 0.0007948939847566261
training loss 1.5477860505905256e-06 mae 0.0007976371057128026
training loss 1.5558562086310433e-06 mae 0.0008040196815417586
Epoch 733, training: loss: 0.0000016, mae: 0.0008094 test: loss0.0001006, mae:0.0072871
training loss 1.0299949053660384e-06 mae 0.0007104361429810524
training loss 1.4726767156336042e-06 mae 0.0007827519541880228
training loss 1.5316284508580704e-06 mae 0.0007937424565494839
training loss 1.525320599939913e-06 mae 0.0007961111951585467
training loss 1.5866968850848168e-06 mae 0.0008121838149399178
Epoch 734, training: loss: 0.0000016, mae: 0.0008104 test: loss0.0001009, mae:0.0073091
training loss 1.5862498230490019e-06 mae 0.0008235282148234546
training loss 1.6187768210329523e-06 mae 0.0008019756824782519
training loss 1.5753865751987792e-06 mae 0.0007997905019449423
training loss 1.5798616463270334e-06 mae 0.0008082280027223692
training loss 1.5751207967868193e-06 mae 0.0008092840927286972
Epoch 735, training: loss: 0.0000016, mae: 0.0008096 test: loss0.0001009, mae:0.0073024
training loss 1.0109332606589305e-06 mae 0.0006932178512215614
training loss 1.6277922797741542e-06 mae 0.0008076190236298477
training loss 1.5449542318735738e-06 mae 0.000796671459707201
training loss 1.56324164555185e-06 mae 0.0008030829352514806
training loss 1.5720725404529041e-06 mae 0.0008054309142626063
Epoch 736, training: loss: 0.0000016, mae: 0.0008047 test: loss0.0001008, mae:0.0072991
training loss 2.396932586634648e-06 mae 0.0009271015296690166
training loss 1.6272713756003284e-06 mae 0.0008084306290245376
training loss 1.588589896636051e-06 mae 0.000801861615238324
training loss 1.5676722113468879e-06 mae 0.0008042861911042636
training loss 1.569712164430019e-06 mae 0.000809389059675107
Epoch 737, training: loss: 0.0000016, mae: 0.0008098 test: loss0.0001027, mae:0.0073454
training loss 8.873653314367402e-07 mae 0.0006806834717281163
training loss 1.6767268686867193e-06 mae 0.0008291381059725786
training loss 1.6561528648726201e-06 mae 0.0008222026315905005
training loss 1.6230909154156377e-06 mae 0.0008178906220733035
training loss 1.566910041513786e-06 mae 0.0008084579722478576
Epoch 738, training: loss: 0.0000016, mae: 0.0008122 test: loss0.0001010, mae:0.0073051
training loss 2.227707454949268e-06 mae 0.0009420377318747342
training loss 1.665483656173213e-06 mae 0.0008237010561952402
training loss 1.5542013875572153e-06 mae 0.0008090399188395252
training loss 1.5740001954056405e-06 mae 0.0008100898942558576
training loss 1.5810495574555147e-06 mae 0.0008108142244430323
Epoch 739, training: loss: 0.0000016, mae: 0.0008069 test: loss0.0001005, mae:0.0072827
training loss 1.0169445658902987e-06 mae 0.0006436140392906964
training loss 1.6281048383283558e-06 mae 0.0007965331783006882
training loss 1.5871069987303715e-06 mae 0.0007998040320563806
training loss 1.5599356345695837e-06 mae 0.0007989738111088165
training loss 1.5714621354593539e-06 mae 0.0008075279285042404
Epoch 740, training: loss: 0.0000016, mae: 0.0008073 test: loss0.0001004, mae:0.0072819
training loss 9.509282676845032e-07 mae 0.0006883405148983002
training loss 1.518063153387665e-06 mae 0.0008044020323009761
training loss 1.5493157964484263e-06 mae 0.0008085782404546395
training loss 1.5336621945396662e-06 mae 0.0008014066982089189
training loss 1.5706705094044302e-06 mae 0.000808757063770546
Epoch 741, training: loss: 0.0000016, mae: 0.0008075 test: loss0.0001008, mae:0.0072984
training loss 1.8526339999880292e-06 mae 0.0008341781795024872
training loss 1.671542723251346e-06 mae 0.000821031676262946
training loss 1.6346435357956922e-06 mae 0.0008085388698877932
training loss 1.5566126051947366e-06 mae 0.0007979377230324025
training loss 1.5704147717113856e-06 mae 0.0008088738771343135
Epoch 742, training: loss: 0.0000016, mae: 0.0008074 test: loss0.0001012, mae:0.0073079
training loss 2.395767069174326e-06 mae 0.000936770171392709
training loss 1.575950522255523e-06 mae 0.0007981770744948995
training loss 1.5490267555428046e-06 mae 0.00080106659713985
training loss 1.5327683216140253e-06 mae 0.0007998292609611338
training loss 1.5638282379474388e-06 mae 0.0008068232428148128
Epoch 743, training: loss: 0.0000016, mae: 0.0008088 test: loss0.0001012, mae:0.0073075
training loss 1.2696882549789734e-06 mae 0.0007011375273577869
training loss 1.4558636750877543e-06 mae 0.0007878766411130186
training loss 1.4688192963338154e-06 mae 0.0007899061682758538
training loss 1.537897311579659e-06 mae 0.0008025332074535286
training loss 1.56583406063946e-06 mae 0.0008057475430874705
Epoch 744, training: loss: 0.0000016, mae: 0.0008056 test: loss0.0001007, mae:0.0072905
training loss 1.1851956287500798e-06 mae 0.0006787587772123516
training loss 1.5218655294922748e-06 mae 0.0008047114105840377
training loss 1.52515035788293e-06 mae 0.0008021813389697656
training loss 1.6009640913634575e-06 mae 0.0008157440796587
training loss 1.569917753077286e-06 mae 0.0008075124580084128
Epoch 745, training: loss: 0.0000016, mae: 0.0008062 test: loss0.0001009, mae:0.0073020
training loss 1.2917040521642775e-06 mae 0.0008396739140152931
training loss 1.4583013516059117e-06 mae 0.0007900738399293201
training loss 1.498322603202895e-06 mae 0.0007904924689745991
training loss 1.490797827449304e-06 mae 0.0007892639456403179
training loss 1.5639779120425572e-06 mae 0.0008063551284076267
Epoch 746, training: loss: 0.0000016, mae: 0.0008060 test: loss0.0001005, mae:0.0072831
training loss 1.9472852272883756e-06 mae 0.001000346033833921
training loss 1.3695553095613814e-06 mae 0.0007635569816236107
training loss 1.4958977721386816e-06 mae 0.0007937932358178685
training loss 1.5327866519225992e-06 mae 0.0007989199455081179
training loss 1.5537058195503115e-06 mae 0.0008045055061365611
Epoch 747, training: loss: 0.0000016, mae: 0.0008064 test: loss0.0001010, mae:0.0072937
training loss 1.1640090633591171e-06 mae 0.0007741609588265419
training loss 1.5101807929675166e-06 mae 0.0007846016270181565
training loss 1.502974225392638e-06 mae 0.0007896908633249563
training loss 1.5135835772131235e-06 mae 0.0007938541163958684
training loss 1.5757743573888079e-06 mae 0.0008057961409821166
Epoch 748, training: loss: 0.0000016, mae: 0.0008036 test: loss0.0001012, mae:0.0072932
training loss 1.3767054269919754e-06 mae 0.0008175934781320393
training loss 1.5617957675364296e-06 mae 0.0007989408811280394
training loss 1.4570435191311806e-06 mae 0.0007787113363790038
training loss 1.5567947800703626e-06 mae 0.0008050010667571072
training loss 1.5403013945e-06 mae 0.0007999387236117426
Epoch 749, training: loss: 0.0000016, mae: 0.0008040 test: loss0.0001007, mae:0.0072941
training loss 9.496245638729306e-07 mae 0.0006440551951527596
training loss 1.4513052085075125e-06 mae 0.0007863112408028661
training loss 1.5654179183812706e-06 mae 0.0008091130812587864
training loss 1.535047792401279e-06 mae 0.0008032886266696037
training loss 1.5491046719966904e-06 mae 0.0008052219962527446
Epoch 750, training: loss: 0.0000016, mae: 0.0008059 test: loss0.0001007, mae:0.0072887
training loss 1.2269675835341332e-06 mae 0.0007382373441942036
training loss 1.4917358882092805e-06 mae 0.0007752379226753963
training loss 1.5273273263146353e-06 mae 0.0007894688688154047
training loss 1.5447043335750294e-06 mae 0.0007958913997543471
training loss 1.5626368244919837e-06 mae 0.0008052396428752784
Epoch 751, training: loss: 0.0000016, mae: 0.0008057 test: loss0.0001007, mae:0.0072904
training loss 1.84088696641993e-06 mae 0.0009306681458838284
training loss 1.4503293886816972e-06 mae 0.0007761677261441946
training loss 1.4665978178167243e-06 mae 0.0007924887249107926
training loss 1.5313754221964382e-06 mae 0.0008038095035127657
training loss 1.565083321122474e-06 mae 0.0008068223030941185
Epoch 752, training: loss: 0.0000016, mae: 0.0008057 test: loss0.0001007, mae:0.0072889
training loss 2.2404201445169747e-06 mae 0.0009183890069834888
training loss 1.6000890310333842e-06 mae 0.0008192801672289622
training loss 1.5895524693022892e-06 mae 0.0008205464374882604
training loss 1.5708105295379327e-06 mae 0.0008096014244315395
training loss 1.5692701351577954e-06 mae 0.0008103301753266831
Epoch 753, training: loss: 0.0000016, mae: 0.0008063 test: loss0.0001014, mae:0.0073129
training loss 9.380329970554158e-07 mae 0.0006708617438562214
training loss 1.4466316562956142e-06 mae 0.0007844684672925401
training loss 1.5718619157170076e-06 mae 0.0008140048231136532
training loss 1.5611389506447691e-06 mae 0.0008119411596146395
training loss 1.5619721654245638e-06 mae 0.0008075324226569256
Epoch 754, training: loss: 0.0000016, mae: 0.0008058 test: loss0.0001012, mae:0.0073084
training loss 1.8212707573184161e-06 mae 0.0008416821365244687
training loss 1.5875276731361849e-06 mae 0.0008105178279163061
training loss 1.5499052672044622e-06 mae 0.0008098685556082148
training loss 1.5466711324312586e-06 mae 0.0008073079015054307
training loss 1.5527525115815377e-06 mae 0.000807017842608861
Epoch 755, training: loss: 0.0000016, mae: 0.0008057 test: loss0.0001009, mae:0.0072951
training loss 1.4013230611453764e-06 mae 0.0007799944723956287
training loss 1.4455258991017445e-06 mae 0.0007791893314752801
training loss 1.464353062201013e-06 mae 0.0007844141651046377
training loss 1.5317144419932207e-06 mae 0.0007977292474763258
training loss 1.5353904248613122e-06 mae 0.0007996748087740045
Epoch 756, training: loss: 0.0000016, mae: 0.0008028 test: loss0.0001010, mae:0.0073022
training loss 2.165557134503615e-06 mae 0.0008747258107177913
training loss 1.5676670730000279e-06 mae 0.0008039251997080796
training loss 1.542099689326242e-06 mae 0.0007992886694950411
training loss 1.5820335023583284e-06 mae 0.0008075211200221683
training loss 1.5619100316085062e-06 mae 0.0008058405776081878
Epoch 757, training: loss: 0.0000016, mae: 0.0008057 test: loss0.0001008, mae:0.0072920
training loss 2.1844600723852636e-06 mae 0.00093003612710163
training loss 1.4926081448988148e-06 mae 0.0007867514539290877
training loss 1.4759244152736578e-06 mae 0.0007816436153025098
training loss 1.5347620190772165e-06 mae 0.0007960371293793645
training loss 1.548169044028001e-06 mae 0.0008027848207845872
Epoch 758, training: loss: 0.0000016, mae: 0.0008032 test: loss0.0001010, mae:0.0072958
training loss 1.854174115578644e-06 mae 0.0008170114015229046
training loss 1.6957544636286414e-06 mae 0.0008194268168653271
training loss 1.5957080149904203e-06 mae 0.0008044365714511624
training loss 1.5655686349903687e-06 mae 0.0008068835123408193
training loss 1.5466113074017426e-06 mae 0.0008012756124076284
Epoch 759, training: loss: 0.0000015, mae: 0.0008034 test: loss0.0001008, mae:0.0072957
training loss 3.044302502530627e-06 mae 0.0011018117656931281
training loss 1.501018782975877e-06 mae 0.0007813769591260042
training loss 1.550512278939068e-06 mae 0.000802437154147405
training loss 1.5100731475090258e-06 mae 0.0007920412212573239
training loss 1.53034797837442e-06 mae 0.000797900376466117
Epoch 760, training: loss: 0.0000015, mae: 0.0008009 test: loss0.0001008, mae:0.0073002
training loss 1.9117453575745458e-06 mae 0.0009443412418477237
training loss 1.4628491572586747e-06 mae 0.0007862840178787856
training loss 1.5432215414731748e-06 mae 0.0007959674026051741
training loss 1.5575232375177794e-06 mae 0.0008002967810387794
training loss 1.5527763914735206e-06 mae 0.0008013816353452591
Epoch 761, training: loss: 0.0000016, mae: 0.0008027 test: loss0.0001014, mae:0.0073173
training loss 1.1135277873108862e-06 mae 0.0006959714810363948
training loss 1.4689912703426097e-06 mae 0.0007995691814678995
training loss 1.4794869848133604e-06 mae 0.0007898269058666239
training loss 1.503014516025444e-06 mae 0.0007944674457003986
training loss 1.5629222613159662e-06 mae 0.0008059406070390821
Epoch 762, training: loss: 0.0000015, mae: 0.0008026 test: loss0.0001008, mae:0.0073000
training loss 2.480175453456468e-06 mae 0.0010127881541848183
training loss 1.4861778011293732e-06 mae 0.0007844828856725465
training loss 1.5694918240209477e-06 mae 0.0008028720319386349
training loss 1.5543171578305405e-06 mae 0.0008027733244627889
training loss 1.5533409260904735e-06 mae 0.0008033689039652772
Epoch 763, training: loss: 0.0000015, mae: 0.0008024 test: loss0.0001010, mae:0.0073092
training loss 7.399578407785157e-07 mae 0.0006272820755839348
training loss 1.6170041071879548e-06 mae 0.00081037831328371
training loss 1.5627665907749638e-06 mae 0.0008033471536997819
training loss 1.5734065312091048e-06 mae 0.000804656920322396
training loss 1.547854143082439e-06 mae 0.0008013049952583898
Epoch 764, training: loss: 0.0000015, mae: 0.0008014 test: loss0.0001014, mae:0.0073134
training loss 9.523383255327644e-07 mae 0.0006665699183940887
training loss 1.5229783408429677e-06 mae 0.0007928698962809994
training loss 1.5188998617358637e-06 mae 0.0007971699826769754
training loss 1.5536372542622167e-06 mae 0.0008023036921409189
training loss 1.5388441762688986e-06 mae 0.0007995436670587618
Epoch 765, training: loss: 0.0000015, mae: 0.0008001 test: loss0.0001013, mae:0.0073173
training loss 9.769074722498772e-07 mae 0.000717461109161377
training loss 1.5296280779176755e-06 mae 0.0007964775854192091
training loss 1.550386618400988e-06 mae 0.0007988539020678415
training loss 1.5476400817729187e-06 mae 0.0008023021673700392
training loss 1.5393849729610143e-06 mae 0.000801460461694497
Epoch 766, training: loss: 0.0000015, mae: 0.0008005 test: loss0.0001008, mae:0.0072950
training loss 1.2634621953111491e-06 mae 0.000760572322178632
training loss 1.5582296298941635e-06 mae 0.0007986602209070149
training loss 1.5405685634378996e-06 mae 0.0007953563472256065
training loss 1.5280445503120197e-06 mae 0.0007922714032596656
training loss 1.529591137381006e-06 mae 0.0007969668550319522
Epoch 767, training: loss: 0.0000015, mae: 0.0007993 test: loss0.0001013, mae:0.0073068
training loss 1.9300312033010414e-06 mae 0.0009051852975971997
training loss 1.5882126140419206e-06 mae 0.0008029885973562213
training loss 1.5582290302518231e-06 mae 0.0008039410424298872
training loss 1.5727776737838102e-06 mae 0.0008067085422322596
training loss 1.544835705929368e-06 mae 0.0008008680800126582
Epoch 768, training: loss: 0.0000015, mae: 0.0008006 test: loss0.0001009, mae:0.0072992
training loss 1.894955516945629e-06 mae 0.0009124437347054482
training loss 1.5183640551399816e-06 mae 0.0008020033824768867
training loss 1.4981304578141854e-06 mae 0.0007945958675936531
training loss 1.492170268882799e-06 mae 0.0007941353330249892
training loss 1.538855701201372e-06 mae 0.0008021131792367525
Epoch 769, training: loss: 0.0000015, mae: 0.0008017 test: loss0.0001012, mae:0.0073166
training loss 9.966721563614556e-07 mae 0.0006063732434995472
training loss 1.4194441545816302e-06 mae 0.000772617999281661
training loss 1.4737988569304615e-06 mae 0.0007887412344409833
training loss 1.5202125390811455e-06 mae 0.0007962279441143503
training loss 1.5326234970710529e-06 mae 0.0008001638360933135
Epoch 770, training: loss: 0.0000015, mae: 0.0008016 test: loss0.0001013, mae:0.0073172
training loss 1.3831871683578356e-06 mae 0.0007332451641559601
training loss 1.5669613240592585e-06 mae 0.000807010145246179
training loss 1.5264615785331391e-06 mae 0.0007946715075845394
training loss 1.5084380813867936e-06 mae 0.0007941088198209242
training loss 1.531848910184436e-06 mae 0.0008001343508723622
Epoch 771, training: loss: 0.0000015, mae: 0.0007998 test: loss0.0001007, mae:0.0072925
training loss 7.822547445357486e-07 mae 0.0005993423983454704
training loss 1.5018295729818167e-06 mae 0.0007889440052631294
training loss 1.4426289513915321e-06 mae 0.0007757561438080698
training loss 1.5184008254469902e-06 mae 0.0007971534013997961
training loss 1.524078049303185e-06 mae 0.0007986953464366809
Epoch 772, training: loss: 0.0000015, mae: 0.0008026 test: loss0.0001015, mae:0.0073179
training loss 1.4561543366653495e-06 mae 0.0007911517168395221
training loss 1.6378249905060594e-06 mae 0.0008169245853673153
training loss 1.5573135788099981e-06 mae 0.0008076683359939862
training loss 1.561777074864901e-06 mae 0.000801831584451535
training loss 1.5440489558237296e-06 mae 0.0008015865764108056
Epoch 773, training: loss: 0.0000015, mae: 0.0008005 test: loss0.0001013, mae:0.0073119
training loss 2.1387904780567624e-06 mae 0.0009165735100395977
training loss 1.5270410274832313e-06 mae 0.000781714527607512
training loss 1.5304443883730467e-06 mae 0.0007968859376482626
training loss 1.4822088830415121e-06 mae 0.0007862397016639765
training loss 1.5313033707432854e-06 mae 0.0007965025862398557
Epoch 774, training: loss: 0.0000015, mae: 0.0007973 test: loss0.0001016, mae:0.0073203
training loss 2.189502538385568e-06 mae 0.0009205981041304767
training loss 1.4857708200835594e-06 mae 0.000786183496672368
training loss 1.5234400502822156e-06 mae 0.0007956799103574145
training loss 1.5142069524482929e-06 mae 0.0007916709611698095
training loss 1.5282843693980733e-06 mae 0.0007994504505305074
Epoch 775, training: loss: 0.0000015, mae: 0.0008022 test: loss0.0001011, mae:0.0073136
training loss 1.4442094879996148e-06 mae 0.0007402384653687477
training loss 1.4722468757313706e-06 mae 0.0007727303834376383
training loss 1.5032448588796547e-06 mae 0.0007849191476839902
training loss 1.5192907690242464e-06 mae 0.0007893653549500651
training loss 1.529585241361014e-06 mae 0.0007948271599505559
Epoch 776, training: loss: 0.0000015, mae: 0.0007980 test: loss0.0001015, mae:0.0073222
training loss 8.61399712448474e-07 mae 0.0006067554350011051
training loss 1.588170384954591e-06 mae 0.000813544464215417
training loss 1.5756975165792336e-06 mae 0.0008081079994923077
training loss 1.555239879544147e-06 mae 0.000803365900037835
training loss 1.5380501603865572e-06 mae 0.0007990204605484725
Epoch 777, training: loss: 0.0000015, mae: 0.0007972 test: loss0.0001013, mae:0.0073109
training loss 1.1400494486224488e-06 mae 0.0007011657580733299
training loss 1.493458969390286e-06 mae 0.0007737663542559628
training loss 1.4823982081285822e-06 mae 0.0007747879619525725
training loss 1.5088731153647305e-06 mae 0.0007894400546530765
training loss 1.5484303816462142e-06 mae 0.0007988253100983687
Epoch 778, training: loss: 0.0000015, mae: 0.0007967 test: loss0.0001011, mae:0.0073024
training loss 1.5434471833941643e-06 mae 0.0007409456302411854
training loss 1.5781537687122607e-06 mae 0.0007866338835846559
training loss 1.5230162538914957e-06 mae 0.0007918696022952102
training loss 1.5330883421568354e-06 mae 0.0007961923830883441
training loss 1.5397373327297949e-06 mae 0.0007988959495835373
Epoch 779, training: loss: 0.0000015, mae: 0.0007990 test: loss0.0001015, mae:0.0073200
training loss 9.777965033208602e-07 mae 0.0006609360571019351
training loss 1.5649488162517967e-06 mae 0.0007972150365365486
training loss 1.5370768236121278e-06 mae 0.0007956555645710023
training loss 1.5365415405139715e-06 mae 0.0007967658892940016
training loss 1.5337797780414156e-06 mae 0.0007963476274556371
Epoch 780, training: loss: 0.0000015, mae: 0.0007961 test: loss0.0001012, mae:0.0073161
training loss 1.4110037227510475e-06 mae 0.0007144895498640835
training loss 1.399429383947755e-06 mae 0.0007628228745934571
training loss 1.46312346250942e-06 mae 0.0007813991419740462
training loss 1.498196715544587e-06 mae 0.0007902139282032894
training loss 1.5271411402140049e-06 mae 0.0007971268163447208
Epoch 781, training: loss: 0.0000015, mae: 0.0007998 test: loss0.0001012, mae:0.0073147
training loss 1.4584087466573692e-06 mae 0.0007270161877386272
training loss 1.655583195556306e-06 mae 0.0008170852795991974
training loss 1.5043842120459239e-06 mae 0.000783860815128791
training loss 1.5553044785134408e-06 mae 0.0007998535445419494
training loss 1.5262880647264758e-06 mae 0.0007956113160381663
Epoch 782, training: loss: 0.0000015, mae: 0.0007966 test: loss0.0001014, mae:0.0073233
training loss 6.708485784656659e-07 mae 0.0005733715370297432
training loss 1.7784715516748236e-06 mae 0.0008282716335801807
training loss 1.6904183332758965e-06 mae 0.000822281099551606
training loss 1.5919930855856345e-06 mae 0.0008084279795744708
training loss 1.539982142063463e-06 mae 0.00079955844051061
Epoch 783, training: loss: 0.0000015, mae: 0.0007987 test: loss0.0001013, mae:0.0073134
training loss 1.6507714235558524e-06 mae 0.000815155915915966
training loss 1.5769097560633766e-06 mae 0.0007877831767294922
training loss 1.6142708330893458e-06 mae 0.0008032469045472251
training loss 1.5392509724745871e-06 mae 0.0007954248688214565
training loss 1.5322245959753708e-06 mae 0.0007942521936533537
Epoch 784, training: loss: 0.0000015, mae: 0.0007940 test: loss0.0001011, mae:0.0073148
training loss 1.4542214330504066e-06 mae 0.0007791140233166516
training loss 1.717253700703355e-06 mae 0.0008297056101226048
training loss 1.5660711635136256e-06 mae 0.000798366975965956
training loss 1.5364303847141762e-06 mae 0.0007954943595170382
training loss 1.5315062625803924e-06 mae 0.000796405534811355
Epoch 785, training: loss: 0.0000015, mae: 0.0007963 test: loss0.0001017, mae:0.0073289
training loss 6.124594733591948e-07 mae 0.0005924279685132205
training loss 1.4364688443528587e-06 mae 0.0007740823830496157
training loss 1.4671451923628532e-06 mae 0.00077934725449817
training loss 1.5054682781460867e-06 mae 0.000786788369966856
training loss 1.5366010409286274e-06 mae 0.0007959913007164405
Epoch 786, training: loss: 0.0000015, mae: 0.0007955 test: loss0.0001016, mae:0.0073353
training loss 1.2827285900129937e-06 mae 0.0007820548489689827
training loss 1.4820222139521732e-06 mae 0.000788400430168372
training loss 1.547410483278264e-06 mae 0.0008014066906710441
training loss 1.545044110029243e-06 mae 0.0008005395108396002
training loss 1.521530220228878e-06 mae 0.0007943248495561492
Epoch 787, training: loss: 0.0000015, mae: 0.0007954 test: loss0.0001012, mae:0.0073114
training loss 1.147530497291882e-06 mae 0.0007099565118551254
training loss 1.3955087874641252e-06 mae 0.0007751447955539047
training loss 1.4789704689271934e-06 mae 0.0007849219580766478
training loss 1.528799245326375e-06 mae 0.0007979366336625367
training loss 1.5126403297644225e-06 mae 0.0007956220882201094
Epoch 788, training: loss: 0.0000015, mae: 0.0008001 test: loss0.0001018, mae:0.0073369
training loss 1.7819921822592732e-06 mae 0.0008759265765547752
training loss 1.5044543512850199e-06 mae 0.0007874498990656553
training loss 1.5722890095888279e-06 mae 0.000804593182620731
training loss 1.5382813012967974e-06 mae 0.0007980147084461786
training loss 1.546431742293799e-06 mae 0.0008003572262684581
Epoch 789, training: loss: 0.0000015, mae: 0.0007953 test: loss0.0001008, mae:0.0072904
training loss 1.4902843759045936e-06 mae 0.0007601386751048267
training loss 1.5819083300277054e-06 mae 0.0008112900073220041
training loss 1.5220023057493262e-06 mae 0.0007937807240523398
training loss 1.521105217165651e-06 mae 0.000794881564682189
training loss 1.5182388824881071e-06 mae 0.0007942626073224404
Epoch 790, training: loss: 0.0000015, mae: 0.0007961 test: loss0.0001012, mae:0.0073106
training loss 1.078639002116688e-06 mae 0.000656522810459137
training loss 1.5295250018514745e-06 mae 0.0007997515255256613
training loss 1.5024787182149949e-06 mae 0.0007915875792623232
training loss 1.4959513246456195e-06 mae 0.0007866667799454258
training loss 1.5233804168107804e-06 mae 0.00079389930507562
Epoch 791, training: loss: 0.0000015, mae: 0.0007954 test: loss0.0001011, mae:0.0073020
training loss 1.0551103741818224e-06 mae 0.0006471779197454453
training loss 1.5082045646362099e-06 mae 0.000795593633608637
training loss 1.468881311659583e-06 mae 0.0007815682303303354
training loss 1.47474937238113e-06 mae 0.0007817062242064523
training loss 1.5107468275470466e-06 mae 0.0007924021713132157
Epoch 792, training: loss: 0.0000015, mae: 0.0007942 test: loss0.0001012, mae:0.0073102
training loss 2.359700829401845e-06 mae 0.001011362881399691
training loss 1.679112477747777e-06 mae 0.0008186095373650244
training loss 1.6059443680097675e-06 mae 0.000807804668709488
training loss 1.558051384592051e-06 mae 0.0007992457152831534
training loss 1.514286593793886e-06 mae 0.0007913550787688747
Epoch 793, training: loss: 0.0000015, mae: 0.0007935 test: loss0.0001022, mae:0.0073519
training loss 1.7182495639644912e-06 mae 0.0007977529312483966
training loss 1.4588733418519289e-06 mae 0.0007675775434073135
training loss 1.4740479259379042e-06 mae 0.0007843620686718072
training loss 1.4816801191596788e-06 mae 0.0007916485080464641
training loss 1.513137549754313e-06 mae 0.0007924368817576983
Epoch 794, training: loss: 0.0000015, mae: 0.0007923 test: loss0.0001008, mae:0.0072886
training loss 1.0641033441061154e-06 mae 0.0007264232262969017
training loss 1.4364517066193605e-06 mae 0.0007742891326665368
training loss 1.444812087334183e-06 mae 0.0007755450266390338
training loss 1.4834200218628275e-06 mae 0.0007827667223015148
training loss 1.5120945458400095e-06 mae 0.0007925974591739423
Epoch 795, training: loss: 0.0000015, mae: 0.0007937 test: loss0.0001020, mae:0.0073490
training loss 1.3219547554399469e-06 mae 0.000709210813511163
training loss 1.5495763038558707e-06 mae 0.0007895239532979973
training loss 1.5564552172961186e-06 mae 0.0008022015047545482
training loss 1.5307851960856267e-06 mae 0.0008001812760308018
training loss 1.5232615896028952e-06 mae 0.000795700872144361
Epoch 796, training: loss: 0.0000015, mae: 0.0007959 test: loss0.0001013, mae:0.0073075
training loss 1.494769094279036e-06 mae 0.0008524864097125828
training loss 1.541805128605726e-06 mae 0.0007968278163496185
training loss 1.5121549978121572e-06 mae 0.0007982347489076452
training loss 1.4672150006321167e-06 mae 0.0007837209037184369
training loss 1.5065193421869293e-06 mae 0.0007906649293567963
Epoch 797, training: loss: 0.0000015, mae: 0.0007917 test: loss0.0001015, mae:0.0073267
training loss 8.445594517070276e-07 mae 0.0006142196361906826
training loss 1.6222355346447634e-06 mae 0.0008127819335855108
training loss 1.534606970871759e-06 mae 0.000793537982545867
training loss 1.5258907475246864e-06 mae 0.0007932705948104627
training loss 1.5230772056718848e-06 mae 0.0007953976735064938
Epoch 798, training: loss: 0.0000015, mae: 0.0007936 test: loss0.0001017, mae:0.0073359
training loss 1.575310875523428e-06 mae 0.0008828869904391468
training loss 1.5776828577680348e-06 mae 0.0007933868933012528
training loss 1.5339106305486218e-06 mae 0.000795272115265287
training loss 1.5083077401802399e-06 mae 0.000791117795843273
training loss 1.5080351632962485e-06 mae 0.0007914010594907073
Epoch 799, training: loss: 0.0000015, mae: 0.0007931 test: loss0.0001015, mae:0.0073145
current learning rate: 1.953125e-06

Process finished with exit code 0



